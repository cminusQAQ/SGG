2023-02-22 11:29:34 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-22 11:29:34 - utils.py[line:261] - INFO: Start init
2023-02-22 11:29:34 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-22 11:29:34 - utils.py[line:261] - INFO: Start init
2023-02-22 11:29:34 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-22 11:29:34 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-22 11:29:34 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-22 11:29:34 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-22 11:29:38 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_caption_coco_vg', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 9, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=9, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_caption_coco_vg', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-22 11:29:38 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-22 11:29:38 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-22 11:29:42 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-22 11:29:42 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-22 11:29:42 - train.py[line:119] - INFO: model: OFAModel
2023-02-22 11:29:42 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-22 11:29:42 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-22 11:29:42 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-22 11:29:43 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-22 11:29:43 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-22 11:29:43 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-22 11:29:43 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-22 11:29:43 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-22 11:29:43 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-22 11:29:45 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-22 11:29:45 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-22 11:29:45 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-02-22 11:29:55 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-22 11:29:55 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-22 11:29:55 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-22 11:29:55 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-22 11:29:55 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-22 11:29:56 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv slice_id 1 row count 43497 total row count 86994
2023-02-22 11:29:56 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv slice_id 0 row count 43497 total row count 86994
2023-02-22 11:29:56 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
2023-02-22 11:29:56 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 19575, warmup steps 978, warmup_factor 0.0010224948875255625
Total steps 19575, warmup steps 978, warmup_factor 0.0010224948875255625
2023-02-22 11:29:57 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-22 11:29:57 - train.py[line:312] - INFO: Start iterating over samples
@@@@ ERROR IN DATA @@@@ hang from
2023-02-22 11:30:19 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 2175 loss=0.996, loss_v1=0, loss_v2=0, nll_loss=0.835, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=88.7, ups=0.79, wpb=112.3, bsz=40, num_updates=10, lr=5.11247e-07, gnorm=12.87, clip=100, loss_scale=128, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=36
2023-02-22 11:30:31 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 2175 loss=1.028, loss_v1=0, loss_v2=0, nll_loss=0.875, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.83, wps=93.6, ups=0.84, wpb=112.1, bsz=40, num_updates=20, lr=1.02249e-06, gnorm=12.27, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=48
2023-02-22 11:30:42 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 2175 loss=1.044, loss_v1=0, loss_v2=0, nll_loss=0.91, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.88, wps=98, ups=0.88, wpb=111, bsz=40, num_updates=30, lr=1.53374e-06, gnorm=12.042, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59
2023-02-22 11:30:54 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 2175 loss=0.891, loss_v1=0, loss_v2=0, nll_loss=0.765, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=97.5, ups=0.87, wpb=112.4, bsz=40, num_updates=40, lr=2.04499e-06, gnorm=8.11, clip=100, loss_scale=128, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=71
@@@@ ERROR IN DATA @@@@ lie on
@@@@ ERROR IN DATA @@@@ eat
2023-02-22 11:31:05 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 2175 loss=0.761, loss_v1=0, loss_v2=0, nll_loss=0.636, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=99, ups=0.88, wpb=112.5, bsz=40, num_updates=50, lr=2.55624e-06, gnorm=5.728, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=82
@@@@ ERROR IN DATA @@@@ cover
@@@@ ERROR IN DATA @@@@ lie on
2023-02-22 11:31:16 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 2175 loss=0.669, loss_v1=0, loss_v2=0, nll_loss=0.532, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=100.6, ups=0.89, wpb=113.1, bsz=40, num_updates=60, lr=3.06748e-06, gnorm=4.75, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=93
2023-02-22 11:31:28 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 2175 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.492, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=97.8, ups=0.87, wpb=111.9, bsz=40, num_updates=70, lr=3.57873e-06, gnorm=4.361, clip=100, loss_scale=128, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=105
@@@@ ERROR IN DATA @@@@ lie on
2023-02-22 11:31:39 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 2175 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=80, lr=4.08998e-06, gnorm=3.767, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=116
2023-02-22 11:31:50 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 2175 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=100.5, ups=0.91, wpb=110.6, bsz=40, num_updates=90, lr=4.60123e-06, gnorm=3.836, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=127
2023-02-22 11:32:01 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 2175 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100, ups=0.89, wpb=111.8, bsz=40, num_updates=100, lr=5.11247e-06, gnorm=3.554, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=138
2023-02-22 11:32:13 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 2175 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.9, ups=0.89, wpb=112.2, bsz=40, num_updates=110, lr=5.62372e-06, gnorm=3.241, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=149
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 60818
Killing subprocess 60819
Main process received SIGINT, exiting
2023-02-22 11:32:59 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-22 11:32:59 - utils.py[line:261] - INFO: Start init
2023-02-22 11:32:59 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-22 11:32:59 - utils.py[line:261] - INFO: Start init
2023-02-22 11:32:59 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-22 11:32:59 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-22 11:32:59 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-22 11:32:59 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-22 11:33:04 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_caption_coco_vg', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 9, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=9, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_caption_coco_vg', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-22 11:33:04 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-22 11:33:04 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-22 11:33:08 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-22 11:33:08 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-22 11:33:08 - train.py[line:119] - INFO: model: OFAModel
2023-02-22 11:33:08 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-22 11:33:08 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-22 11:33:08 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-22 11:33:09 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-22 11:33:09 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-22 11:33:09 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-22 11:33:09 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-22 11:33:09 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-22 11:33:09 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-22 11:33:11 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-22 11:33:11 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-22 11:33:11 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-02-22 11:33:20 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-22 11:33:20 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-22 11:33:20 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-22 11:33:20 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-22 11:33:20 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-22 11:33:21 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-02-22 11:33:21 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv slice_id 0 row count 43497 total row count 86994
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv slice_id 1 row count 43497 total row count 86994
2023-02-22 11:33:21 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
2023-02-22 11:33:21 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 19575, warmup steps 978, warmup_factor 0.0010224948875255625
2023-02-22 11:33:22 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-22 11:33:22 - train.py[line:312] - INFO: Start iterating over samples
Total steps 19575, warmup steps 978, warmup_factor 0.0010224948875255625
2023-02-22 11:33:44 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 2175 loss=0.996, loss_v1=0, loss_v2=0, nll_loss=0.835, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=82.8, ups=0.74, wpb=112.3, bsz=40, num_updates=10, lr=5.11247e-07, gnorm=12.889, clip=100, loss_scale=128, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=34
2023-02-22 11:33:56 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 2175 loss=1.028, loss_v1=0, loss_v2=0, nll_loss=0.875, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.83, wps=95.1, ups=0.85, wpb=112.1, bsz=40, num_updates=20, lr=1.02249e-06, gnorm=12.282, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=46
2023-02-22 11:34:07 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 2175 loss=1.044, loss_v1=0, loss_v2=0, nll_loss=0.91, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.88, wps=96.5, ups=0.87, wpb=111, bsz=40, num_updates=30, lr=1.53374e-06, gnorm=12.014, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58
2023-02-22 11:34:18 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 2175 loss=0.891, loss_v1=0, loss_v2=0, nll_loss=0.765, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=98.2, ups=0.87, wpb=112.4, bsz=40, num_updates=40, lr=2.04499e-06, gnorm=8.087, clip=100, loss_scale=128, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=69
2023-02-22 11:34:30 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 2175 loss=0.761, loss_v1=0, loss_v2=0, nll_loss=0.636, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.55, wps=98.3, ups=0.87, wpb=112.5, bsz=40, num_updates=50, lr=2.55624e-06, gnorm=5.76, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=81
2023-02-22 11:34:41 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 2175 loss=0.669, loss_v1=0, loss_v2=0, nll_loss=0.532, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=103, ups=0.91, wpb=113.1, bsz=40, num_updates=60, lr=3.06748e-06, gnorm=4.745, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=92
2023-02-22 11:34:52 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 2175 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=97, ups=0.87, wpb=111.8, bsz=40, num_updates=70, lr=3.57873e-06, gnorm=4.288, clip=100, loss_scale=128, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=103
2023-02-22 11:35:04 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 2175 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=100.1, ups=0.89, wpb=112, bsz=40, num_updates=80, lr=4.08998e-06, gnorm=3.82, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=114
2023-02-22 11:35:15 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 2175 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=99.7, ups=0.9, wpb=110.6, bsz=40, num_updates=90, lr=4.60123e-06, gnorm=3.804, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=126
2023-02-22 11:35:26 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 2175 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100, ups=0.89, wpb=111.8, bsz=40, num_updates=100, lr=5.11247e-06, gnorm=3.529, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=137
2023-02-22 11:35:37 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 2175 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.2, ups=0.88, wpb=112.2, bsz=40, num_updates=110, lr=5.62372e-06, gnorm=3.228, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=148
2023-02-22 11:35:48 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 2175 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.1, ups=0.9, wpb=111.6, bsz=40, num_updates=120, lr=6.13497e-06, gnorm=3.098, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=159
2023-02-22 11:36:00 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 2175 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.9, ups=0.88, wpb=112.1, bsz=40, num_updates=130, lr=6.64622e-06, gnorm=3.002, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=171
2023-02-22 11:36:11 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 2175 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.3, ups=0.88, wpb=112.7, bsz=40, num_updates=140, lr=7.15746e-06, gnorm=2.784, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=182
2023-02-22 11:36:22 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 2175 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.9, ups=0.88, wpb=112, bsz=40, num_updates=150, lr=7.66871e-06, gnorm=2.9, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=193
2023-02-22 11:36:34 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 2175 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.2, ups=0.89, wpb=112.7, bsz=40, num_updates=160, lr=8.17996e-06, gnorm=3.017, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=204
2023-02-22 11:36:45 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 2175 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=95.5, ups=0.86, wpb=110.9, bsz=40, num_updates=170, lr=8.69121e-06, gnorm=2.75, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=216
2023-02-22 11:36:56 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 2175 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=100.9, ups=0.9, wpb=112, bsz=40, num_updates=180, lr=9.20245e-06, gnorm=2.684, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=227
2023-02-22 11:37:07 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 2175 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=102.2, ups=0.91, wpb=112.3, bsz=40, num_updates=190, lr=9.7137e-06, gnorm=2.691, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=238
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 11:37:19 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 2175 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=97.4, ups=0.87, wpb=112.6, bsz=40, num_updates=200, lr=1.02249e-05, gnorm=2.988, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=250
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 11:37:30 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 2175 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=100.1, ups=0.89, wpb=113, bsz=40, num_updates=210, lr=1.07362e-05, gnorm=2.588, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=261
2023-02-22 11:37:42 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 2175 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=99.4, ups=0.89, wpb=111.9, bsz=40, num_updates=220, lr=1.12474e-05, gnorm=2.451, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=272
2023-02-22 11:37:53 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 2175 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=102.1, ups=0.91, wpb=112, bsz=40, num_updates=230, lr=1.17587e-05, gnorm=2.38, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=283
2023-02-22 11:38:04 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 2175 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.8, ups=0.9, wpb=113.1, bsz=40, num_updates=240, lr=1.22699e-05, gnorm=2.523, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=294
2023-02-22 11:38:15 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 2175 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97.4, ups=0.87, wpb=112.3, bsz=40, num_updates=250, lr=1.27812e-05, gnorm=2.226, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=306
2023-02-22 11:38:27 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 2175 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=97.3, ups=0.87, wpb=111.2, bsz=40, num_updates=260, lr=1.32924e-05, gnorm=2.158, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=317
2023-02-22 11:38:38 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 2175 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.5, ups=0.91, wpb=110.9, bsz=40, num_updates=270, lr=1.38037e-05, gnorm=2.306, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=328
2023-02-22 11:38:49 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 2175 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=102.5, ups=0.91, wpb=112.6, bsz=40, num_updates=280, lr=1.43149e-05, gnorm=2.764, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=339
2023-02-22 11:39:00 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 2175 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.9, ups=0.9, wpb=113.4, bsz=40, num_updates=290, lr=1.48262e-05, gnorm=1.961, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=350
2023-02-22 11:39:11 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 2175 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.4, ups=0.89, wpb=112.1, bsz=40, num_updates=300, lr=1.53374e-05, gnorm=2.11, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=362
2023-02-22 11:39:22 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 2175 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.3, ups=0.88, wpb=112.4, bsz=40, num_updates=310, lr=1.58487e-05, gnorm=2.181, clip=100, loss_scale=128, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=373
2023-02-22 11:39:34 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 2175 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=98, ups=0.87, wpb=112.2, bsz=40, num_updates=320, lr=1.63599e-05, gnorm=2.157, clip=100, loss_scale=128, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=385
2023-02-22 11:39:45 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 2175 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.8, ups=0.9, wpb=111.2, bsz=40, num_updates=330, lr=1.68712e-05, gnorm=2.315, clip=100, loss_scale=128, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=396
2023-02-22 11:39:56 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 2175 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=102.6, ups=0.92, wpb=111.8, bsz=40, num_updates=340, lr=1.73824e-05, gnorm=1.943, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=407
2023-02-22 11:40:07 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 2175 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=100.7, ups=0.9, wpb=111.7, bsz=40, num_updates=350, lr=1.78937e-05, gnorm=2.121, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=418
2023-02-22 11:40:18 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 2175 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.6, ups=0.89, wpb=112.5, bsz=40, num_updates=360, lr=1.84049e-05, gnorm=1.935, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=429
2023-02-22 11:40:29 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 2175 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=370, lr=1.89162e-05, gnorm=1.947, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=440
2023-02-22 11:40:41 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 2175 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.1, ups=0.89, wpb=112.9, bsz=40, num_updates=380, lr=1.94274e-05, gnorm=1.896, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=451
2023-02-22 11:40:52 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 2175 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.9, ups=0.9, wpb=112.4, bsz=40, num_updates=390, lr=1.99387e-05, gnorm=1.887, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=462
2023-02-22 11:41:03 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 2175 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99, ups=0.89, wpb=111.3, bsz=40, num_updates=400, lr=2.04499e-05, gnorm=2.013, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=474
2023-02-22 11:41:15 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 2175 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=97, ups=0.86, wpb=112.3, bsz=40, num_updates=410, lr=2.09611e-05, gnorm=2.203, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=485
2023-02-22 11:41:26 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 2175 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.3, ups=0.9, wpb=111.3, bsz=40, num_updates=420, lr=2.14724e-05, gnorm=1.594, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=496
2023-02-22 11:41:37 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 2175 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.3, ups=0.91, wpb=111.3, bsz=40, num_updates=430, lr=2.19836e-05, gnorm=1.928, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=507
2023-02-22 11:41:48 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 2175 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.4, ups=0.89, wpb=112.1, bsz=40, num_updates=440, lr=2.24949e-05, gnorm=1.955, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=519
2023-02-22 11:41:59 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 2175 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.2, ups=0.89, wpb=112.5, bsz=40, num_updates=450, lr=2.30061e-05, gnorm=1.752, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=530
2023-02-22 11:42:10 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 2175 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.8, ups=0.89, wpb=112.1, bsz=40, num_updates=460, lr=2.35174e-05, gnorm=1.67, clip=90, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=541
2023-02-22 11:42:22 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 2175 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.1, ups=0.89, wpb=112.6, bsz=40, num_updates=470, lr=2.40286e-05, gnorm=1.79, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=552
2023-02-22 11:42:33 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 2175 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=101.5, ups=0.9, wpb=113, bsz=40, num_updates=480, lr=2.45399e-05, gnorm=2.005, clip=100, loss_scale=128, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=564
2023-02-22 11:42:44 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 2175 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.5, ups=0.87, wpb=113.5, bsz=40, num_updates=490, lr=2.50511e-05, gnorm=1.618, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=575
2023-02-22 11:42:56 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 2175 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.7, ups=0.89, wpb=112.4, bsz=40, num_updates=500, lr=2.55624e-05, gnorm=1.545, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=586
2023-02-22 11:43:07 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 2175 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=102.1, ups=0.91, wpb=112.8, bsz=40, num_updates=510, lr=2.60736e-05, gnorm=1.874, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=597
2023-02-22 11:43:17 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 2175 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=103, ups=0.92, wpb=111.8, bsz=40, num_updates=520, lr=2.65849e-05, gnorm=1.421, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=608
2023-02-22 11:43:29 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 2175 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97.8, ups=0.87, wpb=111.9, bsz=40, num_updates=530, lr=2.70961e-05, gnorm=1.908, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=620
2023-02-22 11:43:40 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 2175 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.3, ups=0.88, wpb=113.4, bsz=40, num_updates=540, lr=2.76074e-05, gnorm=1.633, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=631
2023-02-22 11:43:52 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 2175 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=98.7, ups=0.89, wpb=111.3, bsz=40, num_updates=550, lr=2.81186e-05, gnorm=1.84, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=642
2023-02-22 11:44:03 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 2175 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=104, ups=0.92, wpb=113, bsz=40, num_updates=560, lr=2.86299e-05, gnorm=1.97, clip=90, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=653
2023-02-22 11:44:14 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 2175 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.6, ups=0.88, wpb=113.4, bsz=40, num_updates=570, lr=2.91411e-05, gnorm=1.521, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=665
2023-02-22 11:44:25 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 2175 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.2, ups=0.89, wpb=112.9, bsz=40, num_updates=580, lr=2.96524e-05, gnorm=1.512, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=676
2023-02-22 11:44:36 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 2175 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=102.4, ups=0.9, wpb=114.2, bsz=40, num_updates=590, lr=3.01636e-05, gnorm=1.581, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=687
2023-02-22 11:44:47 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 2175 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=102.2, ups=0.91, wpb=112.8, bsz=40, num_updates=600, lr=3.06748e-05, gnorm=1.714, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=698
2023-02-22 11:44:58 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 2175 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=102.7, ups=0.91, wpb=112.7, bsz=40, num_updates=610, lr=3.11861e-05, gnorm=1.805, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=709
2023-02-22 11:45:09 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 2175 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=103.3, ups=0.92, wpb=111.9, bsz=40, num_updates=620, lr=3.16973e-05, gnorm=1.825, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=720
2023-02-22 11:45:21 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 2175 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=97, ups=0.87, wpb=111.8, bsz=40, num_updates=630, lr=3.22086e-05, gnorm=1.727, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=732
2023-02-22 11:45:32 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 2175 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.7, ups=0.9, wpb=113, bsz=40, num_updates=640, lr=3.27198e-05, gnorm=1.519, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=743
2023-02-22 11:45:43 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 2175 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.1, ups=0.89, wpb=112.8, bsz=40, num_updates=650, lr=3.32311e-05, gnorm=2.073, clip=100, loss_scale=256, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=754
2023-02-22 11:45:54 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 2175 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.5, ups=0.89, wpb=111.3, bsz=40, num_updates=660, lr=3.37423e-05, gnorm=1.858, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=765
2023-02-22 11:46:06 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 2175 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.5, ups=0.88, wpb=111.7, bsz=40, num_updates=670, lr=3.42536e-05, gnorm=1.407, clip=90, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=777
2023-02-22 11:46:17 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 2175 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.5, ups=0.89, wpb=111.6, bsz=40, num_updates=680, lr=3.47648e-05, gnorm=1.762, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=788
2023-02-22 11:46:28 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 2175 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99, ups=0.88, wpb=113, bsz=40, num_updates=690, lr=3.52761e-05, gnorm=1.603, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=799
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 11:46:40 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 2175 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.7, ups=0.89, wpb=112.8, bsz=40, num_updates=700, lr=3.57873e-05, gnorm=1.413, clip=90, loss_scale=256, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=810
2023-02-22 11:46:51 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 2175 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.2, ups=0.87, wpb=112.4, bsz=40, num_updates=710, lr=3.62986e-05, gnorm=1.692, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=822
2023-02-22 11:47:03 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 2175 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.8, ups=0.88, wpb=112.8, bsz=40, num_updates=720, lr=3.68098e-05, gnorm=1.38, clip=90, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=833
2023-02-22 11:47:14 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 2175 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.5, ups=0.88, wpb=111.4, bsz=40, num_updates=730, lr=3.73211e-05, gnorm=1.605, clip=90, loss_scale=256, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=845
2023-02-22 11:47:25 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 2175 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.3, ups=0.89, wpb=112, bsz=40, num_updates=740, lr=3.78323e-05, gnorm=1.635, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=856
2023-02-22 11:47:36 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 2175 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101.9, ups=0.9, wpb=113.3, bsz=40, num_updates=750, lr=3.83436e-05, gnorm=1.26, clip=90, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=867
2023-02-22 11:47:47 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 2175 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.7, ups=0.9, wpb=113.5, bsz=40, num_updates=760, lr=3.88548e-05, gnorm=1.029, clip=40, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=878
2023-02-22 11:47:59 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 2175 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=96.5, ups=0.86, wpb=111.7, bsz=40, num_updates=770, lr=3.93661e-05, gnorm=1.406, clip=100, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=890
2023-02-22 11:48:10 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 2175 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.8, ups=0.88, wpb=112.2, bsz=40, num_updates=780, lr=3.98773e-05, gnorm=1.473, clip=80, loss_scale=256, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=901
@@@@ ERROR IN DATA @@@@ watch
2023-02-22 11:48:22 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 2175 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98, ups=0.88, wpb=111, bsz=40, num_updates=790, lr=4.03885e-05, gnorm=1.444, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=912
2023-02-22 11:48:33 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 2175 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.3, ups=0.9, wpb=110.5, bsz=40, num_updates=800, lr=4.08998e-05, gnorm=1.742, clip=100, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=924
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 11:48:44 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 2175 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.4, ups=0.89, wpb=112, bsz=40, num_updates=810, lr=4.1411e-05, gnorm=1.5, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=935
2023-02-22 11:48:55 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 2175 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102.8, ups=0.91, wpb=112.7, bsz=40, num_updates=820, lr=4.19223e-05, gnorm=1.482, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=946
2023-02-22 11:49:06 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 2175 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101.8, ups=0.9, wpb=112.8, bsz=40, num_updates=830, lr=4.24335e-05, gnorm=1.648, clip=90, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=957
2023-02-22 11:49:17 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 2175 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98.9, ups=0.89, wpb=111.6, bsz=40, num_updates=840, lr=4.29448e-05, gnorm=1.471, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=968
2023-02-22 11:49:29 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 2175 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.3, ups=0.89, wpb=112.5, bsz=40, num_updates=850, lr=4.3456e-05, gnorm=1.432, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=979
2023-02-22 11:49:40 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 2175 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.2, ups=0.88, wpb=112.1, bsz=40, num_updates=860, lr=4.39673e-05, gnorm=1.475, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=991
2023-02-22 11:49:51 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 2175 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.5, ups=0.9, wpb=110.7, bsz=40, num_updates=870, lr=4.44785e-05, gnorm=1.355, clip=90, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1002
2023-02-22 11:50:03 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 2175 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=95.4, ups=0.86, wpb=110.6, bsz=40, num_updates=880, lr=4.49898e-05, gnorm=1.525, clip=90, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1014
2023-02-22 11:50:14 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 2175 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.9, ups=0.9, wpb=112.6, bsz=40, num_updates=890, lr=4.5501e-05, gnorm=1.418, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1025
2023-02-22 11:50:25 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 2175 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.6, ups=0.89, wpb=112.2, bsz=40, num_updates=900, lr=4.60123e-05, gnorm=1.267, clip=90, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1036
2023-02-22 11:50:37 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 2175 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.1, ups=0.86, wpb=112.4, bsz=40, num_updates=910, lr=4.65235e-05, gnorm=1.317, clip=90, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1048
2023-02-22 11:50:48 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 2175 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.8, ups=0.88, wpb=112.8, bsz=40, num_updates=920, lr=4.70348e-05, gnorm=1.572, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1059
2023-02-22 11:51:00 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 2175 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=114.6, nsentences=40, sample_size=114.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.8, ups=0.89, wpb=114.6, bsz=40, num_updates=930, lr=4.7546e-05, gnorm=1.267, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1070
2023-02-22 11:51:11 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 2175 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100, ups=0.9, wpb=111.4, bsz=40, num_updates=940, lr=4.80573e-05, gnorm=1.512, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1081
2023-02-22 11:51:22 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 2175 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.9, ups=0.89, wpb=112.8, bsz=40, num_updates=950, lr=4.85685e-05, gnorm=1.471, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1093
2023-02-22 11:51:33 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 2175 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=104.4, ups=0.93, wpb=112.5, bsz=40, num_updates=960, lr=4.90798e-05, gnorm=1.564, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1103
2023-02-22 11:51:44 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 2175 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.6, ups=0.89, wpb=112, bsz=40, num_updates=970, lr=4.9591e-05, gnorm=1.418, clip=90, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1115
2023-02-22 11:51:55 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 2175 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.8, ups=0.88, wpb=112.4, bsz=40, num_updates=980, lr=4.99946e-05, gnorm=1.392, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1126
2023-02-22 11:52:07 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 2175 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.7, ups=0.88, wpb=112.9, bsz=40, num_updates=990, lr=4.99677e-05, gnorm=1.3, clip=70, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1137
2023-02-22 11:52:18 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 2175 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.3, ups=0.89, wpb=111, bsz=40, num_updates=1000, lr=4.99409e-05, gnorm=1.103, clip=70, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1149
2023-02-22 11:52:29 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 2175 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.1, ups=0.9, wpb=111.7, bsz=40, num_updates=1010, lr=4.9914e-05, gnorm=1.493, clip=70, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1160
2023-02-22 11:52:40 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 2175 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.8, ups=0.88, wpb=112.9, bsz=40, num_updates=1020, lr=4.98871e-05, gnorm=1.111, clip=60, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1171
2023-02-22 11:52:52 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 2175 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.5, ups=0.89, wpb=113.4, bsz=40, num_updates=1030, lr=4.98602e-05, gnorm=1.414, clip=80, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1182
2023-02-22 11:53:03 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 2175 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.5, ups=0.87, wpb=113.8, bsz=40, num_updates=1040, lr=4.98333e-05, gnorm=1.449, clip=70, loss_scale=512, train_wall=11, gb_free=9.1, ema_decay=0.9999, wall=1194
2023-02-22 11:53:14 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 2175 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.8, ups=0.9, wpb=112.4, bsz=40, num_updates=1050, lr=4.98064e-05, gnorm=1.5, clip=90, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=1205
2023-02-22 11:53:25 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 2175 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.8, ups=0.91, wpb=111.8, bsz=40, num_updates=1060, lr=4.97795e-05, gnorm=1.306, clip=80, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=1216
2023-02-22 11:53:36 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 2175 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=102.3, ups=0.91, wpb=112.5, bsz=40, num_updates=1070, lr=4.97526e-05, gnorm=1.246, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1227
2023-02-22 11:53:47 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 2175 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.6, ups=0.89, wpb=112.4, bsz=40, num_updates=1080, lr=4.97258e-05, gnorm=1.289, clip=70, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1238
2023-02-22 11:53:59 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 2175 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.2, ups=0.9, wpb=111.8, bsz=40, num_updates=1090, lr=4.96989e-05, gnorm=1.151, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1249
2023-02-22 11:54:10 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 2175 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.3, ups=0.88, wpb=112.1, bsz=40, num_updates=1100, lr=4.9672e-05, gnorm=1.206, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1261
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 11:54:21 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 2175 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=103.9, ups=0.92, wpb=112.4, bsz=40, num_updates=1110, lr=4.96451e-05, gnorm=1.4, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1272
2023-02-22 11:54:32 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 2175 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101, ups=0.9, wpb=112.3, bsz=40, num_updates=1120, lr=4.96182e-05, gnorm=1.211, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1283
2023-02-22 11:54:43 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 2175 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101, ups=0.9, wpb=112.3, bsz=40, num_updates=1130, lr=4.95913e-05, gnorm=1.408, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1294
2023-02-22 11:54:54 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 2175 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=103.2, ups=0.91, wpb=113.2, bsz=40, num_updates=1140, lr=4.95644e-05, gnorm=1.435, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1305
2023-02-22 11:55:05 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 2175 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.2, ups=0.9, wpb=111.1, bsz=40, num_updates=1150, lr=4.95376e-05, gnorm=1.367, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1316
2023-02-22 11:55:17 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 2175 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.2, ups=0.89, wpb=112.1, bsz=40, num_updates=1160, lr=4.95107e-05, gnorm=1.264, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1327
2023-02-22 11:55:28 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 2175 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.6, ups=0.88, wpb=113.7, bsz=40, num_updates=1170, lr=4.94838e-05, gnorm=1.553, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1339
2023-02-22 11:55:39 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 2175 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.4, ups=0.9, wpb=112.8, bsz=40, num_updates=1180, lr=4.94569e-05, gnorm=1.264, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1350
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 11:55:50 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 2175 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=1190, lr=4.943e-05, gnorm=1.397, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1361
2023-02-22 11:56:01 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 2175 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101, ups=0.89, wpb=113.5, bsz=40, num_updates=1200, lr=4.94031e-05, gnorm=1.427, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1372
2023-02-22 11:56:13 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 2175 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.6, ups=0.88, wpb=113.1, bsz=40, num_updates=1210, lr=4.93762e-05, gnorm=1.474, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1384
2023-02-22 11:56:24 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 2175 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.4, ups=0.89, wpb=112.3, bsz=40, num_updates=1220, lr=4.93494e-05, gnorm=1.394, clip=90, loss_scale=512, train_wall=11, gb_free=9.2, ema_decay=0.9999, wall=1395
2023-02-22 11:56:35 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 2175 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=104, ups=0.92, wpb=112.6, bsz=40, num_updates=1230, lr=4.93225e-05, gnorm=1.429, clip=80, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1406
2023-02-22 11:56:46 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 2175 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.8, ups=0.91, wpb=111.9, bsz=40, num_updates=1240, lr=4.92956e-05, gnorm=1.179, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1417
2023-02-22 11:56:57 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 2175 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.4, ups=0.89, wpb=110.9, bsz=40, num_updates=1250, lr=4.92687e-05, gnorm=1.167, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1428
2023-02-22 11:57:09 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 2175 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.8, ups=0.89, wpb=112.8, bsz=40, num_updates=1260, lr=4.92418e-05, gnorm=1.039, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1439
2023-02-22 11:57:20 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 2175 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.7, ups=0.9, wpb=111.3, bsz=40, num_updates=1270, lr=4.92149e-05, gnorm=1.333, clip=90, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1451
2023-02-22 11:57:31 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 2175 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.1, ups=0.89, wpb=112.9, bsz=40, num_updates=1280, lr=4.9188e-05, gnorm=1.146, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1462
2023-02-22 11:57:42 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 2175 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=103.6, ups=0.91, wpb=113.6, bsz=40, num_updates=1290, lr=4.91612e-05, gnorm=1.414, clip=90, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1473
2023-02-22 11:57:53 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 2175 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.6, ups=0.88, wpb=110.5, bsz=40, num_updates=1300, lr=4.91343e-05, gnorm=1.116, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1484
2023-02-22 11:58:05 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 2175 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.5, ups=0.9, wpb=112.2, bsz=40, num_updates=1310, lr=4.91074e-05, gnorm=1.434, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1495
2023-02-22 11:58:16 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 2175 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.7, ups=0.9, wpb=112, bsz=40, num_updates=1320, lr=4.90805e-05, gnorm=1.224, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1506
2023-02-22 11:58:27 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98, ups=0.88, wpb=110.7, bsz=40, num_updates=1330, lr=4.90536e-05, gnorm=1.188, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1518
2023-02-22 11:58:38 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 2175 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.8, ups=0.9, wpb=111.3, bsz=40, num_updates=1340, lr=4.90267e-05, gnorm=1.485, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1529
2023-02-22 11:58:49 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 2175 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100, ups=0.89, wpb=112.6, bsz=40, num_updates=1350, lr=4.89998e-05, gnorm=1.072, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1540
2023-02-22 11:59:01 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 2175 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.7, ups=0.88, wpb=113.7, bsz=40, num_updates=1360, lr=4.8973e-05, gnorm=1.215, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1552
2023-02-22 11:59:11 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 2175 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=105.1, ups=0.94, wpb=111.7, bsz=40, num_updates=1370, lr=4.89461e-05, gnorm=1.249, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1562
2023-02-22 11:59:23 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 2175 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.4, ups=0.89, wpb=112, bsz=40, num_updates=1380, lr=4.89192e-05, gnorm=1.71, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1573
2023-02-22 11:59:34 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 2175 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=102.1, ups=0.91, wpb=112.2, bsz=40, num_updates=1390, lr=4.88923e-05, gnorm=1.18, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1584
2023-02-22 11:59:45 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 2175 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.4, ups=0.87, wpb=113.2, bsz=40, num_updates=1400, lr=4.88654e-05, gnorm=1.095, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1596
2023-02-22 11:59:57 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 2175 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.9, ups=0.89, wpb=113.1, bsz=40, num_updates=1410, lr=4.88385e-05, gnorm=1.256, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1607
2023-02-22 12:00:08 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.88, wpb=112.3, bsz=40, num_updates=1420, lr=4.88116e-05, gnorm=1.13, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1619
2023-02-22 12:00:19 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 2175 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.1, ups=0.9, wpb=112.6, bsz=40, num_updates=1430, lr=4.87848e-05, gnorm=1.291, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1630
2023-02-22 12:00:30 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 2175 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.8, ups=0.89, wpb=112.2, bsz=40, num_updates=1440, lr=4.87579e-05, gnorm=1.208, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1641
2023-02-22 12:00:41 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 2175 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.8, ups=0.92, wpb=110.5, bsz=40, num_updates=1450, lr=4.8731e-05, gnorm=1.2, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1652
2023-02-22 12:00:52 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 2175 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102.3, ups=0.91, wpb=112.5, bsz=40, num_updates=1460, lr=4.87041e-05, gnorm=1.39, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1663
2023-02-22 12:01:03 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 2175 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.8, ups=0.9, wpb=111.1, bsz=40, num_updates=1470, lr=4.86772e-05, gnorm=1.304, clip=80, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1674
2023-02-22 12:01:15 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 2175 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.1, ups=0.89, wpb=111.8, bsz=40, num_updates=1480, lr=4.86503e-05, gnorm=1.059, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1685
2023-02-22 12:01:26 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 2175 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.1, ups=0.89, wpb=112.9, bsz=40, num_updates=1490, lr=4.86234e-05, gnorm=1.293, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1697
2023-02-22 12:01:37 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 2175 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.7, ups=0.87, wpb=113.2, bsz=40, num_updates=1500, lr=4.85965e-05, gnorm=1.086, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1708
2023-02-22 12:01:48 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 2175 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=105, ups=0.94, wpb=112.3, bsz=40, num_updates=1510, lr=4.85697e-05, gnorm=1.342, clip=90, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1719
2023-02-22 12:01:59 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 2175 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.9, ups=0.89, wpb=112.6, bsz=40, num_updates=1520, lr=4.85428e-05, gnorm=1.152, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1730
2023-02-22 12:02:11 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 2175 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.5, ups=0.87, wpb=113.8, bsz=40, num_updates=1530, lr=4.85159e-05, gnorm=1.129, clip=40, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=1742
2023-02-22 12:02:22 - progress_bar.py[line:274] - INFO: epoch 001:   1540 / 2175 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.5, ups=0.9, wpb=111.5, bsz=40, num_updates=1540, lr=4.8489e-05, gnorm=1.19, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1753
2023-02-22 12:02:33 - progress_bar.py[line:274] - INFO: epoch 001:   1550 / 2175 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.1, ups=0.9, wpb=111.5, bsz=40, num_updates=1550, lr=4.84621e-05, gnorm=1.188, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1764
2023-02-22 12:02:44 - progress_bar.py[line:274] - INFO: epoch 001:   1560 / 2175 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.6, ups=0.89, wpb=112.1, bsz=40, num_updates=1560, lr=4.84352e-05, gnorm=1.125, clip=70, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1775
2023-02-22 12:02:56 - progress_bar.py[line:274] - INFO: epoch 001:   1570 / 2175 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.4, ups=0.89, wpb=112.3, bsz=40, num_updates=1570, lr=4.84083e-05, gnorm=1.095, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1786
2023-02-22 12:03:07 - progress_bar.py[line:274] - INFO: epoch 001:   1580 / 2175 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.7, ups=0.9, wpb=113.1, bsz=40, num_updates=1580, lr=4.83815e-05, gnorm=0.899, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1798
2023-02-22 12:03:18 - progress_bar.py[line:274] - INFO: epoch 001:   1590 / 2175 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=102.5, ups=0.91, wpb=112.4, bsz=40, num_updates=1590, lr=4.83546e-05, gnorm=1.204, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1809
2023-02-22 12:03:29 - progress_bar.py[line:274] - INFO: epoch 001:   1600 / 2175 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.1, ups=0.88, wpb=112.3, bsz=40, num_updates=1600, lr=4.83277e-05, gnorm=1.051, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1820
2023-02-22 12:03:40 - progress_bar.py[line:274] - INFO: epoch 001:   1610 / 2175 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.9, ups=0.89, wpb=112.2, bsz=40, num_updates=1610, lr=4.83008e-05, gnorm=0.997, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1831
2023-02-22 12:03:52 - progress_bar.py[line:274] - INFO: epoch 001:   1620 / 2175 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97, ups=0.87, wpb=111.6, bsz=40, num_updates=1620, lr=4.82739e-05, gnorm=0.977, clip=40, loss_scale=1024, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=1843
2023-02-22 12:04:03 - progress_bar.py[line:274] - INFO: epoch 001:   1630 / 2175 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.6, ups=0.9, wpb=112.1, bsz=40, num_updates=1630, lr=4.8247e-05, gnorm=1.091, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1854
2023-02-22 12:04:14 - progress_bar.py[line:274] - INFO: epoch 001:   1640 / 2175 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.2, ups=0.91, wpb=113.3, bsz=40, num_updates=1640, lr=4.82201e-05, gnorm=1.158, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1865
2023-02-22 12:04:25 - progress_bar.py[line:274] - INFO: epoch 001:   1650 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.2, ups=0.9, wpb=113.2, bsz=40, num_updates=1650, lr=4.81933e-05, gnorm=1.252, clip=70, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1876
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 12:04:36 - progress_bar.py[line:274] - INFO: epoch 001:   1660 / 2175 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.89, wpb=112.1, bsz=40, num_updates=1660, lr=4.81664e-05, gnorm=1.184, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1887
2023-02-22 12:04:48 - progress_bar.py[line:274] - INFO: epoch 001:   1670 / 2175 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.6, ups=0.9, wpb=113.1, bsz=40, num_updates=1670, lr=4.81395e-05, gnorm=1.342, clip=70, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1898
2023-02-22 12:04:59 - progress_bar.py[line:274] - INFO: epoch 001:   1680 / 2175 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.1, ups=0.88, wpb=111.8, bsz=40, num_updates=1680, lr=4.81126e-05, gnorm=1.059, clip=60, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1910
2023-02-22 12:05:10 - progress_bar.py[line:274] - INFO: epoch 001:   1690 / 2175 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=102.1, ups=0.91, wpb=112.2, bsz=40, num_updates=1690, lr=4.80857e-05, gnorm=1.022, clip=50, loss_scale=1024, train_wall=11, gb_free=9.2, ema_decay=0.9999, wall=1921
2023-02-22 12:05:22 - progress_bar.py[line:274] - INFO: epoch 001:   1700 / 2175 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.1, ups=0.87, wpb=112.2, bsz=40, num_updates=1700, lr=4.80588e-05, gnorm=1.051, clip=70, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1932
2023-02-22 12:05:33 - progress_bar.py[line:274] - INFO: epoch 001:   1710 / 2175 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.5, ups=0.86, wpb=112.7, bsz=40, num_updates=1710, lr=4.80319e-05, gnorm=0.951, clip=50, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1944
2023-02-22 12:05:44 - progress_bar.py[line:274] - INFO: epoch 001:   1720 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101, ups=0.9, wpb=112, bsz=40, num_updates=1720, lr=4.80051e-05, gnorm=0.864, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1955
2023-02-22 12:05:55 - progress_bar.py[line:274] - INFO: epoch 001:   1730 / 2175 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.8, ups=0.91, wpb=112.7, bsz=40, num_updates=1730, lr=4.79782e-05, gnorm=1.221, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1966
2023-02-22 12:06:06 - progress_bar.py[line:274] - INFO: epoch 001:   1740 / 2175 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=103.2, ups=0.92, wpb=112.5, bsz=40, num_updates=1740, lr=4.79513e-05, gnorm=1.114, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1977
2023-02-22 12:06:18 - progress_bar.py[line:274] - INFO: epoch 001:   1750 / 2175 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.4, ups=0.87, wpb=112.5, bsz=40, num_updates=1750, lr=4.79244e-05, gnorm=1.106, clip=50, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1988
2023-02-22 12:06:29 - progress_bar.py[line:274] - INFO: epoch 001:   1760 / 2175 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.8, ups=0.88, wpb=111.9, bsz=40, num_updates=1760, lr=4.78975e-05, gnorm=1.064, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2000
2023-02-22 12:06:40 - progress_bar.py[line:274] - INFO: epoch 001:   1770 / 2175 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.6, ups=0.9, wpb=112.7, bsz=40, num_updates=1770, lr=4.78706e-05, gnorm=1.166, clip=50, loss_scale=1024, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=2011
2023-02-22 12:06:51 - progress_bar.py[line:274] - INFO: epoch 001:   1780 / 2175 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102.4, ups=0.91, wpb=112.2, bsz=40, num_updates=1780, lr=4.78437e-05, gnorm=1.133, clip=60, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=2022
2023-02-22 12:07:02 - progress_bar.py[line:274] - INFO: epoch 001:   1790 / 2175 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=104.8, ups=0.92, wpb=113.5, bsz=40, num_updates=1790, lr=4.78169e-05, gnorm=1.036, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2033
2023-02-22 12:07:13 - progress_bar.py[line:274] - INFO: epoch 001:   1800 / 2175 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.9, ups=0.9, wpb=112.2, bsz=40, num_updates=1800, lr=4.779e-05, gnorm=1.172, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2044
@@@@ ERROR IN DATA @@@@ play
2023-02-22 12:07:24 - progress_bar.py[line:274] - INFO: epoch 001:   1810 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.3, ups=0.91, wpb=113.5, bsz=40, num_updates=1810, lr=4.77631e-05, gnorm=0.925, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2055
2023-02-22 12:07:35 - progress_bar.py[line:274] - INFO: epoch 001:   1820 / 2175 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.5, ups=0.9, wpb=111.9, bsz=40, num_updates=1820, lr=4.77362e-05, gnorm=0.932, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2066
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 12:07:46 - progress_bar.py[line:274] - INFO: epoch 001:   1830 / 2175 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.89, wpb=112.7, bsz=40, num_updates=1830, lr=4.77093e-05, gnorm=0.954, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2077
2023-02-22 12:07:58 - progress_bar.py[line:274] - INFO: epoch 001:   1840 / 2175 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.4, ups=0.89, wpb=112.8, bsz=40, num_updates=1840, lr=4.76824e-05, gnorm=1.005, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2088
@@@@ ERROR IN DATA @@@@ play
2023-02-22 12:08:09 - progress_bar.py[line:274] - INFO: epoch 001:   1850 / 2175 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.8, ups=0.9, wpb=111.6, bsz=40, num_updates=1850, lr=4.76555e-05, gnorm=1.231, clip=100, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2099
2023-02-22 12:08:20 - progress_bar.py[line:274] - INFO: epoch 001:   1860 / 2175 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.1, ups=0.88, wpb=113.1, bsz=40, num_updates=1860, lr=4.76286e-05, gnorm=0.826, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2111
2023-02-22 12:08:32 - progress_bar.py[line:274] - INFO: epoch 001:   1870 / 2175 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=97.9, ups=0.88, wpb=111.5, bsz=40, num_updates=1870, lr=4.76018e-05, gnorm=1.016, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2122
2023-02-22 12:08:43 - progress_bar.py[line:274] - INFO: epoch 001:   1880 / 2175 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.8, ups=0.88, wpb=114.9, bsz=40, num_updates=1880, lr=4.75749e-05, gnorm=0.941, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2134
2023-02-22 12:08:54 - progress_bar.py[line:274] - INFO: epoch 001:   1890 / 2175 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=98.6, ups=0.88, wpb=112.6, bsz=40, num_updates=1890, lr=4.7548e-05, gnorm=1.325, clip=90, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2145
2023-02-22 12:09:05 - progress_bar.py[line:274] - INFO: epoch 001:   1900 / 2175 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.7, ups=0.91, wpb=111.5, bsz=40, num_updates=1900, lr=4.75211e-05, gnorm=0.944, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2156
2023-02-22 12:09:17 - progress_bar.py[line:274] - INFO: epoch 001:   1910 / 2175 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.7, ups=0.89, wpb=113.1, bsz=40, num_updates=1910, lr=4.74942e-05, gnorm=0.907, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2167
2023-02-22 12:09:28 - progress_bar.py[line:274] - INFO: epoch 001:   1920 / 2175 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.1, ups=0.9, wpb=111.3, bsz=40, num_updates=1920, lr=4.74673e-05, gnorm=1.062, clip=40, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=2178
2023-02-22 12:09:39 - progress_bar.py[line:274] - INFO: epoch 001:   1930 / 2175 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=95.6, ups=0.86, wpb=111.4, bsz=40, num_updates=1930, lr=4.74404e-05, gnorm=0.983, clip=30, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2190
2023-02-22 12:09:47 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 12:09:52 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 2175 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=92.3, ups=0.81, wpb=113.4, bsz=40, num_updates=1940, lr=4.74136e-05, gnorm=0.976, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2202
2023-02-22 12:10:03 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 2175 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.7, ups=0.9, wpb=112.9, bsz=40, num_updates=1950, lr=4.73867e-05, gnorm=1.28, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2214
2023-02-22 12:10:13 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 2175 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=105.8, ups=0.94, wpb=113.1, bsz=40, num_updates=1960, lr=4.73598e-05, gnorm=1.064, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2224
2023-02-22 12:10:24 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 2175 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.6, ups=0.91, wpb=112.8, bsz=40, num_updates=1970, lr=4.73329e-05, gnorm=1.076, clip=60, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=2235
2023-02-22 12:10:36 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 2175 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.9, ups=0.9, wpb=113.4, bsz=40, num_updates=1980, lr=4.7306e-05, gnorm=1.18, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2246
2023-02-22 12:10:47 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 2175 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.6, ups=0.88, wpb=114.2, bsz=40, num_updates=1990, lr=4.72791e-05, gnorm=0.953, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2258
2023-02-22 12:10:58 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 2175 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.4, ups=0.9, wpb=111.9, bsz=40, num_updates=2000, lr=4.72522e-05, gnorm=1.001, clip=50, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=2269
2023-02-22 12:10:58 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 12:10:58 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 12:10:59 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 12:10:59 - train.py[line:551] - INFO: load:1.09 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 12:13:04 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 12:13:04 - train.py[line:551] - INFO: load:1.12 valid_run:124.25 task_valid:120.87 collect_output:2.26
2023-02-22 12:15:04 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 12:15:04 - train.py[line:551] - INFO: load:1.14 valid_run:244.58 task_valid:236.27 collect_output:6.16
2023-02-22 12:17:08 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 12:17:08 - train.py[line:551] - INFO: load:1.17 valid_run:367.87 task_valid:352.53 collect_output:12.15
2023-02-22 12:19:10 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 12:19:10 - train.py[line:551] - INFO: load:1.20 valid_run:490.40 task_valid:465.92 collect_output:20.24
2023-02-22 12:21:11 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 12:21:11 - train.py[line:551] - INFO: load:1.22 valid_run:611.27 task_valid:583.02 collect_output:22.97
2023-02-22 12:23:15 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 12:23:15 - train.py[line:551] - INFO: load:1.25 valid_run:734.79 task_valid:701.67 collect_output:26.77
2023-02-22 12:25:18 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 12:25:18 - train.py[line:551] - INFO: load:1.28 valid_run:858.41 task_valid:819.68 collect_output:31.28
2023-02-22 12:27:21 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 12:27:21 - train.py[line:551] - INFO: load:1.31 valid_run:980.92 task_valid:936.16 collect_output:36.23
2023-02-22 12:29:26 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 12:29:26 - train.py[line:551] - INFO: load:1.34 valid_run:1105.48 task_valid:1053.27 collect_output:42.61
2023-02-22 12:31:28 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 12:31:28 - train.py[line:551] - INFO: load:1.36 valid_run:1228.04 task_valid:1165.61 collect_output:51.76
2023-02-22 12:33:29 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 12:33:29 - train.py[line:551] - INFO: load:1.39 valid_run:1349.18 task_valid:1281.04 collect_output:56.42
2023-02-22 12:35:32 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 12:35:32 - train.py[line:551] - INFO: load:1.42 valid_run:1471.32 task_valid:1397.93 collect_output:60.61
2023-02-22 12:37:31 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 12:37:31 - train.py[line:551] - INFO: load:1.45 valid_run:1590.85 task_valid:1511.50 collect_output:65.52
2023-02-22 12:39:33 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 12:39:33 - train.py[line:551] - INFO: load:1.48 valid_run:1712.30 task_valid:1629.18 collect_output:68.24
2023-02-22 12:41:34 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 12:41:34 - train.py[line:551] - INFO: load:1.51 valid_run:1833.69 task_valid:1745.05 collect_output:72.71
2023-02-22 12:43:36 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 12:43:36 - train.py[line:551] - INFO: load:1.53 valid_run:1955.57 task_valid:1858.78 collect_output:79.77
2023-02-22 12:45:38 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 12:45:38 - train.py[line:551] - INFO: load:1.56 valid_run:2077.37 task_valid:1974.80 collect_output:84.48
2023-02-22 12:47:39 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 12:47:39 - train.py[line:551] - INFO: load:1.59 valid_run:2198.32 task_valid:2092.59 collect_output:86.57
2023-02-22 12:49:41 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 12:49:41 - train.py[line:551] - INFO: load:1.62 valid_run:2319.99 task_valid:2209.46 collect_output:90.26
2023-02-22 12:51:42 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 12:51:42 - train.py[line:551] - INFO: load:1.65 valid_run:2441.15 task_valid:2326.00 collect_output:93.81
2023-02-22 12:53:44 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 12:53:44 - train.py[line:551] - INFO: load:1.68 valid_run:2563.47 task_valid:2442.50 collect_output:98.54
2023-02-22 12:55:47 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 12:55:47 - train.py[line:551] - INFO: load:1.71 valid_run:2685.85 task_valid:2561.23 collect_output:101.13
2023-02-22 12:57:48 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 12:57:48 - train.py[line:551] - INFO: load:1.73 valid_run:2806.74 task_valid:2675.19 collect_output:107.01
2023-02-22 12:59:48 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 12:59:48 - train.py[line:551] - INFO: load:1.76 valid_run:2927.03 task_valid:2791.11 collect_output:110.30
2023-02-22 13:01:50 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 13:01:50 - train.py[line:551] - INFO: load:1.79 valid_run:3049.16 task_valid:2907.14 collect_output:115.34
2023-02-22 13:03:54 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 13:03:54 - train.py[line:551] - INFO: load:1.82 valid_run:3172.67 task_valid:3022.80 collect_output:122.13
2023-02-22 13:05:54 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 13:05:54 - train.py[line:551] - INFO: load:1.85 valid_run:3292.76 task_valid:3136.77 collect_output:127.20
2023-02-22 13:07:56 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 13:07:56 - train.py[line:551] - INFO: load:1.88 valid_run:3414.88 task_valid:3256.00 collect_output:129.04
2023-02-22 13:09:59 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 13:09:59 - train.py[line:551] - INFO: load:1.91 valid_run:3537.44 task_valid:3371.25 collect_output:135.28
2023-02-22 13:12:01 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 13:12:01 - train.py[line:551] - INFO: load:1.93 valid_run:3659.80 task_valid:3489.57 collect_output:138.24
2023-02-22 13:14:02 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 13:14:02 - train.py[line:551] - INFO: load:1.96 valid_run:3781.01 task_valid:3607.84 collect_output:140.13

====================================================================================================
SGG eval:     R @ 50: 0.4360;     R @ 100: 0.4892;     R @ 500: 0.5517;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2205;    mR @ 100: 0.2639;    mR @ 500: 0.3373;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.2049) (covered in:0.0625) (covering:0.1429) (eating:0.6176) (flying in:0.5909) (growing on:0.0000) (hanging from:0.5548) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.5000) (playing:0.0000) (riding:0.6324) (says:0.0000) (sitting on:0.5337) (standing on:0.6725) (using:0.2000) (walking in:0.0000) (walking on:0.2162) (watching:0.0833) 
--------------------------------------------------------
====================================================================================================

2023-02-22 13:14:33 - train.py[line:487] - INFO: 0.4891969696969697

====================================================================================================
SGG eval:     R @ 50: 0.4360;     R @ 100: 0.4892;     R @ 500: 0.5517;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2205;    mR @ 100: 0.2639;    mR @ 500: 0.3373;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.2049) (covered in:0.0625) (covering:0.1429) (eating:0.6176) (flying in:0.5909) (growing on:0.0000) (hanging from:0.5548) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.5000) (playing:0.0000) (riding:0.6324) (says:0.0000) (sitting on:0.5337) (standing on:0.6725) (using:0.2000) (walking in:0.0000) (walking on:0.2162) (watching:0.0833) 
--------------------------------------------------------
====================================================================================================

2023-02-22 13:14:33 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 13:14:33 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.264 | loss_v1 0 | loss_v2 0 | nll_loss 0.089 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.489197 | ppl 1.06 | vqa_score 0.2005 | wps 117.7 | wpb 72 | bsz 24 | num_updates 2000
2023-02-22 13:14:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2023-02-22 13:14:33 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_1_2000.pt
2023-02-22 13:14:39 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_1_2000.pt
2023-02-22 13:14:43 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.4891969696969697) (writing took 10.1952088419348 seconds)
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 13:14:56 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 2175 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=0.3, ups=0, wpb=111.5, bsz=40, num_updates=2010, lr=4.72254e-05, gnorm=0.975, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6105
2023-02-22 13:15:07 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 2175 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=104.1, ups=0.92, wpb=113.2, bsz=40, num_updates=2020, lr=4.71985e-05, gnorm=1.277, clip=60, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=6118
2023-02-22 13:15:19 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 2175 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.7, ups=0.87, wpb=111.7, bsz=40, num_updates=2030, lr=4.71716e-05, gnorm=0.894, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6129
2023-02-22 13:15:30 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 2175 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.2, ups=0.86, wpb=111.6, bsz=40, num_updates=2040, lr=4.71447e-05, gnorm=0.963, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6141
2023-02-22 13:15:41 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 2175 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.1, ups=0.9, wpb=112.3, bsz=40, num_updates=2050, lr=4.71178e-05, gnorm=1.011, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=6152
2023-02-22 13:15:52 - progress_bar.py[line:274] - INFO: epoch 001:   2061 / 2175 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.3, ups=0.9, wpb=111.7, bsz=40, num_updates=2060, lr=4.70909e-05, gnorm=0.998, clip=40, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=6163
2023-02-22 13:16:04 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 2175 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.88, wpb=111.5, bsz=40, num_updates=2070, lr=4.7064e-05, gnorm=1.155, clip=80, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6175
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 13:16:15 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 2175 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99, ups=0.88, wpb=113.1, bsz=40, num_updates=2080, lr=4.70372e-05, gnorm=1.223, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6186
2023-02-22 13:16:26 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 2175 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.3, ups=0.9, wpb=111.7, bsz=40, num_updates=2090, lr=4.70103e-05, gnorm=1.168, clip=80, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=6197
2023-02-22 13:16:38 - progress_bar.py[line:274] - INFO: epoch 001:   2101 / 2175 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.9, wpb=111, bsz=40, num_updates=2100, lr=4.69834e-05, gnorm=1.145, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6208
2023-02-22 13:16:49 - progress_bar.py[line:274] - INFO: epoch 001:   2111 / 2175 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98, ups=0.87, wpb=113, bsz=40, num_updates=2110, lr=4.69565e-05, gnorm=1.364, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6220
2023-02-22 13:17:00 - progress_bar.py[line:274] - INFO: epoch 001:   2121 / 2175 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=2120, lr=4.69296e-05, gnorm=1.142, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6231
2023-02-22 13:17:11 - progress_bar.py[line:274] - INFO: epoch 001:   2131 / 2175 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=104, ups=0.92, wpb=112.8, bsz=40, num_updates=2130, lr=4.69027e-05, gnorm=1.199, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6242
2023-02-22 13:17:22 - progress_bar.py[line:274] - INFO: epoch 001:   2141 / 2175 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.5, ups=0.9, wpb=112.8, bsz=40, num_updates=2140, lr=4.68758e-05, gnorm=1.082, clip=60, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=6253
2023-02-22 13:17:34 - progress_bar.py[line:274] - INFO: epoch 001:   2151 / 2175 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99, ups=0.89, wpb=111.8, bsz=40, num_updates=2150, lr=4.6849e-05, gnorm=1.149, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6264
2023-02-22 13:17:45 - progress_bar.py[line:274] - INFO: epoch 001:   2161 / 2175 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.9, ups=0.87, wpb=112.1, bsz=40, num_updates=2160, lr=4.68221e-05, gnorm=1.012, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6276
2023-02-22 13:17:56 - progress_bar.py[line:274] - INFO: epoch 001:   2171 / 2175 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.1, ups=0.91, wpb=111.2, bsz=40, num_updates=2170, lr=4.67952e-05, gnorm=1.114, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6287
2023-02-22 13:18:00 - train.py[line:339] - INFO: end of epoch 1 (average epoch stats below)
2023-02-22 13:18:00 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.364 | loss_v1 0 | loss_v2 0 | nll_loss 0.197 | ntokens 112.283 | nsentences 39.997 | sample_size 112.283 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.15 | wps 38.9 | ups 0.35 | wpb 112.3 | bsz 40 | num_updates 2174 | lr 4.67844e-05 | gnorm 1.735 | clip 78 | loss_scale 512 | train_wall 2435 | gb_free 14.1 | ema_decay 0.9999 | wall 6291
2023-02-22 13:18:00 - trainer.py[line:694] - INFO: loading train data for epoch 2
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E1.tsv slice_id 1 row count 43497 total row count 86994
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E1.tsv slice_id 0 row count 43497 total row count 86994
2023-02-22 13:18:01 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
2023-02-22 13:18:01 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 13:18:01 - trainer.py[line:758] - INFO: begin training epoch 2
2023-02-22 13:18:01 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 13:18:10 - progress_bar.py[line:274] - INFO: epoch 002:      6 / 2175 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.4, nsentences=39.4, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=80, ups=0.73, wpb=109.4, bsz=39.4, num_updates=2180, lr=4.67683e-05, gnorm=1.066, clip=60, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6300
2023-02-22 13:18:21 - progress_bar.py[line:274] - INFO: epoch 002:     16 / 2175 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.2, ups=0.91, wpb=111.3, bsz=40, num_updates=2190, lr=4.67414e-05, gnorm=0.892, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6311
2023-02-22 13:18:32 - progress_bar.py[line:274] - INFO: epoch 002:     26 / 2175 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.6, ups=0.86, wpb=113.2, bsz=40, num_updates=2200, lr=4.67145e-05, gnorm=0.879, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6323
2023-02-22 13:18:43 - progress_bar.py[line:274] - INFO: epoch 002:     36 / 2175 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.7, ups=0.9, wpb=112.9, bsz=40, num_updates=2210, lr=4.66876e-05, gnorm=0.828, clip=30, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=6334
2023-02-22 13:18:55 - progress_bar.py[line:274] - INFO: epoch 002:     46 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100, ups=0.89, wpb=112.8, bsz=40, num_updates=2220, lr=4.66608e-05, gnorm=1.033, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6346
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 13:19:06 - progress_bar.py[line:274] - INFO: epoch 002:     56 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.88, wpb=112.6, bsz=40, num_updates=2230, lr=4.66339e-05, gnorm=0.892, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6357
2023-02-22 13:19:18 - progress_bar.py[line:274] - INFO: epoch 002:     66 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.88, wpb=112.8, bsz=40, num_updates=2240, lr=4.6607e-05, gnorm=0.91, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6368
2023-02-22 13:19:29 - progress_bar.py[line:274] - INFO: epoch 002:     76 / 2175 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101, ups=0.91, wpb=111.1, bsz=40, num_updates=2250, lr=4.65801e-05, gnorm=1.098, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6379
2023-02-22 13:19:40 - progress_bar.py[line:274] - INFO: epoch 002:     86 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.89, wpb=111.6, bsz=40, num_updates=2260, lr=4.65532e-05, gnorm=1.013, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6391
2023-02-22 13:19:51 - progress_bar.py[line:274] - INFO: epoch 002:     96 / 2175 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=103.8, ups=0.92, wpb=113.1, bsz=40, num_updates=2270, lr=4.65263e-05, gnorm=1.071, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6402
2023-02-22 13:20:02 - progress_bar.py[line:274] - INFO: epoch 002:    106 / 2175 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.87, wpb=113.1, bsz=40, num_updates=2280, lr=4.64994e-05, gnorm=0.933, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=6413
2023-02-22 13:20:13 - progress_bar.py[line:274] - INFO: epoch 002:    116 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.4, ups=0.94, wpb=110.4, bsz=40, num_updates=2290, lr=4.64725e-05, gnorm=1.006, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6424
2023-02-22 13:20:24 - progress_bar.py[line:274] - INFO: epoch 002:    126 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.7, ups=0.89, wpb=113.6, bsz=40, num_updates=2300, lr=4.64457e-05, gnorm=0.818, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6435
2023-02-22 13:20:35 - progress_bar.py[line:274] - INFO: epoch 002:    136 / 2175 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=103.3, ups=0.92, wpb=111.7, bsz=40, num_updates=2310, lr=4.64188e-05, gnorm=0.992, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6446
2023-02-22 13:20:46 - progress_bar.py[line:274] - INFO: epoch 002:    146 / 2175 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.88, wpb=111.7, bsz=40, num_updates=2320, lr=4.63919e-05, gnorm=0.791, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6457
2023-02-22 13:20:57 - progress_bar.py[line:274] - INFO: epoch 002:    156 / 2175 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=105.9, ups=0.93, wpb=114, bsz=40, num_updates=2330, lr=4.6365e-05, gnorm=0.869, clip=40, loss_scale=512, train_wall=11, gb_free=9.3, ema_decay=0.9999, wall=6468
2023-02-22 13:21:08 - progress_bar.py[line:274] - INFO: epoch 002:    166 / 2175 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.6, ups=0.9, wpb=112.4, bsz=40, num_updates=2340, lr=4.63381e-05, gnorm=1.029, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6479
2023-02-22 13:21:20 - progress_bar.py[line:274] - INFO: epoch 002:    176 / 2175 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.1, ups=0.89, wpb=113.8, bsz=40, num_updates=2350, lr=4.63112e-05, gnorm=0.882, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6490
2023-02-22 13:21:31 - progress_bar.py[line:274] - INFO: epoch 002:    186 / 2175 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.87, wpb=111.6, bsz=40, num_updates=2360, lr=4.62843e-05, gnorm=1.299, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6502
2023-02-22 13:21:42 - progress_bar.py[line:274] - INFO: epoch 002:    196 / 2175 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=110.8, bsz=40, num_updates=2370, lr=4.62575e-05, gnorm=1.008, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6513
2023-02-22 13:21:53 - progress_bar.py[line:274] - INFO: epoch 002:    206 / 2175 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.1, ups=0.9, wpb=112.5, bsz=40, num_updates=2380, lr=4.62306e-05, gnorm=0.975, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6524
2023-02-22 13:22:05 - progress_bar.py[line:274] - INFO: epoch 002:    216 / 2175 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.5, ups=0.86, wpb=111.7, bsz=40, num_updates=2390, lr=4.62037e-05, gnorm=1.319, clip=80, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6536
2023-02-22 13:22:16 - progress_bar.py[line:274] - INFO: epoch 002:    226 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.5, ups=0.91, wpb=112.2, bsz=40, num_updates=2400, lr=4.61768e-05, gnorm=1.016, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6547
2023-02-22 13:22:27 - progress_bar.py[line:274] - INFO: epoch 002:    236 / 2175 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.9, wpb=111.8, bsz=40, num_updates=2410, lr=4.61499e-05, gnorm=1.045, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6558
2023-02-22 13:22:38 - progress_bar.py[line:274] - INFO: epoch 002:    246 / 2175 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.9, ups=0.91, wpb=112.7, bsz=40, num_updates=2420, lr=4.6123e-05, gnorm=1.139, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6569
2023-02-22 13:22:49 - progress_bar.py[line:274] - INFO: epoch 002:    256 / 2175 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.4, ups=0.9, wpb=112.2, bsz=40, num_updates=2430, lr=4.60961e-05, gnorm=0.887, clip=20, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=6580
2023-02-22 13:23:00 - progress_bar.py[line:274] - INFO: epoch 002:    266 / 2175 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.9, ups=0.9, wpb=113.1, bsz=40, num_updates=2440, lr=4.60693e-05, gnorm=1.058, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6591
2023-02-22 13:23:12 - progress_bar.py[line:274] - INFO: epoch 002:    276 / 2175 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.89, wpb=112.5, bsz=40, num_updates=2450, lr=4.60424e-05, gnorm=1.016, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6602
2023-02-22 13:23:23 - progress_bar.py[line:274] - INFO: epoch 002:    286 / 2175 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.88, wpb=112.4, bsz=40, num_updates=2460, lr=4.60155e-05, gnorm=0.947, clip=40, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6614
2023-02-22 13:23:34 - progress_bar.py[line:274] - INFO: epoch 002:    296 / 2175 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.2, ups=0.91, wpb=111.9, bsz=40, num_updates=2470, lr=4.59886e-05, gnorm=1.022, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6625
2023-02-22 13:23:45 - progress_bar.py[line:274] - INFO: epoch 002:    306 / 2175 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.3, ups=0.89, wpb=112.5, bsz=40, num_updates=2480, lr=4.59617e-05, gnorm=1.156, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6636
2023-02-22 13:23:56 - progress_bar.py[line:274] - INFO: epoch 002:    316 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103, ups=0.91, wpb=113.1, bsz=40, num_updates=2490, lr=4.59348e-05, gnorm=0.994, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6647
2023-02-22 13:24:07 - progress_bar.py[line:274] - INFO: epoch 002:    326 / 2175 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.89, wpb=111.3, bsz=40, num_updates=2500, lr=4.59079e-05, gnorm=1.183, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6658
2023-02-22 13:24:19 - progress_bar.py[line:274] - INFO: epoch 002:    336 / 2175 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.4, ups=0.89, wpb=110.6, bsz=40, num_updates=2510, lr=4.58811e-05, gnorm=1.053, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6669
2023-02-22 13:24:30 - progress_bar.py[line:274] - INFO: epoch 002:    346 / 2175 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.3, ups=0.89, wpb=110.7, bsz=40, num_updates=2520, lr=4.58542e-05, gnorm=1.125, clip=70, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6681
2023-02-22 13:24:41 - progress_bar.py[line:274] - INFO: epoch 002:    356 / 2175 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.9, ups=0.87, wpb=113, bsz=40, num_updates=2530, lr=4.58273e-05, gnorm=0.969, clip=20, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=6692
2023-02-22 13:24:52 - progress_bar.py[line:274] - INFO: epoch 002:    366 / 2175 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.4, ups=0.9, wpb=112.7, bsz=40, num_updates=2540, lr=4.58004e-05, gnorm=1.431, clip=90, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6703
2023-02-22 13:25:04 - progress_bar.py[line:274] - INFO: epoch 002:    376 / 2175 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.2, ups=0.89, wpb=111.6, bsz=40, num_updates=2550, lr=4.57735e-05, gnorm=1.059, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6714
2023-02-22 13:25:15 - progress_bar.py[line:274] - INFO: epoch 002:    386 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.3, ups=0.92, wpb=112.8, bsz=40, num_updates=2560, lr=4.57466e-05, gnorm=1.224, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6725
2023-02-22 13:25:25 - progress_bar.py[line:274] - INFO: epoch 002:    396 / 2175 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=103.2, ups=0.92, wpb=112.1, bsz=40, num_updates=2570, lr=4.57197e-05, gnorm=1.209, clip=90, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6736
2023-02-22 13:25:37 - progress_bar.py[line:274] - INFO: epoch 002:    406 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100, ups=0.88, wpb=113.6, bsz=40, num_updates=2580, lr=4.56929e-05, gnorm=1.089, clip=60, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6748
2023-02-22 13:25:47 - progress_bar.py[line:274] - INFO: epoch 002:    416 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=106.2, ups=0.94, wpb=112.9, bsz=40, num_updates=2590, lr=4.5666e-05, gnorm=0.866, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6758
2023-02-22 13:25:59 - progress_bar.py[line:274] - INFO: epoch 002:    426 / 2175 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.4, ups=0.88, wpb=111.3, bsz=40, num_updates=2600, lr=4.56391e-05, gnorm=0.974, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6770
@@@@ ERROR IN DATA @@@@ ride
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 13:26:10 - progress_bar.py[line:274] - INFO: epoch 002:    436 / 2175 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.6, ups=0.9, wpb=111.8, bsz=40, num_updates=2610, lr=4.56122e-05, gnorm=1.05, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6781
2023-02-22 13:26:21 - progress_bar.py[line:274] - INFO: epoch 002:    446 / 2175 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.8, ups=0.93, wpb=110.9, bsz=40, num_updates=2620, lr=4.55853e-05, gnorm=1.07, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6792
2023-02-22 13:26:32 - progress_bar.py[line:274] - INFO: epoch 002:    456 / 2175 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.89, wpb=112.5, bsz=40, num_updates=2630, lr=4.55584e-05, gnorm=1.044, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6803
2023-02-22 13:26:43 - progress_bar.py[line:274] - INFO: epoch 002:    466 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.1, ups=0.9, wpb=113.6, bsz=40, num_updates=2640, lr=4.55315e-05, gnorm=1.091, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6814
2023-02-22 13:26:55 - progress_bar.py[line:274] - INFO: epoch 002:    476 / 2175 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.88, wpb=112.1, bsz=40, num_updates=2650, lr=4.55047e-05, gnorm=1.034, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6825
2023-02-22 13:27:06 - progress_bar.py[line:274] - INFO: epoch 002:    486 / 2175 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.7, ups=0.86, wpb=111.9, bsz=40, num_updates=2660, lr=4.54778e-05, gnorm=0.995, clip=50, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6837
2023-02-22 13:27:17 - progress_bar.py[line:274] - INFO: epoch 002:    496 / 2175 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99, ups=0.89, wpb=111.5, bsz=40, num_updates=2670, lr=4.54509e-05, gnorm=0.924, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6848
2023-02-22 13:27:20 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 13:27:30 - progress_bar.py[line:274] - INFO: epoch 002:    507 / 2175 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=91.7, ups=0.81, wpb=113.1, bsz=40, num_updates=2680, lr=4.5424e-05, gnorm=1.196, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6861
2023-02-22 13:27:41 - progress_bar.py[line:274] - INFO: epoch 002:    517 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100, ups=0.89, wpb=112, bsz=40, num_updates=2690, lr=4.53971e-05, gnorm=1.164, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6872
2023-02-22 13:27:52 - progress_bar.py[line:274] - INFO: epoch 002:    527 / 2175 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.9, wpb=111.4, bsz=40, num_updates=2700, lr=4.53702e-05, gnorm=1.093, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6883
2023-02-22 13:28:03 - progress_bar.py[line:274] - INFO: epoch 002:    537 / 2175 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.5, ups=0.91, wpb=113.5, bsz=40, num_updates=2710, lr=4.53433e-05, gnorm=1.207, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6894
2023-02-22 13:28:14 - progress_bar.py[line:274] - INFO: epoch 002:    547 / 2175 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.91, wpb=111.8, bsz=40, num_updates=2720, lr=4.53164e-05, gnorm=1.112, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6905
2023-02-22 13:28:25 - progress_bar.py[line:274] - INFO: epoch 002:    557 / 2175 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.9, ups=0.89, wpb=111.5, bsz=40, num_updates=2730, lr=4.52896e-05, gnorm=1.03, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6916
2023-02-22 13:28:37 - progress_bar.py[line:274] - INFO: epoch 002:    567 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.3, ups=0.9, wpb=111.8, bsz=40, num_updates=2740, lr=4.52627e-05, gnorm=1.065, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6927
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 13:28:48 - progress_bar.py[line:274] - INFO: epoch 002:    577 / 2175 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103, ups=0.91, wpb=113, bsz=40, num_updates=2750, lr=4.52358e-05, gnorm=1.222, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6938
2023-02-22 13:28:58 - progress_bar.py[line:274] - INFO: epoch 002:    587 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.7, ups=0.92, wpb=112.4, bsz=40, num_updates=2760, lr=4.52089e-05, gnorm=1.288, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6949
2023-02-22 13:29:10 - progress_bar.py[line:274] - INFO: epoch 002:    597 / 2175 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.86, wpb=113.2, bsz=40, num_updates=2770, lr=4.5182e-05, gnorm=0.864, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6961
2023-02-22 13:29:21 - progress_bar.py[line:274] - INFO: epoch 002:    607 / 2175 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.7, ups=0.87, wpb=112.2, bsz=40, num_updates=2780, lr=4.51551e-05, gnorm=1.088, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6972
2023-02-22 13:29:33 - progress_bar.py[line:274] - INFO: epoch 002:    617 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.89, wpb=113.6, bsz=40, num_updates=2790, lr=4.51282e-05, gnorm=1.057, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6983
2023-02-22 13:29:44 - progress_bar.py[line:274] - INFO: epoch 002:    627 / 2175 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.9, ups=0.91, wpb=113.1, bsz=40, num_updates=2800, lr=4.51014e-05, gnorm=1.079, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6994
2023-02-22 13:29:54 - progress_bar.py[line:274] - INFO: epoch 002:    637 / 2175 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=105.1, ups=0.94, wpb=112.3, bsz=40, num_updates=2810, lr=4.50745e-05, gnorm=1.092, clip=60, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=7005
2023-02-22 13:30:05 - progress_bar.py[line:274] - INFO: epoch 002:    647 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.4, ups=0.91, wpb=113.3, bsz=40, num_updates=2820, lr=4.50476e-05, gnorm=1.16, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7016
2023-02-22 13:30:16 - progress_bar.py[line:274] - INFO: epoch 002:    657 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.2, ups=0.91, wpb=112.3, bsz=40, num_updates=2830, lr=4.50207e-05, gnorm=1.147, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7027
2023-02-22 13:30:28 - progress_bar.py[line:274] - INFO: epoch 002:    667 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.9, wpb=113.5, bsz=40, num_updates=2840, lr=4.49938e-05, gnorm=1.006, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7038
2023-02-22 13:30:39 - progress_bar.py[line:274] - INFO: epoch 002:    677 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.5, ups=0.89, wpb=113.2, bsz=40, num_updates=2850, lr=4.49669e-05, gnorm=0.93, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7050
2023-02-22 13:30:50 - progress_bar.py[line:274] - INFO: epoch 002:    687 / 2175 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.4, ups=0.9, wpb=111.6, bsz=40, num_updates=2860, lr=4.494e-05, gnorm=1.117, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7061
2023-02-22 13:31:01 - progress_bar.py[line:274] - INFO: epoch 002:    697 / 2175 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.88, wpb=111.5, bsz=40, num_updates=2870, lr=4.49132e-05, gnorm=1.156, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7072
2023-02-22 13:31:13 - progress_bar.py[line:274] - INFO: epoch 002:    707 / 2175 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.87, wpb=113, bsz=40, num_updates=2880, lr=4.48863e-05, gnorm=1.02, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7083
2023-02-22 13:31:24 - progress_bar.py[line:274] - INFO: epoch 002:    717 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.1, ups=0.89, wpb=110.9, bsz=40, num_updates=2890, lr=4.48594e-05, gnorm=1.01, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7095
2023-02-22 13:31:35 - progress_bar.py[line:274] - INFO: epoch 002:    727 / 2175 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.7, ups=0.88, wpb=112.5, bsz=40, num_updates=2900, lr=4.48325e-05, gnorm=0.845, clip=30, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=7106
2023-02-22 13:31:46 - progress_bar.py[line:274] - INFO: epoch 002:    737 / 2175 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.4, ups=0.9, wpb=112.1, bsz=40, num_updates=2910, lr=4.48056e-05, gnorm=1.153, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7117
2023-02-22 13:31:58 - progress_bar.py[line:274] - INFO: epoch 002:    747 / 2175 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.8, ups=0.86, wpb=111.9, bsz=40, num_updates=2920, lr=4.47787e-05, gnorm=1.115, clip=50, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=7129
2023-02-22 13:32:09 - progress_bar.py[line:274] - INFO: epoch 002:    757 / 2175 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.87, wpb=111.8, bsz=40, num_updates=2930, lr=4.47518e-05, gnorm=1.148, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7140
2023-02-22 13:32:21 - progress_bar.py[line:274] - INFO: epoch 002:    767 / 2175 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.4, ups=0.89, wpb=113.3, bsz=40, num_updates=2940, lr=4.4725e-05, gnorm=0.805, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7152
2023-02-22 13:32:32 - progress_bar.py[line:274] - INFO: epoch 002:    777 / 2175 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.1, ups=0.88, wpb=112, bsz=40, num_updates=2950, lr=4.46981e-05, gnorm=1.074, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7163
2023-02-22 13:32:43 - progress_bar.py[line:274] - INFO: epoch 002:    787 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.1, ups=0.89, wpb=110.3, bsz=40, num_updates=2960, lr=4.46712e-05, gnorm=0.987, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7174
2023-02-22 13:32:55 - progress_bar.py[line:274] - INFO: epoch 002:    797 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.88, wpb=111.9, bsz=40, num_updates=2970, lr=4.46443e-05, gnorm=1.179, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7186
2023-02-22 13:33:06 - progress_bar.py[line:274] - INFO: epoch 002:    807 / 2175 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.1, ups=0.88, wpb=112, bsz=40, num_updates=2980, lr=4.46174e-05, gnorm=1.392, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7197
@@@@ ERROR IN DATA @@@@ play
2023-02-22 13:33:18 - progress_bar.py[line:274] - INFO: epoch 002:    817 / 2175 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=2990, lr=4.45905e-05, gnorm=1.206, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7208
2023-02-22 13:33:29 - progress_bar.py[line:274] - INFO: epoch 002:    827 / 2175 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.4, ups=0.9, wpb=112.2, bsz=40, num_updates=3000, lr=4.45636e-05, gnorm=0.973, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7219
2023-02-22 13:33:40 - progress_bar.py[line:274] - INFO: epoch 002:    837 / 2175 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.9, ups=0.9, wpb=111.9, bsz=40, num_updates=3010, lr=4.45368e-05, gnorm=1.001, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7230
2023-02-22 13:33:51 - progress_bar.py[line:274] - INFO: epoch 002:    847 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.8, ups=0.87, wpb=113, bsz=40, num_updates=3020, lr=4.45099e-05, gnorm=0.96, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=7242
2023-02-22 13:34:02 - progress_bar.py[line:274] - INFO: epoch 002:    857 / 2175 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.4, ups=0.9, wpb=113.4, bsz=40, num_updates=3030, lr=4.4483e-05, gnorm=1.419, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7253
2023-02-22 13:34:14 - progress_bar.py[line:274] - INFO: epoch 002:    867 / 2175 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100, ups=0.89, wpb=112.4, bsz=40, num_updates=3040, lr=4.44561e-05, gnorm=0.975, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7264
2023-02-22 13:34:25 - progress_bar.py[line:274] - INFO: epoch 002:    877 / 2175 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.89, wpb=112, bsz=40, num_updates=3050, lr=4.44292e-05, gnorm=0.821, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7276
2023-02-22 13:34:36 - progress_bar.py[line:274] - INFO: epoch 002:    887 / 2175 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.9, ups=0.93, wpb=112.3, bsz=40, num_updates=3060, lr=4.44023e-05, gnorm=0.864, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7286
2023-02-22 13:34:46 - progress_bar.py[line:274] - INFO: epoch 002:    897 / 2175 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.8, ups=0.92, wpb=111.4, bsz=40, num_updates=3070, lr=4.43754e-05, gnorm=0.94, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7297
@@@@ ERROR IN DATA @@@@ watch
2023-02-22 13:34:57 - progress_bar.py[line:274] - INFO: epoch 002:    907 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.91, wpb=112.5, bsz=40, num_updates=3080, lr=4.43486e-05, gnorm=0.984, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7308
2023-02-22 13:35:09 - progress_bar.py[line:274] - INFO: epoch 002:    917 / 2175 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97, ups=0.88, wpb=110.7, bsz=40, num_updates=3090, lr=4.43217e-05, gnorm=1.039, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7320
2023-02-22 13:35:20 - progress_bar.py[line:274] - INFO: epoch 002:    927 / 2175 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.3, ups=0.89, wpb=111, bsz=40, num_updates=3100, lr=4.42948e-05, gnorm=1.079, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7331
2023-02-22 13:35:31 - progress_bar.py[line:274] - INFO: epoch 002:    937 / 2175 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.89, wpb=112.4, bsz=40, num_updates=3110, lr=4.42679e-05, gnorm=1.016, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7342
2023-02-22 13:35:43 - progress_bar.py[line:274] - INFO: epoch 002:    947 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.89, wpb=111.9, bsz=40, num_updates=3120, lr=4.4241e-05, gnorm=0.87, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7353
2023-02-22 13:35:54 - progress_bar.py[line:274] - INFO: epoch 002:    957 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.88, wpb=112.1, bsz=40, num_updates=3130, lr=4.42141e-05, gnorm=1.168, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7365
2023-02-22 13:36:05 - progress_bar.py[line:274] - INFO: epoch 002:    967 / 2175 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.9, ups=0.88, wpb=112.6, bsz=40, num_updates=3140, lr=4.41872e-05, gnorm=1.032, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7376
2023-02-22 13:36:16 - progress_bar.py[line:274] - INFO: epoch 002:    977 / 2175 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.91, wpb=112.2, bsz=40, num_updates=3150, lr=4.41603e-05, gnorm=1.168, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7387
2023-02-22 13:36:28 - progress_bar.py[line:274] - INFO: epoch 002:    987 / 2175 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.4, ups=0.87, wpb=113.8, bsz=40, num_updates=3160, lr=4.41335e-05, gnorm=1.267, clip=80, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=7399
2023-02-22 13:36:39 - progress_bar.py[line:274] - INFO: epoch 002:    997 / 2175 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.9, ups=0.9, wpb=111.6, bsz=40, num_updates=3170, lr=4.41066e-05, gnorm=1.103, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7410
2023-02-22 13:36:51 - progress_bar.py[line:274] - INFO: epoch 002:   1007 / 2175 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.4, ups=0.87, wpb=111.2, bsz=40, num_updates=3180, lr=4.40797e-05, gnorm=1.128, clip=60, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=7421
2023-02-22 13:37:02 - progress_bar.py[line:274] - INFO: epoch 002:   1017 / 2175 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.91, wpb=110.4, bsz=40, num_updates=3190, lr=4.40528e-05, gnorm=0.996, clip=40, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7433
2023-02-22 13:37:13 - progress_bar.py[line:274] - INFO: epoch 002:   1027 / 2175 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.4, ups=0.9, wpb=112.6, bsz=40, num_updates=3200, lr=4.40259e-05, gnorm=1.032, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7444
@@@@ ERROR IN DATA @@@@ play
2023-02-22 13:37:24 - progress_bar.py[line:274] - INFO: epoch 002:   1037 / 2175 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.1, ups=0.88, wpb=113.5, bsz=40, num_updates=3210, lr=4.3999e-05, gnorm=1.23, clip=90, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7455
2023-02-22 13:37:35 - progress_bar.py[line:274] - INFO: epoch 002:   1047 / 2175 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.4, ups=0.9, wpb=111.9, bsz=40, num_updates=3220, lr=4.39721e-05, gnorm=1.14, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7466
2023-02-22 13:37:46 - progress_bar.py[line:274] - INFO: epoch 002:   1057 / 2175 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.9, wpb=111.3, bsz=40, num_updates=3230, lr=4.39453e-05, gnorm=1.058, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7477
2023-02-22 13:37:58 - progress_bar.py[line:274] - INFO: epoch 002:   1067 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.88, wpb=112.6, bsz=40, num_updates=3240, lr=4.39184e-05, gnorm=0.921, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7489
2023-02-22 13:38:09 - progress_bar.py[line:274] - INFO: epoch 002:   1077 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.3, ups=0.92, wpb=113.2, bsz=40, num_updates=3250, lr=4.38915e-05, gnorm=0.926, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7500
2023-02-22 13:38:20 - progress_bar.py[line:274] - INFO: epoch 002:   1087 / 2175 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.2, ups=0.92, wpb=112.9, bsz=40, num_updates=3260, lr=4.38646e-05, gnorm=0.983, clip=70, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7510
2023-02-22 13:38:31 - progress_bar.py[line:274] - INFO: epoch 002:   1097 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.3, ups=0.9, wpb=112.6, bsz=40, num_updates=3270, lr=4.38377e-05, gnorm=1.178, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7521
2023-02-22 13:38:38 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 13:38:43 - progress_bar.py[line:274] - INFO: epoch 002:   1108 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=88.2, ups=0.79, wpb=111.1, bsz=40, num_updates=3280, lr=4.38108e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=7534
2023-02-22 13:38:55 - progress_bar.py[line:274] - INFO: epoch 002:   1118 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.89, wpb=112.1, bsz=40, num_updates=3290, lr=4.37839e-05, gnorm=1.05, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7545
2023-02-22 13:39:06 - progress_bar.py[line:274] - INFO: epoch 002:   1128 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101, ups=0.9, wpb=112.2, bsz=40, num_updates=3300, lr=4.37571e-05, gnorm=1.353, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7556
2023-02-22 13:39:17 - progress_bar.py[line:274] - INFO: epoch 002:   1138 / 2175 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.2, ups=0.87, wpb=113, bsz=40, num_updates=3310, lr=4.37302e-05, gnorm=1.39, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7568
2023-02-22 13:39:29 - progress_bar.py[line:274] - INFO: epoch 002:   1148 / 2175 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.8, ups=0.88, wpb=112.7, bsz=40, num_updates=3320, lr=4.37033e-05, gnorm=0.935, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7579
2023-02-22 13:39:40 - progress_bar.py[line:274] - INFO: epoch 002:   1158 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.89, wpb=113.6, bsz=40, num_updates=3330, lr=4.36764e-05, gnorm=0.915, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7591
2023-02-22 13:39:51 - progress_bar.py[line:274] - INFO: epoch 002:   1168 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.9, wpb=112, bsz=40, num_updates=3340, lr=4.36495e-05, gnorm=0.941, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7602
2023-02-22 13:40:02 - progress_bar.py[line:274] - INFO: epoch 002:   1178 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.4, ups=0.89, wpb=113.2, bsz=40, num_updates=3350, lr=4.36226e-05, gnorm=1.518, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7613
2023-02-22 13:40:13 - progress_bar.py[line:274] - INFO: epoch 002:   1188 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.9, ups=0.9, wpb=112, bsz=40, num_updates=3360, lr=4.35957e-05, gnorm=1.022, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7624
2023-02-22 13:40:25 - progress_bar.py[line:274] - INFO: epoch 002:   1198 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.89, wpb=112.9, bsz=40, num_updates=3370, lr=4.35689e-05, gnorm=1.063, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7636
2023-02-22 13:40:36 - progress_bar.py[line:274] - INFO: epoch 002:   1208 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.4, ups=0.9, wpb=111.8, bsz=40, num_updates=3380, lr=4.3542e-05, gnorm=0.668, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7647
2023-02-22 13:40:47 - progress_bar.py[line:274] - INFO: epoch 002:   1218 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.9, wpb=113.2, bsz=40, num_updates=3390, lr=4.35151e-05, gnorm=0.955, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7658
2023-02-22 13:40:58 - progress_bar.py[line:274] - INFO: epoch 002:   1228 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.92, wpb=111.2, bsz=40, num_updates=3400, lr=4.34882e-05, gnorm=0.914, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7669
2023-02-22 13:41:09 - progress_bar.py[line:274] - INFO: epoch 002:   1238 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.8, ups=0.9, wpb=112.5, bsz=40, num_updates=3410, lr=4.34613e-05, gnorm=1.088, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7680
2023-02-22 13:41:20 - progress_bar.py[line:274] - INFO: epoch 002:   1248 / 2175 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.89, wpb=111.7, bsz=40, num_updates=3420, lr=4.34344e-05, gnorm=1.073, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7691
2023-02-22 13:41:31 - progress_bar.py[line:274] - INFO: epoch 002:   1258 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.89, wpb=112.8, bsz=40, num_updates=3430, lr=4.34075e-05, gnorm=1.032, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7702
2023-02-22 13:41:43 - progress_bar.py[line:274] - INFO: epoch 002:   1268 / 2175 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.2, ups=0.9, wpb=112.8, bsz=40, num_updates=3440, lr=4.33807e-05, gnorm=1.017, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7713
2023-02-22 13:41:54 - progress_bar.py[line:274] - INFO: epoch 002:   1278 / 2175 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.6, ups=0.87, wpb=111.6, bsz=40, num_updates=3450, lr=4.33538e-05, gnorm=1.064, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7725
2023-02-22 13:42:05 - progress_bar.py[line:274] - INFO: epoch 002:   1288 / 2175 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.89, wpb=112.4, bsz=40, num_updates=3460, lr=4.33269e-05, gnorm=1.163, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7736
2023-02-22 13:42:16 - progress_bar.py[line:274] - INFO: epoch 002:   1298 / 2175 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.6, ups=0.92, wpb=112.4, bsz=40, num_updates=3470, lr=4.33e-05, gnorm=0.965, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7747
2023-02-22 13:42:28 - progress_bar.py[line:274] - INFO: epoch 002:   1308 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=112, bsz=40, num_updates=3480, lr=4.32731e-05, gnorm=0.963, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7758
2023-02-22 13:42:39 - progress_bar.py[line:274] - INFO: epoch 002:   1318 / 2175 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.2, ups=0.88, wpb=112, bsz=40, num_updates=3490, lr=4.32462e-05, gnorm=0.96, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7770
2023-02-22 13:42:50 - progress_bar.py[line:274] - INFO: epoch 002:   1328 / 2175 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.2, ups=0.9, wpb=112.8, bsz=40, num_updates=3500, lr=4.32193e-05, gnorm=0.9, clip=40, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=7781
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 13:43:02 - progress_bar.py[line:274] - INFO: epoch 002:   1338 / 2175 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.6, ups=0.88, wpb=112.3, bsz=40, num_updates=3510, lr=4.31925e-05, gnorm=1.171, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7792
2023-02-22 13:43:13 - progress_bar.py[line:274] - INFO: epoch 002:   1348 / 2175 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.4, ups=0.86, wpb=111.7, bsz=40, num_updates=3520, lr=4.31656e-05, gnorm=1.097, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=7804
2023-02-22 13:43:24 - progress_bar.py[line:274] - INFO: epoch 002:   1358 / 2175 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.1, ups=0.91, wpb=113.2, bsz=40, num_updates=3530, lr=4.31387e-05, gnorm=1.017, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7815
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 13:43:35 - progress_bar.py[line:274] - INFO: epoch 002:   1368 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=3540, lr=4.31118e-05, gnorm=0.935, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7826
2023-02-22 13:43:46 - progress_bar.py[line:274] - INFO: epoch 002:   1378 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.9, wpb=110.4, bsz=40, num_updates=3550, lr=4.30849e-05, gnorm=1.285, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7837
2023-02-22 13:43:57 - progress_bar.py[line:274] - INFO: epoch 002:   1388 / 2175 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.7, ups=0.9, wpb=112.2, bsz=40, num_updates=3560, lr=4.3058e-05, gnorm=1.369, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7848
2023-02-22 13:44:08 - progress_bar.py[line:274] - INFO: epoch 002:   1398 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=104, ups=0.93, wpb=112.2, bsz=40, num_updates=3570, lr=4.30311e-05, gnorm=1.307, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7859
2023-02-22 13:44:20 - progress_bar.py[line:274] - INFO: epoch 002:   1408 / 2175 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.89, wpb=112, bsz=40, num_updates=3580, lr=4.30042e-05, gnorm=0.915, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7870
2023-02-22 13:44:31 - progress_bar.py[line:274] - INFO: epoch 002:   1418 / 2175 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.89, wpb=111.4, bsz=40, num_updates=3590, lr=4.29774e-05, gnorm=0.951, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7882
2023-02-22 13:44:42 - progress_bar.py[line:274] - INFO: epoch 002:   1428 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.9, ups=0.89, wpb=112.7, bsz=40, num_updates=3600, lr=4.29505e-05, gnorm=0.913, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7893
2023-02-22 13:44:53 - progress_bar.py[line:274] - INFO: epoch 002:   1438 / 2175 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.88, wpb=112.6, bsz=40, num_updates=3610, lr=4.29236e-05, gnorm=0.887, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7904
2023-02-22 13:45:05 - progress_bar.py[line:274] - INFO: epoch 002:   1448 / 2175 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.87, wpb=112.2, bsz=40, num_updates=3620, lr=4.28967e-05, gnorm=0.842, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7916
2023-02-22 13:45:16 - progress_bar.py[line:274] - INFO: epoch 002:   1458 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.1, ups=0.9, wpb=112.9, bsz=40, num_updates=3630, lr=4.28698e-05, gnorm=1.17, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7927
2023-02-22 13:45:27 - progress_bar.py[line:274] - INFO: epoch 002:   1468 / 2175 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.1, ups=0.89, wpb=111.2, bsz=40, num_updates=3640, lr=4.28429e-05, gnorm=1.044, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7938
2023-02-22 13:45:38 - progress_bar.py[line:274] - INFO: epoch 002:   1478 / 2175 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.3, ups=0.89, wpb=113.9, bsz=40, num_updates=3650, lr=4.2816e-05, gnorm=0.968, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7949
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 13:45:50 - progress_bar.py[line:274] - INFO: epoch 002:   1488 / 2175 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.1, ups=0.9, wpb=112, bsz=40, num_updates=3660, lr=4.27892e-05, gnorm=1.274, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7960
2023-02-22 13:46:01 - progress_bar.py[line:274] - INFO: epoch 002:   1498 / 2175 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.6, ups=0.9, wpb=111.9, bsz=40, num_updates=3670, lr=4.27623e-05, gnorm=0.997, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7971
2023-02-22 13:46:12 - progress_bar.py[line:274] - INFO: epoch 002:   1508 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.7, ups=0.88, wpb=111.6, bsz=40, num_updates=3680, lr=4.27354e-05, gnorm=1.122, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7983
2023-02-22 13:46:24 - progress_bar.py[line:274] - INFO: epoch 002:   1518 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.87, wpb=113.4, bsz=40, num_updates=3690, lr=4.27085e-05, gnorm=0.833, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7994
2023-02-22 13:46:35 - progress_bar.py[line:274] - INFO: epoch 002:   1528 / 2175 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.3, ups=0.91, wpb=111.9, bsz=40, num_updates=3700, lr=4.26816e-05, gnorm=0.94, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8005
2023-02-22 13:46:46 - progress_bar.py[line:274] - INFO: epoch 002:   1538 / 2175 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.88, wpb=112.3, bsz=40, num_updates=3710, lr=4.26547e-05, gnorm=0.925, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=8017
2023-02-22 13:46:57 - progress_bar.py[line:274] - INFO: epoch 002:   1548 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104, ups=0.92, wpb=113.2, bsz=40, num_updates=3720, lr=4.26278e-05, gnorm=1.116, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8028
2023-02-22 13:47:08 - progress_bar.py[line:274] - INFO: epoch 002:   1558 / 2175 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.87, wpb=112.7, bsz=40, num_updates=3730, lr=4.2601e-05, gnorm=1.064, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8039
2023-02-22 13:47:19 - progress_bar.py[line:274] - INFO: epoch 002:   1568 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.8, ups=0.92, wpb=112.5, bsz=40, num_updates=3740, lr=4.25741e-05, gnorm=0.881, clip=40, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=8050
2023-02-22 13:47:31 - progress_bar.py[line:274] - INFO: epoch 002:   1578 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.88, wpb=112.3, bsz=40, num_updates=3750, lr=4.25472e-05, gnorm=0.748, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8061
2023-02-22 13:47:42 - progress_bar.py[line:274] - INFO: epoch 002:   1588 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.88, wpb=112.8, bsz=40, num_updates=3760, lr=4.25203e-05, gnorm=1.332, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8073
2023-02-22 13:47:53 - progress_bar.py[line:274] - INFO: epoch 002:   1598 / 2175 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101, ups=0.9, wpb=111.9, bsz=40, num_updates=3770, lr=4.24934e-05, gnorm=1.057, clip=60, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=8084
2023-02-22 13:48:04 - progress_bar.py[line:274] - INFO: epoch 002:   1608 / 2175 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.5, ups=0.89, wpb=112.1, bsz=40, num_updates=3780, lr=4.24665e-05, gnorm=1.285, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8095
2023-02-22 13:48:16 - progress_bar.py[line:274] - INFO: epoch 002:   1618 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.89, wpb=112.2, bsz=40, num_updates=3790, lr=4.24396e-05, gnorm=1.124, clip=50, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=8106
2023-02-22 13:48:27 - progress_bar.py[line:274] - INFO: epoch 002:   1628 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.8, ups=0.91, wpb=112.9, bsz=40, num_updates=3800, lr=4.24128e-05, gnorm=1.126, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8117
2023-02-22 13:48:38 - progress_bar.py[line:274] - INFO: epoch 002:   1638 / 2175 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.7, ups=0.91, wpb=112, bsz=40, num_updates=3810, lr=4.23859e-05, gnorm=1.163, clip=60, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=8128
2023-02-22 13:48:49 - progress_bar.py[line:274] - INFO: epoch 002:   1648 / 2175 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.89, wpb=113, bsz=40, num_updates=3820, lr=4.2359e-05, gnorm=1.008, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8140
2023-02-22 13:49:00 - progress_bar.py[line:274] - INFO: epoch 002:   1658 / 2175 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104, ups=0.94, wpb=111, bsz=40, num_updates=3830, lr=4.23321e-05, gnorm=1.003, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8150
2023-02-22 13:49:11 - progress_bar.py[line:274] - INFO: epoch 002:   1668 / 2175 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.5, ups=0.89, wpb=113.6, bsz=40, num_updates=3840, lr=4.23052e-05, gnorm=0.977, clip=40, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=8162
2023-02-22 13:49:22 - progress_bar.py[line:274] - INFO: epoch 002:   1678 / 2175 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101, ups=0.89, wpb=113.8, bsz=40, num_updates=3850, lr=4.22783e-05, gnorm=0.952, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8173
2023-02-22 13:49:33 - progress_bar.py[line:274] - INFO: epoch 002:   1688 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.7, ups=0.9, wpb=112.3, bsz=40, num_updates=3860, lr=4.22514e-05, gnorm=0.967, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8184
2023-02-22 13:49:44 - progress_bar.py[line:274] - INFO: epoch 002:   1698 / 2175 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.3, ups=0.9, wpb=113.5, bsz=40, num_updates=3870, lr=4.22246e-05, gnorm=1.055, clip=60, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=8195
2023-02-22 13:49:55 - progress_bar.py[line:274] - INFO: epoch 002:   1708 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102, ups=0.91, wpb=112, bsz=40, num_updates=3880, lr=4.21977e-05, gnorm=1.025, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8206
2023-02-22 13:50:06 - progress_bar.py[line:274] - INFO: epoch 002:   1718 / 2175 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.5, ups=0.91, wpb=111.4, bsz=40, num_updates=3890, lr=4.21708e-05, gnorm=1.317, clip=80, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8217
2023-02-22 13:50:17 - progress_bar.py[line:274] - INFO: epoch 002:   1728 / 2175 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.1, ups=0.9, wpb=111.6, bsz=40, num_updates=3900, lr=4.21439e-05, gnorm=1.102, clip=50, loss_scale=1024, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=8228
2023-02-22 13:50:29 - progress_bar.py[line:274] - INFO: epoch 002:   1738 / 2175 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.89, wpb=113.3, bsz=40, num_updates=3910, lr=4.2117e-05, gnorm=0.91, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8239
2023-02-22 13:50:40 - progress_bar.py[line:274] - INFO: epoch 002:   1748 / 2175 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=111.1, bsz=40, num_updates=3920, lr=4.20901e-05, gnorm=1.019, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8251
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 13:50:51 - progress_bar.py[line:274] - INFO: epoch 002:   1758 / 2175 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.8, ups=0.88, wpb=112.2, bsz=40, num_updates=3930, lr=4.20632e-05, gnorm=0.962, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=8262
2023-02-22 13:51:03 - progress_bar.py[line:274] - INFO: epoch 002:   1768 / 2175 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.9, ups=0.88, wpb=113.7, bsz=40, num_updates=3940, lr=4.20363e-05, gnorm=1.241, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8273
2023-02-22 13:51:14 - progress_bar.py[line:274] - INFO: epoch 002:   1778 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.88, wpb=112.7, bsz=40, num_updates=3950, lr=4.20095e-05, gnorm=1.002, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8285
2023-02-22 13:51:25 - progress_bar.py[line:274] - INFO: epoch 002:   1788 / 2175 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.6, ups=0.93, wpb=111.9, bsz=40, num_updates=3960, lr=4.19826e-05, gnorm=0.792, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8296
2023-02-22 13:51:36 - progress_bar.py[line:274] - INFO: epoch 002:   1798 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.86, wpb=113.1, bsz=40, num_updates=3970, lr=4.19557e-05, gnorm=0.865, clip=20, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=8307
2023-02-22 13:51:48 - progress_bar.py[line:274] - INFO: epoch 002:   1808 / 2175 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.89, wpb=111.3, bsz=40, num_updates=3980, lr=4.19288e-05, gnorm=1.038, clip=70, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8319
2023-02-22 13:51:59 - progress_bar.py[line:274] - INFO: epoch 002:   1818 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99, ups=0.89, wpb=111.4, bsz=40, num_updates=3990, lr=4.19019e-05, gnorm=1.039, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8330
2023-02-22 13:52:10 - progress_bar.py[line:274] - INFO: epoch 002:   1828 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.3, ups=0.9, wpb=112.5, bsz=40, num_updates=4000, lr=4.1875e-05, gnorm=1.054, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8341
2023-02-22 13:52:10 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 13:52:11 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 13:52:11 - train.py[line:551] - INFO: load:0.89 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 13:54:14 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 13:54:14 - train.py[line:551] - INFO: load:0.92 valid_run:122.76 task_valid:118.80 collect_output:2.89
2023-02-22 13:56:14 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 13:56:14 - train.py[line:551] - INFO: load:0.94 valid_run:242.87 task_valid:234.05 collect_output:6.71
2023-02-22 13:58:17 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 13:58:17 - train.py[line:551] - INFO: load:0.97 valid_run:365.52 task_valid:349.93 collect_output:12.42
2023-02-22 14:00:19 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 14:00:19 - train.py[line:551] - INFO: load:0.99 valid_run:487.52 task_valid:463.00 collect_output:20.34
2023-02-22 14:02:20 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 14:02:20 - train.py[line:551] - INFO: load:1.02 valid_run:608.16 task_valid:579.78 collect_output:23.15
2023-02-22 14:04:23 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 14:04:23 - train.py[line:551] - INFO: load:1.05 valid_run:731.27 task_valid:698.15 collect_output:26.82
2023-02-22 14:06:27 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 14:06:27 - train.py[line:551] - INFO: load:1.07 valid_run:854.78 task_valid:815.88 collect_output:31.56
2023-02-22 14:08:29 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 14:08:29 - train.py[line:551] - INFO: load:1.10 valid_run:977.05 task_valid:932.09 collect_output:36.58
2023-02-22 14:10:33 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 14:10:33 - train.py[line:551] - INFO: load:1.12 valid_run:1101.45 task_valid:1049.26 collect_output:42.75
2023-02-22 14:12:36 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 14:12:36 - train.py[line:551] - INFO: load:1.15 valid_run:1223.62 task_valid:1161.42 collect_output:51.72
2023-02-22 14:14:36 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 14:14:36 - train.py[line:551] - INFO: load:1.18 valid_run:1344.02 task_valid:1276.76 collect_output:55.71
2023-02-22 14:16:38 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 14:16:38 - train.py[line:551] - INFO: load:1.20 valid_run:1465.86 task_valid:1393.48 collect_output:59.79
2023-02-22 14:18:37 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 14:18:37 - train.py[line:551] - INFO: load:1.23 valid_run:1585.02 task_valid:1506.91 collect_output:64.46
2023-02-22 14:20:38 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 14:20:38 - train.py[line:551] - INFO: load:1.26 valid_run:1706.04 task_valid:1624.34 collect_output:67.02
2023-02-22 14:22:39 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 14:22:39 - train.py[line:551] - INFO: load:1.28 valid_run:1827.15 task_valid:1739.95 collect_output:71.45
2023-02-22 14:24:41 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 14:24:41 - train.py[line:551] - INFO: load:1.31 valid_run:1948.61 task_valid:1853.39 collect_output:78.42
2023-02-22 14:26:42 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 14:26:42 - train.py[line:551] - INFO: load:1.34 valid_run:2070.09 task_valid:1968.94 collect_output:83.29
2023-02-22 14:28:43 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 14:28:43 - train.py[line:551] - INFO: load:1.37 valid_run:2190.96 task_valid:2086.54 collect_output:85.40
2023-02-22 14:30:45 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 14:30:45 - train.py[line:551] - INFO: load:1.39 valid_run:2312.41 task_valid:2203.26 collect_output:89.07
2023-02-22 14:32:45 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 14:32:45 - train.py[line:551] - INFO: load:1.42 valid_run:2432.91 task_valid:2319.50 collect_output:92.30
2023-02-22 14:34:48 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 14:34:48 - train.py[line:551] - INFO: load:1.45 valid_run:2555.05 task_valid:2435.70 collect_output:97.19
2023-02-22 14:36:50 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 14:36:50 - train.py[line:551] - INFO: load:1.47 valid_run:2676.97 task_valid:2554.26 collect_output:99.51
2023-02-22 14:38:50 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 14:38:50 - train.py[line:551] - INFO: load:1.50 valid_run:2797.42 task_valid:2668.04 collect_output:105.16
2023-02-22 14:40:50 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 14:40:50 - train.py[line:551] - INFO: load:1.53 valid_run:2917.28 task_valid:2783.67 collect_output:108.36
2023-02-22 14:42:52 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 14:42:52 - train.py[line:551] - INFO: load:1.55 valid_run:3039.11 task_valid:2899.28 collect_output:113.55
2023-02-22 14:44:55 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 14:44:55 - train.py[line:551] - INFO: load:1.58 valid_run:3162.33 task_valid:3014.89 collect_output:120.09
2023-02-22 14:46:55 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 14:46:55 - train.py[line:551] - INFO: load:1.61 valid_run:3282.24 task_valid:3128.66 collect_output:125.15
2023-02-22 14:48:57 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 14:48:57 - train.py[line:551] - INFO: load:1.64 valid_run:3404.15 task_valid:3247.70 collect_output:126.98
2023-02-22 14:50:59 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 14:50:59 - train.py[line:551] - INFO: load:1.66 valid_run:3526.20 task_valid:3362.78 collect_output:132.88
2023-02-22 14:53:01 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 14:53:01 - train.py[line:551] - INFO: load:1.69 valid_run:3648.16 task_valid:3480.94 collect_output:135.63
2023-02-22 14:55:03 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 14:55:03 - train.py[line:551] - INFO: load:1.72 valid_run:3769.39 task_valid:3599.29 collect_output:137.43

====================================================================================================
SGG eval:     R @ 50: 0.6231;     R @ 100: 0.6644;     R @ 500: 0.7104;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4019;    mR @ 100: 0.4471;    mR @ 500: 0.5021;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6707) (covered in:0.3125) (covering:0.2857) (eating:0.7353) (flying in:0.7727) (growing on:0.3750) (hanging from:0.4677) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9265) (says:0.0000) (sitting on:0.6672) (standing on:0.6043) (using:0.4000) (walking in:0.0000) (walking on:0.4324) (watching:0.5000) 
--------------------------------------------------------
====================================================================================================

2023-02-22 14:55:33 - train.py[line:487] - INFO: 0.664429564553094
2023-02-22 14:55:33 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.6231;     R @ 100: 0.6644;     R @ 500: 0.7104;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4019;    mR @ 100: 0.4471;    mR @ 500: 0.5021;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6707) (covered in:0.3125) (covering:0.2857) (eating:0.7353) (flying in:0.7727) (growing on:0.3750) (hanging from:0.4677) (lying on:0.5000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9265) (says:0.0000) (sitting on:0.6672) (standing on:0.6043) (using:0.4000) (walking in:0.0000) (walking on:0.4324) (watching:0.5000) 
--------------------------------------------------------
====================================================================================================

2023-02-22 14:55:33 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.247 | loss_v1 0 | loss_v2 0 | nll_loss 0.077 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.66443 | ppl 1.06 | vqa_score 0.4876 | wps 118 | wpb 72 | bsz 24 | num_updates 4000 | best_R@100 0.66443
2023-02-22 14:55:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 4000 updates
2023-02-22 14:55:33 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_2_4000.pt
2023-02-22 14:55:39 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_2_4000.pt
2023-02-22 14:55:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_2_4000.pt (epoch 2 @ 4000 updates, score 0.664429564553094) (writing took 10.524497903883457 seconds)
2023-02-22 14:56:12 - progress_bar.py[line:274] - INFO: epoch 002:   1838 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=0.3, ups=0, wpb=112.8, bsz=40, num_updates=4010, lr=4.18481e-05, gnorm=0.964, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12166
2023-02-22 14:56:23 - progress_bar.py[line:274] - INFO: epoch 002:   1848 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.91, wpb=112.1, bsz=40, num_updates=4020, lr=4.18213e-05, gnorm=0.994, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12194
2023-02-22 14:56:35 - progress_bar.py[line:274] - INFO: epoch 002:   1858 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.7, ups=0.9, wpb=113.1, bsz=40, num_updates=4030, lr=4.17944e-05, gnorm=1.264, clip=80, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12205
2023-02-22 14:56:46 - progress_bar.py[line:274] - INFO: epoch 002:   1868 / 2175 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.3, ups=0.89, wpb=112.1, bsz=40, num_updates=4040, lr=4.17675e-05, gnorm=1.422, clip=70, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12217
2023-02-22 14:56:57 - progress_bar.py[line:274] - INFO: epoch 002:   1878 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.6, ups=0.93, wpb=112.7, bsz=40, num_updates=4050, lr=4.17406e-05, gnorm=1.133, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12227
2023-02-22 14:57:08 - progress_bar.py[line:274] - INFO: epoch 002:   1888 / 2175 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.9, ups=0.91, wpb=111.9, bsz=40, num_updates=4060, lr=4.17137e-05, gnorm=1.194, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12238
2023-02-22 14:57:19 - progress_bar.py[line:274] - INFO: epoch 002:   1898 / 2175 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.4, ups=0.89, wpb=112, bsz=40, num_updates=4070, lr=4.16868e-05, gnorm=1.023, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12250
2023-02-22 14:57:30 - progress_bar.py[line:274] - INFO: epoch 002:   1908 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.7, ups=0.91, wpb=111.7, bsz=40, num_updates=4080, lr=4.16599e-05, gnorm=1.085, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12261
2023-02-22 14:57:41 - progress_bar.py[line:274] - INFO: epoch 002:   1918 / 2175 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.8, ups=0.87, wpb=111.8, bsz=40, num_updates=4090, lr=4.16331e-05, gnorm=1.238, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12272
2023-02-22 14:57:53 - progress_bar.py[line:274] - INFO: epoch 002:   1928 / 2175 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=95.7, ups=0.86, wpb=110.8, bsz=40, num_updates=4100, lr=4.16062e-05, gnorm=1.304, clip=70, loss_scale=1024, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=12284
2023-02-22 14:58:05 - progress_bar.py[line:274] - INFO: epoch 002:   1938 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98, ups=0.87, wpb=112.1, bsz=40, num_updates=4110, lr=4.15793e-05, gnorm=0.925, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12295
2023-02-22 14:58:16 - progress_bar.py[line:274] - INFO: epoch 002:   1948 / 2175 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.1, ups=0.89, wpb=112.4, bsz=40, num_updates=4120, lr=4.15524e-05, gnorm=1.074, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12307
2023-02-22 14:58:27 - progress_bar.py[line:274] - INFO: epoch 002:   1958 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.8, ups=0.87, wpb=111.7, bsz=40, num_updates=4130, lr=4.15255e-05, gnorm=0.911, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12318
2023-02-22 14:58:38 - progress_bar.py[line:274] - INFO: epoch 002:   1968 / 2175 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.9, ups=0.91, wpb=111.8, bsz=40, num_updates=4140, lr=4.14986e-05, gnorm=1.009, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12329
2023-02-22 14:58:50 - progress_bar.py[line:274] - INFO: epoch 002:   1978 / 2175 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.6, ups=0.88, wpb=111.3, bsz=40, num_updates=4150, lr=4.14717e-05, gnorm=1.278, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12340
2023-02-22 14:59:01 - progress_bar.py[line:274] - INFO: epoch 002:   1988 / 2175 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.2, ups=0.9, wpb=111.7, bsz=40, num_updates=4160, lr=4.14449e-05, gnorm=0.934, clip=40, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=12352
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 14:59:12 - progress_bar.py[line:274] - INFO: epoch 002:   1998 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.88, wpb=113.7, bsz=40, num_updates=4170, lr=4.1418e-05, gnorm=0.993, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12363
2023-02-22 14:59:24 - progress_bar.py[line:274] - INFO: epoch 002:   2008 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.7, ups=0.88, wpb=112.8, bsz=40, num_updates=4180, lr=4.13911e-05, gnorm=0.722, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12374
2023-02-22 14:59:35 - progress_bar.py[line:274] - INFO: epoch 002:   2018 / 2175 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.9, ups=0.92, wpb=112.6, bsz=40, num_updates=4190, lr=4.13642e-05, gnorm=1.04, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12385
2023-02-22 14:59:46 - progress_bar.py[line:274] - INFO: epoch 002:   2028 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.9, wpb=113, bsz=40, num_updates=4200, lr=4.13373e-05, gnorm=1.057, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12396
2023-02-22 14:59:57 - progress_bar.py[line:274] - INFO: epoch 002:   2038 / 2175 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.9, ups=0.88, wpb=112.5, bsz=40, num_updates=4210, lr=4.13104e-05, gnorm=1.082, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12408
2023-02-22 15:00:08 - progress_bar.py[line:274] - INFO: epoch 002:   2048 / 2175 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.7, ups=0.9, wpb=112.8, bsz=40, num_updates=4220, lr=4.12835e-05, gnorm=1.003, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12419
2023-02-22 15:00:19 - progress_bar.py[line:274] - INFO: epoch 002:   2058 / 2175 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=102.9, ups=0.91, wpb=113.1, bsz=40, num_updates=4230, lr=4.12567e-05, gnorm=1.112, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12430
2023-02-22 15:00:30 - progress_bar.py[line:274] - INFO: epoch 002:   2068 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.8, ups=0.9, wpb=113.1, bsz=40, num_updates=4240, lr=4.12298e-05, gnorm=0.914, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12441
2023-02-22 15:00:41 - progress_bar.py[line:274] - INFO: epoch 002:   2078 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.9, ups=0.91, wpb=112.9, bsz=40, num_updates=4250, lr=4.12029e-05, gnorm=0.957, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12452
2023-02-22 15:00:53 - progress_bar.py[line:274] - INFO: epoch 002:   2088 / 2175 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.6, ups=0.88, wpb=113.5, bsz=40, num_updates=4260, lr=4.1176e-05, gnorm=1.195, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12463
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 15:01:04 - progress_bar.py[line:274] - INFO: epoch 002:   2098 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.1, ups=0.87, wpb=112.1, bsz=40, num_updates=4270, lr=4.11491e-05, gnorm=1.108, clip=50, loss_scale=1024, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=12475
2023-02-22 15:01:16 - progress_bar.py[line:274] - INFO: epoch 002:   2108 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.1, ups=0.88, wpb=112.9, bsz=40, num_updates=4280, lr=4.11222e-05, gnorm=0.907, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12486
2023-02-22 15:01:27 - progress_bar.py[line:274] - INFO: epoch 002:   2118 / 2175 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.6, ups=0.9, wpb=112, bsz=40, num_updates=4290, lr=4.10953e-05, gnorm=1.047, clip=70, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=12498
2023-02-22 15:01:38 - progress_bar.py[line:274] - INFO: epoch 002:   2128 / 2175 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=110.6, bsz=40, num_updates=4300, lr=4.10685e-05, gnorm=0.984, clip=50, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12509
2023-02-22 15:01:50 - progress_bar.py[line:274] - INFO: epoch 002:   2138 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.88, wpb=112.6, bsz=40, num_updates=4310, lr=4.10416e-05, gnorm=1.057, clip=60, loss_scale=2048, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=12520
2023-02-22 15:01:55 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-22 15:02:02 - progress_bar.py[line:274] - INFO: epoch 002:   2149 / 2175 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=93.8, ups=0.83, wpb=112.6, bsz=40, num_updates=4320, lr=4.10147e-05, gnorm=0.941, clip=30, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=12532
2023-02-22 15:02:13 - progress_bar.py[line:274] - INFO: epoch 002:   2159 / 2175 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.1, ups=0.9, wpb=112.1, bsz=40, num_updates=4330, lr=4.09878e-05, gnorm=1.515, clip=50, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=12543
2023-02-22 15:02:24 - progress_bar.py[line:274] - INFO: epoch 002:   2169 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.8, ups=0.87, wpb=112.9, bsz=40, num_updates=4340, lr=4.09609e-05, gnorm=0.854, clip=30, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=12555
2023-02-22 15:02:31 - train.py[line:339] - INFO: end of epoch 2 (average epoch stats below)
2023-02-22 15:02:31 - progress_bar.py[line:282] - INFO: epoch 002 | loss 0.293 | loss_v1 0 | loss_v2 0 | nll_loss 0.118 | ntokens 112.28 | nsentences 39.997 | sample_size 112.28 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.09 | wps 38.9 | ups 0.35 | wpb 112.3 | bsz 40 | num_updates 4346 | lr 4.09448e-05 | gnorm 1.049 | clip 48.1 | loss_scale 1024 | train_wall 2426 | gb_free 14.2 | ema_decay 0.9999 | wall 12562
2023-02-22 15:02:31 - trainer.py[line:694] - INFO: loading train data for epoch 3
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E2.tsv slice_id 1 row count 43497 total row count 86994
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E2.tsv slice_id 0 row count 43497 total row count 86994
2023-02-22 15:02:31 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
2023-02-22 15:02:31 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 15:02:32 - trainer.py[line:758] - INFO: begin training epoch 3
2023-02-22 15:02:32 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 15:02:38 - progress_bar.py[line:274] - INFO: epoch 003:      4 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=109.8, nsentences=39.4, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=79.9, ups=0.73, wpb=109.8, bsz=39.4, num_updates=4350, lr=4.0934e-05, gnorm=0.888, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12569
2023-02-22 15:02:49 - progress_bar.py[line:274] - INFO: epoch 003:     14 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.88, wpb=111.2, bsz=40, num_updates=4360, lr=4.09071e-05, gnorm=0.85, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12580
2023-02-22 15:03:01 - progress_bar.py[line:274] - INFO: epoch 003:     24 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.8, ups=0.86, wpb=112.9, bsz=40, num_updates=4370, lr=4.08802e-05, gnorm=0.741, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=12592
2023-02-22 15:03:12 - progress_bar.py[line:274] - INFO: epoch 003:     34 / 2175 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.89, wpb=113.1, bsz=40, num_updates=4380, lr=4.08534e-05, gnorm=0.865, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12603
2023-02-22 15:03:24 - progress_bar.py[line:274] - INFO: epoch 003:     44 / 2175 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.86, wpb=112.3, bsz=40, num_updates=4390, lr=4.08265e-05, gnorm=0.923, clip=30, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=12615
2023-02-22 15:03:35 - progress_bar.py[line:274] - INFO: epoch 003:     54 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.89, wpb=112.6, bsz=40, num_updates=4400, lr=4.07996e-05, gnorm=0.791, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12626
2023-02-22 15:03:46 - progress_bar.py[line:274] - INFO: epoch 003:     64 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.5, ups=0.92, wpb=113.3, bsz=40, num_updates=4410, lr=4.07727e-05, gnorm=0.974, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12637
2023-02-22 15:03:57 - progress_bar.py[line:274] - INFO: epoch 003:     74 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.2, ups=0.9, wpb=114.2, bsz=40, num_updates=4420, lr=4.07458e-05, gnorm=0.717, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12648
2023-02-22 15:04:09 - progress_bar.py[line:274] - INFO: epoch 003:     84 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.87, wpb=113.4, bsz=40, num_updates=4430, lr=4.07189e-05, gnorm=0.86, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12659
2023-02-22 15:04:20 - progress_bar.py[line:274] - INFO: epoch 003:     94 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=111.9, bsz=40, num_updates=4440, lr=4.0692e-05, gnorm=1.055, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12671
2023-02-22 15:04:31 - progress_bar.py[line:274] - INFO: epoch 003:    104 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=112, bsz=40, num_updates=4450, lr=4.06652e-05, gnorm=0.967, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12682
2023-02-22 15:04:43 - progress_bar.py[line:274] - INFO: epoch 003:    114 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.89, wpb=112, bsz=40, num_updates=4460, lr=4.06383e-05, gnorm=1.064, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12693
2023-02-22 15:04:54 - progress_bar.py[line:274] - INFO: epoch 003:    124 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.1, ups=0.85, wpb=111.5, bsz=40, num_updates=4470, lr=4.06114e-05, gnorm=1.056, clip=60, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=12705
2023-02-22 15:05:05 - progress_bar.py[line:274] - INFO: epoch 003:    134 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.9, wpb=111.4, bsz=40, num_updates=4480, lr=4.05845e-05, gnorm=1.014, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12716
2023-02-22 15:05:17 - progress_bar.py[line:274] - INFO: epoch 003:    144 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.6, ups=0.89, wpb=113.3, bsz=40, num_updates=4490, lr=4.05576e-05, gnorm=0.8, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12727
2023-02-22 15:05:28 - progress_bar.py[line:274] - INFO: epoch 003:    154 / 2175 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102, ups=0.91, wpb=112.1, bsz=40, num_updates=4500, lr=4.05307e-05, gnorm=1.02, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12738
2023-02-22 15:05:39 - progress_bar.py[line:274] - INFO: epoch 003:    164 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.9, wpb=112.7, bsz=40, num_updates=4510, lr=4.05038e-05, gnorm=0.776, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12750
2023-02-22 15:05:50 - progress_bar.py[line:274] - INFO: epoch 003:    174 / 2175 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.91, wpb=112.7, bsz=40, num_updates=4520, lr=4.0477e-05, gnorm=1.085, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12761
2023-02-22 15:06:01 - progress_bar.py[line:274] - INFO: epoch 003:    184 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.8, ups=0.91, wpb=112.6, bsz=40, num_updates=4530, lr=4.04501e-05, gnorm=0.941, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12772
2023-02-22 15:06:12 - progress_bar.py[line:274] - INFO: epoch 003:    194 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.9, wpb=111.4, bsz=40, num_updates=4540, lr=4.04232e-05, gnorm=1.062, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12783
2023-02-22 15:06:23 - progress_bar.py[line:274] - INFO: epoch 003:    204 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.9, wpb=111.2, bsz=40, num_updates=4550, lr=4.03963e-05, gnorm=0.935, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12794
2023-02-22 15:06:34 - progress_bar.py[line:274] - INFO: epoch 003:    214 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.9, wpb=112.9, bsz=40, num_updates=4560, lr=4.03694e-05, gnorm=0.781, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12805
2023-02-22 15:06:45 - progress_bar.py[line:274] - INFO: epoch 003:    224 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.89, wpb=112.6, bsz=40, num_updates=4570, lr=4.03425e-05, gnorm=0.795, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12816
2023-02-22 15:06:56 - progress_bar.py[line:274] - INFO: epoch 003:    234 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=112.2, bsz=40, num_updates=4580, lr=4.03156e-05, gnorm=0.884, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12827
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 15:07:08 - progress_bar.py[line:274] - INFO: epoch 003:    244 / 2175 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.88, wpb=112.7, bsz=40, num_updates=4590, lr=4.02888e-05, gnorm=0.814, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12839
2023-02-22 15:07:19 - progress_bar.py[line:274] - INFO: epoch 003:    254 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.9, wpb=112.5, bsz=40, num_updates=4600, lr=4.02619e-05, gnorm=0.816, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12850
2023-02-22 15:07:30 - progress_bar.py[line:274] - INFO: epoch 003:    264 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.1, ups=0.92, wpb=112.7, bsz=40, num_updates=4610, lr=4.0235e-05, gnorm=0.835, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=12861
2023-02-22 15:07:41 - progress_bar.py[line:274] - INFO: epoch 003:    274 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.92, wpb=111, bsz=40, num_updates=4620, lr=4.02081e-05, gnorm=0.835, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12871
2023-02-22 15:07:52 - progress_bar.py[line:274] - INFO: epoch 003:    284 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111.9, bsz=40, num_updates=4630, lr=4.01812e-05, gnorm=0.941, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12883
2023-02-22 15:08:03 - progress_bar.py[line:274] - INFO: epoch 003:    294 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.2, ups=0.9, wpb=112.3, bsz=40, num_updates=4640, lr=4.01543e-05, gnorm=1.062, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12894
2023-02-22 15:08:14 - progress_bar.py[line:274] - INFO: epoch 003:    304 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.1, ups=0.91, wpb=111.4, bsz=40, num_updates=4650, lr=4.01274e-05, gnorm=0.843, clip=30, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=12905
2023-02-22 15:08:25 - progress_bar.py[line:274] - INFO: epoch 003:    314 / 2175 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.88, wpb=112.8, bsz=40, num_updates=4660, lr=4.01006e-05, gnorm=1.343, clip=70, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12916
2023-02-22 15:08:37 - progress_bar.py[line:274] - INFO: epoch 003:    324 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.88, wpb=112, bsz=40, num_updates=4670, lr=4.00737e-05, gnorm=0.837, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12928
2023-02-22 15:08:48 - progress_bar.py[line:274] - INFO: epoch 003:    334 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.88, wpb=112.4, bsz=40, num_updates=4680, lr=4.00468e-05, gnorm=1.233, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12939
2023-02-22 15:08:59 - progress_bar.py[line:274] - INFO: epoch 003:    344 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.9, wpb=111.9, bsz=40, num_updates=4690, lr=4.00199e-05, gnorm=1.09, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12950
2023-02-22 15:09:10 - progress_bar.py[line:274] - INFO: epoch 003:    354 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.9, wpb=111.9, bsz=40, num_updates=4700, lr=3.9993e-05, gnorm=1.014, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12961
2023-02-22 15:09:21 - progress_bar.py[line:274] - INFO: epoch 003:    364 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.91, wpb=111.5, bsz=40, num_updates=4710, lr=3.99661e-05, gnorm=0.903, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12972
2023-02-22 15:09:33 - progress_bar.py[line:274] - INFO: epoch 003:    374 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.9, wpb=112.7, bsz=40, num_updates=4720, lr=3.99392e-05, gnorm=0.992, clip=20, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=12983
@@@@ ERROR IN DATA @@@@ watch
2023-02-22 15:09:43 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 15:09:45 - progress_bar.py[line:274] - INFO: epoch 003:    385 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=93.9, ups=0.84, wpb=111.5, bsz=40, num_updates=4730, lr=3.99124e-05, gnorm=0.863, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=12995
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 15:09:56 - progress_bar.py[line:274] - INFO: epoch 003:    395 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=111.6, bsz=40, num_updates=4740, lr=3.98855e-05, gnorm=0.926, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13006
2023-02-22 15:10:07 - progress_bar.py[line:274] - INFO: epoch 003:    405 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.9, wpb=111.4, bsz=40, num_updates=4750, lr=3.98586e-05, gnorm=0.853, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13018
2023-02-22 15:10:18 - progress_bar.py[line:274] - INFO: epoch 003:    415 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.9, wpb=111.3, bsz=40, num_updates=4760, lr=3.98317e-05, gnorm=1.139, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13029
2023-02-22 15:10:29 - progress_bar.py[line:274] - INFO: epoch 003:    425 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.8, ups=0.9, wpb=112.1, bsz=40, num_updates=4770, lr=3.98048e-05, gnorm=1.059, clip=80, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13040
2023-02-22 15:10:40 - progress_bar.py[line:274] - INFO: epoch 003:    435 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102, ups=0.91, wpb=112.7, bsz=40, num_updates=4780, lr=3.97779e-05, gnorm=0.98, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13051
2023-02-22 15:10:51 - progress_bar.py[line:274] - INFO: epoch 003:    445 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.1, ups=0.89, wpb=112.4, bsz=40, num_updates=4790, lr=3.9751e-05, gnorm=0.939, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13062
2023-02-22 15:11:02 - progress_bar.py[line:274] - INFO: epoch 003:    455 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.9, wpb=112.8, bsz=40, num_updates=4800, lr=3.97241e-05, gnorm=1.124, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13073
2023-02-22 15:11:13 - progress_bar.py[line:274] - INFO: epoch 003:    465 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.7, ups=0.93, wpb=112.7, bsz=40, num_updates=4810, lr=3.96973e-05, gnorm=1.066, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13084
2023-02-22 15:11:25 - progress_bar.py[line:274] - INFO: epoch 003:    475 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.88, wpb=112.7, bsz=40, num_updates=4820, lr=3.96704e-05, gnorm=1.003, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13095
2023-02-22 15:11:36 - progress_bar.py[line:274] - INFO: epoch 003:    485 / 2175 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.2, ups=0.9, wpb=112.6, bsz=40, num_updates=4830, lr=3.96435e-05, gnorm=1.22, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13107
2023-02-22 15:11:47 - progress_bar.py[line:274] - INFO: epoch 003:    495 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.91, wpb=112.8, bsz=40, num_updates=4840, lr=3.96166e-05, gnorm=1.032, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13118
2023-02-22 15:11:58 - progress_bar.py[line:274] - INFO: epoch 003:    505 / 2175 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.5, ups=0.89, wpb=113.4, bsz=40, num_updates=4850, lr=3.95897e-05, gnorm=0.979, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13129
2023-02-22 15:12:10 - progress_bar.py[line:274] - INFO: epoch 003:    515 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.88, wpb=112.6, bsz=40, num_updates=4860, lr=3.95628e-05, gnorm=0.953, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13140
2023-02-22 15:12:21 - progress_bar.py[line:274] - INFO: epoch 003:    525 / 2175 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100, ups=0.9, wpb=111.6, bsz=40, num_updates=4870, lr=3.95359e-05, gnorm=0.964, clip=60, loss_scale=512, train_wall=11, gb_free=9.4, ema_decay=0.9999, wall=13152
2023-02-22 15:12:32 - progress_bar.py[line:274] - INFO: epoch 003:    535 / 2175 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.4, ups=0.89, wpb=111.1, bsz=40, num_updates=4880, lr=3.95091e-05, gnorm=0.981, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13163
2023-02-22 15:12:43 - progress_bar.py[line:274] - INFO: epoch 003:    545 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=111, bsz=40, num_updates=4890, lr=3.94822e-05, gnorm=0.814, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13174
2023-02-22 15:12:55 - progress_bar.py[line:274] - INFO: epoch 003:    555 / 2175 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.88, wpb=112.7, bsz=40, num_updates=4900, lr=3.94553e-05, gnorm=1.121, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13185
2023-02-22 15:13:06 - progress_bar.py[line:274] - INFO: epoch 003:    565 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.7, ups=0.91, wpb=112.7, bsz=40, num_updates=4910, lr=3.94284e-05, gnorm=0.795, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13196
2023-02-22 15:13:17 - progress_bar.py[line:274] - INFO: epoch 003:    575 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.1, ups=0.92, wpb=113.5, bsz=40, num_updates=4920, lr=3.94015e-05, gnorm=0.757, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13207
2023-02-22 15:13:28 - progress_bar.py[line:274] - INFO: epoch 003:    585 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.89, wpb=113.2, bsz=40, num_updates=4930, lr=3.93746e-05, gnorm=0.748, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13219
2023-02-22 15:13:39 - progress_bar.py[line:274] - INFO: epoch 003:    595 / 2175 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.3, ups=0.88, wpb=111.1, bsz=40, num_updates=4940, lr=3.93477e-05, gnorm=1.068, clip=40, loss_scale=512, train_wall=11, gb_free=9.3, ema_decay=0.9999, wall=13230
2023-02-22 15:13:50 - progress_bar.py[line:274] - INFO: epoch 003:    605 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.1, ups=0.91, wpb=113, bsz=40, num_updates=4950, lr=3.93209e-05, gnorm=0.763, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13241
2023-02-22 15:14:01 - progress_bar.py[line:274] - INFO: epoch 003:    615 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.4, ups=0.91, wpb=112.2, bsz=40, num_updates=4960, lr=3.9294e-05, gnorm=0.918, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13252
2023-02-22 15:14:13 - progress_bar.py[line:274] - INFO: epoch 003:    625 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97, ups=0.86, wpb=112.3, bsz=40, num_updates=4970, lr=3.92671e-05, gnorm=0.768, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13263
2023-02-22 15:14:24 - progress_bar.py[line:274] - INFO: epoch 003:    635 / 2175 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.6, ups=0.92, wpb=112.4, bsz=40, num_updates=4980, lr=3.92402e-05, gnorm=1, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13274
2023-02-22 15:14:35 - progress_bar.py[line:274] - INFO: epoch 003:    645 / 2175 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.1, ups=0.89, wpb=111.5, bsz=40, num_updates=4990, lr=3.92133e-05, gnorm=0.867, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13286
2023-02-22 15:14:46 - progress_bar.py[line:274] - INFO: epoch 003:    655 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.89, wpb=111.7, bsz=40, num_updates=5000, lr=3.91864e-05, gnorm=0.888, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13297
2023-02-22 15:14:57 - progress_bar.py[line:274] - INFO: epoch 003:    665 / 2175 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.88, wpb=112.7, bsz=40, num_updates=5010, lr=3.91595e-05, gnorm=1.053, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13308
2023-02-22 15:15:08 - progress_bar.py[line:274] - INFO: epoch 003:    675 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.91, wpb=112, bsz=40, num_updates=5020, lr=3.91327e-05, gnorm=0.94, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13319
2023-02-22 15:15:19 - progress_bar.py[line:274] - INFO: epoch 003:    685 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.1, ups=0.91, wpb=113.3, bsz=40, num_updates=5030, lr=3.91058e-05, gnorm=0.939, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13330
2023-02-22 15:15:30 - progress_bar.py[line:274] - INFO: epoch 003:    695 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.2, ups=0.92, wpb=113, bsz=40, num_updates=5040, lr=3.90789e-05, gnorm=0.849, clip=20, loss_scale=512, train_wall=11, gb_free=9, ema_decay=0.9999, wall=13341
2023-02-22 15:15:42 - progress_bar.py[line:274] - INFO: epoch 003:    705 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.87, wpb=112.4, bsz=40, num_updates=5050, lr=3.9052e-05, gnorm=0.954, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=13353
2023-02-22 15:15:53 - progress_bar.py[line:274] - INFO: epoch 003:    715 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104, ups=0.92, wpb=112.5, bsz=40, num_updates=5060, lr=3.90251e-05, gnorm=1.165, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13363
2023-02-22 15:16:04 - progress_bar.py[line:274] - INFO: epoch 003:    725 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.9, wpb=112.4, bsz=40, num_updates=5070, lr=3.89982e-05, gnorm=0.971, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13375
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 15:16:15 - progress_bar.py[line:274] - INFO: epoch 003:    735 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111.8, bsz=40, num_updates=5080, lr=3.89713e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13386
2023-02-22 15:16:26 - progress_bar.py[line:274] - INFO: epoch 003:    745 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.89, wpb=113.1, bsz=40, num_updates=5090, lr=3.89445e-05, gnorm=0.899, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13397
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 15:16:37 - progress_bar.py[line:274] - INFO: epoch 003:    755 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.9, ups=0.91, wpb=113.1, bsz=40, num_updates=5100, lr=3.89176e-05, gnorm=0.787, clip=30, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=13408
2023-02-22 15:16:49 - progress_bar.py[line:274] - INFO: epoch 003:    765 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.89, wpb=112.7, bsz=40, num_updates=5110, lr=3.88907e-05, gnorm=0.901, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13419
2023-02-22 15:17:00 - progress_bar.py[line:274] - INFO: epoch 003:    775 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.4, ups=0.88, wpb=114.5, bsz=40, num_updates=5120, lr=3.88638e-05, gnorm=1.003, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13431
2023-02-22 15:17:11 - progress_bar.py[line:274] - INFO: epoch 003:    785 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.1, ups=0.91, wpb=111.6, bsz=40, num_updates=5130, lr=3.88369e-05, gnorm=0.865, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13442
2023-02-22 15:17:22 - progress_bar.py[line:274] - INFO: epoch 003:    795 / 2175 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.1, ups=0.89, wpb=112.3, bsz=40, num_updates=5140, lr=3.881e-05, gnorm=1.183, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13453
2023-02-22 15:17:33 - progress_bar.py[line:274] - INFO: epoch 003:    805 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.5, ups=0.91, wpb=113.6, bsz=40, num_updates=5150, lr=3.87831e-05, gnorm=0.869, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13464
2023-02-22 15:17:44 - progress_bar.py[line:274] - INFO: epoch 003:    815 / 2175 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.6, ups=0.93, wpb=110.8, bsz=40, num_updates=5160, lr=3.87563e-05, gnorm=1.046, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13475
2023-02-22 15:17:55 - progress_bar.py[line:274] - INFO: epoch 003:    825 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=112.4, bsz=40, num_updates=5170, lr=3.87294e-05, gnorm=0.838, clip=40, loss_scale=512, train_wall=11, gb_free=9.3, ema_decay=0.9999, wall=13486
2023-02-22 15:18:07 - progress_bar.py[line:274] - INFO: epoch 003:    835 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.7, ups=0.9, wpb=112.3, bsz=40, num_updates=5180, lr=3.87025e-05, gnorm=0.984, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13497
2023-02-22 15:18:18 - progress_bar.py[line:274] - INFO: epoch 003:    845 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.87, wpb=112.6, bsz=40, num_updates=5190, lr=3.86756e-05, gnorm=0.977, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13509
2023-02-22 15:18:29 - progress_bar.py[line:274] - INFO: epoch 003:    855 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.9, ups=0.92, wpb=113.3, bsz=40, num_updates=5200, lr=3.86487e-05, gnorm=0.919, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13520
2023-02-22 15:18:40 - progress_bar.py[line:274] - INFO: epoch 003:    865 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.87, wpb=112, bsz=40, num_updates=5210, lr=3.86218e-05, gnorm=0.834, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13531
2023-02-22 15:18:52 - progress_bar.py[line:274] - INFO: epoch 003:    875 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.89, wpb=112.7, bsz=40, num_updates=5220, lr=3.85949e-05, gnorm=1.081, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13542
2023-02-22 15:19:03 - progress_bar.py[line:274] - INFO: epoch 003:    885 / 2175 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.7, ups=0.87, wpb=111.5, bsz=40, num_updates=5230, lr=3.8568e-05, gnorm=1.01, clip=50, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=13554
2023-02-22 15:19:14 - progress_bar.py[line:274] - INFO: epoch 003:    895 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.9, wpb=110.9, bsz=40, num_updates=5240, lr=3.85412e-05, gnorm=0.912, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13565
2023-02-22 15:19:26 - progress_bar.py[line:274] - INFO: epoch 003:    905 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.9, wpb=113, bsz=40, num_updates=5250, lr=3.85143e-05, gnorm=0.765, clip=20, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13576
2023-02-22 15:19:37 - progress_bar.py[line:274] - INFO: epoch 003:    915 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.88, wpb=111.5, bsz=40, num_updates=5260, lr=3.84874e-05, gnorm=0.738, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13588
2023-02-22 15:19:48 - progress_bar.py[line:274] - INFO: epoch 003:    925 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.87, wpb=112.2, bsz=40, num_updates=5270, lr=3.84605e-05, gnorm=0.918, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13599
2023-02-22 15:19:59 - progress_bar.py[line:274] - INFO: epoch 003:    935 / 2175 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.91, wpb=111.6, bsz=40, num_updates=5280, lr=3.84336e-05, gnorm=1.08, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13610
2023-02-22 15:20:11 - progress_bar.py[line:274] - INFO: epoch 003:    945 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.9, wpb=113.3, bsz=40, num_updates=5290, lr=3.84067e-05, gnorm=1.037, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13621
2023-02-22 15:20:22 - progress_bar.py[line:274] - INFO: epoch 003:    955 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.89, wpb=112.1, bsz=40, num_updates=5300, lr=3.83798e-05, gnorm=1.11, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13633
2023-02-22 15:20:33 - progress_bar.py[line:274] - INFO: epoch 003:    965 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.88, wpb=112.3, bsz=40, num_updates=5310, lr=3.8353e-05, gnorm=0.824, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13644
2023-02-22 15:20:45 - progress_bar.py[line:274] - INFO: epoch 003:    975 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=5320, lr=3.83261e-05, gnorm=1.025, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13655
2023-02-22 15:20:56 - progress_bar.py[line:274] - INFO: epoch 003:    985 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.87, wpb=113.9, bsz=40, num_updates=5330, lr=3.82992e-05, gnorm=0.929, clip=20, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13667
2023-02-22 15:21:07 - progress_bar.py[line:274] - INFO: epoch 003:    995 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.89, wpb=111.8, bsz=40, num_updates=5340, lr=3.82723e-05, gnorm=0.866, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13678
2023-02-22 15:21:19 - progress_bar.py[line:274] - INFO: epoch 003:   1005 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.89, wpb=112.2, bsz=40, num_updates=5350, lr=3.82454e-05, gnorm=0.91, clip=30, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13689
2023-02-22 15:21:30 - progress_bar.py[line:274] - INFO: epoch 003:   1015 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=110.8, bsz=40, num_updates=5360, lr=3.82185e-05, gnorm=1.071, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13700
2023-02-22 15:21:41 - progress_bar.py[line:274] - INFO: epoch 003:   1025 / 2175 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.3, ups=0.91, wpb=111.6, bsz=40, num_updates=5370, lr=3.81916e-05, gnorm=1.16, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13712
2023-02-22 15:21:52 - progress_bar.py[line:274] - INFO: epoch 003:   1035 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.89, wpb=113.6, bsz=40, num_updates=5380, lr=3.81648e-05, gnorm=0.847, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13723
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 15:22:03 - progress_bar.py[line:274] - INFO: epoch 003:   1045 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.88, wpb=112.3, bsz=40, num_updates=5390, lr=3.81379e-05, gnorm=0.881, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13734
2023-02-22 15:22:15 - progress_bar.py[line:274] - INFO: epoch 003:   1055 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.89, wpb=113.7, bsz=40, num_updates=5400, lr=3.8111e-05, gnorm=0.95, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13745
2023-02-22 15:22:26 - progress_bar.py[line:274] - INFO: epoch 003:   1065 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=110.7, bsz=40, num_updates=5410, lr=3.80841e-05, gnorm=0.939, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13756
2023-02-22 15:22:37 - progress_bar.py[line:274] - INFO: epoch 003:   1075 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.9, wpb=112.7, bsz=40, num_updates=5420, lr=3.80572e-05, gnorm=0.766, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13768
2023-02-22 15:22:48 - progress_bar.py[line:274] - INFO: epoch 003:   1085 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.87, wpb=112.2, bsz=40, num_updates=5430, lr=3.80303e-05, gnorm=0.918, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13779
2023-02-22 15:22:59 - progress_bar.py[line:274] - INFO: epoch 003:   1095 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103, ups=0.91, wpb=112.9, bsz=40, num_updates=5440, lr=3.80034e-05, gnorm=1.113, clip=70, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13790
2023-02-22 15:23:11 - progress_bar.py[line:274] - INFO: epoch 003:   1105 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=112.5, bsz=40, num_updates=5450, lr=3.79766e-05, gnorm=0.712, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13802
2023-02-22 15:23:22 - progress_bar.py[line:274] - INFO: epoch 003:   1115 / 2175 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98, ups=0.87, wpb=112.1, bsz=40, num_updates=5460, lr=3.79497e-05, gnorm=1.096, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13813
2023-02-22 15:23:34 - progress_bar.py[line:274] - INFO: epoch 003:   1125 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.6, ups=0.88, wpb=111.5, bsz=40, num_updates=5470, lr=3.79228e-05, gnorm=0.98, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13824
2023-02-22 15:23:45 - progress_bar.py[line:274] - INFO: epoch 003:   1135 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.6, ups=0.9, wpb=111.8, bsz=40, num_updates=5480, lr=3.78959e-05, gnorm=1.083, clip=40, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=13835
2023-02-22 15:23:56 - progress_bar.py[line:274] - INFO: epoch 003:   1145 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.91, wpb=111.7, bsz=40, num_updates=5490, lr=3.7869e-05, gnorm=0.805, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13847
2023-02-22 15:24:06 - progress_bar.py[line:274] - INFO: epoch 003:   1155 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=105.9, ups=0.94, wpb=112.4, bsz=40, num_updates=5500, lr=3.78421e-05, gnorm=0.826, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13857
2023-02-22 15:24:18 - progress_bar.py[line:274] - INFO: epoch 003:   1165 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.87, wpb=112.5, bsz=40, num_updates=5510, lr=3.78152e-05, gnorm=0.81, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13869
2023-02-22 15:24:29 - progress_bar.py[line:274] - INFO: epoch 003:   1175 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.7, bsz=40, num_updates=5520, lr=3.77884e-05, gnorm=0.978, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13880
2023-02-22 15:24:40 - progress_bar.py[line:274] - INFO: epoch 003:   1185 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.7, ups=0.9, wpb=112.5, bsz=40, num_updates=5530, lr=3.77615e-05, gnorm=0.737, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13891
2023-02-22 15:24:51 - progress_bar.py[line:274] - INFO: epoch 003:   1195 / 2175 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.8, ups=0.92, wpb=112.7, bsz=40, num_updates=5540, lr=3.77346e-05, gnorm=0.886, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13902
2023-02-22 15:25:02 - progress_bar.py[line:274] - INFO: epoch 003:   1205 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.89, wpb=113.1, bsz=40, num_updates=5550, lr=3.77077e-05, gnorm=0.724, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13913
2023-02-22 15:25:14 - progress_bar.py[line:274] - INFO: epoch 003:   1215 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=112.7, bsz=40, num_updates=5560, lr=3.76808e-05, gnorm=0.793, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13925
2023-02-22 15:25:25 - progress_bar.py[line:274] - INFO: epoch 003:   1225 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=112.5, bsz=40, num_updates=5570, lr=3.76539e-05, gnorm=0.842, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13936
2023-02-22 15:25:36 - progress_bar.py[line:274] - INFO: epoch 003:   1235 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.9, wpb=112.3, bsz=40, num_updates=5580, lr=3.7627e-05, gnorm=0.938, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13947
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 15:25:47 - progress_bar.py[line:274] - INFO: epoch 003:   1245 / 2175 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.3, ups=0.89, wpb=111.6, bsz=40, num_updates=5590, lr=3.76002e-05, gnorm=0.919, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13958
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 15:25:58 - progress_bar.py[line:274] - INFO: epoch 003:   1255 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.8, ups=0.92, wpb=111.1, bsz=40, num_updates=5600, lr=3.75733e-05, gnorm=0.658, clip=10, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=13969
2023-02-22 15:26:09 - progress_bar.py[line:274] - INFO: epoch 003:   1265 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.88, wpb=112.9, bsz=40, num_updates=5610, lr=3.75464e-05, gnorm=0.761, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13980
2023-02-22 15:26:21 - progress_bar.py[line:274] - INFO: epoch 003:   1275 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.9, wpb=113.5, bsz=40, num_updates=5620, lr=3.75195e-05, gnorm=0.8, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13991
2023-02-22 15:26:32 - progress_bar.py[line:274] - INFO: epoch 003:   1285 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.9, wpb=111.8, bsz=40, num_updates=5630, lr=3.74926e-05, gnorm=0.983, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14002
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 15:26:43 - progress_bar.py[line:274] - INFO: epoch 003:   1295 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=112.5, bsz=40, num_updates=5640, lr=3.74657e-05, gnorm=0.759, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14014
2023-02-22 15:26:54 - progress_bar.py[line:274] - INFO: epoch 003:   1305 / 2175 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.5, ups=0.92, wpb=111.5, bsz=40, num_updates=5650, lr=3.74388e-05, gnorm=0.867, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14025
2023-02-22 15:27:05 - progress_bar.py[line:274] - INFO: epoch 003:   1315 / 2175 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.3, ups=0.91, wpb=112.1, bsz=40, num_updates=5660, lr=3.74119e-05, gnorm=1.204, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14036
2023-02-22 15:27:16 - progress_bar.py[line:274] - INFO: epoch 003:   1325 / 2175 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.3, ups=0.9, wpb=113.8, bsz=40, num_updates=5670, lr=3.73851e-05, gnorm=1.009, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14047
2023-02-22 15:27:27 - progress_bar.py[line:274] - INFO: epoch 003:   1335 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.89, wpb=113, bsz=40, num_updates=5680, lr=3.73582e-05, gnorm=0.857, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14058
2023-02-22 15:27:39 - progress_bar.py[line:274] - INFO: epoch 003:   1345 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.88, wpb=111.8, bsz=40, num_updates=5690, lr=3.73313e-05, gnorm=0.58, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14069
2023-02-22 15:27:50 - progress_bar.py[line:274] - INFO: epoch 003:   1355 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.89, wpb=113.2, bsz=40, num_updates=5700, lr=3.73044e-05, gnorm=0.982, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14081
2023-02-22 15:28:01 - progress_bar.py[line:274] - INFO: epoch 003:   1365 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.6, ups=0.91, wpb=112.6, bsz=40, num_updates=5710, lr=3.72775e-05, gnorm=0.982, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14092
2023-02-22 15:28:12 - progress_bar.py[line:274] - INFO: epoch 003:   1375 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=113.5, bsz=40, num_updates=5720, lr=3.72506e-05, gnorm=0.835, clip=40, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=14103
2023-02-22 15:28:20 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 15:28:24 - progress_bar.py[line:274] - INFO: epoch 003:   1386 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=91.7, ups=0.81, wpb=112.8, bsz=40, num_updates=5730, lr=3.72237e-05, gnorm=0.814, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=14115
2023-02-22 15:28:36 - progress_bar.py[line:274] - INFO: epoch 003:   1396 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.6, ups=0.87, wpb=112.6, bsz=40, num_updates=5740, lr=3.71969e-05, gnorm=0.893, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14127
2023-02-22 15:28:47 - progress_bar.py[line:274] - INFO: epoch 003:   1406 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.9, wpb=113.3, bsz=40, num_updates=5750, lr=3.717e-05, gnorm=0.948, clip=40, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=14138
2023-02-22 15:28:58 - progress_bar.py[line:274] - INFO: epoch 003:   1416 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=112.3, bsz=40, num_updates=5760, lr=3.71431e-05, gnorm=1.023, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14149
2023-02-22 15:29:09 - progress_bar.py[line:274] - INFO: epoch 003:   1426 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.9, wpb=112.3, bsz=40, num_updates=5770, lr=3.71162e-05, gnorm=1.011, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14160
2023-02-22 15:29:20 - progress_bar.py[line:274] - INFO: epoch 003:   1436 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.9, wpb=112.5, bsz=40, num_updates=5780, lr=3.70893e-05, gnorm=0.779, clip=20, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=14171
2023-02-22 15:29:31 - progress_bar.py[line:274] - INFO: epoch 003:   1446 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.92, wpb=111.8, bsz=40, num_updates=5790, lr=3.70624e-05, gnorm=0.94, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14182
2023-02-22 15:29:42 - progress_bar.py[line:274] - INFO: epoch 003:   1456 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.9, wpb=111.6, bsz=40, num_updates=5800, lr=3.70355e-05, gnorm=0.927, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14193
2023-02-22 15:29:54 - progress_bar.py[line:274] - INFO: epoch 003:   1466 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.9, wpb=112.2, bsz=40, num_updates=5810, lr=3.70087e-05, gnorm=0.903, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14204
2023-02-22 15:30:05 - progress_bar.py[line:274] - INFO: epoch 003:   1476 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.91, wpb=111.2, bsz=40, num_updates=5820, lr=3.69818e-05, gnorm=0.812, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14215
2023-02-22 15:30:16 - progress_bar.py[line:274] - INFO: epoch 003:   1486 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.88, wpb=113.1, bsz=40, num_updates=5830, lr=3.69549e-05, gnorm=0.853, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14227
2023-02-22 15:30:27 - progress_bar.py[line:274] - INFO: epoch 003:   1496 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.88, wpb=113.6, bsz=40, num_updates=5840, lr=3.6928e-05, gnorm=0.735, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=14238
2023-02-22 15:30:39 - progress_bar.py[line:274] - INFO: epoch 003:   1506 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.9, wpb=112.4, bsz=40, num_updates=5850, lr=3.69011e-05, gnorm=0.83, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14249
2023-02-22 15:30:50 - progress_bar.py[line:274] - INFO: epoch 003:   1516 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.87, wpb=111.8, bsz=40, num_updates=5860, lr=3.68742e-05, gnorm=0.903, clip=20, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=14261
2023-02-22 15:31:01 - progress_bar.py[line:274] - INFO: epoch 003:   1526 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.88, wpb=113.8, bsz=40, num_updates=5870, lr=3.68473e-05, gnorm=0.694, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14272
2023-02-22 15:31:12 - progress_bar.py[line:274] - INFO: epoch 003:   1536 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.2, ups=0.91, wpb=112.9, bsz=40, num_updates=5880, lr=3.68205e-05, gnorm=0.983, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14283
2023-02-22 15:31:23 - progress_bar.py[line:274] - INFO: epoch 003:   1546 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104, ups=0.92, wpb=112.4, bsz=40, num_updates=5890, lr=3.67936e-05, gnorm=0.788, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14294
2023-02-22 15:31:34 - progress_bar.py[line:274] - INFO: epoch 003:   1556 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.9, wpb=111.9, bsz=40, num_updates=5900, lr=3.67667e-05, gnorm=0.767, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14305
2023-02-22 15:31:46 - progress_bar.py[line:274] - INFO: epoch 003:   1566 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.1, ups=0.89, wpb=111.9, bsz=40, num_updates=5910, lr=3.67398e-05, gnorm=1.024, clip=40, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=14316
@@@@ ERROR IN DATA @@@@ play
2023-02-22 15:31:57 - progress_bar.py[line:274] - INFO: epoch 003:   1576 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.88, wpb=112.5, bsz=40, num_updates=5920, lr=3.67129e-05, gnorm=0.845, clip=30, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=14328
2023-02-22 15:32:08 - progress_bar.py[line:274] - INFO: epoch 003:   1586 / 2175 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.9, wpb=112.9, bsz=40, num_updates=5930, lr=3.6686e-05, gnorm=1.119, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14339
2023-02-22 15:32:20 - progress_bar.py[line:274] - INFO: epoch 003:   1596 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.88, wpb=112.3, bsz=40, num_updates=5940, lr=3.66591e-05, gnorm=1.33, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14350
2023-02-22 15:32:30 - progress_bar.py[line:274] - INFO: epoch 003:   1606 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.91, wpb=112.2, bsz=40, num_updates=5950, lr=3.66323e-05, gnorm=0.796, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14361
2023-02-22 15:32:42 - progress_bar.py[line:274] - INFO: epoch 003:   1616 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.9, wpb=112.2, bsz=40, num_updates=5960, lr=3.66054e-05, gnorm=0.742, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14372
2023-02-22 15:32:53 - progress_bar.py[line:274] - INFO: epoch 003:   1626 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.9, wpb=112.4, bsz=40, num_updates=5970, lr=3.65785e-05, gnorm=0.919, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14383
2023-02-22 15:33:04 - progress_bar.py[line:274] - INFO: epoch 003:   1636 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.89, wpb=113.8, bsz=40, num_updates=5980, lr=3.65516e-05, gnorm=0.968, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14395
2023-02-22 15:33:15 - progress_bar.py[line:274] - INFO: epoch 003:   1646 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=112.5, bsz=40, num_updates=5990, lr=3.65247e-05, gnorm=0.839, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=14406
2023-02-22 15:33:27 - progress_bar.py[line:274] - INFO: epoch 003:   1656 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.89, wpb=112.7, bsz=40, num_updates=6000, lr=3.64978e-05, gnorm=0.869, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=14417
2023-02-22 15:33:27 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 15:33:28 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 15:33:28 - train.py[line:551] - INFO: load:1.06 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 15:35:30 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 15:35:30 - train.py[line:551] - INFO: load:1.09 valid_run:121.72 task_valid:118.53 collect_output:2.12
2023-02-22 15:37:30 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 15:37:30 - train.py[line:551] - INFO: load:1.11 valid_run:241.66 task_valid:233.70 collect_output:5.87
2023-02-22 15:39:32 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 15:39:32 - train.py[line:551] - INFO: load:1.14 valid_run:364.34 task_valid:349.78 collect_output:11.43
2023-02-22 15:41:35 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 15:41:35 - train.py[line:551] - INFO: load:1.16 valid_run:486.44 task_valid:462.99 collect_output:19.30
2023-02-22 15:43:35 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 15:43:35 - train.py[line:551] - INFO: load:1.19 valid_run:606.93 task_valid:579.91 collect_output:21.81
2023-02-22 15:45:38 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 15:45:38 - train.py[line:551] - INFO: load:1.21 valid_run:729.72 task_valid:698.14 collect_output:25.38
2023-02-22 15:47:41 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 15:47:41 - train.py[line:551] - INFO: load:1.24 valid_run:852.71 task_valid:815.79 collect_output:29.71
2023-02-22 15:49:43 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 15:49:43 - train.py[line:551] - INFO: load:1.26 valid_run:974.81 task_valid:931.99 collect_output:34.54
2023-02-22 15:51:47 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 15:51:47 - train.py[line:551] - INFO: load:1.29 valid_run:1098.69 task_valid:1048.68 collect_output:40.73
2023-02-22 15:53:49 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 15:53:49 - train.py[line:551] - INFO: load:1.31 valid_run:1220.46 task_valid:1160.95 collect_output:49.21
2023-02-22 15:55:49 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 15:55:49 - train.py[line:551] - INFO: load:1.34 valid_run:1340.62 task_valid:1276.14 collect_output:53.14
2023-02-22 15:57:51 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 15:57:51 - train.py[line:551] - INFO: load:1.36 valid_run:1462.29 task_valid:1392.77 collect_output:57.17
2023-02-22 15:59:50 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 15:59:50 - train.py[line:551] - INFO: load:1.39 valid_run:1581.48 task_valid:1506.32 collect_output:61.76
2023-02-22 16:01:51 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 16:01:51 - train.py[line:551] - INFO: load:1.42 valid_run:1702.41 task_valid:1623.74 collect_output:64.26
2023-02-22 16:03:52 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 16:03:52 - train.py[line:551] - INFO: load:1.44 valid_run:1823.37 task_valid:1739.20 collect_output:68.73
2023-02-22 16:05:54 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 16:05:54 - train.py[line:551] - INFO: load:1.47 valid_run:1944.71 task_valid:1852.57 collect_output:75.68
2023-02-22 16:07:55 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 16:07:55 - train.py[line:551] - INFO: load:1.49 valid_run:2066.06 task_valid:1968.33 collect_output:80.20
2023-02-22 16:09:56 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 16:09:56 - train.py[line:551] - INFO: load:1.52 valid_run:2186.69 task_valid:2085.95 collect_output:82.17
2023-02-22 16:11:57 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 16:11:57 - train.py[line:551] - INFO: load:1.54 valid_run:2307.79 task_valid:2202.39 collect_output:85.81
2023-02-22 16:13:57 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 16:13:57 - train.py[line:551] - INFO: load:1.57 valid_run:2428.13 task_valid:2318.63 collect_output:88.90
2023-02-22 16:15:59 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 16:15:59 - train.py[line:551] - INFO: load:1.59 valid_run:2549.88 task_valid:2434.77 collect_output:93.49
2023-02-22 16:18:01 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 16:18:01 - train.py[line:551] - INFO: load:1.62 valid_run:2671.98 task_valid:2553.39 collect_output:95.93
2023-02-22 16:20:02 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 16:20:02 - train.py[line:551] - INFO: load:1.64 valid_run:2792.45 task_valid:2667.34 collect_output:101.43
2023-02-22 16:22:02 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 16:22:02 - train.py[line:551] - INFO: load:1.67 valid_run:2912.32 task_valid:2783.07 collect_output:104.53
2023-02-22 16:24:04 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 16:24:04 - train.py[line:551] - INFO: load:1.69 valid_run:3034.27 task_valid:2898.87 collect_output:109.66
2023-02-22 16:26:07 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 16:26:07 - train.py[line:551] - INFO: load:1.72 valid_run:3157.58 task_valid:3014.77 collect_output:115.97
2023-02-22 16:28:07 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 16:28:07 - train.py[line:551] - INFO: load:1.75 valid_run:3277.48 task_valid:3128.81 collect_output:120.79
2023-02-22 16:30:09 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 16:30:09 - train.py[line:551] - INFO: load:1.77 valid_run:3399.39 task_valid:3248.04 collect_output:122.46
2023-02-22 16:32:11 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 16:32:11 - train.py[line:551] - INFO: load:1.80 valid_run:3521.25 task_valid:3363.21 collect_output:128.09
2023-02-22 16:34:13 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 16:34:13 - train.py[line:551] - INFO: load:1.82 valid_run:3643.20 task_valid:3481.40 collect_output:130.77
2023-02-22 16:36:14 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 16:36:14 - train.py[line:551] - INFO: load:1.85 valid_run:3764.48 task_valid:3599.63 collect_output:132.79

====================================================================================================
SGG eval:     R @ 50: 0.6543;     R @ 100: 0.6948;     R @ 500: 0.7237;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4448;    mR @ 100: 0.4937;    mR @ 500: 0.5380;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.5417) (covering:0.2286) (eating:0.8235) (flying in:0.8636) (growing on:0.3750) (hanging from:0.5323) (lying on:0.4000) (mounted on:0.1429) (painted on:0.4167) (parked on:0.9583) (playing:0.0000) (riding:0.9265) (says:0.0000) (sitting on:0.7262) (standing on:0.5460) (using:0.5000) (walking in:0.0000) (walking on:0.4865) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6543;     R @ 100: 0.6948;     R @ 500: 0.7237;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4448;    mR @ 100: 0.4937;    mR @ 500: 0.5380;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.5417) (covering:0.2286) (eating:0.8235) (flying in:0.8636) (growing on:0.3750) (hanging from:0.5323) (lying on:0.4000) (mounted on:0.1429) (painted on:0.4167) (parked on:0.9583) (playing:0.0000) (riding:0.9265) (says:0.0000) (sitting on:0.7262) (standing on:0.5460) (using:0.5000) (walking in:0.0000) (walking on:0.4865) (watching:0.6389) 
--------------------------------------------------------
====================================================================================================

2023-02-22 16:36:45 - train.py[line:487] - INFO: 0.6947693913929207
2023-02-22 16:36:45 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 16:36:46 - progress_bar.py[line:282] - INFO: epoch 003 | valid on 'valid' subset | loss 0.247 | loss_v1 0 | loss_v2 0 | nll_loss 0.083 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.694769 | ppl 1.06 | vqa_score 0.5541 | wps 118.1 | wpb 72 | bsz 24 | num_updates 6000 | best_R@100 0.694769
2023-02-22 16:36:46 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 3 @ 6000 updates
2023-02-22 16:36:46 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_3_6000.pt
2023-02-22 16:36:52 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_3_6000.pt
2023-02-22 16:36:56 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_3_6000.pt (epoch 3 @ 6000 updates, score 0.6947693913929207) (writing took 10.41541125997901 seconds)
2023-02-22 16:37:08 - progress_bar.py[line:274] - INFO: epoch 003:   1666 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=111.6, bsz=40, num_updates=6010, lr=3.64709e-05, gnorm=0.885, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=18239
2023-02-22 16:37:19 - progress_bar.py[line:274] - INFO: epoch 003:   1676 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.88, wpb=112.9, bsz=40, num_updates=6020, lr=3.64441e-05, gnorm=0.953, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=18250
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 16:37:31 - progress_bar.py[line:274] - INFO: epoch 003:   1686 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.88, wpb=112.4, bsz=40, num_updates=6030, lr=3.64172e-05, gnorm=0.791, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=18261
2023-02-22 16:37:42 - progress_bar.py[line:274] - INFO: epoch 003:   1696 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.9, wpb=112.6, bsz=40, num_updates=6040, lr=3.63903e-05, gnorm=0.765, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18273
2023-02-22 16:37:53 - progress_bar.py[line:274] - INFO: epoch 003:   1706 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.9, wpb=110.7, bsz=40, num_updates=6050, lr=3.63634e-05, gnorm=0.785, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=18284
2023-02-22 16:38:04 - progress_bar.py[line:274] - INFO: epoch 003:   1716 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.9, wpb=112.6, bsz=40, num_updates=6060, lr=3.63365e-05, gnorm=0.997, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=18295
2023-02-22 16:38:15 - progress_bar.py[line:274] - INFO: epoch 003:   1726 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.9, wpb=113.3, bsz=40, num_updates=6070, lr=3.63096e-05, gnorm=0.814, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18306
2023-02-22 16:38:26 - progress_bar.py[line:274] - INFO: epoch 003:   1736 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.91, wpb=111, bsz=40, num_updates=6080, lr=3.62827e-05, gnorm=0.697, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18317
2023-02-22 16:38:37 - progress_bar.py[line:274] - INFO: epoch 003:   1746 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.9, wpb=112.3, bsz=40, num_updates=6090, lr=3.62558e-05, gnorm=1.003, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=18328
2023-02-22 16:38:49 - progress_bar.py[line:274] - INFO: epoch 003:   1756 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.89, wpb=112.9, bsz=40, num_updates=6100, lr=3.6229e-05, gnorm=0.863, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=18339
2023-02-22 16:39:02 - progress_bar.py[line:274] - INFO: epoch 003:   1766 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=81.1, ups=0.72, wpb=112, bsz=40, num_updates=6110, lr=3.62021e-05, gnorm=0.922, clip=30, loss_scale=512, train_wall=14, gb_free=10.3, ema_decay=0.9999, wall=18353
2023-02-22 16:39:14 - progress_bar.py[line:274] - INFO: epoch 003:   1776 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.2, ups=0.86, wpb=112.8, bsz=40, num_updates=6120, lr=3.61752e-05, gnorm=0.97, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=18365
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 16:39:26 - progress_bar.py[line:274] - INFO: epoch 003:   1786 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.87, wpb=112.3, bsz=40, num_updates=6130, lr=3.61483e-05, gnorm=0.869, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18376
2023-02-22 16:39:37 - progress_bar.py[line:274] - INFO: epoch 003:   1796 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=93.3, ups=0.84, wpb=111, bsz=40, num_updates=6140, lr=3.61214e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=18388
2023-02-22 16:39:49 - progress_bar.py[line:274] - INFO: epoch 003:   1806 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=94.9, ups=0.84, wpb=112.6, bsz=40, num_updates=6150, lr=3.60945e-05, gnorm=0.93, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=18400
2023-02-22 16:40:03 - progress_bar.py[line:274] - INFO: epoch 003:   1816 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=79.3, ups=0.71, wpb=111.5, bsz=40, num_updates=6160, lr=3.60676e-05, gnorm=1.018, clip=40, loss_scale=512, train_wall=14, gb_free=10.4, ema_decay=0.9999, wall=18414
2023-02-22 16:40:17 - progress_bar.py[line:274] - INFO: epoch 003:   1826 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=79.9, ups=0.72, wpb=111.3, bsz=40, num_updates=6170, lr=3.60408e-05, gnorm=0.934, clip=50, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18428
2023-02-22 16:40:31 - progress_bar.py[line:274] - INFO: epoch 003:   1836 / 2175 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=82.4, ups=0.73, wpb=113.6, bsz=40, num_updates=6180, lr=3.60139e-05, gnorm=0.884, clip=40, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18442
2023-02-22 16:40:45 - progress_bar.py[line:274] - INFO: epoch 003:   1846 / 2175 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=81, ups=0.73, wpb=110.9, bsz=40, num_updates=6190, lr=3.5987e-05, gnorm=1.117, clip=70, loss_scale=512, train_wall=14, gb_free=10.5, ema_decay=0.9999, wall=18456
2023-02-22 16:40:58 - progress_bar.py[line:274] - INFO: epoch 003:   1856 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=84, ups=0.74, wpb=112.8, bsz=40, num_updates=6200, lr=3.59601e-05, gnorm=0.829, clip=30, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=18469
2023-02-22 16:41:12 - progress_bar.py[line:274] - INFO: epoch 003:   1866 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=81.7, ups=0.74, wpb=110.8, bsz=40, num_updates=6210, lr=3.59332e-05, gnorm=0.974, clip=30, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18483
2023-02-22 16:41:27 - progress_bar.py[line:274] - INFO: epoch 003:   1876 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=78, ups=0.69, wpb=113.1, bsz=40, num_updates=6220, lr=3.59063e-05, gnorm=1.039, clip=30, loss_scale=512, train_wall=14, gb_free=10.5, ema_decay=0.9999, wall=18497
2023-02-22 16:41:40 - progress_bar.py[line:274] - INFO: epoch 003:   1886 / 2175 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=80.8, ups=0.73, wpb=110.3, bsz=40, num_updates=6230, lr=3.58794e-05, gnorm=0.943, clip=50, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18511
2023-02-22 16:41:54 - progress_bar.py[line:274] - INFO: epoch 003:   1896 / 2175 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=79.2, ups=0.71, wpb=111.9, bsz=40, num_updates=6240, lr=3.58526e-05, gnorm=0.798, clip=20, loss_scale=1024, train_wall=14, gb_free=10.3, ema_decay=0.9999, wall=18525
2023-02-22 16:42:08 - progress_bar.py[line:274] - INFO: epoch 003:   1906 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=82.6, ups=0.73, wpb=112.5, bsz=40, num_updates=6250, lr=3.58257e-05, gnorm=0.828, clip=30, loss_scale=1024, train_wall=14, gb_free=10.5, ema_decay=0.9999, wall=18539
2023-02-22 16:42:22 - progress_bar.py[line:274] - INFO: epoch 003:   1916 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=81.2, ups=0.72, wpb=112.3, bsz=40, num_updates=6260, lr=3.57988e-05, gnorm=0.684, clip=0, loss_scale=1024, train_wall=14, gb_free=10.4, ema_decay=0.9999, wall=18553
2023-02-22 16:42:35 - progress_bar.py[line:274] - INFO: epoch 003:   1926 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=84.3, ups=0.75, wpb=112.2, bsz=40, num_updates=6270, lr=3.57719e-05, gnorm=0.663, clip=10, loss_scale=1024, train_wall=13, gb_free=10.3, ema_decay=0.9999, wall=18566
2023-02-22 16:42:43 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 16:42:52 - progress_bar.py[line:274] - INFO: epoch 003:   1937 / 2175 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=75.6, ups=0.68, wpb=111.4, bsz=40, num_updates=6280, lr=3.5745e-05, gnorm=1.006, clip=50, loss_scale=512, train_wall=15, gb_free=9.5, ema_decay=0.9999, wall=18581
2023-02-22 16:43:05 - progress_bar.py[line:274] - INFO: epoch 003:   1947 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=84.6, ups=0.75, wpb=112.4, bsz=40, num_updates=6290, lr=3.57181e-05, gnorm=0.812, clip=30, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=18596
2023-02-22 16:43:19 - progress_bar.py[line:274] - INFO: epoch 003:   1957 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=81.1, ups=0.72, wpb=112.2, bsz=40, num_updates=6300, lr=3.56912e-05, gnorm=0.734, clip=20, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18610
2023-02-22 16:43:32 - progress_bar.py[line:274] - INFO: epoch 003:   1967 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=84.6, ups=0.76, wpb=111.9, bsz=40, num_updates=6310, lr=3.56644e-05, gnorm=0.816, clip=30, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=18623
2023-02-22 16:43:46 - progress_bar.py[line:274] - INFO: epoch 003:   1977 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=82.3, ups=0.74, wpb=111.4, bsz=40, num_updates=6320, lr=3.56375e-05, gnorm=0.937, clip=30, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18637
2023-02-22 16:44:00 - progress_bar.py[line:274] - INFO: epoch 003:   1987 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=80.9, ups=0.72, wpb=112.7, bsz=40, num_updates=6330, lr=3.56106e-05, gnorm=0.783, clip=20, loss_scale=512, train_wall=14, gb_free=10, ema_decay=0.9999, wall=18651
2023-02-22 16:44:14 - progress_bar.py[line:274] - INFO: epoch 003:   1997 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78, ups=0.7, wpb=111.4, bsz=40, num_updates=6340, lr=3.55837e-05, gnorm=0.801, clip=20, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18665
2023-02-22 16:44:28 - progress_bar.py[line:274] - INFO: epoch 003:   2007 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=83.2, ups=0.74, wpb=112.4, bsz=40, num_updates=6350, lr=3.55568e-05, gnorm=0.881, clip=40, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18678
2023-02-22 16:44:41 - progress_bar.py[line:274] - INFO: epoch 003:   2017 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=86.3, ups=0.77, wpb=112.2, bsz=40, num_updates=6360, lr=3.55299e-05, gnorm=0.888, clip=30, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18691
2023-02-22 16:44:54 - progress_bar.py[line:274] - INFO: epoch 003:   2027 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=85.4, ups=0.75, wpb=113.2, bsz=40, num_updates=6370, lr=3.5503e-05, gnorm=0.792, clip=30, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=18705
2023-02-22 16:45:08 - progress_bar.py[line:274] - INFO: epoch 003:   2037 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=84.2, ups=0.75, wpb=112.3, bsz=40, num_updates=6380, lr=3.54762e-05, gnorm=0.743, clip=20, loss_scale=512, train_wall=13, gb_free=9.7, ema_decay=0.9999, wall=18718
2023-02-22 16:45:22 - progress_bar.py[line:274] - INFO: epoch 003:   2047 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=79.6, ups=0.72, wpb=111.3, bsz=40, num_updates=6390, lr=3.54493e-05, gnorm=0.871, clip=30, loss_scale=512, train_wall=14, gb_free=10.5, ema_decay=0.9999, wall=18732
2023-02-22 16:45:35 - progress_bar.py[line:274] - INFO: epoch 003:   2057 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=81.5, ups=0.72, wpb=112.4, bsz=40, num_updates=6400, lr=3.54224e-05, gnorm=0.802, clip=20, loss_scale=512, train_wall=14, gb_free=10.5, ema_decay=0.9999, wall=18746
2023-02-22 16:45:49 - progress_bar.py[line:274] - INFO: epoch 003:   2067 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=80.7, ups=0.72, wpb=111.9, bsz=40, num_updates=6410, lr=3.53955e-05, gnorm=0.994, clip=40, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18760
2023-02-22 16:46:03 - progress_bar.py[line:274] - INFO: epoch 003:   2077 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=82.2, ups=0.75, wpb=110.1, bsz=40, num_updates=6420, lr=3.53686e-05, gnorm=0.719, clip=20, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18773
2023-02-22 16:46:17 - progress_bar.py[line:274] - INFO: epoch 003:   2087 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=82.1, ups=0.73, wpb=112.1, bsz=40, num_updates=6430, lr=3.53417e-05, gnorm=0.85, clip=20, loss_scale=512, train_wall=14, gb_free=10.2, ema_decay=0.9999, wall=18787
2023-02-22 16:46:30 - progress_bar.py[line:274] - INFO: epoch 003:   2097 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=84.6, ups=0.75, wpb=112.1, bsz=40, num_updates=6440, lr=3.53148e-05, gnorm=0.775, clip=30, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=18801
2023-02-22 16:46:43 - progress_bar.py[line:274] - INFO: epoch 003:   2107 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=83, ups=0.75, wpb=111.1, bsz=40, num_updates=6450, lr=3.52879e-05, gnorm=1.148, clip=70, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18814
2023-02-22 16:46:57 - progress_bar.py[line:274] - INFO: epoch 003:   2117 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=82.4, ups=0.73, wpb=112.8, bsz=40, num_updates=6460, lr=3.52611e-05, gnorm=1.045, clip=40, loss_scale=512, train_wall=14, gb_free=10.5, ema_decay=0.9999, wall=18828
2023-02-22 16:47:11 - progress_bar.py[line:274] - INFO: epoch 003:   2127 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=83.1, ups=0.73, wpb=113.4, bsz=40, num_updates=6470, lr=3.52342e-05, gnorm=0.86, clip=40, loss_scale=512, train_wall=14, gb_free=10.4, ema_decay=0.9999, wall=18842
2023-02-22 16:47:25 - progress_bar.py[line:274] - INFO: epoch 003:   2137 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=80.9, ups=0.73, wpb=111.2, bsz=40, num_updates=6480, lr=3.52073e-05, gnorm=0.74, clip=20, loss_scale=512, train_wall=14, gb_free=10.2, ema_decay=0.9999, wall=18855
@@@@ ERROR IN DATA @@@@ play
2023-02-22 16:47:38 - progress_bar.py[line:274] - INFO: epoch 003:   2147 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=85.8, ups=0.76, wpb=112.4, bsz=40, num_updates=6490, lr=3.51804e-05, gnorm=0.836, clip=20, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18869
2023-02-22 16:47:51 - progress_bar.py[line:274] - INFO: epoch 003:   2157 / 2175 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=86.9, ups=0.77, wpb=113.4, bsz=40, num_updates=6500, lr=3.51535e-05, gnorm=1.006, clip=40, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=18882
2023-02-22 16:48:04 - progress_bar.py[line:274] - INFO: epoch 003:   2167 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=83, ups=0.74, wpb=111.4, bsz=40, num_updates=6510, lr=3.51266e-05, gnorm=0.827, clip=30, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=18895
2023-02-22 16:48:15 - train.py[line:339] - INFO: end of epoch 3 (average epoch stats below)
2023-02-22 16:48:15 - progress_bar.py[line:282] - INFO: epoch 003 | loss 0.274 | loss_v1 0 | loss_v2 0 | nll_loss 0.097 | ntokens 112.284 | nsentences 39.997 | sample_size 112.284 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.07 | wps 38.4 | ups 0.34 | wpb 112.3 | bsz 40 | num_updates 6518 | lr 3.51051e-05 | gnorm 0.909 | clip 33.7 | loss_scale 512 | train_wall 2513 | gb_free 14.2 | ema_decay 0.9999 | wall 18905
2023-02-22 16:48:15 - trainer.py[line:694] - INFO: loading train data for epoch 4
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E3.tsv slice_id 1 row count 43497 total row count 86994
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E3.tsv slice_id 0 row count 43497 total row count 86994
2023-02-22 16:48:15 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
2023-02-22 16:48:15 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 16:48:16 - trainer.py[line:758] - INFO: begin training epoch 4
2023-02-22 16:48:16 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 16:48:22 - progress_bar.py[line:274] - INFO: epoch 004:      2 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.5, nsentences=39.4, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=64.1, ups=0.58, wpb=110.5, bsz=39.4, num_updates=6520, lr=3.50997e-05, gnorm=0.701, clip=20, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=18912
2023-02-22 16:48:35 - progress_bar.py[line:274] - INFO: epoch 004:     12 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=86.9, ups=0.78, wpb=111.5, bsz=40, num_updates=6530, lr=3.50729e-05, gnorm=0.564, clip=0, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18925
2023-02-22 16:48:48 - progress_bar.py[line:274] - INFO: epoch 004:     22 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=84.6, ups=0.75, wpb=112.5, bsz=40, num_updates=6540, lr=3.5046e-05, gnorm=1.136, clip=40, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18939
2023-02-22 16:49:02 - progress_bar.py[line:274] - INFO: epoch 004:     32 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=83.7, ups=0.75, wpb=111, bsz=40, num_updates=6550, lr=3.50191e-05, gnorm=0.807, clip=20, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18952
2023-02-22 16:49:15 - progress_bar.py[line:274] - INFO: epoch 004:     42 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=86.6, ups=0.77, wpb=113.1, bsz=40, num_updates=6560, lr=3.49922e-05, gnorm=0.646, clip=10, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18965
2023-02-22 16:49:28 - progress_bar.py[line:274] - INFO: epoch 004:     52 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=82.4, ups=0.73, wpb=112.6, bsz=40, num_updates=6570, lr=3.49653e-05, gnorm=0.684, clip=10, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=18979
2023-02-22 16:49:42 - progress_bar.py[line:274] - INFO: epoch 004:     62 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=83.4, ups=0.74, wpb=112.2, bsz=40, num_updates=6580, lr=3.49384e-05, gnorm=0.607, clip=10, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=18993
2023-02-22 16:49:55 - progress_bar.py[line:274] - INFO: epoch 004:     72 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=85.8, ups=0.77, wpb=112.1, bsz=40, num_updates=6590, lr=3.49115e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=13, gb_free=10, ema_decay=0.9999, wall=19006
2023-02-22 16:50:08 - progress_bar.py[line:274] - INFO: epoch 004:     82 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=87.1, ups=0.78, wpb=111.6, bsz=40, num_updates=6600, lr=3.48847e-05, gnorm=0.731, clip=10, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19019
2023-02-22 16:50:21 - progress_bar.py[line:274] - INFO: epoch 004:     92 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=88, ups=0.79, wpb=111.7, bsz=40, num_updates=6610, lr=3.48578e-05, gnorm=0.981, clip=50, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19031
2023-02-22 16:50:34 - progress_bar.py[line:274] - INFO: epoch 004:    102 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=83.8, ups=0.74, wpb=113.7, bsz=40, num_updates=6620, lr=3.48309e-05, gnorm=0.764, clip=10, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19045
2023-02-22 16:50:47 - progress_bar.py[line:274] - INFO: epoch 004:    112 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=90.2, ups=0.8, wpb=112.2, bsz=40, num_updates=6630, lr=3.4804e-05, gnorm=0.774, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19057
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 16:50:59 - progress_bar.py[line:274] - INFO: epoch 004:    122 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=93, ups=0.83, wpb=112.1, bsz=40, num_updates=6640, lr=3.47771e-05, gnorm=0.842, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19070
2023-02-22 16:51:11 - progress_bar.py[line:274] - INFO: epoch 004:    132 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=92.3, ups=0.82, wpb=113.1, bsz=40, num_updates=6650, lr=3.47502e-05, gnorm=0.841, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19082
2023-02-22 16:51:24 - progress_bar.py[line:274] - INFO: epoch 004:    142 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=87.5, ups=0.78, wpb=112.2, bsz=40, num_updates=6660, lr=3.47233e-05, gnorm=0.949, clip=30, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19095
2023-02-22 16:51:38 - progress_bar.py[line:274] - INFO: epoch 004:    152 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=78.6, ups=0.7, wpb=112.1, bsz=40, num_updates=6670, lr=3.46965e-05, gnorm=0.685, clip=10, loss_scale=512, train_wall=14, gb_free=10.4, ema_decay=0.9999, wall=19109
2023-02-22 16:51:53 - progress_bar.py[line:274] - INFO: epoch 004:    162 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=85.1, ups=0.75, wpb=113.1, bsz=40, num_updates=6680, lr=3.46696e-05, gnorm=0.866, clip=40, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=19122
2023-02-22 16:52:05 - progress_bar.py[line:274] - INFO: epoch 004:    172 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=89.2, ups=0.81, wpb=110.4, bsz=40, num_updates=6690, lr=3.46427e-05, gnorm=0.74, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19136
2023-02-22 16:52:18 - progress_bar.py[line:274] - INFO: epoch 004:    182 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=89.8, ups=0.79, wpb=114, bsz=40, num_updates=6700, lr=3.46158e-05, gnorm=0.856, clip=20, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=19149
2023-02-22 16:52:31 - progress_bar.py[line:274] - INFO: epoch 004:    192 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=85, ups=0.76, wpb=111.9, bsz=40, num_updates=6710, lr=3.45889e-05, gnorm=0.751, clip=20, loss_scale=512, train_wall=13, gb_free=10.7, ema_decay=0.9999, wall=19162
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 16:52:45 - progress_bar.py[line:274] - INFO: epoch 004:    202 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=82.9, ups=0.73, wpb=112.9, bsz=40, num_updates=6720, lr=3.4562e-05, gnorm=0.648, clip=20, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=19175
2023-02-22 16:52:59 - progress_bar.py[line:274] - INFO: epoch 004:    212 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=78.6, ups=0.71, wpb=111, bsz=40, num_updates=6730, lr=3.45351e-05, gnorm=0.7, clip=30, loss_scale=512, train_wall=14, gb_free=10.6, ema_decay=0.9999, wall=19190
2023-02-22 16:53:13 - progress_bar.py[line:274] - INFO: epoch 004:    222 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=78.5, ups=0.7, wpb=111.7, bsz=40, num_updates=6740, lr=3.45083e-05, gnorm=1.05, clip=60, loss_scale=512, train_wall=14, gb_free=10.4, ema_decay=0.9999, wall=19204
2023-02-22 16:53:26 - progress_bar.py[line:274] - INFO: epoch 004:    232 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=85, ups=0.76, wpb=112.1, bsz=40, num_updates=6750, lr=3.44814e-05, gnorm=0.904, clip=40, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19217
2023-02-22 16:53:39 - progress_bar.py[line:274] - INFO: epoch 004:    242 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=88.9, ups=0.79, wpb=112.2, bsz=40, num_updates=6760, lr=3.44545e-05, gnorm=1.055, clip=50, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19230
2023-02-22 16:53:52 - progress_bar.py[line:274] - INFO: epoch 004:    252 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=88.9, ups=0.79, wpb=113, bsz=40, num_updates=6770, lr=3.44276e-05, gnorm=0.647, clip=0, loss_scale=512, train_wall=13, gb_free=10.2, ema_decay=0.9999, wall=19242
2023-02-22 16:54:05 - progress_bar.py[line:274] - INFO: epoch 004:    262 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=85.3, ups=0.76, wpb=111.5, bsz=40, num_updates=6780, lr=3.44007e-05, gnorm=0.98, clip=40, loss_scale=512, train_wall=13, gb_free=10.2, ema_decay=0.9999, wall=19256
2023-02-22 16:54:18 - progress_bar.py[line:274] - INFO: epoch 004:    272 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=84.8, ups=0.75, wpb=113.1, bsz=40, num_updates=6790, lr=3.43738e-05, gnorm=0.795, clip=40, loss_scale=1024, train_wall=13, gb_free=10.3, ema_decay=0.9999, wall=19269
2023-02-22 16:54:31 - progress_bar.py[line:274] - INFO: epoch 004:    282 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=83.7, ups=0.75, wpb=111.6, bsz=40, num_updates=6800, lr=3.43469e-05, gnorm=0.886, clip=40, loss_scale=1024, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=19282
2023-02-22 16:54:45 - progress_bar.py[line:274] - INFO: epoch 004:    292 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=85.2, ups=0.75, wpb=112.9, bsz=40, num_updates=6810, lr=3.43201e-05, gnorm=0.774, clip=10, loss_scale=1024, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=19295
2023-02-22 16:54:59 - progress_bar.py[line:274] - INFO: epoch 004:    302 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=79.4, ups=0.71, wpb=111.5, bsz=40, num_updates=6820, lr=3.42932e-05, gnorm=0.916, clip=40, loss_scale=1024, train_wall=14, gb_free=10.3, ema_decay=0.9999, wall=19310
2023-02-22 16:55:12 - progress_bar.py[line:274] - INFO: epoch 004:    312 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=86.9, ups=0.78, wpb=111.8, bsz=40, num_updates=6830, lr=3.42663e-05, gnorm=0.627, clip=10, loss_scale=1024, train_wall=13, gb_free=9.1, ema_decay=0.9999, wall=19322
2023-02-22 16:55:24 - progress_bar.py[line:274] - INFO: epoch 004:    322 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=90.4, ups=0.8, wpb=113.1, bsz=40, num_updates=6840, lr=3.42394e-05, gnorm=0.746, clip=30, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19335
2023-02-22 16:55:36 - progress_bar.py[line:274] - INFO: epoch 004:    332 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=93.5, ups=0.83, wpb=112.8, bsz=40, num_updates=6850, lr=3.42125e-05, gnorm=0.884, clip=40, loss_scale=1024, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=19347
2023-02-22 16:55:49 - progress_bar.py[line:274] - INFO: epoch 004:    342 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=87.5, ups=0.77, wpb=113, bsz=40, num_updates=6860, lr=3.41856e-05, gnorm=0.711, clip=20, loss_scale=1024, train_wall=13, gb_free=10.3, ema_decay=0.9999, wall=19360
2023-02-22 16:56:02 - progress_bar.py[line:274] - INFO: epoch 004:    352 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=87.5, ups=0.77, wpb=113.2, bsz=40, num_updates=6870, lr=3.41587e-05, gnorm=1.043, clip=40, loss_scale=1024, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19373
2023-02-22 16:56:15 - progress_bar.py[line:274] - INFO: epoch 004:    362 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=85.7, ups=0.76, wpb=112.1, bsz=40, num_updates=6880, lr=3.41318e-05, gnorm=0.705, clip=10, loss_scale=1024, train_wall=13, gb_free=10.1, ema_decay=0.9999, wall=19386
2023-02-22 16:56:28 - progress_bar.py[line:274] - INFO: epoch 004:    372 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=85.8, ups=0.77, wpb=111.7, bsz=40, num_updates=6890, lr=3.4105e-05, gnorm=0.814, clip=20, loss_scale=1024, train_wall=13, gb_free=10.3, ema_decay=0.9999, wall=19399
2023-02-22 16:56:41 - progress_bar.py[line:274] - INFO: epoch 004:    382 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=89.9, ups=0.8, wpb=112.6, bsz=40, num_updates=6900, lr=3.40781e-05, gnorm=0.804, clip=30, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19412
2023-02-22 16:56:54 - progress_bar.py[line:274] - INFO: epoch 004:    392 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=87.3, ups=0.78, wpb=112, bsz=40, num_updates=6910, lr=3.40512e-05, gnorm=0.933, clip=30, loss_scale=1024, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=19425
2023-02-22 16:57:07 - progress_bar.py[line:274] - INFO: epoch 004:    402 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=88.8, ups=0.78, wpb=113.2, bsz=40, num_updates=6920, lr=3.40243e-05, gnorm=0.857, clip=10, loss_scale=1024, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=19437
2023-02-22 16:57:19 - progress_bar.py[line:274] - INFO: epoch 004:    412 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=87.2, ups=0.79, wpb=110.9, bsz=40, num_updates=6930, lr=3.39974e-05, gnorm=0.812, clip=20, loss_scale=1024, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=19450
2023-02-22 16:57:30 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 16:57:32 - progress_bar.py[line:274] - INFO: epoch 004:    423 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=88.4, ups=0.8, wpb=111.1, bsz=40, num_updates=6940, lr=3.39705e-05, gnorm=0.714, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19463
2023-02-22 16:57:43 - progress_bar.py[line:274] - INFO: epoch 004:    433 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111.7, bsz=40, num_updates=6950, lr=3.39436e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19474
2023-02-22 16:57:54 - progress_bar.py[line:274] - INFO: epoch 004:    443 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.8, bsz=40, num_updates=6960, lr=3.39168e-05, gnorm=0.962, clip=40, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=19485
2023-02-22 16:58:06 - progress_bar.py[line:274] - INFO: epoch 004:    453 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=112.9, bsz=40, num_updates=6970, lr=3.38899e-05, gnorm=0.867, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19496
@@@@ ERROR IN DATA @@@@ play
2023-02-22 16:58:17 - progress_bar.py[line:274] - INFO: epoch 004:    463 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.9, wpb=112.7, bsz=40, num_updates=6980, lr=3.3863e-05, gnorm=1.081, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19507
2023-02-22 16:58:28 - progress_bar.py[line:274] - INFO: epoch 004:    473 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.89, wpb=113, bsz=40, num_updates=6990, lr=3.38361e-05, gnorm=0.927, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19519
2023-02-22 16:58:39 - progress_bar.py[line:274] - INFO: epoch 004:    483 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.88, wpb=113, bsz=40, num_updates=7000, lr=3.38092e-05, gnorm=0.932, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19530
2023-02-22 16:58:50 - progress_bar.py[line:274] - INFO: epoch 004:    493 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.2, ups=0.91, wpb=111.6, bsz=40, num_updates=7010, lr=3.37823e-05, gnorm=1.014, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19541
2023-02-22 16:59:02 - progress_bar.py[line:274] - INFO: epoch 004:    503 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.7, ups=0.87, wpb=111.7, bsz=40, num_updates=7020, lr=3.37554e-05, gnorm=0.692, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19553
2023-02-22 16:59:13 - progress_bar.py[line:274] - INFO: epoch 004:    513 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.88, wpb=111.8, bsz=40, num_updates=7030, lr=3.37286e-05, gnorm=0.671, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19564
2023-02-22 16:59:25 - progress_bar.py[line:274] - INFO: epoch 004:    523 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.89, wpb=113.4, bsz=40, num_updates=7040, lr=3.37017e-05, gnorm=0.551, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19575
2023-02-22 16:59:36 - progress_bar.py[line:274] - INFO: epoch 004:    533 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.9, wpb=112.3, bsz=40, num_updates=7050, lr=3.36748e-05, gnorm=0.804, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19586
2023-02-22 16:59:47 - progress_bar.py[line:274] - INFO: epoch 004:    543 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.3, ups=0.92, wpb=112.8, bsz=40, num_updates=7060, lr=3.36479e-05, gnorm=0.915, clip=40, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=19597
2023-02-22 16:59:58 - progress_bar.py[line:274] - INFO: epoch 004:    553 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.88, wpb=113.2, bsz=40, num_updates=7070, lr=3.3621e-05, gnorm=0.621, clip=10, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=19609
2023-02-22 17:00:09 - progress_bar.py[line:274] - INFO: epoch 004:    563 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.88, wpb=111.2, bsz=40, num_updates=7080, lr=3.35941e-05, gnorm=0.801, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19620
2023-02-22 17:00:21 - progress_bar.py[line:274] - INFO: epoch 004:    573 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.7, ups=0.88, wpb=110.4, bsz=40, num_updates=7090, lr=3.35672e-05, gnorm=0.66, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19631
2023-02-22 17:00:32 - progress_bar.py[line:274] - INFO: epoch 004:    583 / 2175 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.1, ups=0.86, wpb=111.8, bsz=40, num_updates=7100, lr=3.35404e-05, gnorm=0.917, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=19643
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 17:00:44 - progress_bar.py[line:274] - INFO: epoch 004:    593 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.88, wpb=112.2, bsz=40, num_updates=7110, lr=3.35135e-05, gnorm=0.86, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19655
2023-02-22 17:00:57 - progress_bar.py[line:274] - INFO: epoch 004:    603 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.88, wpb=113.8, bsz=40, num_updates=7120, lr=3.34866e-05, gnorm=0.756, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19666
2023-02-22 17:01:08 - progress_bar.py[line:274] - INFO: epoch 004:    613 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=7130, lr=3.34597e-05, gnorm=0.74, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19679
2023-02-22 17:01:19 - progress_bar.py[line:274] - INFO: epoch 004:    623 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.9, wpb=111.4, bsz=40, num_updates=7140, lr=3.34328e-05, gnorm=0.701, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19690
2023-02-22 17:01:31 - progress_bar.py[line:274] - INFO: epoch 004:    633 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.88, wpb=112.4, bsz=40, num_updates=7150, lr=3.34059e-05, gnorm=0.788, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19702
2023-02-22 17:01:42 - progress_bar.py[line:274] - INFO: epoch 004:    643 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=111.8, bsz=40, num_updates=7160, lr=3.3379e-05, gnorm=0.733, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19713
2023-02-22 17:01:53 - progress_bar.py[line:274] - INFO: epoch 004:    653 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=112, bsz=40, num_updates=7170, lr=3.33522e-05, gnorm=0.687, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19724
2023-02-22 17:02:07 - progress_bar.py[line:274] - INFO: epoch 004:    663 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.87, wpb=113.8, bsz=40, num_updates=7180, lr=3.33253e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19735
@@@@ ERROR IN DATA @@@@ watch
2023-02-22 17:02:18 - progress_bar.py[line:274] - INFO: epoch 004:    673 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=112.5, bsz=40, num_updates=7190, lr=3.32984e-05, gnorm=0.992, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19749
2023-02-22 17:02:29 - progress_bar.py[line:274] - INFO: epoch 004:    683 / 2175 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.9, ups=0.91, wpb=111.8, bsz=40, num_updates=7200, lr=3.32715e-05, gnorm=0.769, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19760
2023-02-22 17:02:41 - progress_bar.py[line:274] - INFO: epoch 004:    693 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.88, wpb=113.8, bsz=40, num_updates=7210, lr=3.32446e-05, gnorm=0.597, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19771
2023-02-22 17:02:52 - progress_bar.py[line:274] - INFO: epoch 004:    703 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=112.7, bsz=40, num_updates=7220, lr=3.32177e-05, gnorm=0.865, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19783
2023-02-22 17:03:03 - progress_bar.py[line:274] - INFO: epoch 004:    713 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.88, wpb=113.6, bsz=40, num_updates=7230, lr=3.31908e-05, gnorm=0.724, clip=0, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=19794
2023-02-22 17:03:15 - progress_bar.py[line:274] - INFO: epoch 004:    723 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.89, wpb=112.5, bsz=40, num_updates=7240, lr=3.3164e-05, gnorm=1.067, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19805
2023-02-22 17:03:26 - progress_bar.py[line:274] - INFO: epoch 004:    733 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.88, wpb=111.7, bsz=40, num_updates=7250, lr=3.31371e-05, gnorm=1.137, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19817
2023-02-22 17:03:37 - progress_bar.py[line:274] - INFO: epoch 004:    743 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.89, wpb=112.7, bsz=40, num_updates=7260, lr=3.31102e-05, gnorm=1.051, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19828
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 17:03:49 - progress_bar.py[line:274] - INFO: epoch 004:    753 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111.7, bsz=40, num_updates=7270, lr=3.30833e-05, gnorm=0.838, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19839
2023-02-22 17:04:00 - progress_bar.py[line:274] - INFO: epoch 004:    763 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=112.4, bsz=40, num_updates=7280, lr=3.30564e-05, gnorm=0.909, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19850
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 17:04:11 - progress_bar.py[line:274] - INFO: epoch 004:    773 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.4, ups=0.92, wpb=112, bsz=40, num_updates=7290, lr=3.30295e-05, gnorm=1.143, clip=50, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=19861
2023-02-22 17:04:22 - progress_bar.py[line:274] - INFO: epoch 004:    783 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.89, wpb=113.2, bsz=40, num_updates=7300, lr=3.30026e-05, gnorm=0.814, clip=30, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=19873
2023-02-22 17:04:33 - progress_bar.py[line:274] - INFO: epoch 004:    793 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=112.9, bsz=40, num_updates=7310, lr=3.29757e-05, gnorm=0.485, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19884
2023-02-22 17:04:44 - progress_bar.py[line:274] - INFO: epoch 004:    803 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.5, ups=0.92, wpb=113.1, bsz=40, num_updates=7320, lr=3.29489e-05, gnorm=0.702, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=19895
2023-02-22 17:04:55 - progress_bar.py[line:274] - INFO: epoch 004:    813 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.88, wpb=113.2, bsz=40, num_updates=7330, lr=3.2922e-05, gnorm=0.916, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19906
2023-02-22 17:05:06 - progress_bar.py[line:274] - INFO: epoch 004:    823 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.9, wpb=111.8, bsz=40, num_updates=7340, lr=3.28951e-05, gnorm=0.819, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=19917
2023-02-22 17:05:18 - progress_bar.py[line:274] - INFO: epoch 004:    833 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.88, wpb=111.5, bsz=40, num_updates=7350, lr=3.28682e-05, gnorm=0.769, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19929
2023-02-22 17:05:29 - progress_bar.py[line:274] - INFO: epoch 004:    843 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.87, wpb=113.8, bsz=40, num_updates=7360, lr=3.28413e-05, gnorm=0.608, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19940
2023-02-22 17:05:40 - progress_bar.py[line:274] - INFO: epoch 004:    853 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.4, ups=0.92, wpb=113.1, bsz=40, num_updates=7370, lr=3.28144e-05, gnorm=0.731, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19951
2023-02-22 17:05:51 - progress_bar.py[line:274] - INFO: epoch 004:    863 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111.9, bsz=40, num_updates=7380, lr=3.27875e-05, gnorm=0.578, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=19962
2023-02-22 17:06:03 - progress_bar.py[line:274] - INFO: epoch 004:    873 / 2175 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.87, wpb=112.9, bsz=40, num_updates=7390, lr=3.27607e-05, gnorm=0.969, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19974
2023-02-22 17:06:14 - progress_bar.py[line:274] - INFO: epoch 004:    883 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=111.7, bsz=40, num_updates=7400, lr=3.27338e-05, gnorm=0.696, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19985
2023-02-22 17:06:26 - progress_bar.py[line:274] - INFO: epoch 004:    893 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.86, wpb=112.7, bsz=40, num_updates=7410, lr=3.27069e-05, gnorm=0.853, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19996
2023-02-22 17:06:37 - progress_bar.py[line:274] - INFO: epoch 004:    903 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=112.7, bsz=40, num_updates=7420, lr=3.268e-05, gnorm=0.91, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20008
2023-02-22 17:06:48 - progress_bar.py[line:274] - INFO: epoch 004:    913 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.89, wpb=111.3, bsz=40, num_updates=7430, lr=3.26531e-05, gnorm=1.055, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20019
2023-02-22 17:06:59 - progress_bar.py[line:274] - INFO: epoch 004:    923 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.87, wpb=112.6, bsz=40, num_updates=7440, lr=3.26262e-05, gnorm=1.164, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20030
2023-02-22 17:07:10 - progress_bar.py[line:274] - INFO: epoch 004:    933 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.6, ups=0.92, wpb=112.3, bsz=40, num_updates=7450, lr=3.25993e-05, gnorm=0.738, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20041
2023-02-22 17:07:21 - progress_bar.py[line:274] - INFO: epoch 004:    943 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=111.6, bsz=40, num_updates=7460, lr=3.25725e-05, gnorm=0.986, clip=40, loss_scale=1024, train_wall=11, gb_free=9.3, ema_decay=0.9999, wall=20052
2023-02-22 17:07:32 - progress_bar.py[line:274] - INFO: epoch 004:    953 / 2175 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.92, wpb=111.1, bsz=40, num_updates=7470, lr=3.25456e-05, gnorm=0.825, clip=40, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=20063
2023-02-22 17:07:44 - progress_bar.py[line:274] - INFO: epoch 004:    963 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.9, wpb=112.7, bsz=40, num_updates=7480, lr=3.25187e-05, gnorm=0.814, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20074
2023-02-22 17:07:55 - progress_bar.py[line:274] - INFO: epoch 004:    973 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.91, wpb=111, bsz=40, num_updates=7490, lr=3.24918e-05, gnorm=0.832, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20085
2023-02-22 17:08:06 - progress_bar.py[line:274] - INFO: epoch 004:    983 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97, ups=0.86, wpb=112.6, bsz=40, num_updates=7500, lr=3.24649e-05, gnorm=0.699, clip=20, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=20097
2023-02-22 17:08:18 - progress_bar.py[line:274] - INFO: epoch 004:    993 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.88, wpb=112.3, bsz=40, num_updates=7510, lr=3.2438e-05, gnorm=0.839, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20108
2023-02-22 17:08:29 - progress_bar.py[line:274] - INFO: epoch 004:   1003 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112.6, bsz=40, num_updates=7520, lr=3.24111e-05, gnorm=0.977, clip=40, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=20120
2023-02-22 17:08:40 - progress_bar.py[line:274] - INFO: epoch 004:   1013 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.89, wpb=111.6, bsz=40, num_updates=7530, lr=3.23843e-05, gnorm=0.806, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20131
2023-02-22 17:08:51 - progress_bar.py[line:274] - INFO: epoch 004:   1023 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.89, wpb=112.7, bsz=40, num_updates=7540, lr=3.23574e-05, gnorm=0.877, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20142
2023-02-22 17:09:03 - progress_bar.py[line:274] - INFO: epoch 004:   1033 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=111.4, bsz=40, num_updates=7550, lr=3.23305e-05, gnorm=0.887, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20153
2023-02-22 17:09:14 - progress_bar.py[line:274] - INFO: epoch 004:   1043 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.6, bsz=40, num_updates=7560, lr=3.23036e-05, gnorm=0.567, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20165
2023-02-22 17:09:25 - progress_bar.py[line:274] - INFO: epoch 004:   1053 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.89, wpb=112.9, bsz=40, num_updates=7570, lr=3.22767e-05, gnorm=0.882, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20176
2023-02-22 17:09:37 - progress_bar.py[line:274] - INFO: epoch 004:   1063 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.87, wpb=114.2, bsz=40, num_updates=7580, lr=3.22498e-05, gnorm=0.854, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20187
2023-02-22 17:09:48 - progress_bar.py[line:274] - INFO: epoch 004:   1073 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.89, wpb=112.1, bsz=40, num_updates=7590, lr=3.22229e-05, gnorm=0.909, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20199
2023-02-22 17:09:59 - progress_bar.py[line:274] - INFO: epoch 004:   1083 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.88, wpb=111.9, bsz=40, num_updates=7600, lr=3.21961e-05, gnorm=1.042, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20210
2023-02-22 17:10:10 - progress_bar.py[line:274] - INFO: epoch 004:   1093 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.89, wpb=111.7, bsz=40, num_updates=7610, lr=3.21692e-05, gnorm=1.021, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20221
2023-02-22 17:10:22 - progress_bar.py[line:274] - INFO: epoch 004:   1103 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.87, wpb=111.9, bsz=40, num_updates=7620, lr=3.21423e-05, gnorm=1.18, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20233
@@@@ ERROR IN DATA @@@@ play
2023-02-22 17:10:33 - progress_bar.py[line:274] - INFO: epoch 004:   1113 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.7, ups=0.87, wpb=110.9, bsz=40, num_updates=7630, lr=3.21154e-05, gnorm=0.782, clip=30, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=20244
2023-02-22 17:10:45 - progress_bar.py[line:274] - INFO: epoch 004:   1123 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.89, wpb=110.4, bsz=40, num_updates=7640, lr=3.20885e-05, gnorm=0.805, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20255
2023-02-22 17:10:56 - progress_bar.py[line:274] - INFO: epoch 004:   1133 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=112.6, bsz=40, num_updates=7650, lr=3.20616e-05, gnorm=0.815, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20267
2023-02-22 17:11:07 - progress_bar.py[line:274] - INFO: epoch 004:   1143 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=113.3, bsz=40, num_updates=7660, lr=3.20347e-05, gnorm=0.597, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20278
2023-02-22 17:11:16 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 17:11:19 - progress_bar.py[line:274] - INFO: epoch 004:   1154 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.6, ups=0.85, wpb=112.3, bsz=40, num_updates=7670, lr=3.20079e-05, gnorm=0.916, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=20290
2023-02-22 17:11:30 - progress_bar.py[line:274] - INFO: epoch 004:   1164 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.88, wpb=112.5, bsz=40, num_updates=7680, lr=3.1981e-05, gnorm=0.667, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20301
2023-02-22 17:11:42 - progress_bar.py[line:274] - INFO: epoch 004:   1174 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.88, wpb=112, bsz=40, num_updates=7690, lr=3.19541e-05, gnorm=0.985, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=20312
2023-02-22 17:11:53 - progress_bar.py[line:274] - INFO: epoch 004:   1184 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.89, wpb=111.6, bsz=40, num_updates=7700, lr=3.19272e-05, gnorm=0.727, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20324
2023-02-22 17:12:04 - progress_bar.py[line:274] - INFO: epoch 004:   1194 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=112.2, bsz=40, num_updates=7710, lr=3.19003e-05, gnorm=0.926, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20335
2023-02-22 17:12:16 - progress_bar.py[line:274] - INFO: epoch 004:   1204 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.89, wpb=111.1, bsz=40, num_updates=7720, lr=3.18734e-05, gnorm=0.793, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20346
2023-02-22 17:12:27 - progress_bar.py[line:274] - INFO: epoch 004:   1214 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.88, wpb=112, bsz=40, num_updates=7730, lr=3.18465e-05, gnorm=0.797, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20358
2023-02-22 17:12:38 - progress_bar.py[line:274] - INFO: epoch 004:   1224 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=112.2, bsz=40, num_updates=7740, lr=3.18196e-05, gnorm=0.809, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20369
2023-02-22 17:12:50 - progress_bar.py[line:274] - INFO: epoch 004:   1234 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.88, wpb=111.8, bsz=40, num_updates=7750, lr=3.17928e-05, gnorm=0.911, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20381
2023-02-22 17:13:01 - progress_bar.py[line:274] - INFO: epoch 004:   1244 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.88, wpb=111.9, bsz=40, num_updates=7760, lr=3.17659e-05, gnorm=0.648, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20392
2023-02-22 17:13:12 - progress_bar.py[line:274] - INFO: epoch 004:   1254 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.1, ups=0.92, wpb=113.5, bsz=40, num_updates=7770, lr=3.1739e-05, gnorm=0.806, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20403
2023-02-22 17:13:23 - progress_bar.py[line:274] - INFO: epoch 004:   1264 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.9, wpb=112.1, bsz=40, num_updates=7780, lr=3.17121e-05, gnorm=0.879, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20414
2023-02-22 17:13:34 - progress_bar.py[line:274] - INFO: epoch 004:   1274 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.2, ups=0.93, wpb=112.2, bsz=40, num_updates=7790, lr=3.16852e-05, gnorm=0.802, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20425
2023-02-22 17:13:45 - progress_bar.py[line:274] - INFO: epoch 004:   1284 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.88, wpb=112.4, bsz=40, num_updates=7800, lr=3.16583e-05, gnorm=0.862, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20436
2023-02-22 17:13:57 - progress_bar.py[line:274] - INFO: epoch 004:   1294 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.9, wpb=112.8, bsz=40, num_updates=7810, lr=3.16314e-05, gnorm=0.652, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20447
2023-02-22 17:14:07 - progress_bar.py[line:274] - INFO: epoch 004:   1304 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=111.6, bsz=40, num_updates=7820, lr=3.16046e-05, gnorm=0.654, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20458
2023-02-22 17:14:19 - progress_bar.py[line:274] - INFO: epoch 004:   1314 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.9, bsz=40, num_updates=7830, lr=3.15777e-05, gnorm=0.584, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20470
2023-02-22 17:14:30 - progress_bar.py[line:274] - INFO: epoch 004:   1324 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.89, wpb=112.5, bsz=40, num_updates=7840, lr=3.15508e-05, gnorm=0.937, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20481
2023-02-22 17:14:42 - progress_bar.py[line:274] - INFO: epoch 004:   1334 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.88, wpb=111.9, bsz=40, num_updates=7850, lr=3.15239e-05, gnorm=0.572, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20492
2023-02-22 17:14:53 - progress_bar.py[line:274] - INFO: epoch 004:   1344 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=112.1, bsz=40, num_updates=7860, lr=3.1497e-05, gnorm=1.037, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20504
2023-02-22 17:15:04 - progress_bar.py[line:274] - INFO: epoch 004:   1354 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=7870, lr=3.14701e-05, gnorm=0.566, clip=0, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=20515
2023-02-22 17:15:16 - progress_bar.py[line:274] - INFO: epoch 004:   1364 / 2175 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.88, wpb=111.9, bsz=40, num_updates=7880, lr=3.14432e-05, gnorm=0.995, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20526
2023-02-22 17:15:27 - progress_bar.py[line:274] - INFO: epoch 004:   1374 / 2175 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.91, wpb=111.3, bsz=40, num_updates=7890, lr=3.14164e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20537
2023-02-22 17:15:38 - progress_bar.py[line:274] - INFO: epoch 004:   1384 / 2175 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.9, wpb=112.6, bsz=40, num_updates=7900, lr=3.13895e-05, gnorm=0.737, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20548
2023-02-22 17:15:49 - progress_bar.py[line:274] - INFO: epoch 004:   1394 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.9, wpb=112.7, bsz=40, num_updates=7910, lr=3.13626e-05, gnorm=0.951, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20560
2023-02-22 17:16:00 - progress_bar.py[line:274] - INFO: epoch 004:   1404 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.9, wpb=112, bsz=40, num_updates=7920, lr=3.13357e-05, gnorm=0.986, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20571
2023-02-22 17:16:11 - progress_bar.py[line:274] - INFO: epoch 004:   1414 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=112.4, bsz=40, num_updates=7930, lr=3.13088e-05, gnorm=0.86, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20582
2023-02-22 17:16:22 - progress_bar.py[line:274] - INFO: epoch 004:   1424 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.7, ups=0.92, wpb=111.9, bsz=40, num_updates=7940, lr=3.12819e-05, gnorm=0.631, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20593
2023-02-22 17:16:33 - progress_bar.py[line:274] - INFO: epoch 004:   1434 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.92, wpb=112, bsz=40, num_updates=7950, lr=3.1255e-05, gnorm=0.594, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20604
2023-02-22 17:16:44 - progress_bar.py[line:274] - INFO: epoch 004:   1444 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112.3, bsz=40, num_updates=7960, lr=3.12282e-05, gnorm=0.588, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20615
2023-02-22 17:16:55 - progress_bar.py[line:274] - INFO: epoch 004:   1454 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=112.5, bsz=40, num_updates=7970, lr=3.12013e-05, gnorm=0.698, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20626
2023-02-22 17:17:07 - progress_bar.py[line:274] - INFO: epoch 004:   1464 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=111.8, bsz=40, num_updates=7980, lr=3.11744e-05, gnorm=0.669, clip=20, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=20637
2023-02-22 17:17:18 - progress_bar.py[line:274] - INFO: epoch 004:   1474 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.7, ups=0.9, wpb=111.7, bsz=40, num_updates=7990, lr=3.11475e-05, gnorm=1.012, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20649
2023-02-22 17:17:29 - progress_bar.py[line:274] - INFO: epoch 004:   1484 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.88, wpb=113.1, bsz=40, num_updates=8000, lr=3.11206e-05, gnorm=0.842, clip=30, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=20660
2023-02-22 17:17:29 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 17:17:30 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 17:17:30 - train.py[line:551] - INFO: load:1.04 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 17:19:32 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 17:19:32 - train.py[line:551] - INFO: load:1.07 valid_run:121.87 task_valid:118.51 collect_output:2.27
2023-02-22 17:21:33 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 17:21:33 - train.py[line:551] - INFO: load:1.09 valid_run:242.11 task_valid:233.99 collect_output:5.98
2023-02-22 17:23:35 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 17:23:35 - train.py[line:551] - INFO: load:1.12 valid_run:364.45 task_valid:350.14 collect_output:11.05
2023-02-22 17:25:37 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 17:25:37 - train.py[line:551] - INFO: load:1.14 valid_run:486.85 task_valid:463.17 collect_output:19.36
2023-02-22 17:27:38 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 17:27:38 - train.py[line:551] - INFO: load:1.17 valid_run:607.45 task_valid:579.88 collect_output:22.21
2023-02-22 17:29:41 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 17:29:41 - train.py[line:551] - INFO: load:1.20 valid_run:730.38 task_valid:698.17 collect_output:25.82
2023-02-22 17:31:45 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 17:31:45 - train.py[line:551] - INFO: load:1.22 valid_run:853.81 task_valid:816.22 collect_output:30.17
2023-02-22 17:33:47 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 17:33:47 - train.py[line:551] - INFO: load:1.25 valid_run:975.94 task_valid:932.44 collect_output:35.02
2023-02-22 17:35:51 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 17:35:51 - train.py[line:551] - INFO: load:1.27 valid_run:1100.04 task_valid:1049.10 collect_output:41.42
2023-02-22 17:37:53 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 17:37:53 - train.py[line:551] - INFO: load:1.30 valid_run:1222.33 task_valid:1161.21 collect_output:50.55
2023-02-22 17:39:54 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 17:39:54 - train.py[line:551] - INFO: load:1.33 valid_run:1342.72 task_valid:1276.30 collect_output:54.84
2023-02-22 17:41:56 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 17:41:56 - train.py[line:551] - INFO: load:1.35 valid_run:1464.54 task_valid:1392.80 collect_output:59.11
2023-02-22 17:43:55 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 17:43:55 - train.py[line:551] - INFO: load:1.38 valid_run:1583.63 task_valid:1506.04 collect_output:63.92
2023-02-22 17:45:56 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 17:45:56 - train.py[line:551] - INFO: load:1.40 valid_run:1704.72 task_valid:1623.46 collect_output:66.54
2023-02-22 17:47:57 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 17:47:57 - train.py[line:551] - INFO: load:1.43 valid_run:1825.95 task_valid:1739.37 collect_output:70.81
2023-02-22 17:49:59 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 17:49:59 - train.py[line:551] - INFO: load:1.46 valid_run:1947.52 task_valid:1852.88 collect_output:77.81
2023-02-22 17:52:01 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 17:52:01 - train.py[line:551] - INFO: load:1.48 valid_run:2069.25 task_valid:1968.59 collect_output:82.78
2023-02-22 17:54:02 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 17:54:02 - train.py[line:551] - INFO: load:1.51 valid_run:2190.07 task_valid:2086.15 collect_output:85.02
2023-02-22 17:56:03 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 17:56:03 - train.py[line:551] - INFO: load:1.54 valid_run:2311.46 task_valid:2202.67 collect_output:88.83
2023-02-22 17:58:04 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 17:58:04 - train.py[line:551] - INFO: load:1.56 valid_run:2431.99 task_valid:2318.78 collect_output:92.18
2023-02-22 18:00:06 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 18:00:06 - train.py[line:551] - INFO: load:1.59 valid_run:2553.87 task_valid:2434.97 collect_output:96.82
2023-02-22 18:02:08 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 18:02:08 - train.py[line:551] - INFO: load:1.62 valid_run:2676.05 task_valid:2553.61 collect_output:99.29
2023-02-22 18:04:08 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 18:04:08 - train.py[line:551] - INFO: load:1.65 valid_run:2796.68 task_valid:2667.37 collect_output:105.10
2023-02-22 18:06:09 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 18:06:09 - train.py[line:551] - INFO: load:1.68 valid_run:2917.00 task_valid:2783.12 collect_output:108.60
2023-02-22 18:08:11 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 18:08:11 - train.py[line:551] - INFO: load:1.70 valid_run:3038.96 task_valid:2898.85 collect_output:113.73
2023-02-22 18:10:14 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 18:10:14 - train.py[line:551] - INFO: load:1.73 valid_run:3162.37 task_valid:3014.25 collect_output:120.65
2023-02-22 18:12:14 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 18:12:14 - train.py[line:551] - INFO: load:1.76 valid_run:3282.25 task_valid:3127.93 collect_output:125.78
2023-02-22 18:14:16 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 18:14:16 - train.py[line:551] - INFO: load:1.79 valid_run:3404.18 task_valid:3247.10 collect_output:127.46
2023-02-22 18:16:18 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 18:16:18 - train.py[line:551] - INFO: load:1.81 valid_run:3526.08 task_valid:3362.08 collect_output:133.35
2023-02-22 18:18:20 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 18:18:20 - train.py[line:551] - INFO: load:1.84 valid_run:3647.85 task_valid:3479.94 collect_output:136.21
2023-02-22 18:20:21 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 18:20:21 - train.py[line:551] - INFO: load:1.87 valid_run:3768.83 task_valid:3597.95 collect_output:138.12

====================================================================================================
SGG eval:     R @ 50: 0.6553;     R @ 100: 0.6991;     R @ 500: 0.7275;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4447;    mR @ 100: 0.4969;    mR @ 500: 0.5435;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6250) (covering:0.2286) (eating:0.8235) (flying in:0.7727) (growing on:0.2500) (hanging from:0.4839) (lying on:0.4000) (mounted on:0.1429) (painted on:0.5000) (parked on:0.9583) (playing:0.0000) (riding:0.9353) (says:0.0000) (sitting on:0.7066) (standing on:0.5677) (using:0.6000) (walking in:0.0000) (walking on:0.5225) (watching:0.6528) 
--------------------------------------------------------
====================================================================================================

2023-02-22 18:20:52 - train.py[line:487] - INFO: 0.6990724216959511
2023-02-22 18:20:52 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.6553;     R @ 100: 0.6991;     R @ 500: 0.7275;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4447;    mR @ 100: 0.4969;    mR @ 500: 0.5435;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6250) (covering:0.2286) (eating:0.8235) (flying in:0.7727) (growing on:0.2500) (hanging from:0.4839) (lying on:0.4000) (mounted on:0.1429) (painted on:0.5000) (parked on:0.9583) (playing:0.0000) (riding:0.9353) (says:0.0000) (sitting on:0.7066) (standing on:0.5677) (using:0.6000) (walking in:0.0000) (walking on:0.5225) (watching:0.6528) 
--------------------------------------------------------
====================================================================================================

2023-02-22 18:20:52 - progress_bar.py[line:282] - INFO: epoch 004 | valid on 'valid' subset | loss 0.248 | loss_v1 0 | loss_v2 0 | nll_loss 0.078 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.699072 | ppl 1.06 | vqa_score 0.5417 | wps 118 | wpb 72 | bsz 24 | num_updates 8000 | best_R@100 0.699072
2023-02-22 18:20:52 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 4 @ 8000 updates
2023-02-22 18:20:52 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_4_8000.pt
2023-02-22 18:20:58 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_4_8000.pt
2023-02-22 18:21:03 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_4_8000.pt (epoch 4 @ 8000 updates, score 0.6990724216959511) (writing took 10.912326108664274 seconds)
2023-02-22 18:21:14 - progress_bar.py[line:274] - INFO: epoch 004:   1494 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=111.9, bsz=40, num_updates=8010, lr=3.10937e-05, gnorm=0.78, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24485
2023-02-22 18:21:26 - progress_bar.py[line:274] - INFO: epoch 004:   1504 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=112.4, bsz=40, num_updates=8020, lr=3.10668e-05, gnorm=0.88, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24496
2023-02-22 18:21:37 - progress_bar.py[line:274] - INFO: epoch 004:   1514 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.4, ups=0.92, wpb=112.4, bsz=40, num_updates=8030, lr=3.104e-05, gnorm=0.972, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24508
2023-02-22 18:21:49 - progress_bar.py[line:274] - INFO: epoch 004:   1524 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.89, wpb=111.7, bsz=40, num_updates=8040, lr=3.10131e-05, gnorm=0.756, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24519
2023-02-22 18:22:00 - progress_bar.py[line:274] - INFO: epoch 004:   1534 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.89, wpb=113.2, bsz=40, num_updates=8050, lr=3.09862e-05, gnorm=0.856, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24531
2023-02-22 18:22:11 - progress_bar.py[line:274] - INFO: epoch 004:   1544 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.6, ups=0.94, wpb=112.5, bsz=40, num_updates=8060, lr=3.09593e-05, gnorm=0.869, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24541
2023-02-22 18:22:22 - progress_bar.py[line:274] - INFO: epoch 004:   1554 / 2175 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.88, wpb=112.5, bsz=40, num_updates=8070, lr=3.09324e-05, gnorm=1.003, clip=40, loss_scale=512, train_wall=11, gb_free=9.2, ema_decay=0.9999, wall=24553
2023-02-22 18:22:33 - progress_bar.py[line:274] - INFO: epoch 004:   1564 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.89, wpb=112.1, bsz=40, num_updates=8080, lr=3.09055e-05, gnorm=0.697, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=24564
2023-02-22 18:22:44 - progress_bar.py[line:274] - INFO: epoch 004:   1574 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.3, ups=0.91, wpb=113.4, bsz=40, num_updates=8090, lr=3.08786e-05, gnorm=0.673, clip=10, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=24575
2023-02-22 18:22:56 - progress_bar.py[line:274] - INFO: epoch 004:   1584 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.87, wpb=111.8, bsz=40, num_updates=8100, lr=3.08518e-05, gnorm=0.657, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=24587
2023-02-22 18:23:07 - progress_bar.py[line:274] - INFO: epoch 004:   1594 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.9, wpb=112.4, bsz=40, num_updates=8110, lr=3.08249e-05, gnorm=0.828, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24598
2023-02-22 18:23:18 - progress_bar.py[line:274] - INFO: epoch 004:   1604 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.93, wpb=110.2, bsz=40, num_updates=8120, lr=3.0798e-05, gnorm=0.581, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24608
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 18:23:29 - progress_bar.py[line:274] - INFO: epoch 004:   1614 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.9, wpb=113.2, bsz=40, num_updates=8130, lr=3.07711e-05, gnorm=0.837, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24620
2023-02-22 18:23:40 - progress_bar.py[line:274] - INFO: epoch 004:   1624 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.89, wpb=112.6, bsz=40, num_updates=8140, lr=3.07442e-05, gnorm=0.843, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=24631
2023-02-22 18:23:51 - progress_bar.py[line:274] - INFO: epoch 004:   1634 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.9, wpb=112.6, bsz=40, num_updates=8150, lr=3.07173e-05, gnorm=0.859, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24642
2023-02-22 18:24:03 - progress_bar.py[line:274] - INFO: epoch 004:   1644 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=112.8, bsz=40, num_updates=8160, lr=3.06904e-05, gnorm=0.906, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24653
2023-02-22 18:24:14 - progress_bar.py[line:274] - INFO: epoch 004:   1654 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.87, wpb=111.7, bsz=40, num_updates=8170, lr=3.06635e-05, gnorm=0.994, clip=30, loss_scale=512, train_wall=11, gb_free=9.2, ema_decay=0.9999, wall=24665
2023-02-22 18:24:25 - progress_bar.py[line:274] - INFO: epoch 004:   1664 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.9, wpb=113.7, bsz=40, num_updates=8180, lr=3.06367e-05, gnorm=0.9, clip=50, loss_scale=1024, train_wall=11, gb_free=9.2, ema_decay=0.9999, wall=24676
2023-02-22 18:24:37 - progress_bar.py[line:274] - INFO: epoch 004:   1674 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.87, wpb=113.3, bsz=40, num_updates=8190, lr=3.06098e-05, gnorm=0.695, clip=30, loss_scale=1024, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=24687
2023-02-22 18:24:48 - progress_bar.py[line:274] - INFO: epoch 004:   1684 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.8, ups=0.93, wpb=112.2, bsz=40, num_updates=8200, lr=3.05829e-05, gnorm=0.886, clip=50, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=24698
2023-02-22 18:24:59 - progress_bar.py[line:274] - INFO: epoch 004:   1694 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.4, ups=0.9, wpb=111.9, bsz=40, num_updates=8210, lr=3.0556e-05, gnorm=1.076, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24710
2023-02-22 18:25:09 - progress_bar.py[line:274] - INFO: epoch 004:   1704 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=105.8, ups=0.94, wpb=112.7, bsz=40, num_updates=8220, lr=3.05291e-05, gnorm=0.971, clip=60, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=24720
2023-02-22 18:25:20 - progress_bar.py[line:274] - INFO: epoch 004:   1714 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=112.4, bsz=40, num_updates=8230, lr=3.05022e-05, gnorm=0.58, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24731
2023-02-22 18:25:32 - progress_bar.py[line:274] - INFO: epoch 004:   1724 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.9, wpb=112.9, bsz=40, num_updates=8240, lr=3.04753e-05, gnorm=0.687, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24742
2023-02-22 18:25:43 - progress_bar.py[line:274] - INFO: epoch 004:   1734 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.9, wpb=113.1, bsz=40, num_updates=8250, lr=3.04485e-05, gnorm=0.775, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=24753
2023-02-22 18:25:54 - progress_bar.py[line:274] - INFO: epoch 004:   1744 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.87, wpb=112.7, bsz=40, num_updates=8260, lr=3.04216e-05, gnorm=0.872, clip=50, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=24765
2023-02-22 18:26:05 - progress_bar.py[line:274] - INFO: epoch 004:   1754 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.7, ups=0.92, wpb=112.4, bsz=40, num_updates=8270, lr=3.03947e-05, gnorm=1.042, clip=20, loss_scale=1024, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=24776
2023-02-22 18:26:16 - progress_bar.py[line:274] - INFO: epoch 004:   1764 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=112.2, bsz=40, num_updates=8280, lr=3.03678e-05, gnorm=0.894, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24787
2023-02-22 18:26:28 - progress_bar.py[line:274] - INFO: epoch 004:   1774 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.88, wpb=113, bsz=40, num_updates=8290, lr=3.03409e-05, gnorm=0.661, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24799
2023-02-22 18:26:39 - progress_bar.py[line:274] - INFO: epoch 004:   1784 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.4, ups=0.92, wpb=113.2, bsz=40, num_updates=8300, lr=3.0314e-05, gnorm=0.95, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24809
2023-02-22 18:26:50 - progress_bar.py[line:274] - INFO: epoch 004:   1794 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.87, wpb=112.7, bsz=40, num_updates=8310, lr=3.02871e-05, gnorm=0.61, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24821
2023-02-22 18:27:01 - progress_bar.py[line:274] - INFO: epoch 004:   1804 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=112.3, bsz=40, num_updates=8320, lr=3.02603e-05, gnorm=0.764, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24832
2023-02-22 18:27:13 - progress_bar.py[line:274] - INFO: epoch 004:   1814 / 2175 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.89, wpb=111.9, bsz=40, num_updates=8330, lr=3.02334e-05, gnorm=1.025, clip=50, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=24844
2023-02-22 18:27:24 - progress_bar.py[line:274] - INFO: epoch 004:   1824 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=112.5, bsz=40, num_updates=8340, lr=3.02065e-05, gnorm=0.961, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24855
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 18:27:35 - progress_bar.py[line:274] - INFO: epoch 004:   1834 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.91, wpb=110.6, bsz=40, num_updates=8350, lr=3.01796e-05, gnorm=0.825, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24866
2023-02-22 18:27:47 - progress_bar.py[line:274] - INFO: epoch 004:   1844 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=112.3, bsz=40, num_updates=8360, lr=3.01527e-05, gnorm=0.66, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24877
2023-02-22 18:27:58 - progress_bar.py[line:274] - INFO: epoch 004:   1854 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.91, wpb=112.5, bsz=40, num_updates=8370, lr=3.01258e-05, gnorm=0.493, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24889
2023-02-22 18:28:09 - progress_bar.py[line:274] - INFO: epoch 004:   1864 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.89, wpb=112.8, bsz=40, num_updates=8380, lr=3.00989e-05, gnorm=0.809, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24900
2023-02-22 18:28:20 - progress_bar.py[line:274] - INFO: epoch 004:   1874 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=112.2, bsz=40, num_updates=8390, lr=3.00721e-05, gnorm=0.722, clip=20, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=24911
2023-02-22 18:28:32 - progress_bar.py[line:274] - INFO: epoch 004:   1884 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.86, wpb=112.8, bsz=40, num_updates=8400, lr=3.00452e-05, gnorm=0.723, clip=30, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=24923
2023-02-22 18:28:43 - progress_bar.py[line:274] - INFO: epoch 004:   1894 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=112.5, bsz=40, num_updates=8410, lr=3.00183e-05, gnorm=0.74, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24934
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 18:28:54 - progress_bar.py[line:274] - INFO: epoch 004:   1904 / 2175 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.9, wpb=111.5, bsz=40, num_updates=8420, lr=2.99914e-05, gnorm=0.722, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24945
2023-02-22 18:29:05 - progress_bar.py[line:274] - INFO: epoch 004:   1914 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.8, ups=0.91, wpb=112.9, bsz=40, num_updates=8430, lr=2.99645e-05, gnorm=1.128, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24956
2023-02-22 18:29:17 - progress_bar.py[line:274] - INFO: epoch 004:   1924 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.89, wpb=113.3, bsz=40, num_updates=8440, lr=2.99376e-05, gnorm=0.697, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24967
2023-02-22 18:29:28 - progress_bar.py[line:274] - INFO: epoch 004:   1934 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.5, ups=0.92, wpb=112, bsz=40, num_updates=8450, lr=2.99107e-05, gnorm=0.963, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24978
2023-02-22 18:29:39 - progress_bar.py[line:274] - INFO: epoch 004:   1944 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=113, bsz=40, num_updates=8460, lr=2.98839e-05, gnorm=0.649, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24990
2023-02-22 18:29:43 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 18:29:51 - progress_bar.py[line:274] - INFO: epoch 004:   1955 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=91.6, ups=0.81, wpb=112.4, bsz=40, num_updates=8470, lr=2.9857e-05, gnorm=0.822, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=25002
2023-02-22 18:30:02 - progress_bar.py[line:274] - INFO: epoch 004:   1965 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.89, wpb=114.1, bsz=40, num_updates=8480, lr=2.98301e-05, gnorm=0.722, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25013
2023-02-22 18:30:14 - progress_bar.py[line:274] - INFO: epoch 004:   1975 / 2175 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.87, wpb=111.8, bsz=40, num_updates=8490, lr=2.98032e-05, gnorm=0.773, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25025
2023-02-22 18:30:25 - progress_bar.py[line:274] - INFO: epoch 004:   1985 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.9, wpb=112.5, bsz=40, num_updates=8500, lr=2.97763e-05, gnorm=0.747, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25036
2023-02-22 18:30:36 - progress_bar.py[line:274] - INFO: epoch 004:   1995 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=106.6, ups=0.94, wpb=113.8, bsz=40, num_updates=8510, lr=2.97494e-05, gnorm=0.784, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25046
2023-02-22 18:30:47 - progress_bar.py[line:274] - INFO: epoch 004:   2005 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.3, ups=0.92, wpb=112.5, bsz=40, num_updates=8520, lr=2.97225e-05, gnorm=0.786, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25057
2023-02-22 18:30:58 - progress_bar.py[line:274] - INFO: epoch 004:   2015 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=8530, lr=2.96956e-05, gnorm=0.915, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25069
2023-02-22 18:31:09 - progress_bar.py[line:274] - INFO: epoch 004:   2025 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.89, wpb=112.7, bsz=40, num_updates=8540, lr=2.96688e-05, gnorm=0.817, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25080
2023-02-22 18:31:20 - progress_bar.py[line:274] - INFO: epoch 004:   2035 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=112.2, bsz=40, num_updates=8550, lr=2.96419e-05, gnorm=0.69, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25091
2023-02-22 18:31:31 - progress_bar.py[line:274] - INFO: epoch 004:   2045 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=111.6, bsz=40, num_updates=8560, lr=2.9615e-05, gnorm=0.901, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25102
2023-02-22 18:31:42 - progress_bar.py[line:274] - INFO: epoch 004:   2055 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.91, wpb=112.7, bsz=40, num_updates=8570, lr=2.95881e-05, gnorm=0.759, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25113
2023-02-22 18:31:53 - progress_bar.py[line:274] - INFO: epoch 004:   2065 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.89, wpb=112.2, bsz=40, num_updates=8580, lr=2.95612e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25124
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 18:32:04 - progress_bar.py[line:274] - INFO: epoch 004:   2075 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.3, ups=0.94, wpb=112.3, bsz=40, num_updates=8590, lr=2.95343e-05, gnorm=0.723, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25135
2023-02-22 18:32:15 - progress_bar.py[line:274] - INFO: epoch 004:   2085 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.92, wpb=112.4, bsz=40, num_updates=8600, lr=2.95074e-05, gnorm=0.711, clip=20, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=25146
2023-02-22 18:32:26 - progress_bar.py[line:274] - INFO: epoch 004:   2095 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.92, wpb=112.7, bsz=40, num_updates=8610, lr=2.94806e-05, gnorm=0.731, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25157
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 18:32:37 - progress_bar.py[line:274] - INFO: epoch 004:   2105 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103, ups=0.92, wpb=111.9, bsz=40, num_updates=8620, lr=2.94537e-05, gnorm=0.85, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25168
2023-02-22 18:32:48 - progress_bar.py[line:274] - INFO: epoch 004:   2115 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.5, bsz=40, num_updates=8630, lr=2.94268e-05, gnorm=0.81, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25179
2023-02-22 18:32:59 - progress_bar.py[line:274] - INFO: epoch 004:   2125 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.88, wpb=112.3, bsz=40, num_updates=8640, lr=2.93999e-05, gnorm=0.704, clip=30, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=25190
2023-02-22 18:33:11 - progress_bar.py[line:274] - INFO: epoch 004:   2135 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=8650, lr=2.9373e-05, gnorm=0.875, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25201
2023-02-22 18:33:23 - progress_bar.py[line:274] - INFO: epoch 004:   2145 / 2175 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.88, wpb=111.5, bsz=40, num_updates=8660, lr=2.93461e-05, gnorm=1.025, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25213
2023-02-22 18:33:34 - progress_bar.py[line:274] - INFO: epoch 004:   2155 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.8, ups=0.92, wpb=113.4, bsz=40, num_updates=8670, lr=2.93192e-05, gnorm=0.775, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25224
2023-02-22 18:33:45 - progress_bar.py[line:274] - INFO: epoch 004:   2165 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=112.9, bsz=40, num_updates=8680, lr=2.92924e-05, gnorm=0.854, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25236
2023-02-22 18:33:56 - progress_bar.py[line:274] - INFO: epoch 004:   2175 / 2175 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=108.6, nsentences=39.4, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.91, wpb=108.6, bsz=39.4, num_updates=8690, lr=2.92655e-05, gnorm=0.896, clip=30, loss_scale=512, train_wall=11, gb_free=14.3, ema_decay=0.9999, wall=25247
2023-02-22 18:33:56 - train.py[line:339] - INFO: end of epoch 4 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E4.tsv slice_id 1 row count 43497 total row count 86994
2023-02-22 18:33:56 - progress_bar.py[line:282] - INFO: epoch 004 | loss 0.263 | loss_v1 0 | loss_v2 0 | nll_loss 0.085 | ntokens 112.286 | nsentences 39.997 | sample_size 112.286 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.06 | wps 38.5 | ups 0.34 | wpb 112.3 | bsz 40 | num_updates 8690 | lr 2.92655e-05 | gnorm 0.814 | clip 27.3 | loss_scale 512 | train_wall 2498 | gb_free 14.3 | ema_decay 0.9999 | wall 25247
2023-02-22 18:33:56 - trainer.py[line:694] - INFO: loading train data for epoch 5
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E4.tsv slice_id 0 row count 43497 total row count 86994
2023-02-22 18:33:56 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
2023-02-22 18:33:56 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 18:33:57 - trainer.py[line:758] - INFO: begin training epoch 5
2023-02-22 18:33:57 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 18:34:10 - progress_bar.py[line:274] - INFO: epoch 005:     10 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=77, ups=0.69, wpb=111.2, bsz=40, num_updates=8700, lr=2.92386e-05, gnorm=0.481, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=25261
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 18:34:21 - progress_bar.py[line:274] - INFO: epoch 005:     20 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.9, ups=0.92, wpb=113.2, bsz=40, num_updates=8710, lr=2.92117e-05, gnorm=0.619, clip=0, loss_scale=512, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=25272
2023-02-22 18:34:33 - progress_bar.py[line:274] - INFO: epoch 005:     30 / 2175 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.2, ups=0.91, wpb=112.2, bsz=40, num_updates=8720, lr=2.91848e-05, gnorm=1.025, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25283
2023-02-22 18:34:44 - progress_bar.py[line:274] - INFO: epoch 005:     40 / 2175 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.91, wpb=112.3, bsz=40, num_updates=8730, lr=2.91579e-05, gnorm=0.618, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25294
@@@@ ERROR IN DATA @@@@ watch
2023-02-22 18:34:55 - progress_bar.py[line:274] - INFO: epoch 005:     50 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.9, wpb=112.8, bsz=40, num_updates=8740, lr=2.9131e-05, gnorm=0.917, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25306
2023-02-22 18:35:06 - progress_bar.py[line:274] - INFO: epoch 005:     60 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.91, wpb=112.1, bsz=40, num_updates=8750, lr=2.91042e-05, gnorm=0.734, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25317
2023-02-22 18:35:17 - progress_bar.py[line:274] - INFO: epoch 005:     70 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.7, ups=0.87, wpb=111.5, bsz=40, num_updates=8760, lr=2.90773e-05, gnorm=0.893, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25328
2023-02-22 18:35:29 - progress_bar.py[line:274] - INFO: epoch 005:     80 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=112.2, bsz=40, num_updates=8770, lr=2.90504e-05, gnorm=0.629, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25339
2023-02-22 18:35:40 - progress_bar.py[line:274] - INFO: epoch 005:     90 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=113.1, bsz=40, num_updates=8780, lr=2.90235e-05, gnorm=0.665, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25350
2023-02-22 18:35:51 - progress_bar.py[line:274] - INFO: epoch 005:    100 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.9, wpb=113.6, bsz=40, num_updates=8790, lr=2.89966e-05, gnorm=0.822, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25362
2023-02-22 18:36:02 - progress_bar.py[line:274] - INFO: epoch 005:    110 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.89, wpb=112.5, bsz=40, num_updates=8800, lr=2.89697e-05, gnorm=0.868, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25373
2023-02-22 18:36:13 - progress_bar.py[line:274] - INFO: epoch 005:    120 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.9, wpb=112.8, bsz=40, num_updates=8810, lr=2.89428e-05, gnorm=0.787, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25384
2023-02-22 18:36:24 - progress_bar.py[line:274] - INFO: epoch 005:    130 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=110.9, bsz=40, num_updates=8820, lr=2.8916e-05, gnorm=0.598, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25395
2023-02-22 18:36:35 - progress_bar.py[line:274] - INFO: epoch 005:    140 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.4, ups=0.94, wpb=111.4, bsz=40, num_updates=8830, lr=2.88891e-05, gnorm=0.739, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25406
2023-02-22 18:36:47 - progress_bar.py[line:274] - INFO: epoch 005:    150 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.86, wpb=112.7, bsz=40, num_updates=8840, lr=2.88622e-05, gnorm=0.673, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=25417
2023-02-22 18:36:58 - progress_bar.py[line:274] - INFO: epoch 005:    160 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=112.2, bsz=40, num_updates=8850, lr=2.88353e-05, gnorm=0.69, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25428
2023-02-22 18:37:10 - progress_bar.py[line:274] - INFO: epoch 005:    170 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.88, wpb=112.5, bsz=40, num_updates=8860, lr=2.88084e-05, gnorm=0.561, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25440
2023-02-22 18:37:21 - progress_bar.py[line:274] - INFO: epoch 005:    180 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.91, wpb=112, bsz=40, num_updates=8870, lr=2.87815e-05, gnorm=0.634, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25451
2023-02-22 18:37:32 - progress_bar.py[line:274] - INFO: epoch 005:    190 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.9, wpb=113.8, bsz=40, num_updates=8880, lr=2.87546e-05, gnorm=0.635, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25463
2023-02-22 18:37:43 - progress_bar.py[line:274] - INFO: epoch 005:    200 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.5, ups=0.91, wpb=113.3, bsz=40, num_updates=8890, lr=2.87278e-05, gnorm=0.565, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25474
2023-02-22 18:37:54 - progress_bar.py[line:274] - INFO: epoch 005:    210 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111.4, bsz=40, num_updates=8900, lr=2.87009e-05, gnorm=0.729, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25485
2023-02-22 18:38:05 - progress_bar.py[line:274] - INFO: epoch 005:    220 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.89, wpb=111.1, bsz=40, num_updates=8910, lr=2.8674e-05, gnorm=0.75, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25496
2023-02-22 18:38:17 - progress_bar.py[line:274] - INFO: epoch 005:    230 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.88, wpb=112.1, bsz=40, num_updates=8920, lr=2.86471e-05, gnorm=0.481, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25508
2023-02-22 18:38:28 - progress_bar.py[line:274] - INFO: epoch 005:    240 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.88, wpb=111.6, bsz=40, num_updates=8930, lr=2.86202e-05, gnorm=0.675, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25519
2023-02-22 18:38:39 - progress_bar.py[line:274] - INFO: epoch 005:    250 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.9, ups=0.93, wpb=113.2, bsz=40, num_updates=8940, lr=2.85933e-05, gnorm=0.698, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25530
2023-02-22 18:38:50 - progress_bar.py[line:274] - INFO: epoch 005:    260 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.88, wpb=113.4, bsz=40, num_updates=8950, lr=2.85664e-05, gnorm=0.576, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25541
2023-02-22 18:39:02 - progress_bar.py[line:274] - INFO: epoch 005:    270 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=111.4, bsz=40, num_updates=8960, lr=2.85395e-05, gnorm=0.649, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25552
2023-02-22 18:39:14 - progress_bar.py[line:274] - INFO: epoch 005:    280 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.89, wpb=112, bsz=40, num_updates=8970, lr=2.85127e-05, gnorm=0.612, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25564
2023-02-22 18:39:25 - progress_bar.py[line:274] - INFO: epoch 005:    290 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.89, wpb=112.2, bsz=40, num_updates=8980, lr=2.84858e-05, gnorm=0.975, clip=30, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25576
2023-02-22 18:39:37 - progress_bar.py[line:274] - INFO: epoch 005:    300 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.9, bsz=40, num_updates=8990, lr=2.84589e-05, gnorm=0.676, clip=20, loss_scale=1024, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=25587
2023-02-22 18:39:48 - progress_bar.py[line:274] - INFO: epoch 005:    310 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=112.6, bsz=40, num_updates=9000, lr=2.8432e-05, gnorm=0.831, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25599
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 18:39:59 - progress_bar.py[line:274] - INFO: epoch 005:    320 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.89, wpb=112.9, bsz=40, num_updates=9010, lr=2.84051e-05, gnorm=0.571, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25610
2023-02-22 18:40:11 - progress_bar.py[line:274] - INFO: epoch 005:    330 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.88, wpb=112.4, bsz=40, num_updates=9020, lr=2.83782e-05, gnorm=0.595, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25621
2023-02-22 18:40:22 - progress_bar.py[line:274] - INFO: epoch 005:    340 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.89, wpb=111.3, bsz=40, num_updates=9030, lr=2.83513e-05, gnorm=0.743, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25633
2023-02-22 18:40:33 - progress_bar.py[line:274] - INFO: epoch 005:    350 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.91, wpb=113.6, bsz=40, num_updates=9040, lr=2.83245e-05, gnorm=0.674, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25644
2023-02-22 18:40:44 - progress_bar.py[line:274] - INFO: epoch 005:    360 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=112.3, bsz=40, num_updates=9050, lr=2.82976e-05, gnorm=0.565, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25655
2023-02-22 18:40:55 - progress_bar.py[line:274] - INFO: epoch 005:    370 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.92, wpb=112, bsz=40, num_updates=9060, lr=2.82707e-05, gnorm=0.566, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25666
2023-02-22 18:41:06 - progress_bar.py[line:274] - INFO: epoch 005:    380 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=112.7, bsz=40, num_updates=9070, lr=2.82438e-05, gnorm=0.687, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25677
2023-02-22 18:41:17 - progress_bar.py[line:274] - INFO: epoch 005:    390 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.91, wpb=112.6, bsz=40, num_updates=9080, lr=2.82169e-05, gnorm=0.677, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25688
2023-02-22 18:41:29 - progress_bar.py[line:274] - INFO: epoch 005:    400 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=113.3, bsz=40, num_updates=9090, lr=2.819e-05, gnorm=0.574, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25699
2023-02-22 18:41:40 - progress_bar.py[line:274] - INFO: epoch 005:    410 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111.6, bsz=40, num_updates=9100, lr=2.81631e-05, gnorm=0.719, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25711
2023-02-22 18:41:51 - progress_bar.py[line:274] - INFO: epoch 005:    420 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=9110, lr=2.81363e-05, gnorm=0.796, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25722
2023-02-22 18:42:03 - progress_bar.py[line:274] - INFO: epoch 005:    430 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.91, wpb=112.6, bsz=40, num_updates=9120, lr=2.81094e-05, gnorm=0.731, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25733
2023-02-22 18:42:14 - progress_bar.py[line:274] - INFO: epoch 005:    440 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.4, ups=0.86, wpb=112.2, bsz=40, num_updates=9130, lr=2.80825e-05, gnorm=0.642, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=25745
2023-02-22 18:42:25 - progress_bar.py[line:274] - INFO: epoch 005:    450 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=111.8, bsz=40, num_updates=9140, lr=2.80556e-05, gnorm=0.902, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25756
2023-02-22 18:42:37 - progress_bar.py[line:274] - INFO: epoch 005:    460 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.89, wpb=113.6, bsz=40, num_updates=9150, lr=2.80287e-05, gnorm=0.804, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25767
2023-02-22 18:42:48 - progress_bar.py[line:274] - INFO: epoch 005:    470 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=112.3, bsz=40, num_updates=9160, lr=2.80018e-05, gnorm=0.946, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25779
2023-02-22 18:42:59 - progress_bar.py[line:274] - INFO: epoch 005:    480 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.9, wpb=113.6, bsz=40, num_updates=9170, lr=2.79749e-05, gnorm=0.776, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25790
2023-02-22 18:43:11 - progress_bar.py[line:274] - INFO: epoch 005:    490 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=112.2, bsz=40, num_updates=9180, lr=2.79481e-05, gnorm=0.581, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25801
2023-02-22 18:43:22 - progress_bar.py[line:274] - INFO: epoch 005:    500 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.89, wpb=112.1, bsz=40, num_updates=9190, lr=2.79212e-05, gnorm=0.661, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25813
2023-02-22 18:43:33 - progress_bar.py[line:274] - INFO: epoch 005:    510 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.87, wpb=112.9, bsz=40, num_updates=9200, lr=2.78943e-05, gnorm=0.911, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25824
2023-02-22 18:43:44 - progress_bar.py[line:274] - INFO: epoch 005:    520 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=112.4, bsz=40, num_updates=9210, lr=2.78674e-05, gnorm=0.839, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25835
2023-02-22 18:43:56 - progress_bar.py[line:274] - INFO: epoch 005:    530 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.89, wpb=113.2, bsz=40, num_updates=9220, lr=2.78405e-05, gnorm=0.708, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25847
2023-02-22 18:44:07 - progress_bar.py[line:274] - INFO: epoch 005:    540 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.88, wpb=112.5, bsz=40, num_updates=9230, lr=2.78136e-05, gnorm=0.583, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25858
2023-02-22 18:44:18 - progress_bar.py[line:274] - INFO: epoch 005:    550 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.91, wpb=111.5, bsz=40, num_updates=9240, lr=2.77867e-05, gnorm=0.604, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25869
2023-02-22 18:44:29 - progress_bar.py[line:274] - INFO: epoch 005:    560 / 2175 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.9, wpb=112.3, bsz=40, num_updates=9250, lr=2.77599e-05, gnorm=0.774, clip=10, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25880
2023-02-22 18:44:41 - progress_bar.py[line:274] - INFO: epoch 005:    570 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.87, wpb=112.4, bsz=40, num_updates=9260, lr=2.7733e-05, gnorm=0.69, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25891
2023-02-22 18:44:52 - progress_bar.py[line:274] - INFO: epoch 005:    580 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.87, wpb=111.8, bsz=40, num_updates=9270, lr=2.77061e-05, gnorm=0.602, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25903
2023-02-22 18:45:03 - progress_bar.py[line:274] - INFO: epoch 005:    590 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=112.6, bsz=40, num_updates=9280, lr=2.76792e-05, gnorm=0.809, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25914
2023-02-22 18:45:15 - progress_bar.py[line:274] - INFO: epoch 005:    600 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=112.8, bsz=40, num_updates=9290, lr=2.76523e-05, gnorm=0.735, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25925
2023-02-22 18:45:26 - progress_bar.py[line:274] - INFO: epoch 005:    610 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.1, bsz=40, num_updates=9300, lr=2.76254e-05, gnorm=0.767, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25936
2023-02-22 18:45:37 - progress_bar.py[line:274] - INFO: epoch 005:    620 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.1, ups=0.93, wpb=112.5, bsz=40, num_updates=9310, lr=2.75985e-05, gnorm=0.802, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25947
2023-02-22 18:45:48 - progress_bar.py[line:274] - INFO: epoch 005:    630 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.6, ups=0.88, wpb=109.7, bsz=40, num_updates=9320, lr=2.75717e-05, gnorm=0.711, clip=20, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25959
2023-02-22 18:45:59 - progress_bar.py[line:274] - INFO: epoch 005:    640 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.91, wpb=112.8, bsz=40, num_updates=9330, lr=2.75448e-05, gnorm=0.631, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25970
2023-02-22 18:46:10 - progress_bar.py[line:274] - INFO: epoch 005:    650 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=111.3, bsz=40, num_updates=9340, lr=2.75179e-05, gnorm=0.649, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25981
2023-02-22 18:46:22 - progress_bar.py[line:274] - INFO: epoch 005:    660 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.9, ups=0.87, wpb=111.6, bsz=40, num_updates=9350, lr=2.7491e-05, gnorm=0.731, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25993
2023-02-22 18:46:34 - progress_bar.py[line:274] - INFO: epoch 005:    670 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.2, ups=0.86, wpb=112.6, bsz=40, num_updates=9360, lr=2.74641e-05, gnorm=0.546, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=26004
2023-02-22 18:46:44 - progress_bar.py[line:274] - INFO: epoch 005:    680 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.8, ups=0.91, wpb=112.8, bsz=40, num_updates=9370, lr=2.74372e-05, gnorm=0.726, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26015
2023-02-22 18:46:56 - progress_bar.py[line:274] - INFO: epoch 005:    690 / 2175 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=9380, lr=2.74103e-05, gnorm=0.512, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26027
2023-02-22 18:47:07 - progress_bar.py[line:274] - INFO: epoch 005:    700 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.8, bsz=40, num_updates=9390, lr=2.73834e-05, gnorm=0.793, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26038
2023-02-22 18:47:18 - progress_bar.py[line:274] - INFO: epoch 005:    710 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.89, wpb=113.1, bsz=40, num_updates=9400, lr=2.73566e-05, gnorm=0.708, clip=30, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=26049
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 18:47:29 - progress_bar.py[line:274] - INFO: epoch 005:    720 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.2, ups=0.92, wpb=112.2, bsz=40, num_updates=9410, lr=2.73297e-05, gnorm=0.866, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26060
2023-02-22 18:47:40 - progress_bar.py[line:274] - INFO: epoch 005:    730 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.6, bsz=40, num_updates=9420, lr=2.73028e-05, gnorm=0.604, clip=20, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=26071
2023-02-22 18:47:52 - progress_bar.py[line:274] - INFO: epoch 005:    740 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.9, wpb=112.2, bsz=40, num_updates=9430, lr=2.72759e-05, gnorm=0.615, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26082
2023-02-22 18:48:03 - progress_bar.py[line:274] - INFO: epoch 005:    750 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112.7, bsz=40, num_updates=9440, lr=2.7249e-05, gnorm=1.155, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26094
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 18:48:14 - progress_bar.py[line:274] - INFO: epoch 005:    760 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.9, wpb=113.5, bsz=40, num_updates=9450, lr=2.72221e-05, gnorm=0.763, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26105
2023-02-22 18:48:25 - progress_bar.py[line:274] - INFO: epoch 005:    770 / 2175 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.91, wpb=113.6, bsz=40, num_updates=9460, lr=2.71952e-05, gnorm=0.548, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26116
2023-02-22 18:48:26 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 18:48:37 - progress_bar.py[line:274] - INFO: epoch 005:    781 / 2175 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=90.2, ups=0.8, wpb=112.3, bsz=40, num_updates=9470, lr=2.71684e-05, gnorm=0.526, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26128
2023-02-22 18:48:49 - progress_bar.py[line:274] - INFO: epoch 005:    791 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.89, wpb=110.9, bsz=40, num_updates=9480, lr=2.71415e-05, gnorm=0.499, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26139
2023-02-22 18:49:00 - progress_bar.py[line:274] - INFO: epoch 005:    801 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.1, ups=0.91, wpb=113.2, bsz=40, num_updates=9490, lr=2.71146e-05, gnorm=0.721, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26150
2023-02-22 18:49:11 - progress_bar.py[line:274] - INFO: epoch 005:    811 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.9, wpb=112.9, bsz=40, num_updates=9500, lr=2.70877e-05, gnorm=0.661, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26162
2023-02-22 18:49:22 - progress_bar.py[line:274] - INFO: epoch 005:    821 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.9, wpb=112.4, bsz=40, num_updates=9510, lr=2.70608e-05, gnorm=0.598, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26173
2023-02-22 18:49:33 - progress_bar.py[line:274] - INFO: epoch 005:    831 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.2, ups=0.91, wpb=114, bsz=40, num_updates=9520, lr=2.70339e-05, gnorm=0.519, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26184
2023-02-22 18:49:44 - progress_bar.py[line:274] - INFO: epoch 005:    841 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.87, wpb=113.9, bsz=40, num_updates=9530, lr=2.7007e-05, gnorm=0.571, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26195
2023-02-22 18:49:56 - progress_bar.py[line:274] - INFO: epoch 005:    851 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.9, wpb=113.3, bsz=40, num_updates=9540, lr=2.69802e-05, gnorm=0.584, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26206
2023-02-22 18:50:07 - progress_bar.py[line:274] - INFO: epoch 005:    861 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=112.2, bsz=40, num_updates=9550, lr=2.69533e-05, gnorm=0.626, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26218
2023-02-22 18:50:18 - progress_bar.py[line:274] - INFO: epoch 005:    871 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.9, ups=0.91, wpb=113.9, bsz=40, num_updates=9560, lr=2.69264e-05, gnorm=0.595, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26229
2023-02-22 18:50:29 - progress_bar.py[line:274] - INFO: epoch 005:    881 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.9, wpb=111.6, bsz=40, num_updates=9570, lr=2.68995e-05, gnorm=0.863, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26240
2023-02-22 18:50:40 - progress_bar.py[line:274] - INFO: epoch 005:    891 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.6, ups=0.92, wpb=112.3, bsz=40, num_updates=9580, lr=2.68726e-05, gnorm=0.923, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=26251
2023-02-22 18:50:51 - progress_bar.py[line:274] - INFO: epoch 005:    901 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.88, wpb=112.4, bsz=40, num_updates=9590, lr=2.68457e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26262
2023-02-22 18:51:02 - progress_bar.py[line:274] - INFO: epoch 005:    911 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103, ups=0.9, wpb=114.3, bsz=40, num_updates=9600, lr=2.68188e-05, gnorm=0.908, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26273
2023-02-22 18:51:14 - progress_bar.py[line:274] - INFO: epoch 005:    921 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.88, wpb=111.9, bsz=40, num_updates=9610, lr=2.6792e-05, gnorm=0.919, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26285
2023-02-22 18:51:25 - progress_bar.py[line:274] - INFO: epoch 005:    931 / 2175 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=112.6, bsz=40, num_updates=9620, lr=2.67651e-05, gnorm=0.518, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26296
2023-02-22 18:51:36 - progress_bar.py[line:274] - INFO: epoch 005:    941 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.9, wpb=111.7, bsz=40, num_updates=9630, lr=2.67382e-05, gnorm=0.69, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=26307
2023-02-22 18:51:48 - progress_bar.py[line:274] - INFO: epoch 005:    951 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.88, wpb=113.3, bsz=40, num_updates=9640, lr=2.67113e-05, gnorm=0.571, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26318
2023-02-22 18:51:59 - progress_bar.py[line:274] - INFO: epoch 005:    961 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=113, bsz=40, num_updates=9650, lr=2.66844e-05, gnorm=0.824, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26330
2023-02-22 18:52:10 - progress_bar.py[line:274] - INFO: epoch 005:    971 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.5, bsz=40, num_updates=9660, lr=2.66575e-05, gnorm=1.008, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26341
2023-02-22 18:52:21 - progress_bar.py[line:274] - INFO: epoch 005:    981 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.9, ups=0.91, wpb=113.7, bsz=40, num_updates=9670, lr=2.66306e-05, gnorm=0.745, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26352
2023-02-22 18:52:32 - progress_bar.py[line:274] - INFO: epoch 005:    991 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=111.6, bsz=40, num_updates=9680, lr=2.66038e-05, gnorm=0.947, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26363
2023-02-22 18:52:44 - progress_bar.py[line:274] - INFO: epoch 005:   1001 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=111.7, bsz=40, num_updates=9690, lr=2.65769e-05, gnorm=0.665, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26374
2023-02-22 18:52:55 - progress_bar.py[line:274] - INFO: epoch 005:   1011 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.9, wpb=110.5, bsz=40, num_updates=9700, lr=2.655e-05, gnorm=0.888, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26385
2023-02-22 18:53:06 - progress_bar.py[line:274] - INFO: epoch 005:   1021 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=111.8, bsz=40, num_updates=9710, lr=2.65231e-05, gnorm=0.61, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26397
2023-02-22 18:53:17 - progress_bar.py[line:274] - INFO: epoch 005:   1031 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.9, wpb=112.7, bsz=40, num_updates=9720, lr=2.64962e-05, gnorm=0.666, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=26408
2023-02-22 18:53:28 - progress_bar.py[line:274] - INFO: epoch 005:   1041 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.89, wpb=112.9, bsz=40, num_updates=9730, lr=2.64693e-05, gnorm=0.808, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26419
2023-02-22 18:53:40 - progress_bar.py[line:274] - INFO: epoch 005:   1051 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.9, ups=0.88, wpb=110.3, bsz=40, num_updates=9740, lr=2.64424e-05, gnorm=0.856, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26430
2023-02-22 18:53:51 - progress_bar.py[line:274] - INFO: epoch 005:   1061 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.9, wpb=112.7, bsz=40, num_updates=9750, lr=2.64156e-05, gnorm=0.644, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26441
2023-02-22 18:54:02 - progress_bar.py[line:274] - INFO: epoch 005:   1071 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.92, wpb=111.8, bsz=40, num_updates=9760, lr=2.63887e-05, gnorm=0.697, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26452
@@@@ ERROR IN DATA @@@@ play
2023-02-22 18:54:17 - progress_bar.py[line:274] - INFO: epoch 005:   1081 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112.8, bsz=40, num_updates=9770, lr=2.63618e-05, gnorm=0.701, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26464
2023-02-22 18:54:28 - progress_bar.py[line:274] - INFO: epoch 005:   1091 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.87, wpb=112.2, bsz=40, num_updates=9780, lr=2.63349e-05, gnorm=1.287, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26479
2023-02-22 18:54:39 - progress_bar.py[line:274] - INFO: epoch 005:   1101 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=112.2, bsz=40, num_updates=9790, lr=2.6308e-05, gnorm=0.631, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26490
2023-02-22 18:54:51 - progress_bar.py[line:274] - INFO: epoch 005:   1111 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.9, wpb=113.1, bsz=40, num_updates=9800, lr=2.62811e-05, gnorm=0.758, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26501
2023-02-22 18:55:03 - progress_bar.py[line:274] - INFO: epoch 005:   1121 / 2175 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.89, wpb=111.5, bsz=40, num_updates=9810, lr=2.62542e-05, gnorm=0.401, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26513
2023-02-22 18:55:14 - progress_bar.py[line:274] - INFO: epoch 005:   1131 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.1, ups=0.87, wpb=111.9, bsz=40, num_updates=9820, lr=2.62273e-05, gnorm=0.519, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26525
2023-02-22 18:55:26 - progress_bar.py[line:274] - INFO: epoch 005:   1141 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.87, wpb=112.4, bsz=40, num_updates=9830, lr=2.62005e-05, gnorm=0.801, clip=20, loss_scale=512, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=26536
2023-02-22 18:55:37 - progress_bar.py[line:274] - INFO: epoch 005:   1151 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.89, wpb=112.8, bsz=40, num_updates=9840, lr=2.61736e-05, gnorm=0.755, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=26548
2023-02-22 18:55:48 - progress_bar.py[line:274] - INFO: epoch 005:   1161 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.6, bsz=40, num_updates=9850, lr=2.61467e-05, gnorm=0.81, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=26559
2023-02-22 18:56:00 - progress_bar.py[line:274] - INFO: epoch 005:   1171 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.3, ups=0.87, wpb=111.3, bsz=40, num_updates=9860, lr=2.61198e-05, gnorm=0.862, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26570
2023-02-22 18:56:11 - progress_bar.py[line:274] - INFO: epoch 005:   1181 / 2175 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.88, wpb=111.2, bsz=40, num_updates=9870, lr=2.60929e-05, gnorm=1.782, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26582
2023-02-22 18:56:23 - progress_bar.py[line:274] - INFO: epoch 005:   1191 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.88, wpb=112.4, bsz=40, num_updates=9880, lr=2.6066e-05, gnorm=0.691, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26594
2023-02-22 18:56:34 - progress_bar.py[line:274] - INFO: epoch 005:   1201 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.3, ups=0.93, wpb=112.7, bsz=40, num_updates=9890, lr=2.60391e-05, gnorm=0.692, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26604
2023-02-22 18:56:45 - progress_bar.py[line:274] - INFO: epoch 005:   1211 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=112.6, bsz=40, num_updates=9900, lr=2.60123e-05, gnorm=0.672, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26616
2023-02-22 18:56:56 - progress_bar.py[line:274] - INFO: epoch 005:   1221 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.5, ups=0.92, wpb=112.6, bsz=40, num_updates=9910, lr=2.59854e-05, gnorm=0.713, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26627
2023-02-22 18:57:07 - progress_bar.py[line:274] - INFO: epoch 005:   1231 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.9, wpb=111.5, bsz=40, num_updates=9920, lr=2.59585e-05, gnorm=0.644, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26638
2023-02-22 18:57:18 - progress_bar.py[line:274] - INFO: epoch 005:   1241 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.88, wpb=112, bsz=40, num_updates=9930, lr=2.59316e-05, gnorm=0.849, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26649
2023-02-22 18:57:30 - progress_bar.py[line:274] - INFO: epoch 005:   1251 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=112.1, bsz=40, num_updates=9940, lr=2.59047e-05, gnorm=0.659, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26660
2023-02-22 18:57:41 - progress_bar.py[line:274] - INFO: epoch 005:   1261 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.9, wpb=113.1, bsz=40, num_updates=9950, lr=2.58778e-05, gnorm=1.071, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26671
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 18:57:52 - progress_bar.py[line:274] - INFO: epoch 005:   1271 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.87, wpb=113.2, bsz=40, num_updates=9960, lr=2.58509e-05, gnorm=0.864, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26683
2023-02-22 18:58:03 - progress_bar.py[line:274] - INFO: epoch 005:   1281 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.92, wpb=112, bsz=40, num_updates=9970, lr=2.58241e-05, gnorm=0.557, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26694
2023-02-22 18:58:14 - progress_bar.py[line:274] - INFO: epoch 005:   1291 / 2175 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=112.3, bsz=40, num_updates=9980, lr=2.57972e-05, gnorm=0.512, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26705
2023-02-22 18:58:26 - progress_bar.py[line:274] - INFO: epoch 005:   1301 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.89, wpb=113.6, bsz=40, num_updates=9990, lr=2.57703e-05, gnorm=0.753, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26716
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 18:58:37 - progress_bar.py[line:274] - INFO: epoch 005:   1311 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.7, ups=0.91, wpb=112.7, bsz=40, num_updates=10000, lr=2.57434e-05, gnorm=0.839, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26727
2023-02-22 18:58:37 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 18:58:38 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 18:58:38 - train.py[line:551] - INFO: load:1.07 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 19:00:40 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 19:00:40 - train.py[line:551] - INFO: load:1.10 valid_run:122.02 task_valid:119.17 collect_output:1.78
2023-02-22 19:02:40 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 19:02:40 - train.py[line:551] - INFO: load:1.12 valid_run:242.20 task_valid:235.10 collect_output:4.99
2023-02-22 19:04:43 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 19:04:43 - train.py[line:551] - INFO: load:1.15 valid_run:364.59 task_valid:351.72 collect_output:9.71
2023-02-22 19:06:45 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 19:06:45 - train.py[line:551] - INFO: load:1.17 valid_run:486.90 task_valid:465.46 collect_output:17.27
2023-02-22 19:08:46 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 19:08:46 - train.py[line:551] - INFO: load:1.20 valid_run:607.75 task_valid:582.93 collect_output:19.57
2023-02-22 19:10:50 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 19:10:50 - train.py[line:551] - INFO: load:1.22 valid_run:731.02 task_valid:701.80 collect_output:22.92
2023-02-22 19:12:53 - train.py[line:549] - INFO: 1400 / 6234
2023-02-22 19:12:53 - train.py[line:551] - INFO: load:1.25 valid_run:854.38 task_valid:820.07 collect_output:26.99
2023-02-22 19:14:55 - train.py[line:549] - INFO: 1600 / 6234
2023-02-22 19:14:55 - train.py[line:551] - INFO: load:1.28 valid_run:976.45 task_valid:936.51 collect_output:31.60
2023-02-22 19:16:59 - train.py[line:549] - INFO: 1800 / 6234
2023-02-22 19:16:59 - train.py[line:551] - INFO: load:1.30 valid_run:1100.59 task_valid:1053.67 collect_output:37.56
2023-02-22 19:19:01 - train.py[line:549] - INFO: 2000 / 6234
2023-02-22 19:19:01 - train.py[line:551] - INFO: load:1.32 valid_run:1222.52 task_valid:1166.31 collect_output:45.81
2023-02-22 19:21:02 - train.py[line:549] - INFO: 2200 / 6234
2023-02-22 19:21:02 - train.py[line:551] - INFO: load:1.35 valid_run:1342.92 task_valid:1281.96 collect_output:49.54
2023-02-22 19:23:04 - train.py[line:549] - INFO: 2400 / 6234
2023-02-22 19:23:04 - train.py[line:551] - INFO: load:1.37 valid_run:1464.74 task_valid:1398.92 collect_output:53.38
2023-02-22 19:25:03 - train.py[line:549] - INFO: 2600 / 6234
2023-02-22 19:25:03 - train.py[line:551] - INFO: load:1.40 valid_run:1584.52 task_valid:1513.41 collect_output:57.62
2023-02-22 19:27:07 - train.py[line:549] - INFO: 2800 / 6234
2023-02-22 19:27:07 - train.py[line:551] - INFO: load:1.43 valid_run:1707.80 task_valid:1633.38 collect_output:59.82
2023-02-22 19:29:10 - train.py[line:549] - INFO: 3000 / 6234
2023-02-22 19:29:10 - train.py[line:551] - INFO: load:1.45 valid_run:1831.23 task_valid:1752.12 collect_output:63.36
2023-02-22 19:31:14 - train.py[line:549] - INFO: 3200 / 6234
2023-02-22 19:31:14 - train.py[line:551] - INFO: load:1.48 valid_run:1954.54 task_valid:1868.62 collect_output:69.02
2023-02-22 19:33:17 - train.py[line:549] - INFO: 3400 / 6234
2023-02-22 19:33:17 - train.py[line:551] - INFO: load:1.51 valid_run:2078.20 task_valid:1987.11 collect_output:73.07
2023-02-22 19:35:20 - train.py[line:549] - INFO: 3600 / 6234
2023-02-22 19:35:20 - train.py[line:551] - INFO: load:1.53 valid_run:2200.94 task_valid:2107.10 collect_output:74.69
2023-02-22 19:37:24 - train.py[line:549] - INFO: 3800 / 6234
2023-02-22 19:37:24 - train.py[line:551] - INFO: load:1.56 valid_run:2324.49 task_valid:2226.40 collect_output:77.79
2023-02-22 19:39:26 - train.py[line:549] - INFO: 4000 / 6234
2023-02-22 19:39:26 - train.py[line:551] - INFO: load:1.59 valid_run:2447.09 task_valid:2345.36 collect_output:80.29
2023-02-22 19:41:31 - train.py[line:549] - INFO: 4200 / 6234
2023-02-22 19:41:31 - train.py[line:551] - INFO: load:1.62 valid_run:2571.09 task_valid:2464.37 collect_output:84.14
2023-02-22 19:43:35 - train.py[line:549] - INFO: 4400 / 6234
2023-02-22 19:43:35 - train.py[line:551] - INFO: load:1.65 valid_run:2695.28 task_valid:2585.36 collect_output:86.22
2023-02-22 19:45:37 - train.py[line:549] - INFO: 4600 / 6234
2023-02-22 19:45:37 - train.py[line:551] - INFO: load:1.67 valid_run:2817.61 task_valid:2701.94 collect_output:90.86
2023-02-22 19:47:39 - train.py[line:549] - INFO: 4800 / 6234
2023-02-22 19:47:39 - train.py[line:551] - INFO: load:1.70 valid_run:2939.75 task_valid:2820.42 collect_output:93.38
2023-02-22 19:49:43 - train.py[line:549] - INFO: 5000 / 6234
2023-02-22 19:49:43 - train.py[line:551] - INFO: load:1.73 valid_run:3063.46 task_valid:2938.88 collect_output:97.50
2023-02-22 19:51:48 - train.py[line:549] - INFO: 5200 / 6234
2023-02-22 19:51:48 - train.py[line:551] - INFO: load:1.75 valid_run:3188.39 task_valid:3057.26 collect_output:102.92
2023-02-22 19:53:50 - train.py[line:549] - INFO: 5400 / 6234
2023-02-22 19:53:50 - train.py[line:551] - INFO: load:1.78 valid_run:3310.25 task_valid:3173.89 collect_output:107.02
2023-02-22 19:55:53 - train.py[line:549] - INFO: 5600 / 6234
2023-02-22 19:55:53 - train.py[line:551] - INFO: load:1.81 valid_run:3433.20 task_valid:3293.94 collect_output:108.89
2023-02-22 19:57:55 - train.py[line:549] - INFO: 5800 / 6234
2023-02-22 19:57:55 - train.py[line:551] - INFO: load:1.83 valid_run:3555.21 task_valid:3409.51 collect_output:114.28
2023-02-22 19:59:58 - train.py[line:549] - INFO: 6000 / 6234
2023-02-22 19:59:58 - train.py[line:551] - INFO: load:1.86 valid_run:3677.98 task_valid:3528.57 collect_output:116.93
2023-02-22 20:02:00 - train.py[line:549] - INFO: 6200 / 6234
2023-02-22 20:02:00 - train.py[line:551] - INFO: load:1.88 valid_run:3799.97 task_valid:3647.70 collect_output:118.72

====================================================================================================
SGG eval:     R @ 50: 0.6639;     R @ 100: 0.6986;     R @ 500: 0.7247;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4483;    mR @ 100: 0.4807;    mR @ 500: 0.5318;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6250) (covering:0.2286) (eating:0.8235) (flying in:0.5000) (growing on:0.2500) (hanging from:0.4839) (lying on:0.4000) (mounted on:0.1429) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9353) (says:0.0000) (sitting on:0.7044) (standing on:0.5677) (using:0.6000) (walking in:0.0000) (walking on:0.4595) (watching:0.7083) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6639;     R @ 100: 0.6986;     R @ 500: 0.7247;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4483;    mR @ 100: 0.4807;    mR @ 500: 0.5318;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6250) (covering:0.2286) (eating:0.8235) (flying in:0.5000) (growing on:0.2500) (hanging from:0.4839) (lying on:0.4000) (mounted on:0.1429) (painted on:0.4167) (parked on:1.0000) (playing:0.0000) (riding:0.9353) (says:0.0000) (sitting on:0.7044) (standing on:0.5677) (using:0.6000) (walking in:0.0000) (walking on:0.4595) (watching:0.7083) 
--------------------------------------------------------
====================================================================================================

2023-02-22 20:02:32 - train.py[line:487] - INFO: 0.6986481792717086
2023-02-22 20:02:32 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-22 20:02:32 - progress_bar.py[line:282] - INFO: epoch 005 | valid on 'valid' subset | loss 0.237 | loss_v1 0 | loss_v2 0 | nll_loss 0.066 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.698648 | ppl 1.05 | vqa_score 0.5383 | wps 117 | wpb 72 | bsz 24 | num_updates 10000 | best_R@100 0.699072
2023-02-22 20:02:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 5 @ 10000 updates
2023-02-22 20:02:33 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_5_10000.pt
2023-02-22 20:02:39 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_5_10000.pt
2023-02-22 20:02:42 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_caption_coco_vg/1_B20_A1_E9_0.05_5e-5_480/checkpoint_5_10000.pt (epoch 5 @ 10000 updates, score 0.6986481792717086) (writing took 9.305200483649969 seconds)
2023-02-22 20:02:53 - progress_bar.py[line:274] - INFO: epoch 005:   1321 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=111.8, bsz=40, num_updates=10010, lr=2.57165e-05, gnorm=0.707, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30584
2023-02-22 20:03:05 - progress_bar.py[line:274] - INFO: epoch 005:   1331 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=112.7, bsz=40, num_updates=10020, lr=2.56896e-05, gnorm=0.675, clip=20, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=30595
2023-02-22 20:03:16 - progress_bar.py[line:274] - INFO: epoch 005:   1341 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.1, ups=0.85, wpb=112.5, bsz=40, num_updates=10030, lr=2.56627e-05, gnorm=0.912, clip=20, loss_scale=1024, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=30607
2023-02-22 20:03:28 - progress_bar.py[line:274] - INFO: epoch 005:   1351 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.9, ups=0.87, wpb=111.5, bsz=40, num_updates=10040, lr=2.56359e-05, gnorm=0.612, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30619
2023-02-22 20:03:40 - progress_bar.py[line:274] - INFO: epoch 005:   1361 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.87, wpb=113.1, bsz=40, num_updates=10050, lr=2.5609e-05, gnorm=0.561, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30631
2023-02-22 20:03:51 - progress_bar.py[line:274] - INFO: epoch 005:   1371 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.8, ups=0.87, wpb=111.7, bsz=40, num_updates=10060, lr=2.55821e-05, gnorm=0.633, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30642
2023-02-22 20:03:56 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 20:04:04 - progress_bar.py[line:274] - INFO: epoch 005:   1382 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=92.6, ups=0.82, wpb=112.5, bsz=40, num_updates=10070, lr=2.55552e-05, gnorm=0.656, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=30654
2023-02-22 20:04:15 - progress_bar.py[line:274] - INFO: epoch 005:   1392 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.9, wpb=113.2, bsz=40, num_updates=10080, lr=2.55283e-05, gnorm=0.849, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=30665
2023-02-22 20:04:26 - progress_bar.py[line:274] - INFO: epoch 005:   1402 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.91, wpb=111.2, bsz=40, num_updates=10090, lr=2.55014e-05, gnorm=0.712, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30676
2023-02-22 20:04:37 - progress_bar.py[line:274] - INFO: epoch 005:   1412 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=111.2, bsz=40, num_updates=10100, lr=2.54745e-05, gnorm=0.728, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30687
2023-02-22 20:04:48 - progress_bar.py[line:274] - INFO: epoch 005:   1422 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.87, wpb=112.5, bsz=40, num_updates=10110, lr=2.54477e-05, gnorm=0.618, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30699
2023-02-22 20:05:00 - progress_bar.py[line:274] - INFO: epoch 005:   1432 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.88, wpb=112.5, bsz=40, num_updates=10120, lr=2.54208e-05, gnorm=0.916, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=30710
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 20:05:10 - progress_bar.py[line:274] - INFO: epoch 005:   1442 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.7, ups=0.94, wpb=112.3, bsz=40, num_updates=10130, lr=2.53939e-05, gnorm=0.813, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30721
2023-02-22 20:05:22 - progress_bar.py[line:274] - INFO: epoch 005:   1452 / 2175 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=96.7, ups=0.87, wpb=111.4, bsz=40, num_updates=10140, lr=2.5367e-05, gnorm=0.546, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30733
2023-02-22 20:05:33 - progress_bar.py[line:274] - INFO: epoch 005:   1462 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=112.6, bsz=40, num_updates=10150, lr=2.53401e-05, gnorm=0.881, clip=40, loss_scale=512, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=30744
2023-02-22 20:05:45 - progress_bar.py[line:274] - INFO: epoch 005:   1472 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.86, wpb=113.2, bsz=40, num_updates=10160, lr=2.53132e-05, gnorm=0.809, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30756
2023-02-22 20:05:56 - progress_bar.py[line:274] - INFO: epoch 005:   1482 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.87, wpb=112.2, bsz=40, num_updates=10170, lr=2.52863e-05, gnorm=0.767, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30767
2023-02-22 20:06:09 - progress_bar.py[line:274] - INFO: epoch 005:   1492 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=92.5, ups=0.82, wpb=112.7, bsz=40, num_updates=10180, lr=2.52595e-05, gnorm=0.636, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=30779
2023-02-22 20:06:21 - progress_bar.py[line:274] - INFO: epoch 005:   1502 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=93.7, ups=0.83, wpb=112.5, bsz=40, num_updates=10190, lr=2.52326e-05, gnorm=1.149, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30791
2023-02-22 20:06:32 - progress_bar.py[line:274] - INFO: epoch 005:   1512 / 2175 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.7, ups=0.86, wpb=111.4, bsz=40, num_updates=10200, lr=2.52057e-05, gnorm=0.921, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30803
2023-02-22 20:06:44 - progress_bar.py[line:274] - INFO: epoch 005:   1522 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.87, wpb=112.3, bsz=40, num_updates=10210, lr=2.51788e-05, gnorm=0.713, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30815
2023-02-22 20:06:55 - progress_bar.py[line:274] - INFO: epoch 005:   1532 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.87, wpb=113.9, bsz=40, num_updates=10220, lr=2.51519e-05, gnorm=0.729, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30826
2023-02-22 20:07:07 - progress_bar.py[line:274] - INFO: epoch 005:   1542 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.87, wpb=112.4, bsz=40, num_updates=10230, lr=2.5125e-05, gnorm=0.794, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30838
2023-02-22 20:07:19 - progress_bar.py[line:274] - INFO: epoch 005:   1552 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.5, ups=0.86, wpb=111.7, bsz=40, num_updates=10240, lr=2.50981e-05, gnorm=0.712, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30849
2023-02-22 20:07:30 - progress_bar.py[line:274] - INFO: epoch 005:   1562 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=94.5, ups=0.85, wpb=111.7, bsz=40, num_updates=10250, lr=2.50712e-05, gnorm=0.786, clip=30, loss_scale=512, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=30861
2023-02-22 20:07:42 - progress_bar.py[line:274] - INFO: epoch 005:   1572 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.88, wpb=113, bsz=40, num_updates=10260, lr=2.50444e-05, gnorm=0.656, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30873
2023-02-22 20:07:54 - progress_bar.py[line:274] - INFO: epoch 005:   1582 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.8, ups=0.85, wpb=112.5, bsz=40, num_updates=10270, lr=2.50175e-05, gnorm=0.643, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30884
2023-02-22 20:08:05 - progress_bar.py[line:274] - INFO: epoch 005:   1592 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.4, ups=0.87, wpb=111.4, bsz=40, num_updates=10280, lr=2.49906e-05, gnorm=0.849, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30896
2023-02-22 20:08:17 - progress_bar.py[line:274] - INFO: epoch 005:   1602 / 2175 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.5, ups=0.85, wpb=113.7, bsz=40, num_updates=10290, lr=2.49637e-05, gnorm=0.586, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30908
2023-02-22 20:08:29 - progress_bar.py[line:274] - INFO: epoch 005:   1612 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=94.6, ups=0.85, wpb=111.5, bsz=40, num_updates=10300, lr=2.49368e-05, gnorm=0.649, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=30919
2023-02-22 20:08:40 - progress_bar.py[line:274] - INFO: epoch 005:   1622 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=94.6, ups=0.85, wpb=110.9, bsz=40, num_updates=10310, lr=2.49099e-05, gnorm=0.554, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=30931
2023-02-22 20:08:52 - progress_bar.py[line:274] - INFO: epoch 005:   1632 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=112.6, bsz=40, num_updates=10320, lr=2.4883e-05, gnorm=0.713, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30942
@@@@ ERROR IN DATA @@@@ play
2023-02-22 20:09:03 - progress_bar.py[line:274] - INFO: epoch 005:   1642 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.89, wpb=113, bsz=40, num_updates=10330, lr=2.48562e-05, gnorm=0.638, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30954
2023-02-22 20:09:14 - progress_bar.py[line:274] - INFO: epoch 005:   1652 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.91, wpb=112.4, bsz=40, num_updates=10340, lr=2.48293e-05, gnorm=0.714, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30965
2023-02-22 20:09:26 - progress_bar.py[line:274] - INFO: epoch 005:   1662 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.9, wpb=111.1, bsz=40, num_updates=10350, lr=2.48024e-05, gnorm=0.861, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30976
2023-02-22 20:09:37 - progress_bar.py[line:274] - INFO: epoch 005:   1672 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.88, wpb=113.5, bsz=40, num_updates=10360, lr=2.47755e-05, gnorm=0.63, clip=10, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=30988
2023-02-22 20:09:48 - progress_bar.py[line:274] - INFO: epoch 005:   1682 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.2, ups=0.92, wpb=113.2, bsz=40, num_updates=10370, lr=2.47486e-05, gnorm=0.593, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30999
2023-02-22 20:09:59 - progress_bar.py[line:274] - INFO: epoch 005:   1692 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.89, wpb=111.7, bsz=40, num_updates=10380, lr=2.47217e-05, gnorm=0.893, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31010
2023-02-22 20:10:10 - progress_bar.py[line:274] - INFO: epoch 005:   1702 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=106.3, ups=0.93, wpb=113.7, bsz=40, num_updates=10390, lr=2.46948e-05, gnorm=0.879, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31021
2023-02-22 20:10:21 - progress_bar.py[line:274] - INFO: epoch 005:   1712 / 2175 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.88, wpb=112.5, bsz=40, num_updates=10400, lr=2.4668e-05, gnorm=0.536, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31032
2023-02-22 20:10:33 - progress_bar.py[line:274] - INFO: epoch 005:   1722 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.87, wpb=113.6, bsz=40, num_updates=10410, lr=2.46411e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31044
2023-02-22 20:10:44 - progress_bar.py[line:274] - INFO: epoch 005:   1732 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112.6, bsz=40, num_updates=10420, lr=2.46142e-05, gnorm=0.629, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31055
2023-02-22 20:10:55 - progress_bar.py[line:274] - INFO: epoch 005:   1742 / 2175 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.89, wpb=109.9, bsz=40, num_updates=10430, lr=2.45873e-05, gnorm=0.618, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31066
2023-02-22 20:11:06 - progress_bar.py[line:274] - INFO: epoch 005:   1752 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.2, ups=0.9, wpb=113.5, bsz=40, num_updates=10440, lr=2.45604e-05, gnorm=0.785, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31077
2023-02-22 20:11:17 - progress_bar.py[line:274] - INFO: epoch 005:   1762 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=112.7, bsz=40, num_updates=10450, lr=2.45335e-05, gnorm=0.723, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31088
2023-02-22 20:11:29 - progress_bar.py[line:274] - INFO: epoch 005:   1772 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=112.5, bsz=40, num_updates=10460, lr=2.45066e-05, gnorm=0.675, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31099
2023-02-22 20:11:39 - progress_bar.py[line:274] - INFO: epoch 005:   1782 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.92, wpb=111.3, bsz=40, num_updates=10470, lr=2.44798e-05, gnorm=0.654, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31110
2023-02-22 20:11:51 - progress_bar.py[line:274] - INFO: epoch 005:   1792 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=112.3, bsz=40, num_updates=10480, lr=2.44529e-05, gnorm=0.724, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31121
2023-02-22 20:12:02 - progress_bar.py[line:274] - INFO: epoch 005:   1802 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.3, ups=0.88, wpb=110.7, bsz=40, num_updates=10490, lr=2.4426e-05, gnorm=0.666, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31133
2023-02-22 20:12:13 - progress_bar.py[line:274] - INFO: epoch 005:   1812 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111.6, bsz=40, num_updates=10500, lr=2.43991e-05, gnorm=0.749, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31144
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 20:12:25 - progress_bar.py[line:274] - INFO: epoch 005:   1822 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.88, wpb=111.4, bsz=40, num_updates=10510, lr=2.43722e-05, gnorm=0.596, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=31155
2023-02-22 20:12:36 - progress_bar.py[line:274] - INFO: epoch 005:   1832 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.1, ups=0.88, wpb=110.8, bsz=40, num_updates=10520, lr=2.43453e-05, gnorm=0.785, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31167
2023-02-22 20:12:48 - progress_bar.py[line:274] - INFO: epoch 005:   1842 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.88, wpb=112.8, bsz=40, num_updates=10530, lr=2.43184e-05, gnorm=0.568, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31178
2023-02-22 20:12:59 - progress_bar.py[line:274] - INFO: epoch 005:   1852 / 2175 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.9, wpb=112, bsz=40, num_updates=10540, lr=2.42916e-05, gnorm=0.666, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31189
2023-02-22 20:13:10 - progress_bar.py[line:274] - INFO: epoch 005:   1862 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=112.7, bsz=40, num_updates=10550, lr=2.42647e-05, gnorm=0.844, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=31201
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 20:13:21 - progress_bar.py[line:274] - INFO: epoch 005:   1872 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.87, wpb=114.2, bsz=40, num_updates=10560, lr=2.42378e-05, gnorm=0.601, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=31212
2023-02-22 20:13:33 - progress_bar.py[line:274] - INFO: epoch 005:   1882 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.89, wpb=111.4, bsz=40, num_updates=10570, lr=2.42109e-05, gnorm=0.532, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31224
2023-02-22 20:13:44 - progress_bar.py[line:274] - INFO: epoch 005:   1892 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.89, wpb=111.5, bsz=40, num_updates=10580, lr=2.4184e-05, gnorm=0.79, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31235
2023-02-22 20:13:55 - progress_bar.py[line:274] - INFO: epoch 005:   1902 / 2175 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=112.1, bsz=40, num_updates=10590, lr=2.41571e-05, gnorm=0.768, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31246
2023-02-22 20:14:07 - progress_bar.py[line:274] - INFO: epoch 005:   1912 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=10600, lr=2.41302e-05, gnorm=0.877, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31257
2023-02-22 20:14:18 - progress_bar.py[line:274] - INFO: epoch 005:   1922 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=110.7, bsz=40, num_updates=10610, lr=2.41034e-05, gnorm=0.931, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31269
2023-02-22 20:14:29 - progress_bar.py[line:274] - INFO: epoch 005:   1932 / 2175 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.89, wpb=113.3, bsz=40, num_updates=10620, lr=2.40765e-05, gnorm=0.673, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31280
2023-02-22 20:14:40 - progress_bar.py[line:274] - INFO: epoch 005:   1942 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.92, wpb=111, bsz=40, num_updates=10630, lr=2.40496e-05, gnorm=0.626, clip=20, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=31291
2023-02-22 20:14:52 - progress_bar.py[line:274] - INFO: epoch 005:   1952 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.3, ups=0.86, wpb=111.5, bsz=40, num_updates=10640, lr=2.40227e-05, gnorm=0.772, clip=20, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=31302
2023-02-22 20:15:03 - progress_bar.py[line:274] - INFO: epoch 005:   1962 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=111.5, bsz=40, num_updates=10650, lr=2.39958e-05, gnorm=0.774, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31313
2023-02-22 20:15:14 - progress_bar.py[line:274] - INFO: epoch 005:   1972 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.91, wpb=111.7, bsz=40, num_updates=10660, lr=2.39689e-05, gnorm=0.705, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31325
2023-02-22 20:15:25 - progress_bar.py[line:274] - INFO: epoch 005:   1982 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=111.7, bsz=40, num_updates=10670, lr=2.3942e-05, gnorm=1.016, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31336
2023-02-22 20:15:36 - progress_bar.py[line:274] - INFO: epoch 005:   1992 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.5, ups=0.91, wpb=113.5, bsz=40, num_updates=10680, lr=2.39151e-05, gnorm=0.798, clip=30, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=31347
2023-02-22 20:15:47 - progress_bar.py[line:274] - INFO: epoch 005:   2002 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.9, wpb=112.3, bsz=40, num_updates=10690, lr=2.38883e-05, gnorm=1.059, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31358
2023-02-22 20:15:59 - progress_bar.py[line:274] - INFO: epoch 005:   2012 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.88, wpb=112.8, bsz=40, num_updates=10700, lr=2.38614e-05, gnorm=0.891, clip=50, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31369
2023-02-22 20:16:10 - progress_bar.py[line:274] - INFO: epoch 005:   2022 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=112.3, bsz=40, num_updates=10710, lr=2.38345e-05, gnorm=0.628, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31381
2023-02-22 20:16:21 - progress_bar.py[line:274] - INFO: epoch 005:   2032 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.88, wpb=112.1, bsz=40, num_updates=10720, lr=2.38076e-05, gnorm=0.624, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31392
2023-02-22 20:16:32 - progress_bar.py[line:274] - INFO: epoch 005:   2042 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.9, wpb=111.7, bsz=40, num_updates=10730, lr=2.37807e-05, gnorm=0.627, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31403
2023-02-22 20:16:44 - progress_bar.py[line:274] - INFO: epoch 005:   2052 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=112.7, bsz=40, num_updates=10740, lr=2.37538e-05, gnorm=0.737, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31415
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 20:16:55 - progress_bar.py[line:274] - INFO: epoch 005:   2062 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.9, wpb=112.1, bsz=40, num_updates=10750, lr=2.37269e-05, gnorm=0.628, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31426
2023-02-22 20:17:06 - progress_bar.py[line:274] - INFO: epoch 005:   2072 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=111.7, bsz=40, num_updates=10760, lr=2.37001e-05, gnorm=0.731, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31437
2023-02-22 20:17:17 - progress_bar.py[line:274] - INFO: epoch 005:   2082 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=112.4, bsz=40, num_updates=10770, lr=2.36732e-05, gnorm=1.016, clip=30, loss_scale=1024, train_wall=11, gb_free=9.4, ema_decay=0.9999, wall=31448
2023-02-22 20:17:29 - progress_bar.py[line:274] - INFO: epoch 005:   2092 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.86, wpb=113.7, bsz=40, num_updates=10780, lr=2.36463e-05, gnorm=0.687, clip=20, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31460
2023-02-22 20:17:40 - progress_bar.py[line:274] - INFO: epoch 005:   2102 / 2175 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.88, wpb=112, bsz=40, num_updates=10790, lr=2.36194e-05, gnorm=0.569, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31471
2023-02-22 20:17:51 - progress_bar.py[line:274] - INFO: epoch 005:   2112 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.91, wpb=112.6, bsz=40, num_updates=10800, lr=2.35925e-05, gnorm=0.54, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31482
2023-02-22 20:18:02 - progress_bar.py[line:274] - INFO: epoch 005:   2122 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=111, bsz=40, num_updates=10810, lr=2.35656e-05, gnorm=0.873, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31493
2023-02-22 20:18:13 - progress_bar.py[line:274] - INFO: epoch 005:   2132 / 2175 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.5, ups=0.95, wpb=111.1, bsz=40, num_updates=10820, lr=2.35387e-05, gnorm=0.822, clip=30, loss_scale=1024, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=31504
2023-02-22 20:18:25 - progress_bar.py[line:274] - INFO: epoch 005:   2142 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.5, ups=0.86, wpb=112.6, bsz=40, num_updates=10830, lr=2.35119e-05, gnorm=0.691, clip=20, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=31515
2023-02-22 20:18:36 - progress_bar.py[line:274] - INFO: epoch 005:   2152 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=111.3, bsz=40, num_updates=10840, lr=2.3485e-05, gnorm=0.81, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31527
2023-02-22 20:18:47 - progress_bar.py[line:274] - INFO: epoch 005:   2162 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.9, wpb=112.8, bsz=40, num_updates=10850, lr=2.34581e-05, gnorm=0.753, clip=20, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=31538
2023-02-22 20:18:58 - progress_bar.py[line:274] - INFO: epoch 005:   2172 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.9, wpb=112.2, bsz=40, num_updates=10860, lr=2.34312e-05, gnorm=0.83, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31549
2023-02-22 20:19:01 - train.py[line:339] - INFO: end of epoch 5 (average epoch stats below)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E5.tsv slice_id 1 row count 43497 total row count 86994
2023-02-22 20:19:02 - progress_bar.py[line:282] - INFO: epoch 005 | loss 0.256 | loss_v1 0 | loss_v2 0 | nll_loss 0.077 | ntokens 112.284 | nsentences 39.997 | sample_size 112.284 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.05 | wps 38.7 | ups 0.34 | wpb 112.3 | bsz 40 | num_updates 10863 | lr 2.34231e-05 | gnorm 0.725 | clip 19.8 | loss_scale 1024 | train_wall 2434 | gb_free 14.4 | ema_decay 0.9999 | wall 31552
2023-02-22 20:19:02 - trainer.py[line:694] - INFO: loading train data for epoch 6
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E5.tsv slice_id 0 row count 43497 total row count 86994
2023-02-22 20:19:02 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
2023-02-22 20:19:02 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 20:19:03 - trainer.py[line:758] - INFO: begin training epoch 6
2023-02-22 20:19:03 - train.py[line:312] - INFO: Start iterating over samples
2023-02-22 20:19:12 - progress_bar.py[line:274] - INFO: epoch 006:      7 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=109.6, nsentences=39.4, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=78, ups=0.71, wpb=109.6, bsz=39.4, num_updates=10870, lr=2.34043e-05, gnorm=0.702, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31563
2023-02-22 20:19:24 - progress_bar.py[line:274] - INFO: epoch 006:     17 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=112.3, bsz=40, num_updates=10880, lr=2.33774e-05, gnorm=0.638, clip=20, loss_scale=1024, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=31574
2023-02-22 20:19:35 - progress_bar.py[line:274] - INFO: epoch 006:     27 / 2175 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=111.7, bsz=40, num_updates=10890, lr=2.33505e-05, gnorm=0.504, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31586
2023-02-22 20:19:46 - progress_bar.py[line:274] - INFO: epoch 006:     37 / 2175 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.9, wpb=111.5, bsz=40, num_updates=10900, lr=2.33237e-05, gnorm=0.888, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31597
2023-02-22 20:19:57 - progress_bar.py[line:274] - INFO: epoch 006:     47 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=107.2, ups=0.95, wpb=112.8, bsz=40, num_updates=10910, lr=2.32968e-05, gnorm=0.863, clip=20, loss_scale=1024, train_wall=10, gb_free=10.4, ema_decay=0.9999, wall=31607
2023-02-22 20:20:08 - progress_bar.py[line:274] - INFO: epoch 006:     57 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=112.1, bsz=40, num_updates=10920, lr=2.32699e-05, gnorm=0.604, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31619
2023-02-22 20:20:19 - progress_bar.py[line:274] - INFO: epoch 006:     67 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.87, wpb=111.9, bsz=40, num_updates=10930, lr=2.3243e-05, gnorm=0.572, clip=10, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=31630
2023-02-22 20:20:31 - progress_bar.py[line:274] - INFO: epoch 006:     77 / 2175 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.89, wpb=113.8, bsz=40, num_updates=10940, lr=2.32161e-05, gnorm=0.573, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31641
2023-02-22 20:20:42 - progress_bar.py[line:274] - INFO: epoch 006:     87 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.88, wpb=113.2, bsz=40, num_updates=10950, lr=2.31892e-05, gnorm=0.679, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31653
2023-02-22 20:20:53 - progress_bar.py[line:274] - INFO: epoch 006:     97 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.5, ups=0.92, wpb=113.4, bsz=40, num_updates=10960, lr=2.31623e-05, gnorm=0.766, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31664
2023-02-22 20:21:04 - progress_bar.py[line:274] - INFO: epoch 006:    107 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=111.8, bsz=40, num_updates=10970, lr=2.31355e-05, gnorm=0.67, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31675
2023-02-22 20:21:10 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 20:21:17 - progress_bar.py[line:274] - INFO: epoch 006:    118 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=90.6, ups=0.81, wpb=112.4, bsz=40, num_updates=10980, lr=2.31086e-05, gnorm=0.671, clip=30, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=31688
2023-02-22 20:21:28 - progress_bar.py[line:274] - INFO: epoch 006:    128 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.91, wpb=110.5, bsz=40, num_updates=10990, lr=2.30817e-05, gnorm=0.964, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31699
2023-02-22 20:21:39 - progress_bar.py[line:274] - INFO: epoch 006:    138 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111.5, bsz=40, num_updates=11000, lr=2.30548e-05, gnorm=0.606, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31710
2023-02-22 20:21:51 - progress_bar.py[line:274] - INFO: epoch 006:    148 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.87, wpb=112.4, bsz=40, num_updates=11010, lr=2.30279e-05, gnorm=0.502, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31721
2023-02-22 20:22:02 - progress_bar.py[line:274] - INFO: epoch 006:    158 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.86, wpb=113, bsz=40, num_updates=11020, lr=2.3001e-05, gnorm=0.714, clip=20, loss_scale=512, train_wall=12, gb_free=9.5, ema_decay=0.9999, wall=31733
2023-02-22 20:22:13 - progress_bar.py[line:274] - INFO: epoch 006:    168 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=112.3, bsz=40, num_updates=11030, lr=2.29741e-05, gnorm=0.545, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31744
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 20:22:24 - progress_bar.py[line:274] - INFO: epoch 006:    178 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.92, wpb=112.3, bsz=40, num_updates=11040, lr=2.29472e-05, gnorm=0.572, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=31755
2023-02-22 20:22:36 - progress_bar.py[line:274] - INFO: epoch 006:    188 / 2175 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.89, wpb=111.6, bsz=40, num_updates=11050, lr=2.29204e-05, gnorm=0.381, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31766
2023-02-22 20:22:47 - progress_bar.py[line:274] - INFO: epoch 006:    198 / 2175 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.91, wpb=112.2, bsz=40, num_updates=11060, lr=2.28935e-05, gnorm=0.596, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31777
2023-02-22 20:22:58 - progress_bar.py[line:274] - INFO: epoch 006:    208 / 2175 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.92, wpb=112.9, bsz=40, num_updates=11070, lr=2.28666e-05, gnorm=0.564, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31788
2023-02-22 20:23:09 - progress_bar.py[line:274] - INFO: epoch 006:    218 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.88, wpb=112.2, bsz=40, num_updates=11080, lr=2.28397e-05, gnorm=0.621, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31800
2023-02-22 20:23:20 - progress_bar.py[line:274] - INFO: epoch 006:    228 / 2175 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=112.6, bsz=40, num_updates=11090, lr=2.28128e-05, gnorm=0.692, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31811
2023-02-22 20:23:32 - progress_bar.py[line:274] - INFO: epoch 006:    238 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.89, wpb=113.1, bsz=40, num_updates=11100, lr=2.27859e-05, gnorm=0.675, clip=10, loss_scale=512, train_wall=11, gb_free=9.3, ema_decay=0.9999, wall=31822
2023-02-22 20:23:43 - progress_bar.py[line:274] - INFO: epoch 006:    248 / 2175 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.87, wpb=113.3, bsz=40, num_updates=11110, lr=2.2759e-05, gnorm=0.43, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=31834
2023-02-22 20:23:54 - progress_bar.py[line:274] - INFO: epoch 006:    258 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.9, wpb=110.8, bsz=40, num_updates=11120, lr=2.27322e-05, gnorm=0.808, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31845
2023-02-22 20:24:06 - progress_bar.py[line:274] - INFO: epoch 006:    268 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.87, wpb=112.6, bsz=40, num_updates=11130, lr=2.27053e-05, gnorm=0.7, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31856
2023-02-22 20:24:17 - progress_bar.py[line:274] - INFO: epoch 006:    278 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=112.2, bsz=40, num_updates=11140, lr=2.26784e-05, gnorm=0.946, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31868
2023-02-22 20:24:29 - progress_bar.py[line:274] - INFO: epoch 006:    288 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.88, wpb=111.8, bsz=40, num_updates=11150, lr=2.26515e-05, gnorm=0.693, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31879
2023-02-22 20:24:40 - progress_bar.py[line:274] - INFO: epoch 006:    298 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.87, wpb=112.7, bsz=40, num_updates=11160, lr=2.26246e-05, gnorm=0.461, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31891
2023-02-22 20:24:51 - progress_bar.py[line:274] - INFO: epoch 006:    308 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.87, wpb=113.1, bsz=40, num_updates=11170, lr=2.25977e-05, gnorm=0.74, clip=40, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=31902
2023-02-22 20:25:03 - progress_bar.py[line:274] - INFO: epoch 006:    318 / 2175 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=112.3, bsz=40, num_updates=11180, lr=2.25708e-05, gnorm=0.691, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31913
2023-02-22 20:25:14 - progress_bar.py[line:274] - INFO: epoch 006:    328 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.87, wpb=113.3, bsz=40, num_updates=11190, lr=2.2544e-05, gnorm=0.846, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31925
2023-02-22 20:25:25 - progress_bar.py[line:274] - INFO: epoch 006:    338 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=112.4, bsz=40, num_updates=11200, lr=2.25171e-05, gnorm=0.751, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31936
2023-02-22 20:25:37 - progress_bar.py[line:274] - INFO: epoch 006:    348 / 2175 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.87, wpb=113.6, bsz=40, num_updates=11210, lr=2.24902e-05, gnorm=0.539, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31948
2023-02-22 20:25:48 - progress_bar.py[line:274] - INFO: epoch 006:    358 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.88, wpb=113.3, bsz=40, num_updates=11220, lr=2.24633e-05, gnorm=0.604, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31959
2023-02-22 20:25:59 - progress_bar.py[line:274] - INFO: epoch 006:    368 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=112.9, bsz=40, num_updates=11230, lr=2.24364e-05, gnorm=0.387, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31970
2023-02-22 20:26:11 - progress_bar.py[line:274] - INFO: epoch 006:    378 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.88, wpb=112.9, bsz=40, num_updates=11240, lr=2.24095e-05, gnorm=0.961, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31982
2023-02-22 20:26:22 - progress_bar.py[line:274] - INFO: epoch 006:    388 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.88, wpb=112.4, bsz=40, num_updates=11250, lr=2.23826e-05, gnorm=0.776, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31993
2023-02-22 20:26:33 - progress_bar.py[line:274] - INFO: epoch 006:    398 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.9, wpb=111, bsz=40, num_updates=11260, lr=2.23558e-05, gnorm=0.917, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=32004
2023-02-22 20:26:45 - progress_bar.py[line:274] - INFO: epoch 006:    408 / 2175 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.9, wpb=112.4, bsz=40, num_updates=11270, lr=2.23289e-05, gnorm=0.471, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32015
2023-02-22 20:26:56 - progress_bar.py[line:274] - INFO: epoch 006:    418 / 2175 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.6, ups=0.9, wpb=111.3, bsz=40, num_updates=11280, lr=2.2302e-05, gnorm=0.666, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32027
2023-02-22 20:27:07 - progress_bar.py[line:274] - INFO: epoch 006:    428 / 2175 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.9, wpb=111.8, bsz=40, num_updates=11290, lr=2.22751e-05, gnorm=0.703, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32038
2023-02-22 20:27:18 - progress_bar.py[line:274] - INFO: epoch 006:    438 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.92, wpb=112.5, bsz=40, num_updates=11300, lr=2.22482e-05, gnorm=0.645, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32049
2023-02-22 20:27:29 - progress_bar.py[line:274] - INFO: epoch 006:    448 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.92, wpb=111.8, bsz=40, num_updates=11310, lr=2.22213e-05, gnorm=0.655, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32059
2023-02-22 20:27:40 - progress_bar.py[line:274] - INFO: epoch 006:    458 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.9, wpb=113.3, bsz=40, num_updates=11320, lr=2.21944e-05, gnorm=0.794, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32071
2023-02-22 20:27:51 - progress_bar.py[line:274] - INFO: epoch 006:    468 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=111.5, bsz=40, num_updates=11330, lr=2.21676e-05, gnorm=0.535, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32082
2023-02-22 20:28:02 - progress_bar.py[line:274] - INFO: epoch 006:    478 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.92, wpb=112, bsz=40, num_updates=11340, lr=2.21407e-05, gnorm=0.637, clip=10, loss_scale=512, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=32093
2023-02-22 20:28:14 - progress_bar.py[line:274] - INFO: epoch 006:    488 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.88, wpb=113, bsz=40, num_updates=11350, lr=2.21138e-05, gnorm=0.547, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32104
2023-02-22 20:28:25 - progress_bar.py[line:274] - INFO: epoch 006:    498 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.91, wpb=111.3, bsz=40, num_updates=11360, lr=2.20869e-05, gnorm=0.71, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=32115
2023-02-22 20:28:35 - progress_bar.py[line:274] - INFO: epoch 006:    508 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=111.8, bsz=40, num_updates=11370, lr=2.206e-05, gnorm=0.646, clip=10, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=32126
2023-02-22 20:28:47 - progress_bar.py[line:274] - INFO: epoch 006:    518 / 2175 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.9, wpb=113.1, bsz=40, num_updates=11380, lr=2.20331e-05, gnorm=0.528, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32137
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 20:28:57 - progress_bar.py[line:274] - INFO: epoch 006:    528 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=111.4, bsz=40, num_updates=11390, lr=2.20062e-05, gnorm=0.639, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=32148
2023-02-22 20:29:09 - progress_bar.py[line:274] - INFO: epoch 006:    538 / 2175 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.88, wpb=112.8, bsz=40, num_updates=11400, lr=2.19794e-05, gnorm=0.438, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32160
2023-02-22 20:29:21 - progress_bar.py[line:274] - INFO: epoch 006:    548 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.9, ups=0.87, wpb=111.9, bsz=40, num_updates=11410, lr=2.19525e-05, gnorm=0.65, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32171
2023-02-22 20:29:32 - progress_bar.py[line:274] - INFO: epoch 006:    558 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.89, wpb=111.3, bsz=40, num_updates=11420, lr=2.19256e-05, gnorm=0.806, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32183
2023-02-22 20:29:43 - progress_bar.py[line:274] - INFO: epoch 006:    568 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.89, wpb=111.5, bsz=40, num_updates=11430, lr=2.18987e-05, gnorm=0.576, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32194
2023-02-22 20:29:54 - progress_bar.py[line:274] - INFO: epoch 006:    578 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112.7, bsz=40, num_updates=11440, lr=2.18718e-05, gnorm=0.516, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32205
2023-02-22 20:30:05 - progress_bar.py[line:274] - INFO: epoch 006:    588 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.92, wpb=112.5, bsz=40, num_updates=11450, lr=2.18449e-05, gnorm=0.908, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32216
2023-02-22 20:30:17 - progress_bar.py[line:274] - INFO: epoch 006:    598 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.91, wpb=111.7, bsz=40, num_updates=11460, lr=2.1818e-05, gnorm=0.705, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32227
2023-02-22 20:30:28 - progress_bar.py[line:274] - INFO: epoch 006:    608 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.9, wpb=113.1, bsz=40, num_updates=11470, lr=2.17911e-05, gnorm=0.865, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32238
2023-02-22 20:30:39 - progress_bar.py[line:274] - INFO: epoch 006:    618 / 2175 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.2, ups=0.92, wpb=113.1, bsz=40, num_updates=11480, lr=2.17643e-05, gnorm=0.571, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32249
2023-02-22 20:30:50 - progress_bar.py[line:274] - INFO: epoch 006:    628 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.9, wpb=112.6, bsz=40, num_updates=11490, lr=2.17374e-05, gnorm=0.605, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32260
2023-02-22 20:31:01 - progress_bar.py[line:274] - INFO: epoch 006:    638 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=112.1, bsz=40, num_updates=11500, lr=2.17105e-05, gnorm=0.608, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32272
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 20:31:12 - progress_bar.py[line:274] - INFO: epoch 006:    648 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.87, wpb=114.1, bsz=40, num_updates=11510, lr=2.16836e-05, gnorm=0.503, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32283
2023-02-22 20:31:23 - progress_bar.py[line:274] - INFO: epoch 006:    658 / 2175 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.91, wpb=111.6, bsz=40, num_updates=11520, lr=2.16567e-05, gnorm=1.167, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32294
2023-02-22 20:31:27 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-22 20:31:36 - progress_bar.py[line:274] - INFO: epoch 006:    669 / 2175 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=90.2, ups=0.81, wpb=112.1, bsz=40, num_updates=11530, lr=2.16298e-05, gnorm=0.871, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32307
2023-02-22 20:31:47 - progress_bar.py[line:274] - INFO: epoch 006:    679 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.88, wpb=112, bsz=40, num_updates=11540, lr=2.16029e-05, gnorm=0.557, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32318
2023-02-22 20:31:58 - progress_bar.py[line:274] - INFO: epoch 006:    689 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.9, wpb=111.4, bsz=40, num_updates=11550, lr=2.15761e-05, gnorm=0.757, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32329
2023-02-22 20:32:10 - progress_bar.py[line:274] - INFO: epoch 006:    699 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.87, wpb=113.1, bsz=40, num_updates=11560, lr=2.15492e-05, gnorm=0.659, clip=10, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=32341
2023-02-22 20:32:22 - progress_bar.py[line:274] - INFO: epoch 006:    709 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.6, ups=0.87, wpb=111.6, bsz=40, num_updates=11570, lr=2.15223e-05, gnorm=0.502, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32352
2023-02-22 20:32:33 - progress_bar.py[line:274] - INFO: epoch 006:    719 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111.8, bsz=40, num_updates=11580, lr=2.14954e-05, gnorm=0.835, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=32364
2023-02-22 20:32:44 - progress_bar.py[line:274] - INFO: epoch 006:    729 / 2175 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.9, wpb=113.3, bsz=40, num_updates=11590, lr=2.14685e-05, gnorm=0.612, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32375
2023-02-22 20:32:55 - progress_bar.py[line:274] - INFO: epoch 006:    739 / 2175 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=111.4, bsz=40, num_updates=11600, lr=2.14416e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32386
2023-02-22 20:33:06 - progress_bar.py[line:274] - INFO: epoch 006:    749 / 2175 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.91, wpb=110.7, bsz=40, num_updates=11610, lr=2.14147e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32397
2023-02-22 20:33:18 - progress_bar.py[line:274] - INFO: epoch 006:    759 / 2175 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.89, wpb=110.5, bsz=40, num_updates=11620, lr=2.13879e-05, gnorm=0.699, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32408
2023-02-22 20:33:29 - progress_bar.py[line:274] - INFO: epoch 006:    769 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.89, wpb=111.6, bsz=40, num_updates=11630, lr=2.1361e-05, gnorm=0.624, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32420
2023-02-22 20:33:40 - progress_bar.py[line:274] - INFO: epoch 006:    779 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=111.9, bsz=40, num_updates=11640, lr=2.13341e-05, gnorm=0.682, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32431
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 20:33:52 - progress_bar.py[line:274] - INFO: epoch 006:    789 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.88, wpb=111.6, bsz=40, num_updates=11650, lr=2.13072e-05, gnorm=0.784, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32442
2023-02-22 20:34:03 - progress_bar.py[line:274] - INFO: epoch 006:    799 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=112.3, bsz=40, num_updates=11660, lr=2.12803e-05, gnorm=0.488, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32453
2023-02-22 20:34:14 - progress_bar.py[line:274] - INFO: epoch 006:    809 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.9, wpb=111.3, bsz=40, num_updates=11670, lr=2.12534e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=32465
2023-02-22 20:34:25 - progress_bar.py[line:274] - INFO: epoch 006:    819 / 2175 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.91, wpb=113.2, bsz=40, num_updates=11680, lr=2.12265e-05, gnorm=0.74, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=32476
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 20:34:36 - progress_bar.py[line:274] - INFO: epoch 006:    829 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=111.8, bsz=40, num_updates=11690, lr=2.11997e-05, gnorm=0.551, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32487
2023-02-22 20:34:47 - progress_bar.py[line:274] - INFO: epoch 006:    839 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=112, bsz=40, num_updates=11700, lr=2.11728e-05, gnorm=0.627, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=32498
2023-02-22 20:34:58 - progress_bar.py[line:274] - INFO: epoch 006:    849 / 2175 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.91, wpb=112.3, bsz=40, num_updates=11710, lr=2.11459e-05, gnorm=0.459, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32509
2023-02-22 20:35:09 - progress_bar.py[line:274] - INFO: epoch 006:    859 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=112.3, bsz=40, num_updates=11720, lr=2.1119e-05, gnorm=0.756, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32520
2023-02-22 20:35:21 - progress_bar.py[line:274] - INFO: epoch 006:    869 / 2175 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=112.7, bsz=40, num_updates=11730, lr=2.10921e-05, gnorm=0.581, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32531
2023-02-22 20:35:31 - progress_bar.py[line:274] - INFO: epoch 006:    879 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.91, wpb=111.3, bsz=40, num_updates=11740, lr=2.10652e-05, gnorm=0.809, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32542
2023-02-22 20:35:43 - progress_bar.py[line:274] - INFO: epoch 006:    889 / 2175 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.9, wpb=112.3, bsz=40, num_updates=11750, lr=2.10383e-05, gnorm=0.605, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32553
2023-02-22 20:35:54 - progress_bar.py[line:274] - INFO: epoch 006:    899 / 2175 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.7, ups=0.87, wpb=111.2, bsz=40, num_updates=11760, lr=2.10115e-05, gnorm=0.653, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32565
2023-02-22 20:36:06 - progress_bar.py[line:274] - INFO: epoch 006:    909 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.88, wpb=113.5, bsz=40, num_updates=11770, lr=2.09846e-05, gnorm=0.497, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=32576
2023-02-22 20:36:17 - progress_bar.py[line:274] - INFO: epoch 006:    919 / 2175 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.89, wpb=113.1, bsz=40, num_updates=11780, lr=2.09577e-05, gnorm=0.804, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32588
2023-02-22 20:36:28 - progress_bar.py[line:274] - INFO: epoch 006:    929 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=111.9, bsz=40, num_updates=11790, lr=2.09308e-05, gnorm=0.726, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32599
2023-02-22 20:36:39 - progress_bar.py[line:274] - INFO: epoch 006:    939 / 2175 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.88, wpb=112.1, bsz=40, num_updates=11800, lr=2.09039e-05, gnorm=0.719, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32610
2023-02-22 20:36:50 - progress_bar.py[line:274] - INFO: epoch 006:    949 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.9, wpb=113, bsz=40, num_updates=11810, lr=2.0877e-05, gnorm=0.637, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32621
2023-02-22 20:37:02 - progress_bar.py[line:274] - INFO: epoch 006:    959 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.9, bsz=40, num_updates=11820, lr=2.08501e-05, gnorm=0.772, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32632
2023-02-22 20:37:13 - progress_bar.py[line:274] - INFO: epoch 006:    969 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=113, bsz=40, num_updates=11830, lr=2.08233e-05, gnorm=0.653, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32644
2023-02-22 20:37:24 - progress_bar.py[line:274] - INFO: epoch 006:    979 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=111.8, bsz=40, num_updates=11840, lr=2.07964e-05, gnorm=0.699, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32655
2023-02-22 20:37:35 - progress_bar.py[line:274] - INFO: epoch 006:    989 / 2175 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.89, wpb=111.7, bsz=40, num_updates=11850, lr=2.07695e-05, gnorm=0.691, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32666
2023-02-22 20:37:47 - progress_bar.py[line:274] - INFO: epoch 006:    999 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=111.9, bsz=40, num_updates=11860, lr=2.07426e-05, gnorm=0.794, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32677
2023-02-22 20:37:58 - progress_bar.py[line:274] - INFO: epoch 006:   1009 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.88, wpb=111.8, bsz=40, num_updates=11870, lr=2.07157e-05, gnorm=0.578, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32689
@@@@ ERROR IN DATA @@@@ ride
2023-02-22 20:38:09 - progress_bar.py[line:274] - INFO: epoch 006:   1019 / 2175 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.2, ups=0.91, wpb=113, bsz=40, num_updates=11880, lr=2.06888e-05, gnorm=0.737, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32700
2023-02-22 20:38:20 - progress_bar.py[line:274] - INFO: epoch 006:   1029 / 2175 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.92, wpb=111.9, bsz=40, num_updates=11890, lr=2.06619e-05, gnorm=0.697, clip=20, loss_scale=512, train_wall=11, gb_free=9.4, ema_decay=0.9999, wall=32711
2023-02-22 20:38:31 - progress_bar.py[line:274] - INFO: epoch 006:   1039 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=113.1, bsz=40, num_updates=11900, lr=2.0635e-05, gnorm=0.567, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=32722
2023-02-22 20:38:42 - progress_bar.py[line:274] - INFO: epoch 006:   1049 / 2175 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.92, wpb=111.6, bsz=40, num_updates=11910, lr=2.06082e-05, gnorm=0.494, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32733
2023-02-22 20:38:56 - progress_bar.py[line:274] - INFO: epoch 006:   1059 / 2175 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.5, ups=0.86, wpb=111.6, bsz=40, num_updates=11920, lr=2.05813e-05, gnorm=0.914, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32744
2023-02-22 20:39:08 - progress_bar.py[line:274] - INFO: epoch 006:   1069 / 2175 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=112, bsz=40, num_updates=11930, lr=2.05544e-05, gnorm=0.448, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32758
2023-02-22 20:39:19 - progress_bar.py[line:274] - INFO: epoch 006:   1079 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.89, wpb=111.7, bsz=40, num_updates=11940, lr=2.05275e-05, gnorm=0.657, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32770
2023-02-22 20:39:30 - progress_bar.py[line:274] - INFO: epoch 006:   1089 / 2175 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.9, wpb=111.3, bsz=40, num_updates=11950, lr=2.05006e-05, gnorm=0.719, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32781
2023-02-22 20:39:41 - progress_bar.py[line:274] - INFO: epoch 006:   1099 / 2175 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.91, wpb=112.9, bsz=40, num_updates=11960, lr=2.04737e-05, gnorm=0.631, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32792
2023-02-22 20:39:52 - progress_bar.py[line:274] - INFO: epoch 006:   1109 / 2175 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.7, bsz=40, num_updates=11970, lr=2.04468e-05, gnorm=0.652, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=32803
@@@@ ERROR IN DATA @@@@ play
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 20:40:04 - progress_bar.py[line:274] - INFO: epoch 006:   1119 / 2175 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=113.1, bsz=40, num_updates=11980, lr=2.042e-05, gnorm=0.446, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32814
2023-02-22 20:40:15 - progress_bar.py[line:274] - INFO: epoch 006:   1129 / 2175 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.9, ups=0.93, wpb=112.8, bsz=40, num_updates=11990, lr=2.03931e-05, gnorm=0.534, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32825
2023-02-22 20:40:26 - progress_bar.py[line:274] - INFO: epoch 006:   1139 / 2175 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=112.8, bsz=40, num_updates=12000, lr=2.03662e-05, gnorm=0.704, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32837
2023-02-22 20:40:26 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 20:40:27 - train.py[line:549] - INFO: 0 / 6234
2023-02-22 20:40:27 - train.py[line:551] - INFO: load:1.04 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 20:42:30 - train.py[line:549] - INFO: 200 / 6234
2023-02-22 20:42:30 - train.py[line:551] - INFO: load:1.07 valid_run:122.12 task_valid:118.50 collect_output:2.52
2023-02-22 20:44:30 - train.py[line:549] - INFO: 400 / 6234
2023-02-22 20:44:30 - train.py[line:551] - INFO: load:1.10 valid_run:242.10 task_valid:233.72 collect_output:6.25
2023-02-22 20:46:32 - train.py[line:549] - INFO: 600 / 6234
2023-02-22 20:46:32 - train.py[line:551] - INFO: load:1.12 valid_run:364.10 task_valid:349.69 collect_output:11.25
2023-02-22 20:48:34 - train.py[line:549] - INFO: 800 / 6234
2023-02-22 20:48:34 - train.py[line:551] - INFO: load:1.15 valid_run:486.27 task_valid:462.73 collect_output:19.34
2023-02-22 20:50:34 - train.py[line:549] - INFO: 1000 / 6234
2023-02-22 20:50:34 - train.py[line:551] - INFO: load:1.17 valid_run:606.64 task_valid:579.39 collect_output:22.02
2023-02-22 20:52:37 - train.py[line:549] - INFO: 1200 / 6234
2023-02-22 20:52:37 - train.py[line:551] - INFO: load:1.19 valid_run:729.54 task_valid:697.56 collect_output:25.73
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 62426
Killing subprocess 62428
Main process received SIGINT, exiting
