2023-02-22 21:10:22 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-22 21:10:22 - utils.py[line:261] - INFO: Start init
2023-02-22 21:10:22 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-22 21:10:22 - utils.py[line:261] - INFO: Start init
2023-02-22 21:10:23 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-22 21:10:23 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-22 21:10:23 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-22 21:10:23 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-22 21:10:32 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/temp_test_caption_vg_full_val', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 200, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 9, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/temp_test_caption_vg_full_val/1_B20_A1_E9_0.05_0e-5_480', 'restore_file': '/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_best.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 200, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_1566.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[0.0], lr_scheduler='polynomial_decay', max_epoch=9, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_best.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/temp_test_caption_vg_full_val/1_B20_A1_E9_0.05_0e-5_480', save_interval=10, save_interval_updates=200, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/temp_test_caption_vg_full_val', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=200, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_1566.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [0.0]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-22 21:10:32 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-22 21:10:32 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-22 21:10:36 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-22 21:10:36 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-22 21:10:36 - train.py[line:119] - INFO: model: OFAModel
2023-02-22 21:10:36 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-22 21:10:36 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-22 21:10:37 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_1566.tsv slice_id 0 row count 210450 total row count 420900
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_val_1566.tsv slice_id 1 row count 210450 total row count 420900
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-22 21:10:38 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-22 21:10:38 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-22 21:10:39 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-22 21:10:39 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-22 21:10:39 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-22 21:10:39 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-02-22 21:10:41 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-22 21:10:41 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-22 21:10:41 - trainer.py[line:500] - INFO: Preparing to load checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_best.pt
2023-02-22 21:11:26 - trainer.py[line:565] - INFO: Load Model_m together with Model 2
2023-02-22 21:11:26 - trainer.py[line:657] - INFO: Loading EMA from checkpoint
2023-02-22 21:11:26 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-22 21:11:26 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-22 21:11:27 - trainer.py[line:664] - INFO: Loading EMA fp32 params from checkpoint
2023-02-22 21:11:27 - trainer.py[line:675] - INFO: Loaded checkpoint /data/private/yutianyu/OFA/run_scripts/vqa/vqa_checkpoints/test_caption_opt_new/1_B20_A1_E50_0.04_5e-5_480/checkpoint_best.pt (epoch 15 @ 0 updates)
2023-02-22 21:11:27 - trainer.py[line:695] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv slice_id 0 row count 43497 total row count 86994
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_caption_five_filtered/query_combine_coco_vg_train_NA1_E0.tsv slice_id 1 row count 43497 total row count 86994
2023-02-22 21:11:27 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/COCO/b64_feat.lineidx
2023-02-22 21:11:27 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 19575, warmup steps 978, warmup_factor 0.0010224948875255625
2023-02-22 21:11:32 - trainer.py[line:759] - INFO: begin training epoch 1
2023-02-22 21:11:32 - train.py[line:312] - INFO: Start iterating over samples
Total steps 19575, warmup steps 978, warmup_factor 0.0010224948875255625
2023-02-22 21:12:01 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 2175 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=69.6, ups=0.62, wpb=112.3, bsz=40, num_updates=10, lr=0, gnorm=2.466, clip=100, loss_scale=128, train_wall=18, gb_free=9.6, ema_decay=0.9999, wall=82
2023-02-22 21:12:14 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 2175 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=84.4, ups=0.75, wpb=112.1, bsz=40, num_updates=20, lr=0, gnorm=2.385, clip=100, loss_scale=128, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=95
2023-02-22 21:12:27 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 2175 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=87.4, ups=0.79, wpb=111, bsz=40, num_updates=30, lr=0, gnorm=3.246, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=108
2023-02-22 21:12:40 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 2175 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=87.7, ups=0.78, wpb=112.4, bsz=40, num_updates=40, lr=0, gnorm=3.004, clip=100, loss_scale=128, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=120
2023-02-22 21:12:53 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 2175 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=88.4, ups=0.79, wpb=112.5, bsz=40, num_updates=50, lr=0, gnorm=2.566, clip=100, loss_scale=128, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=133
2023-02-22 21:13:04 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 2175 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=94.9, ups=0.84, wpb=113.1, bsz=40, num_updates=60, lr=0, gnorm=2.764, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=145
2023-02-22 21:13:16 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 2175 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=94.2, ups=0.84, wpb=111.8, bsz=40, num_updates=70, lr=0, gnorm=3.117, clip=100, loss_scale=128, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=157
2023-02-22 21:13:28 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 2175 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.4, ups=0.89, wpb=112, bsz=40, num_updates=80, lr=0, gnorm=2.775, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=169
2023-02-22 21:13:39 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 2175 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=98.1, ups=0.89, wpb=110.6, bsz=40, num_updates=90, lr=0, gnorm=2.825, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=180
2023-02-22 21:13:50 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 2175 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=99.5, ups=0.89, wpb=111.8, bsz=40, num_updates=100, lr=0, gnorm=2.509, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=191
2023-02-22 21:14:02 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 2175 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=100.1, ups=0.89, wpb=112.2, bsz=40, num_updates=110, lr=0, gnorm=3.066, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=203
2023-02-22 21:14:13 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 2175 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=99.7, ups=0.89, wpb=111.6, bsz=40, num_updates=120, lr=0, gnorm=2.552, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=214
2023-02-22 21:14:24 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 2175 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.9, ups=0.88, wpb=112.1, bsz=40, num_updates=130, lr=0, gnorm=2.642, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=225
2023-02-22 21:14:36 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 2175 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=98.3, ups=0.87, wpb=112.7, bsz=40, num_updates=140, lr=0, gnorm=2.675, clip=90, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=237
2023-02-22 21:14:47 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 2175 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.2, ups=0.89, wpb=112, bsz=40, num_updates=150, lr=0, gnorm=2.601, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=248
2023-02-22 21:14:58 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 2175 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100.1, ups=0.89, wpb=112.7, bsz=40, num_updates=160, lr=0, gnorm=2.645, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=259
2023-02-22 21:15:10 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 2175 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=95.8, ups=0.86, wpb=110.9, bsz=40, num_updates=170, lr=0, gnorm=2.772, clip=90, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=271
2023-02-22 21:15:21 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 2175 loss=0.441, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.4, ups=0.89, wpb=112, bsz=40, num_updates=180, lr=0, gnorm=2.952, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=282
2023-02-22 21:15:32 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 2175 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101.7, ups=0.91, wpb=112.3, bsz=40, num_updates=190, lr=0, gnorm=2.748, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=293
@@@@ ERROR IN DATA @@@@ stand on
2023-02-22 21:15:44 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 2175 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.5, ups=0.87, wpb=112.6, bsz=40, num_updates=200, lr=0, gnorm=2.382, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=305
2023-02-22 21:15:44 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-22 21:15:44 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-22 21:15:45 - train.py[line:549] - INFO: 0 / 17538
2023-02-22 21:15:45 - train.py[line:551] - INFO: load:1.02 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-22 21:17:52 - train.py[line:549] - INFO: 200 / 17538
2023-02-22 21:17:52 - train.py[line:551] - INFO: load:1.05 valid_run:127.13 task_valid:121.51 collect_output:4.55
2023-02-22 21:19:52 - train.py[line:549] - INFO: 400 / 17538
2023-02-22 21:19:52 - train.py[line:551] - INFO: load:1.07 valid_run:246.98 task_valid:237.96 collect_output:6.92
2023-02-22 21:21:53 - train.py[line:549] - INFO: 600 / 17538
2023-02-22 21:21:53 - train.py[line:551] - INFO: load:1.09 valid_run:368.06 task_valid:354.94 collect_output:9.99
2023-02-22 21:23:52 - train.py[line:549] - INFO: 800 / 17538
2023-02-22 21:23:52 - train.py[line:551] - INFO: load:1.12 valid_run:487.07 task_valid:469.47 collect_output:13.41
2023-02-22 21:25:54 - train.py[line:549] - INFO: 1000 / 17538
2023-02-22 21:25:54 - train.py[line:551] - INFO: load:1.15 valid_run:608.34 task_valid:587.44 collect_output:15.69
2023-02-22 21:27:57 - train.py[line:549] - INFO: 1200 / 17538
2023-02-22 21:27:57 - train.py[line:551] - INFO: load:1.17 valid_run:731.24 task_valid:706.91 collect_output:18.09
2023-02-22 21:29:58 - train.py[line:549] - INFO: 1400 / 17538
2023-02-22 21:29:58 - train.py[line:551] - INFO: load:1.20 valid_run:852.82 task_valid:825.89 collect_output:19.62
2023-02-22 21:31:59 - train.py[line:549] - INFO: 1600 / 17538
2023-02-22 21:31:59 - train.py[line:551] - INFO: load:1.22 valid_run:973.29 task_valid:942.93 collect_output:22.00
2023-02-22 21:34:01 - train.py[line:549] - INFO: 1800 / 17538
2023-02-22 21:34:01 - train.py[line:551] - INFO: load:1.25 valid_run:1095.55 task_valid:1060.64 collect_output:25.52
2023-02-22 21:36:01 - train.py[line:549] - INFO: 2000 / 17538
2023-02-22 21:36:01 - train.py[line:551] - INFO: load:1.27 valid_run:1215.43 task_valid:1173.97 collect_output:31.01
2023-02-22 21:38:01 - train.py[line:549] - INFO: 2200 / 17538
2023-02-22 21:38:01 - train.py[line:551] - INFO: load:1.30 valid_run:1335.05 task_valid:1290.10 collect_output:33.44
2023-02-22 21:40:02 - train.py[line:549] - INFO: 2400 / 17538
2023-02-22 21:40:02 - train.py[line:551] - INFO: load:1.33 valid_run:1456.03 task_valid:1407.59 collect_output:35.90
2023-02-22 21:42:02 - train.py[line:549] - INFO: 2600 / 17538
2023-02-22 21:42:02 - train.py[line:551] - INFO: load:1.35 valid_run:1575.87 task_valid:1521.85 collect_output:40.46
2023-02-22 21:44:03 - train.py[line:549] - INFO: 2800 / 17538
2023-02-22 21:44:03 - train.py[line:551] - INFO: load:1.38 valid_run:1697.39 task_valid:1640.27 collect_output:42.51
2023-02-22 21:46:05 - train.py[line:549] - INFO: 3000 / 17538
2023-02-22 21:46:05 - train.py[line:551] - INFO: load:1.40 valid_run:1818.98 task_valid:1757.13 collect_output:46.21
2023-02-22 21:48:05 - train.py[line:549] - INFO: 3200 / 17538
2023-02-22 21:48:05 - train.py[line:551] - INFO: load:1.43 valid_run:1938.59 task_valid:1871.67 collect_output:50.25
2023-02-22 21:50:07 - train.py[line:549] - INFO: 3400 / 17538
2023-02-22 21:50:07 - train.py[line:551] - INFO: load:1.45 valid_run:2060.60 task_valid:1988.32 collect_output:54.57
2023-02-22 21:52:09 - train.py[line:549] - INFO: 3600 / 17538
2023-02-22 21:52:09 - train.py[line:551] - INFO: load:1.48 valid_run:2183.04 task_valid:2106.73 collect_output:57.58
2023-02-22 21:54:11 - train.py[line:549] - INFO: 3800 / 17538
2023-02-22 21:54:11 - train.py[line:551] - INFO: load:1.51 valid_run:2305.04 task_valid:2224.20 collect_output:61.09
2023-02-22 21:56:14 - train.py[line:549] - INFO: 4000 / 17538
2023-02-22 21:56:14 - train.py[line:551] - INFO: load:1.53 valid_run:2427.71 task_valid:2341.47 collect_output:65.48
2023-02-22 21:58:17 - train.py[line:549] - INFO: 4200 / 17538
2023-02-22 21:58:17 - train.py[line:551] - INFO: load:1.56 valid_run:2550.62 task_valid:2458.55 collect_output:70.25
2023-02-22 22:00:19 - train.py[line:549] - INFO: 4400 / 17538
2023-02-22 22:00:19 - train.py[line:551] - INFO: load:1.58 valid_run:2672.67 task_valid:2578.19 collect_output:71.61
2023-02-22 22:02:19 - train.py[line:549] - INFO: 4600 / 17538
2023-02-22 22:02:19 - train.py[line:551] - INFO: load:1.61 valid_run:2792.26 task_valid:2693.16 collect_output:75.19
2023-02-22 22:04:19 - train.py[line:549] - INFO: 4800 / 17538
2023-02-22 22:04:19 - train.py[line:551] - INFO: load:1.63 valid_run:2912.84 task_valid:2809.91 collect_output:77.97
2023-02-22 22:06:20 - train.py[line:549] - INFO: 5000 / 17538
2023-02-22 22:06:20 - train.py[line:551] - INFO: load:1.66 valid_run:3033.23 task_valid:2926.68 collect_output:80.56
2023-02-22 22:08:21 - train.py[line:549] - INFO: 5200 / 17538
2023-02-22 22:08:21 - train.py[line:551] - INFO: load:1.68 valid_run:3154.11 task_valid:3043.34 collect_output:83.74
2023-02-22 22:10:22 - train.py[line:549] - INFO: 5400 / 17538
2023-02-22 22:10:22 - train.py[line:551] - INFO: load:1.71 valid_run:3274.83 task_valid:3158.17 collect_output:88.61
2023-02-22 22:12:24 - train.py[line:549] - INFO: 5600 / 17538
2023-02-22 22:12:24 - train.py[line:551] - INFO: load:1.74 valid_run:3397.43 task_valid:3278.35 collect_output:89.99
2023-02-22 22:14:26 - train.py[line:549] - INFO: 5800 / 17538
2023-02-22 22:14:26 - train.py[line:551] - INFO: load:1.76 valid_run:3519.13 task_valid:3394.59 collect_output:94.42
2023-02-22 22:16:28 - train.py[line:549] - INFO: 6000 / 17538
2023-02-22 22:16:28 - train.py[line:551] - INFO: load:1.79 valid_run:3641.09 task_valid:3513.60 collect_output:96.35
2023-02-22 22:18:30 - train.py[line:549] - INFO: 6200 / 17538
2023-02-22 22:18:30 - train.py[line:551] - INFO: load:1.81 valid_run:3762.75 task_valid:3632.75 collect_output:97.82
2023-02-22 22:20:32 - train.py[line:549] - INFO: 6400 / 17538
2023-02-22 22:20:32 - train.py[line:551] - INFO: load:1.84 valid_run:3884.64 task_valid:3749.53 collect_output:101.92
2023-02-22 22:22:32 - train.py[line:549] - INFO: 6600 / 17538
2023-02-22 22:22:32 - train.py[line:551] - INFO: load:1.86 valid_run:4004.94 task_valid:3865.62 collect_output:105.09
2023-02-22 22:24:35 - train.py[line:549] - INFO: 6800 / 17538
2023-02-22 22:24:35 - train.py[line:551] - INFO: load:1.89 valid_run:4127.52 task_valid:3983.64 collect_output:108.63
2023-02-22 22:26:38 - train.py[line:549] - INFO: 7000 / 17538
2023-02-22 22:26:38 - train.py[line:551] - INFO: load:1.92 valid_run:4251.19 task_valid:4104.47 collect_output:110.46
2023-02-22 22:28:38 - train.py[line:549] - INFO: 7200 / 17538
2023-02-22 22:28:38 - train.py[line:551] - INFO: load:1.94 valid_run:4370.67 task_valid:4220.15 collect_output:113.24
2023-02-22 22:30:41 - train.py[line:549] - INFO: 7400 / 17538
2023-02-22 22:30:41 - train.py[line:551] - INFO: load:1.97 valid_run:4493.83 task_valid:4337.33 collect_output:118.18
2023-02-22 22:32:44 - train.py[line:549] - INFO: 7600 / 17538
2023-02-22 22:32:44 - train.py[line:551] - INFO: load:1.99 valid_run:4616.14 task_valid:4457.29 collect_output:119.52
2023-02-22 22:34:46 - train.py[line:549] - INFO: 7800 / 17538
2023-02-22 22:34:46 - train.py[line:551] - INFO: load:2.02 valid_run:4738.48 task_valid:4576.48 collect_output:121.65
2023-02-22 22:36:50 - train.py[line:549] - INFO: 8000 / 17538
2023-02-22 22:36:50 - train.py[line:551] - INFO: load:2.04 valid_run:4862.33 task_valid:4698.68 collect_output:122.28
2023-02-22 22:38:53 - train.py[line:549] - INFO: 8200 / 17538
2023-02-22 22:38:53 - train.py[line:551] - INFO: load:2.07 valid_run:4985.43 task_valid:4819.38 collect_output:123.68
2023-02-22 22:40:54 - train.py[line:549] - INFO: 8400 / 17538
2023-02-22 22:40:54 - train.py[line:551] - INFO: load:2.09 valid_run:5106.42 task_valid:4936.09 collect_output:126.93
2023-02-22 22:42:57 - train.py[line:549] - INFO: 8600 / 17538
2023-02-22 22:42:57 - train.py[line:551] - INFO: load:2.12 valid_run:5229.67 task_valid:5054.90 collect_output:130.34
2023-02-22 22:44:59 - train.py[line:549] - INFO: 8800 / 17538
2023-02-22 22:44:59 - train.py[line:551] - INFO: load:2.14 valid_run:5351.19 task_valid:5170.61 collect_output:135.12
2023-02-22 22:47:02 - train.py[line:549] - INFO: 9000 / 17538
2023-02-22 22:47:02 - train.py[line:551] - INFO: load:2.17 valid_run:5473.72 task_valid:5287.18 collect_output:140.05
2023-02-22 22:49:04 - train.py[line:549] - INFO: 9200 / 17538
2023-02-22 22:49:04 - train.py[line:551] - INFO: load:2.19 valid_run:5596.30 task_valid:5404.21 collect_output:144.57
2023-02-22 22:51:06 - train.py[line:549] - INFO: 9400 / 17538
2023-02-22 22:51:06 - train.py[line:551] - INFO: load:2.22 valid_run:5718.45 task_valid:5523.10 collect_output:146.81
2023-02-22 22:53:07 - train.py[line:549] - INFO: 9600 / 17538
2023-02-22 22:53:07 - train.py[line:551] - INFO: load:2.25 valid_run:5838.94 task_valid:5639.75 collect_output:149.62
2023-02-22 22:55:06 - train.py[line:549] - INFO: 9800 / 17538
2023-02-22 22:55:06 - train.py[line:551] - INFO: load:2.27 valid_run:5958.27 task_valid:5754.47 collect_output:153.19
2023-02-22 22:57:06 - train.py[line:549] - INFO: 10000 / 17538
2023-02-22 22:57:06 - train.py[line:551] - INFO: load:2.30 valid_run:6077.57 task_valid:5870.23 collect_output:155.68
2023-02-22 22:59:07 - train.py[line:549] - INFO: 10200 / 17538
2023-02-22 22:59:07 - train.py[line:551] - INFO: load:2.33 valid_run:6198.38 task_valid:5986.92 collect_output:158.76
2023-02-22 23:01:09 - train.py[line:549] - INFO: 10400 / 17538
2023-02-22 23:01:09 - train.py[line:551] - INFO: load:2.35 valid_run:6320.46 task_valid:6104.50 collect_output:162.24
2023-02-22 23:03:11 - train.py[line:549] - INFO: 10600 / 17538
2023-02-22 23:03:11 - train.py[line:551] - INFO: load:2.38 valid_run:6442.50 task_valid:6223.21 collect_output:164.53
2023-02-22 23:05:11 - train.py[line:549] - INFO: 10800 / 17538
2023-02-22 23:05:11 - train.py[line:551] - INFO: load:2.40 valid_run:6562.82 task_valid:6338.97 collect_output:168.05
2023-02-22 23:07:12 - train.py[line:549] - INFO: 11000 / 17538
2023-02-22 23:07:12 - train.py[line:551] - INFO: load:2.43 valid_run:6683.29 task_valid:6456.09 collect_output:170.37
2023-02-22 23:09:13 - train.py[line:549] - INFO: 11200 / 17538
2023-02-22 23:09:13 - train.py[line:551] - INFO: load:2.45 valid_run:6804.11 task_valid:6573.65 collect_output:172.57
2023-02-22 23:11:16 - train.py[line:549] - INFO: 11400 / 17538
2023-02-22 23:11:16 - train.py[line:551] - INFO: load:2.48 valid_run:6927.40 task_valid:6695.11 collect_output:173.37
2023-02-22 23:13:19 - train.py[line:549] - INFO: 11600 / 17538
2023-02-22 23:13:19 - train.py[line:551] - INFO: load:2.51 valid_run:7050.11 task_valid:6811.59 collect_output:178.57
2023-02-22 23:15:20 - train.py[line:549] - INFO: 11800 / 17538
2023-02-22 23:15:20 - train.py[line:551] - INFO: load:2.53 valid_run:7171.59 task_valid:6926.14 collect_output:184.47
2023-02-22 23:17:23 - train.py[line:549] - INFO: 12000 / 17538
2023-02-22 23:17:23 - train.py[line:551] - INFO: load:2.56 valid_run:7294.49 task_valid:7044.57 collect_output:187.91
2023-02-22 23:19:25 - train.py[line:549] - INFO: 12200 / 17538
2023-02-22 23:19:25 - train.py[line:551] - INFO: load:2.58 valid_run:7415.94 task_valid:7160.15 collect_output:192.74
2023-02-22 23:21:24 - train.py[line:549] - INFO: 12400 / 17538
2023-02-22 23:21:24 - train.py[line:551] - INFO: load:2.61 valid_run:7534.65 task_valid:7273.93 collect_output:196.65
2023-02-22 23:23:25 - train.py[line:549] - INFO: 12600 / 17538
2023-02-22 23:23:25 - train.py[line:551] - INFO: load:2.63 valid_run:7656.03 task_valid:7392.06 collect_output:198.88
2023-02-22 23:25:26 - train.py[line:549] - INFO: 12800 / 17538
2023-02-22 23:25:26 - train.py[line:551] - INFO: load:2.66 valid_run:7777.21 task_valid:7508.55 collect_output:202.54
2023-02-22 23:27:28 - train.py[line:549] - INFO: 13000 / 17538
2023-02-22 23:27:28 - train.py[line:551] - INFO: load:2.68 valid_run:7898.50 task_valid:7624.49 collect_output:206.85
2023-02-22 23:29:30 - train.py[line:549] - INFO: 13200 / 17538
2023-02-22 23:29:30 - train.py[line:551] - INFO: load:2.71 valid_run:8020.61 task_valid:7740.23 collect_output:212.17
2023-02-22 23:31:31 - train.py[line:549] - INFO: 13400 / 17538
2023-02-22 23:31:31 - train.py[line:551] - INFO: load:2.74 valid_run:8141.90 task_valid:7857.43 collect_output:215.24
2023-02-22 23:33:32 - train.py[line:549] - INFO: 13600 / 17538
2023-02-22 23:33:32 - train.py[line:551] - INFO: load:2.76 valid_run:8262.78 task_valid:7974.41 collect_output:218.10
2023-02-22 23:35:34 - train.py[line:549] - INFO: 13800 / 17538
2023-02-22 23:35:34 - train.py[line:551] - INFO: load:2.79 valid_run:8384.37 task_valid:8092.15 collect_output:220.90
2023-02-22 23:37:34 - train.py[line:549] - INFO: 14000 / 17538
2023-02-22 23:37:34 - train.py[line:551] - INFO: load:2.82 valid_run:8504.93 task_valid:8207.48 collect_output:225.11
2023-02-22 23:39:37 - train.py[line:549] - INFO: 14200 / 17538
2023-02-22 23:39:37 - train.py[line:551] - INFO: load:2.84 valid_run:8627.73 task_valid:8326.55 collect_output:227.80
2023-02-22 23:41:41 - train.py[line:549] - INFO: 14400 / 17538
2023-02-22 23:41:41 - train.py[line:551] - INFO: load:2.87 valid_run:8751.48 task_valid:8446.38 collect_output:230.68
2023-02-22 23:43:42 - train.py[line:549] - INFO: 14600 / 17538
2023-02-22 23:43:42 - train.py[line:551] - INFO: load:2.89 valid_run:8872.51 task_valid:8561.83 collect_output:235.25
2023-02-22 23:45:43 - train.py[line:549] - INFO: 14800 / 17538
2023-02-22 23:45:43 - train.py[line:551] - INFO: load:2.92 valid_run:8993.38 task_valid:8678.61 collect_output:238.30
2023-02-22 23:47:44 - train.py[line:549] - INFO: 15000 / 17538
2023-02-22 23:47:44 - train.py[line:551] - INFO: load:2.95 valid_run:9114.04 task_valid:8795.70 collect_output:240.84
2023-02-22 23:49:46 - train.py[line:549] - INFO: 15200 / 17538
2023-02-22 23:49:46 - train.py[line:551] - INFO: load:2.98 valid_run:9235.76 task_valid:8912.21 collect_output:245.00
2023-02-22 23:51:47 - train.py[line:549] - INFO: 15400 / 17538
2023-02-22 23:51:47 - train.py[line:551] - INFO: load:3.00 valid_run:9357.16 task_valid:9028.90 collect_output:248.69
2023-02-22 23:53:48 - train.py[line:549] - INFO: 15600 / 17538
2023-02-22 23:53:48 - train.py[line:551] - INFO: load:3.03 valid_run:9478.46 task_valid:9144.16 collect_output:253.69
2023-02-22 23:55:49 - train.py[line:549] - INFO: 15800 / 17538
2023-02-22 23:55:49 - train.py[line:551] - INFO: load:3.06 valid_run:9598.91 task_valid:9261.70 collect_output:255.57
2023-02-22 23:57:48 - train.py[line:549] - INFO: 16000 / 17538
2023-02-22 23:57:48 - train.py[line:551] - INFO: load:3.08 valid_run:9717.99 task_valid:9377.26 collect_output:258.06
2023-02-22 23:59:50 - train.py[line:549] - INFO: 16200 / 17538
2023-02-22 23:59:50 - train.py[line:551] - INFO: load:3.11 valid_run:9839.47 task_valid:9493.94 collect_output:261.84
2023-02-23 00:01:53 - train.py[line:549] - INFO: 16400 / 17538
2023-02-23 00:01:53 - train.py[line:551] - INFO: load:3.14 valid_run:9962.66 task_valid:9610.21 collect_output:267.71
2023-02-23 00:03:55 - train.py[line:549] - INFO: 16600 / 17538
2023-02-23 00:03:55 - train.py[line:551] - INFO: load:3.16 valid_run:10084.56 task_valid:9728.33 collect_output:270.46
2023-02-23 00:05:57 - train.py[line:549] - INFO: 16800 / 17538
2023-02-23 00:05:57 - train.py[line:551] - INFO: load:3.19 valid_run:10206.77 task_valid:9845.34 collect_output:274.60
2023-02-23 00:07:58 - train.py[line:549] - INFO: 17000 / 17538
2023-02-23 00:07:58 - train.py[line:551] - INFO: load:3.22 valid_run:10327.94 task_valid:9961.61 collect_output:278.46
2023-02-23 00:09:57 - train.py[line:549] - INFO: 17200 / 17538
2023-02-23 00:09:57 - train.py[line:551] - INFO: load:3.24 valid_run:10447.05 task_valid:10077.36 collect_output:280.76
2023-02-23 00:12:01 - train.py[line:549] - INFO: 17400 / 17538
2023-02-23 00:12:01 - train.py[line:551] - INFO: load:3.27 valid_run:10570.90 task_valid:10198.70 collect_output:282.24

====================================================================================================
SGG eval:     R @ 50: 0.6353;     R @ 100: 0.6744;     R @ 500: 0.6989;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.5078;    mR @ 100: 0.5546;    mR @ 500: 0.5946;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7889) (covered in:0.5392) (covering:0.2667) (eating:0.8491) (flying in:1.0000) (growing on:0.2857) (hanging from:0.5258) (lying on:0.4952) (mounted on:0.0061) (painted on:0.5250) (parked on:1.0000) (playing:0.6667) (riding:0.9600) (says:0.0000) (sitting on:0.6778) (standing on:0.4535) (using:0.6429) (walking in:0.3077) (walking on:0.5606) (watching:0.5417) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6353;     R @ 100: 0.6744;     R @ 500: 0.6989;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.5078;    mR @ 100: 0.5546;    mR @ 500: 0.5946;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7889) (covered in:0.5392) (covering:0.2667) (eating:0.8491) (flying in:1.0000) (growing on:0.2857) (hanging from:0.5258) (lying on:0.4952) (mounted on:0.0061) (painted on:0.5250) (parked on:1.0000) (playing:0.6667) (riding:0.9600) (says:0.0000) (sitting on:0.6778) (standing on:0.4535) (using:0.6429) (walking in:0.3077) (walking on:0.5606) (watching:0.5417) 
--------------------------------------------------------
====================================================================================================

2023-02-23 00:13:56 - train.py[line:487] - INFO: 0.6743550804740798
2023-02-23 00:13:56 - train.py[line:575] - INFO: logits:torch.Size([420900, 21]) sample_ids:torch.Size([420900])
2023-02-23 00:13:56 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.23 | loss_v1 0 | loss_v2 0 | nll_loss 0.061 | ntokens 71.943 | nsentences 23.999 | sample_size 71.943 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.674355 | ppl 1.04 | vqa_score 0.4664 | wps 118 | wpb 71.9 | bsz 24 | num_updates 200
2023-02-23 00:13:56 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 200 updates
2023-02-23 00:13:56 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/temp_test_caption_vg_full_val/1_B20_A1_E9_0.05_0e-5_480/checkpoint_1_200.pt
2023-02-23 00:14:04 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/temp_test_caption_vg_full_val/1_B20_A1_E9_0.05_0e-5_480/checkpoint_1_200.pt
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 211900
Killing subprocess 211902
Main process received SIGINT, exiting
