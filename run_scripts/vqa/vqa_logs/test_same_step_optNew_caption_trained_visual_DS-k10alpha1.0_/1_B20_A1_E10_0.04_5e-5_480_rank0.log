/home/yutianyu/miniconda3/bin/python3: Error while finding module specification for 'torch.distributed.launch' (ModuleNotFoundError: No module named 'torch')
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/__init__.py", line 196, in <module>
    from torch._C import *
RuntimeError: KeyboardInterrupt: 
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1552146
Killing subprocess 1552147
Main process received SIGINT, exiting
2023-02-16 15:20:48 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-16 15:20:48 - utils.py[line:261] - INFO: Start init
2023-02-16 15:20:48 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-16 15:20:48 - utils.py[line:261] - INFO: Start init
2023-02-16 15:20:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-16 15:20:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-16 15:20:49 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-16 15:20:49 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-16 15:21:00 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-16 15:21:00 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-16 15:21:00 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-16 15:21:05 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-16 15:21:05 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-16 15:21:05 - train.py[line:119] - INFO: model: OFAModel
2023-02-16 15:21:05 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-16 15:21:05 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-16 15:21:05 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-16 15:21:05 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-16 15:21:05 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-16 15:21:06 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-16 15:21:06 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-16 15:21:06 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-16 15:21:06 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
2023-02-16 15:21:07 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-16 15:21:07 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-16 15:21:07 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
Done 0.95 cuda cpu, cpu
2023-02-16 15:21:16 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-16 15:21:16 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-16 15:21:16 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-16 15:21:16 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-16 15:21:16 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-16 15:21:17 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-02-16 15:21:17 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv slice_id 1 row count 231280 total row count 462560
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv slice_id 0 row count 231280 total row count 462560
2023-02-16 15:21:18 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 115640, warmup steps 4625, warmup_factor 0.00021621621621621621
2023-02-16 15:21:19 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-16 15:21:19 - train.py[line:312] - INFO: Start iterating over samples
Total steps 115640, warmup steps 4625, warmup_factor 0.00021621621621621621
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
    cli_main()
  File "../../train.py", line 625, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 206, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "../../train.py", line 317, in train
    log_output = trainer.train_step(samples)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/data/private/yutianyu/OFA/trainer.py", line 834, in train_step
    assert sample['id'][0] in ['visualDS', 'caption_']
AssertionError
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
    cli_main()
  File "../../train.py", line 625, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 206, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "../../train.py", line 317, in train
    log_output = trainer.train_step(samples)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/data/private/yutianyu/OFA/trainer.py", line 834, in train_step
    assert sample['id'][0] in ['visualDS', 'caption_']
AssertionError
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=12', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=10', '--warmup-ratio=0.04', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=1000', '--validate-interval-updates=1000', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1552387
Killing subprocess 1552388
2023-02-16 15:46:24 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-16 15:46:24 - utils.py[line:261] - INFO: Start init
2023-02-16 15:46:24 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-16 15:46:24 - utils.py[line:261] - INFO: Start init
2023-02-16 15:46:24 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-16 15:46:24 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-16 15:46:24 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-16 15:46:24 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-16 15:46:34 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-16 15:46:34 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-16 15:46:34 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-16 15:46:38 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-16 15:46:38 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-16 15:46:38 - train.py[line:119] - INFO: model: OFAModel
2023-02-16 15:46:38 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-16 15:46:38 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-16 15:46:38 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-16 15:46:39 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-16 15:46:39 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-16 15:46:39 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-16 15:46:39 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-16 15:46:39 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-16 15:46:39 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
2023-02-16 15:46:40 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-16 15:46:40 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-16 15:46:40 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
Done 0.95 cuda cpu, cpu
2023-02-16 15:46:48 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-16 15:46:48 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-16 15:46:48 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-16 15:46:49 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-16 15:46:49 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-16 15:46:49 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-02-16 15:46:49 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv slice_id 0 row count 231280 total row count 462560
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv slice_id 1 row count 231280 total row count 462560
2023-02-16 15:46:50 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 115640, warmup steps 4625, warmup_factor 0.00021621621621621621
2023-02-16 15:46:51 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-16 15:46:51 - train.py[line:312] - INFO: Start iterating over samples
Total steps 115640, warmup steps 4625, warmup_factor 0.00021621621621621621
2023-02-16 15:47:10 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 11564 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=89.3, ups=0.79, wpb=112.7, bsz=40, num_updates=10, lr=1.08108e-07, gnorm=8.225, clip=100, loss_scale=128, train_wall=15, gb_free=11.1, ema_decay=0.9999, wall=31
2023-02-16 15:47:21 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 11564 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.508, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=95.3, ups=0.86, wpb=111.2, bsz=40, num_updates=20, lr=2.16216e-07, gnorm=9.294, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=42
2023-02-16 15:47:33 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 11564 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=96.7, ups=0.87, wpb=111.7, bsz=40, num_updates=30, lr=3.24324e-07, gnorm=7.825, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54
2023-02-16 15:47:45 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 11564 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=94.5, ups=0.85, wpb=111.8, bsz=40, num_updates=40, lr=4.32432e-07, gnorm=8.458, clip=100, loss_scale=128, train_wall=12, gb_free=11, ema_decay=0.9999, wall=66
2023-02-16 15:47:57 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 11564 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=95.9, ups=0.86, wpb=111.4, bsz=40, num_updates=50, lr=5.40541e-07, gnorm=8.477, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=78
2023-02-16 15:48:08 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 11564 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.1, ups=0.88, wpb=113.4, bsz=40, num_updates=60, lr=6.48649e-07, gnorm=6.872, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=89
2023-02-16 15:48:20 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 11564 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.2, ups=0.87, wpb=112.7, bsz=40, num_updates=70, lr=7.56757e-07, gnorm=7.194, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=100
2023-02-16 15:48:31 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 11564 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.5, ups=0.88, wpb=112.1, bsz=40, num_updates=80, lr=8.64865e-07, gnorm=6.839, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=112
2023-02-16 15:48:43 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 11564 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=96.2, ups=0.85, wpb=112.8, bsz=40, num_updates=90, lr=9.72973e-07, gnorm=6.172, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=124
2023-02-16 15:48:54 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 11564 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.1, ups=0.89, wpb=111.3, bsz=40, num_updates=100, lr=1.08108e-06, gnorm=5.642, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=135
2023-02-16 15:49:05 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 11564 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.2, ups=0.89, wpb=113.1, bsz=40, num_updates=110, lr=1.18919e-06, gnorm=4.748, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=146
2023-02-16 15:49:17 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 11564 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=98.8, ups=0.89, wpb=111.4, bsz=40, num_updates=120, lr=1.2973e-06, gnorm=4.529, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=157
2023-02-16 15:49:28 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 11564 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=98.6, ups=0.89, wpb=111.4, bsz=40, num_updates=130, lr=1.40541e-06, gnorm=4.01, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=169
2023-02-16 15:49:39 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 11564 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=99.3, ups=0.88, wpb=112.8, bsz=40, num_updates=140, lr=1.51351e-06, gnorm=3.362, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=180
2023-02-16 15:49:50 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 11564 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=150, lr=1.62162e-06, gnorm=3.516, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=191
2023-02-16 15:50:01 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 11564 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=103.3, ups=0.92, wpb=111.9, bsz=40, num_updates=160, lr=1.72973e-06, gnorm=3.008, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=202
2023-02-16 15:50:12 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 11564 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=101, ups=0.91, wpb=111.4, bsz=40, num_updates=170, lr=1.83784e-06, gnorm=2.944, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=213
2023-02-16 15:50:24 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 11564 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.6, ups=0.9, wpb=112, bsz=40, num_updates=180, lr=1.94595e-06, gnorm=2.939, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=224
2023-02-16 15:50:35 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 11564 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=190, lr=2.05405e-06, gnorm=2.858, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=236
2023-02-16 15:50:46 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 11564 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=103.3, ups=0.92, wpb=112.5, bsz=40, num_updates=200, lr=2.16216e-06, gnorm=2.809, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=247
2023-02-16 15:50:57 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 11564 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=100.8, ups=0.89, wpb=112.7, bsz=40, num_updates=210, lr=2.27027e-06, gnorm=2.526, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=258
2023-02-16 15:51:08 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 11564 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.5, ups=0.9, wpb=112.4, bsz=40, num_updates=220, lr=2.37838e-06, gnorm=2.39, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=269
2023-02-16 15:51:19 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 11564 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=100.5, ups=0.91, wpb=110.9, bsz=40, num_updates=230, lr=2.48649e-06, gnorm=2.734, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=280
2023-02-16 15:51:30 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 11564 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=98.2, ups=0.87, wpb=112.9, bsz=40, num_updates=240, lr=2.59459e-06, gnorm=2.582, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=291
2023-02-16 15:51:41 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 11564 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=105.8, ups=0.94, wpb=112.1, bsz=40, num_updates=250, lr=2.7027e-06, gnorm=2.422, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=302
2023-02-16 15:51:52 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 11564 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.17, wps=103.4, ups=0.92, wpb=112.5, bsz=40, num_updates=260, lr=2.81081e-06, gnorm=2.469, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=313
2023-02-16 15:52:03 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 11564 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=101.2, ups=0.91, wpb=111.2, bsz=40, num_updates=270, lr=2.91892e-06, gnorm=2.443, clip=100, loss_scale=128, train_wall=11, gb_free=10, ema_decay=0.9999, wall=324
2023-02-16 15:52:14 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 11564 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=103.5, ups=0.92, wpb=112.2, bsz=40, num_updates=280, lr=3.02703e-06, gnorm=2.162, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=335
2023-02-16 15:52:25 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 11564 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=97.5, ups=0.87, wpb=111.7, bsz=40, num_updates=290, lr=3.13514e-06, gnorm=2.379, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=346
2023-02-16 15:52:36 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 11564 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=101.9, ups=0.9, wpb=112.7, bsz=40, num_updates=300, lr=3.24324e-06, gnorm=2.197, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=357
2023-02-16 15:52:48 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 11564 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.6, ups=0.88, wpb=112.1, bsz=40, num_updates=310, lr=3.35135e-06, gnorm=2.139, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=369
2023-02-16 15:52:59 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 11564 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=102.6, ups=0.91, wpb=113.2, bsz=40, num_updates=320, lr=3.45946e-06, gnorm=2.129, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=380
2023-02-16 15:53:10 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 11564 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=100.7, ups=0.9, wpb=112.4, bsz=40, num_updates=330, lr=3.56757e-06, gnorm=2.201, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=391
2023-02-16 15:53:21 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 11564 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.6, ups=0.91, wpb=112.1, bsz=40, num_updates=340, lr=3.67568e-06, gnorm=1.929, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=402
2023-02-16 15:53:32 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 11564 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.3, ups=0.9, wpb=111.9, bsz=40, num_updates=350, lr=3.78378e-06, gnorm=1.797, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=413
2023-02-16 15:53:44 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 11564 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=99.4, ups=0.89, wpb=111.1, bsz=40, num_updates=360, lr=3.89189e-06, gnorm=2.304, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=424
2023-02-16 15:53:55 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 11564 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=102.3, ups=0.91, wpb=112.6, bsz=40, num_updates=370, lr=4e-06, gnorm=2.325, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=435
2023-02-16 15:54:06 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 11564 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=100.6, ups=0.89, wpb=112.9, bsz=40, num_updates=380, lr=4.10811e-06, gnorm=2.11, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=447
2023-02-16 15:54:17 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 11564 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=102.3, ups=0.92, wpb=111.4, bsz=40, num_updates=390, lr=4.21622e-06, gnorm=2.166, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=458
2023-02-16 15:54:28 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 11564 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=98.9, ups=0.88, wpb=112, bsz=40, num_updates=400, lr=4.32432e-06, gnorm=1.904, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=469
2023-02-16 15:54:39 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 11564 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=100.1, ups=0.9, wpb=111.7, bsz=40, num_updates=410, lr=4.43243e-06, gnorm=1.883, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=480
2023-02-16 15:54:51 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 11564 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=99.7, ups=0.89, wpb=111.7, bsz=40, num_updates=420, lr=4.54054e-06, gnorm=1.877, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=491
2023-02-16 15:55:02 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 11564 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=430, lr=4.64865e-06, gnorm=1.709, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=502
2023-02-16 15:55:13 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 11564 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.2, ups=0.89, wpb=111.1, bsz=40, num_updates=440, lr=4.75676e-06, gnorm=1.871, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=514
2023-02-16 15:55:24 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 11564 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.7, ups=0.89, wpb=111.7, bsz=40, num_updates=450, lr=4.86486e-06, gnorm=1.698, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=525
2023-02-16 15:55:35 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 11564 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.8, ups=0.91, wpb=111.9, bsz=40, num_updates=460, lr=4.97297e-06, gnorm=1.903, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=536
2023-02-16 15:55:46 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 11564 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102.4, ups=0.91, wpb=113, bsz=40, num_updates=470, lr=5.08108e-06, gnorm=1.734, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=547
2023-02-16 15:55:58 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 11564 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.8, ups=0.88, wpb=113.3, bsz=40, num_updates=480, lr=5.18919e-06, gnorm=1.793, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=559
2023-02-16 15:56:08 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 11564 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=104.3, ups=0.93, wpb=111.9, bsz=40, num_updates=490, lr=5.2973e-06, gnorm=2.013, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=569
2023-02-16 15:56:19 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 11564 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=102.9, ups=0.91, wpb=113.3, bsz=40, num_updates=500, lr=5.40541e-06, gnorm=1.941, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=580
2023-02-16 15:56:31 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 11564 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.9, ups=0.87, wpb=111, bsz=40, num_updates=510, lr=5.51351e-06, gnorm=1.761, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=592
2023-02-16 15:56:42 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 11564 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.9, ups=0.92, wpb=112.2, bsz=40, num_updates=520, lr=5.62162e-06, gnorm=1.706, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=603
2023-02-16 15:56:53 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 11564 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.3, ups=0.9, wpb=112, bsz=40, num_updates=530, lr=5.72973e-06, gnorm=1.937, clip=100, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=614
2023-02-16 15:57:04 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 11564 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=96.3, ups=0.87, wpb=110.4, bsz=40, num_updates=540, lr=5.83784e-06, gnorm=1.778, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=625
2023-02-16 15:57:15 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 11564 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=104.5, ups=0.93, wpb=112.3, bsz=40, num_updates=550, lr=5.94595e-06, gnorm=1.867, clip=90, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=636
2023-02-16 15:57:26 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 11564 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.6, ups=0.9, wpb=112.2, bsz=40, num_updates=560, lr=6.05405e-06, gnorm=1.537, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=647
2023-02-16 15:57:38 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 11564 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.8, ups=0.87, wpb=112.4, bsz=40, num_updates=570, lr=6.16216e-06, gnorm=1.796, clip=100, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=659
2023-02-16 15:57:49 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 11564 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.4, ups=0.9, wpb=113.5, bsz=40, num_updates=580, lr=6.27027e-06, gnorm=1.684, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=670
2023-02-16 15:58:00 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 11564 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.1, ups=0.89, wpb=112.1, bsz=40, num_updates=590, lr=6.37838e-06, gnorm=2.051, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=681
2023-02-16 15:58:11 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 11564 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.8, ups=0.9, wpb=112, bsz=40, num_updates=600, lr=6.48649e-06, gnorm=1.787, clip=100, loss_scale=256, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=692
2023-02-16 15:58:23 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 11564 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.9, ups=0.88, wpb=112.1, bsz=40, num_updates=610, lr=6.59459e-06, gnorm=1.655, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=704
2023-02-16 15:58:33 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 11564 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=103.1, ups=0.92, wpb=112, bsz=40, num_updates=620, lr=6.7027e-06, gnorm=1.615, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=714
2023-02-16 15:58:45 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 11564 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.88, wpb=112.3, bsz=40, num_updates=630, lr=6.81081e-06, gnorm=1.74, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=726
2023-02-16 15:58:56 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 11564 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=101, ups=0.89, wpb=113, bsz=40, num_updates=640, lr=6.91892e-06, gnorm=1.673, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=737
2023-02-16 15:59:07 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 11564 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.4, ups=0.89, wpb=111.1, bsz=40, num_updates=650, lr=7.02703e-06, gnorm=1.488, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=748
2023-02-16 15:59:19 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 11564 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.1, ups=0.87, wpb=113.5, bsz=40, num_updates=660, lr=7.13514e-06, gnorm=1.57, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=760
2023-02-16 15:59:30 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 11564 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.5, ups=0.87, wpb=112, bsz=40, num_updates=670, lr=7.24324e-06, gnorm=1.685, clip=90, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=771
2023-02-16 15:59:41 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 11564 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100, ups=0.9, wpb=110.8, bsz=40, num_updates=680, lr=7.35135e-06, gnorm=1.657, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=782
2023-02-16 15:59:53 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 11564 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.3, ups=0.88, wpb=112.3, bsz=40, num_updates=690, lr=7.45946e-06, gnorm=1.583, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=793
2023-02-16 16:00:04 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 11564 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98, ups=0.88, wpb=110.8, bsz=40, num_updates=700, lr=7.56757e-06, gnorm=1.713, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=805
2023-02-16 16:00:15 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 11564 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.7, ups=0.9, wpb=110.7, bsz=40, num_updates=710, lr=7.67568e-06, gnorm=1.593, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=816
2023-02-16 16:00:26 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 11564 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=104.7, ups=0.93, wpb=112.6, bsz=40, num_updates=720, lr=7.78378e-06, gnorm=1.65, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=827
2023-02-16 16:00:37 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 11564 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.2, ups=0.9, wpb=111.8, bsz=40, num_updates=730, lr=7.89189e-06, gnorm=1.575, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=838
2023-02-16 16:00:48 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 11564 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.91, wpb=110.7, bsz=40, num_updates=740, lr=8e-06, gnorm=1.442, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=849
2023-02-16 16:00:59 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 11564 loss=0.255, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.6, ups=0.9, wpb=112.3, bsz=40, num_updates=750, lr=8.10811e-06, gnorm=1.521, clip=90, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=860
2023-02-16 16:01:10 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 11564 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.6, ups=0.88, wpb=113, bsz=40, num_updates=760, lr=8.21622e-06, gnorm=1.486, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=871
2023-02-16 16:01:22 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 11564 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.2, ups=0.9, wpb=112, bsz=40, num_updates=770, lr=8.32432e-06, gnorm=1.557, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=883
2023-02-16 16:01:33 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 11564 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.4, ups=0.88, wpb=112, bsz=40, num_updates=780, lr=8.43243e-06, gnorm=1.445, clip=80, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=894
2023-02-16 16:01:44 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 11564 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=107.6, ups=0.96, wpb=112.4, bsz=40, num_updates=790, lr=8.54054e-06, gnorm=1.675, clip=100, loss_scale=256, train_wall=10, gb_free=10.7, ema_decay=0.9999, wall=904
2023-02-16 16:01:54 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 11564 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=105.5, ups=0.94, wpb=112.2, bsz=40, num_updates=800, lr=8.64865e-06, gnorm=1.389, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=915
2023-02-16 16:02:05 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 11564 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.3, ups=0.9, wpb=113.1, bsz=40, num_updates=810, lr=8.75676e-06, gnorm=1.414, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=926
2023-02-16 16:02:16 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 11564 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=105.2, ups=0.95, wpb=111.1, bsz=40, num_updates=820, lr=8.86486e-06, gnorm=1.51, clip=80, loss_scale=256, train_wall=10, gb_free=10.7, ema_decay=0.9999, wall=937
2023-02-16 16:02:27 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 11564 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.88, wpb=113.1, bsz=40, num_updates=830, lr=8.97297e-06, gnorm=1.576, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=948
2023-02-16 16:02:38 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.1, ups=0.91, wpb=113.7, bsz=40, num_updates=840, lr=9.08108e-06, gnorm=1.382, clip=80, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=959
2023-02-16 16:02:49 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 11564 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.4, ups=0.89, wpb=111.1, bsz=40, num_updates=850, lr=9.18919e-06, gnorm=1.571, clip=100, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=970
2023-02-16 16:03:01 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 11564 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.3, ups=0.87, wpb=111.6, bsz=40, num_updates=860, lr=9.2973e-06, gnorm=1.273, clip=80, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=982
2023-02-16 16:03:12 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 11564 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.87, wpb=113.3, bsz=40, num_updates=870, lr=9.40541e-06, gnorm=1.431, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=993
2023-02-16 16:03:23 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 11564 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.9, ups=0.92, wpb=112, bsz=40, num_updates=880, lr=9.51351e-06, gnorm=1.358, clip=80, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1004
2023-02-16 16:03:34 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 11564 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.4, ups=0.92, wpb=111.8, bsz=40, num_updates=890, lr=9.62162e-06, gnorm=1.357, clip=100, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1015
2023-02-16 16:03:45 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 11564 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.5, ups=0.89, wpb=111.9, bsz=40, num_updates=900, lr=9.72973e-06, gnorm=1.365, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1026
2023-02-16 16:03:57 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 11564 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.88, wpb=111.4, bsz=40, num_updates=910, lr=9.83784e-06, gnorm=1.384, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1038
2023-02-16 16:04:08 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.2, ups=0.92, wpb=112.1, bsz=40, num_updates=920, lr=9.94595e-06, gnorm=1.295, clip=90, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1049
2023-02-16 16:04:19 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.9, wpb=111.9, bsz=40, num_updates=930, lr=1.00541e-05, gnorm=1.169, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1060
2023-02-16 16:04:30 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.9, wpb=110.3, bsz=40, num_updates=940, lr=1.01622e-05, gnorm=1.237, clip=80, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1071
2023-02-16 16:04:41 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.88, wpb=111.9, bsz=40, num_updates=950, lr=1.02703e-05, gnorm=1.339, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1082
2023-02-16 16:04:53 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.89, wpb=113, bsz=40, num_updates=960, lr=1.03784e-05, gnorm=1.292, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1093
2023-02-16 16:05:04 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.7, ups=0.9, wpb=112.5, bsz=40, num_updates=970, lr=1.04865e-05, gnorm=1.224, clip=60, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1105
2023-02-16 16:05:15 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 11564 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.7, ups=0.89, wpb=112.6, bsz=40, num_updates=980, lr=1.05946e-05, gnorm=1.443, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1116
2023-02-16 16:05:26 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 11564 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101.9, ups=0.91, wpb=111.6, bsz=40, num_updates=990, lr=1.07027e-05, gnorm=1.508, clip=90, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1127
2023-02-16 16:05:37 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.89, wpb=111.3, bsz=40, num_updates=1000, lr=1.08108e-05, gnorm=1.189, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1138
2023-02-16 16:05:37 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 16:05:37 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-16 16:05:38 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 16:05:38 - train.py[line:551] - INFO: load:0.85 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 16:07:43 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 16:07:43 - train.py[line:551] - INFO: load:0.87 valid_run:124.24 task_valid:120.11 collect_output:3.07
2023-02-16 16:09:43 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 16:09:43 - train.py[line:551] - INFO: load:0.90 valid_run:244.32 task_valid:235.95 collect_output:6.27
2023-02-16 16:11:46 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 16:11:46 - train.py[line:551] - INFO: load:0.92 valid_run:366.95 task_valid:352.50 collect_output:11.34
2023-02-16 16:13:47 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 16:13:47 - train.py[line:551] - INFO: load:0.95 valid_run:488.72 task_valid:466.07 collect_output:18.52
2023-02-16 16:15:48 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 16:15:48 - train.py[line:551] - INFO: load:0.98 valid_run:609.03 task_valid:583.33 collect_output:20.56
2023-02-16 16:17:51 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 16:17:51 - train.py[line:551] - INFO: load:1.00 valid_run:731.87 task_valid:701.93 collect_output:23.79
2023-02-16 16:19:53 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 16:19:53 - train.py[line:551] - INFO: load:1.03 valid_run:854.68 task_valid:819.90 collect_output:27.64
2023-02-16 16:21:55 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 16:21:55 - train.py[line:551] - INFO: load:1.05 valid_run:976.37 task_valid:936.28 collect_output:31.93
2023-02-16 16:23:59 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 16:23:59 - train.py[line:551] - INFO: load:1.08 valid_run:1100.03 task_valid:1053.37 collect_output:37.49
2023-02-16 16:26:01 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 16:26:01 - train.py[line:551] - INFO: load:1.11 valid_run:1221.58 task_valid:1165.78 collect_output:45.63
2023-02-16 16:28:01 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 16:28:01 - train.py[line:551] - INFO: load:1.13 valid_run:1341.77 task_valid:1281.38 collect_output:49.18
2023-02-16 16:30:03 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 16:30:03 - train.py[line:551] - INFO: load:1.16 valid_run:1463.41 task_valid:1398.25 collect_output:52.91
2023-02-16 16:32:02 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 16:32:02 - train.py[line:551] - INFO: load:1.19 valid_run:1582.35 task_valid:1512.10 collect_output:56.98
2023-02-16 16:34:02 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 16:34:02 - train.py[line:551] - INFO: load:1.21 valid_run:1703.16 task_valid:1629.75 collect_output:59.12
2023-02-16 16:36:03 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 16:36:03 - train.py[line:551] - INFO: load:1.24 valid_run:1823.85 task_valid:1745.76 collect_output:62.78
2023-02-16 16:38:04 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 16:38:04 - train.py[line:551] - INFO: load:1.27 valid_run:1944.85 task_valid:1859.47 collect_output:69.05
2023-02-16 16:40:06 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 16:40:06 - train.py[line:551] - INFO: load:1.30 valid_run:2066.11 task_valid:1975.62 collect_output:73.14
2023-02-16 16:42:06 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 16:42:06 - train.py[line:551] - INFO: load:1.32 valid_run:2186.94 task_valid:2093.61 collect_output:74.93
2023-02-16 16:44:08 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 16:44:08 - train.py[line:551] - INFO: load:1.35 valid_run:2308.16 task_valid:2210.43 collect_output:78.32
2023-02-16 16:46:08 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 16:46:08 - train.py[line:551] - INFO: load:1.37 valid_run:2428.58 task_valid:2326.81 collect_output:81.32
2023-02-16 16:48:10 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 16:48:10 - train.py[line:551] - INFO: load:1.40 valid_run:2550.20 task_valid:2443.35 collect_output:85.37
2023-02-16 16:50:12 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 16:50:12 - train.py[line:551] - INFO: load:1.43 valid_run:2672.12 task_valid:2562.06 collect_output:87.55
2023-02-16 16:52:12 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 16:52:12 - train.py[line:551] - INFO: load:1.46 valid_run:2792.36 task_valid:2676.28 collect_output:92.55
2023-02-16 16:54:12 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 16:54:12 - train.py[line:551] - INFO: load:1.48 valid_run:2912.34 task_valid:2792.46 collect_output:95.28
2023-02-16 16:56:14 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 16:56:14 - train.py[line:551] - INFO: load:1.51 valid_run:3034.05 task_valid:2908.73 collect_output:99.69
2023-02-16 16:58:17 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 16:58:17 - train.py[line:551] - INFO: load:1.54 valid_run:3157.16 task_valid:3024.62 collect_output:105.91
2023-02-16 17:00:17 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 17:00:17 - train.py[line:551] - INFO: load:1.57 valid_run:3276.93 task_valid:3138.73 collect_output:110.56
2023-02-16 17:02:19 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 17:02:19 - train.py[line:551] - INFO: load:1.59 valid_run:3398.84 task_valid:3258.10 collect_output:112.09
2023-02-16 17:04:21 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 17:04:21 - train.py[line:551] - INFO: load:1.62 valid_run:3520.46 task_valid:3373.51 collect_output:117.30
2023-02-16 17:06:23 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 17:06:23 - train.py[line:551] - INFO: load:1.65 valid_run:3642.39 task_valid:3491.89 collect_output:119.82
2023-02-16 17:08:24 - train.py[line:549] - INFO: 6200 / 6234
2023-02-16 17:08:24 - train.py[line:551] - INFO: load:1.67 valid_run:3763.38 task_valid:3610.19 collect_output:121.50

====================================================================================================
SGG eval:     R @ 50: 0.1703;     R @ 100: 0.2582;     R @ 500: 0.3450;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0791;    mR @ 100: 0.1319;    mR @ 500: 0.1869;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0293) (covered in:0.0000) (covering:0.0000) (eating:0.2941) (flying in:0.5000) (growing on:0.1250) (hanging from:0.3613) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0000) (parked on:0.1250) (playing:0.0000) (riding:0.2069) (says:0.0000) (sitting on:0.3039) (standing on:0.5075) (using:0.1500) (walking in:0.0000) (walking on:0.0360) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-02-16 17:08:54 - train.py[line:487] - INFO: 0.25816666666666666

====================================================================================================
SGG eval:     R @ 50: 0.1703;     R @ 100: 0.2582;     R @ 500: 0.3450;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0791;    mR @ 100: 0.1319;    mR @ 500: 0.1869;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0293) (covered in:0.0000) (covering:0.0000) (eating:0.2941) (flying in:0.5000) (growing on:0.1250) (hanging from:0.3613) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0000) (parked on:0.1250) (playing:0.0000) (riding:0.2069) (says:0.0000) (sitting on:0.3039) (standing on:0.5075) (using:0.1500) (walking in:0.0000) (walking on:0.0360) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-02-16 17:08:54 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-16 17:08:54 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.271 | loss_v1 0 | loss_v2 0 | nll_loss 0.101 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.258167 | ppl 1.07 | vqa_score 0.1802 | wps 118.2 | wpb 72 | bsz 24 | num_updates 1000
2023-02-16 17:08:54 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 1000 updates
2023-02-16 17:08:54 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_1000.pt
2023-02-16 17:08:59 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_1000.pt
2023-02-16 17:09:04 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 0.25816666666666666) (writing took 10.203011199831963 seconds)
2023-02-16 17:09:15 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 11564 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=0.3, ups=0, wpb=112.8, bsz=40, num_updates=1010, lr=1.09189e-05, gnorm=1.258, clip=70, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=4956
2023-02-16 17:09:26 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=105.1, ups=0.94, wpb=111.9, bsz=40, num_updates=1020, lr=1.1027e-05, gnorm=1.179, clip=70, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4967
2023-02-16 17:09:37 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 11564 loss=0.266, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.9, ups=0.9, wpb=111.6, bsz=40, num_updates=1030, lr=1.11351e-05, gnorm=1.649, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=4978
2023-02-16 17:09:48 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 11564 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=106.4, ups=0.93, wpb=114, bsz=40, num_updates=1040, lr=1.12432e-05, gnorm=1.32, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4989
2023-02-16 17:09:59 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.87, wpb=112.7, bsz=40, num_updates=1050, lr=1.13514e-05, gnorm=1.316, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5000
2023-02-16 17:10:10 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 11564 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103.6, ups=0.93, wpb=110.8, bsz=40, num_updates=1060, lr=1.14595e-05, gnorm=1.261, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5011
2023-02-16 17:10:21 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 11564 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.3, ups=0.91, wpb=113.4, bsz=40, num_updates=1070, lr=1.15676e-05, gnorm=1.362, clip=70, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=5022
2023-02-16 17:10:32 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.3, ups=0.92, wpb=112.2, bsz=40, num_updates=1080, lr=1.16757e-05, gnorm=1.354, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5033
2023-02-16 17:10:43 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 11564 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.5, ups=0.9, wpb=112, bsz=40, num_updates=1090, lr=1.17838e-05, gnorm=1.405, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5044
2023-02-16 17:10:54 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 11564 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102, ups=0.91, wpb=112.2, bsz=40, num_updates=1100, lr=1.18919e-05, gnorm=1.179, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5055
2023-02-16 17:11:05 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.9, bsz=40, num_updates=1110, lr=1.2e-05, gnorm=1.082, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5066
2023-02-16 17:11:16 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 11564 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.1, ups=0.9, wpb=111.9, bsz=40, num_updates=1120, lr=1.21081e-05, gnorm=1.256, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=5077
2023-02-16 17:11:27 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 11564 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.9, wpb=111.4, bsz=40, num_updates=1130, lr=1.22162e-05, gnorm=1.356, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5088
2023-02-16 17:11:39 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 11564 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.3, ups=0.88, wpb=111.5, bsz=40, num_updates=1140, lr=1.23243e-05, gnorm=1.271, clip=80, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5100
2023-02-16 17:11:50 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.88, wpb=111, bsz=40, num_updates=1150, lr=1.24324e-05, gnorm=0.998, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5111
2023-02-16 17:12:01 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.9, wpb=111.6, bsz=40, num_updates=1160, lr=1.25405e-05, gnorm=0.939, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5122
2023-02-16 17:12:12 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.92, wpb=112, bsz=40, num_updates=1170, lr=1.26486e-05, gnorm=1.08, clip=60, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=5133
2023-02-16 17:12:23 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.9, wpb=112.6, bsz=40, num_updates=1180, lr=1.27568e-05, gnorm=1.099, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5144
2023-02-16 17:12:35 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.88, wpb=111.6, bsz=40, num_updates=1190, lr=1.28649e-05, gnorm=1.381, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5156
2023-02-16 17:12:46 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 11564 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.88, wpb=112.1, bsz=40, num_updates=1200, lr=1.2973e-05, gnorm=1.207, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5167
2023-02-16 17:12:57 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 11564 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.7, ups=0.93, wpb=112.4, bsz=40, num_updates=1210, lr=1.30811e-05, gnorm=1.154, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5178
2023-02-16 17:13:08 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 11564 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.7, ups=0.92, wpb=111.5, bsz=40, num_updates=1220, lr=1.31892e-05, gnorm=1.176, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5188
2023-02-16 17:13:18 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.8, ups=0.92, wpb=112.1, bsz=40, num_updates=1230, lr=1.32973e-05, gnorm=1.122, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5199
2023-02-16 17:13:30 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.88, wpb=112.6, bsz=40, num_updates=1240, lr=1.34054e-05, gnorm=1.004, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5211
2023-02-16 17:13:41 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 11564 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.8, ups=0.89, wpb=112.6, bsz=40, num_updates=1250, lr=1.35135e-05, gnorm=1.221, clip=90, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=5222
2023-02-16 17:13:52 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.9, wpb=112.9, bsz=40, num_updates=1260, lr=1.36216e-05, gnorm=0.92, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5233
2023-02-16 17:14:03 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.9, wpb=111.5, bsz=40, num_updates=1270, lr=1.37297e-05, gnorm=0.989, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5244
2023-02-16 17:14:14 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.91, wpb=110.7, bsz=40, num_updates=1280, lr=1.38378e-05, gnorm=1.111, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5255
2023-02-16 17:14:25 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=105.7, ups=0.94, wpb=112.7, bsz=40, num_updates=1290, lr=1.39459e-05, gnorm=1.133, clip=50, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5266
2023-02-16 17:14:36 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=111.5, bsz=40, num_updates=1300, lr=1.40541e-05, gnorm=1.065, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5277
2023-02-16 17:14:47 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 11564 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.2, ups=0.91, wpb=111.7, bsz=40, num_updates=1310, lr=1.41622e-05, gnorm=1.254, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5288
2023-02-16 17:14:58 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.91, wpb=112, bsz=40, num_updates=1320, lr=1.42703e-05, gnorm=1.124, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5299
2023-02-16 17:15:10 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.89, wpb=111.7, bsz=40, num_updates=1330, lr=1.43784e-05, gnorm=0.957, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5311
2023-02-16 17:15:21 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 11564 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.7, ups=0.91, wpb=111.1, bsz=40, num_updates=1340, lr=1.44865e-05, gnorm=1.485, clip=100, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=5322
2023-02-16 17:15:32 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 11564 loss=0.247, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.3, ups=0.87, wpb=112.5, bsz=40, num_updates=1350, lr=1.45946e-05, gnorm=1.064, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5333
2023-02-16 17:15:43 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.7, ups=0.92, wpb=111.6, bsz=40, num_updates=1360, lr=1.47027e-05, gnorm=1.258, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5344
2023-02-16 17:15:54 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=110.5, bsz=40, num_updates=1370, lr=1.48108e-05, gnorm=1.255, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5355
2023-02-16 17:16:05 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.91, wpb=112.4, bsz=40, num_updates=1380, lr=1.49189e-05, gnorm=1.07, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5366
2023-02-16 17:16:16 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 11564 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.91, wpb=110.6, bsz=40, num_updates=1390, lr=1.5027e-05, gnorm=1.275, clip=80, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=5377
2023-02-16 17:16:27 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 11564 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.9, wpb=112, bsz=40, num_updates=1400, lr=1.51351e-05, gnorm=1.262, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5388
2023-02-16 17:16:38 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.91, wpb=112, bsz=40, num_updates=1410, lr=1.52432e-05, gnorm=1.187, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5399
2023-02-16 17:16:50 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 11564 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.3, ups=0.88, wpb=111.4, bsz=40, num_updates=1420, lr=1.53514e-05, gnorm=1.162, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5411
2023-02-16 17:17:01 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.88, wpb=112.7, bsz=40, num_updates=1430, lr=1.54595e-05, gnorm=0.857, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5422
2023-02-16 17:17:12 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 11564 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.9, wpb=111.9, bsz=40, num_updates=1440, lr=1.55676e-05, gnorm=1.013, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5433
2023-02-16 17:17:24 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.9, ups=0.88, wpb=113, bsz=40, num_updates=1450, lr=1.56757e-05, gnorm=1.24, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5445
2023-02-16 17:17:35 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.5, ups=0.88, wpb=110.2, bsz=40, num_updates=1460, lr=1.57838e-05, gnorm=1.012, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5456
2023-02-16 17:17:46 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.89, wpb=111.5, bsz=40, num_updates=1470, lr=1.58919e-05, gnorm=0.955, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5467
2023-02-16 17:17:57 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.9, ups=0.91, wpb=112.5, bsz=40, num_updates=1480, lr=1.6e-05, gnorm=1.079, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5478
2023-02-16 17:18:08 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.91, wpb=111.2, bsz=40, num_updates=1490, lr=1.61081e-05, gnorm=1.114, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5489
2023-02-16 17:18:19 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.89, wpb=111.6, bsz=40, num_updates=1500, lr=1.62162e-05, gnorm=0.992, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5500
2023-02-16 17:18:30 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.91, wpb=111.8, bsz=40, num_updates=1510, lr=1.63243e-05, gnorm=1.196, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5511
2023-02-16 17:18:42 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.9, wpb=110.7, bsz=40, num_updates=1520, lr=1.64324e-05, gnorm=1.189, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5522
2023-02-16 17:18:53 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 11564 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.9, wpb=112.7, bsz=40, num_updates=1530, lr=1.65405e-05, gnorm=1.189, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5534
2023-02-16 17:19:04 - progress_bar.py[line:274] - INFO: epoch 001:   1540 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.87, wpb=112.7, bsz=40, num_updates=1540, lr=1.66486e-05, gnorm=1.186, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5545
2023-02-16 17:19:15 - progress_bar.py[line:274] - INFO: epoch 001:   1550 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.91, wpb=110.8, bsz=40, num_updates=1550, lr=1.67568e-05, gnorm=0.918, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5556
2023-02-16 17:19:26 - progress_bar.py[line:274] - INFO: epoch 001:   1560 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.9, wpb=109.9, bsz=40, num_updates=1560, lr=1.68649e-05, gnorm=1.14, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5567
2023-02-16 17:19:38 - progress_bar.py[line:274] - INFO: epoch 001:   1570 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=111.3, bsz=40, num_updates=1570, lr=1.6973e-05, gnorm=1.174, clip=60, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5579
2023-02-16 17:19:49 - progress_bar.py[line:274] - INFO: epoch 001:   1580 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.89, wpb=113.6, bsz=40, num_updates=1580, lr=1.70811e-05, gnorm=0.932, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5590
2023-02-16 17:19:52 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 17:20:01 - progress_bar.py[line:274] - INFO: epoch 001:   1591 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=91.2, ups=0.82, wpb=111.5, bsz=40, num_updates=1590, lr=1.71892e-05, gnorm=1.045, clip=50, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5602
2023-02-16 17:20:12 - progress_bar.py[line:274] - INFO: epoch 001:   1601 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.89, wpb=112.2, bsz=40, num_updates=1600, lr=1.72973e-05, gnorm=1.088, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5613
2023-02-16 17:20:23 - progress_bar.py[line:274] - INFO: epoch 001:   1611 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.9, wpb=112.7, bsz=40, num_updates=1610, lr=1.74054e-05, gnorm=0.928, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=5624
2023-02-16 17:20:35 - progress_bar.py[line:274] - INFO: epoch 001:   1621 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.89, wpb=111.9, bsz=40, num_updates=1620, lr=1.75135e-05, gnorm=1.098, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5635
2023-02-16 17:20:46 - progress_bar.py[line:274] - INFO: epoch 001:   1631 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.89, wpb=112.8, bsz=40, num_updates=1630, lr=1.76216e-05, gnorm=0.893, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5647
2023-02-16 17:20:57 - progress_bar.py[line:274] - INFO: epoch 001:   1641 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.89, wpb=112.3, bsz=40, num_updates=1640, lr=1.77297e-05, gnorm=1.118, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5658
2023-02-16 17:21:08 - progress_bar.py[line:274] - INFO: epoch 001:   1651 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.8, ups=0.91, wpb=113.5, bsz=40, num_updates=1650, lr=1.78378e-05, gnorm=0.931, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5669
2023-02-16 17:21:19 - progress_bar.py[line:274] - INFO: epoch 001:   1661 / 11564 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.1, ups=0.89, wpb=112.1, bsz=40, num_updates=1660, lr=1.79459e-05, gnorm=1.203, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5680
2023-02-16 17:21:30 - progress_bar.py[line:274] - INFO: epoch 001:   1671 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102, ups=0.91, wpb=112.7, bsz=40, num_updates=1670, lr=1.80541e-05, gnorm=1.042, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5691
2023-02-16 17:21:41 - progress_bar.py[line:274] - INFO: epoch 001:   1681 / 11564 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.89, wpb=111.9, bsz=40, num_updates=1680, lr=1.81622e-05, gnorm=1.097, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=5702
2023-02-16 17:21:52 - progress_bar.py[line:274] - INFO: epoch 001:   1691 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.91, wpb=111.8, bsz=40, num_updates=1690, lr=1.82703e-05, gnorm=0.979, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5713
2023-02-16 17:22:03 - progress_bar.py[line:274] - INFO: epoch 001:   1701 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=105.1, ups=0.93, wpb=112.8, bsz=40, num_updates=1700, lr=1.83784e-05, gnorm=1.044, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5724
2023-02-16 17:22:14 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.92, wpb=112.5, bsz=40, num_updates=1710, lr=1.84865e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5735
2023-02-16 17:22:25 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 11564 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=1720, lr=1.85946e-05, gnorm=0.972, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5746
2023-02-16 17:22:37 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.87, wpb=112.3, bsz=40, num_updates=1730, lr=1.87027e-05, gnorm=0.948, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5758
2023-02-16 17:22:48 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.6, ups=0.91, wpb=112.2, bsz=40, num_updates=1740, lr=1.88108e-05, gnorm=1.121, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5769
2023-02-16 17:22:59 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.92, wpb=109.6, bsz=40, num_updates=1750, lr=1.89189e-05, gnorm=0.9, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5780
2023-02-16 17:23:10 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.91, wpb=112.9, bsz=40, num_updates=1760, lr=1.9027e-05, gnorm=1.025, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5791
2023-02-16 17:23:21 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.91, wpb=112.1, bsz=40, num_updates=1770, lr=1.91351e-05, gnorm=1.058, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5802
2023-02-16 17:23:32 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.9, ups=0.91, wpb=112.5, bsz=40, num_updates=1780, lr=1.92432e-05, gnorm=1, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5813
2023-02-16 17:23:43 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=111.7, bsz=40, num_updates=1790, lr=1.93514e-05, gnorm=0.859, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5824
2023-02-16 17:23:55 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.9, wpb=113.4, bsz=40, num_updates=1800, lr=1.94595e-05, gnorm=1.037, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5835
2023-02-16 17:24:06 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.88, wpb=112.1, bsz=40, num_updates=1810, lr=1.95676e-05, gnorm=0.9, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5847
2023-02-16 17:24:17 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.6, ups=0.92, wpb=112.8, bsz=40, num_updates=1820, lr=1.96757e-05, gnorm=0.758, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5858
2023-02-16 17:24:28 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.88, wpb=112.4, bsz=40, num_updates=1830, lr=1.97838e-05, gnorm=0.994, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5869
2023-02-16 17:24:39 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 11564 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.91, wpb=110.7, bsz=40, num_updates=1840, lr=1.98919e-05, gnorm=1.107, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5880
2023-02-16 17:24:50 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.4, ups=0.91, wpb=111.9, bsz=40, num_updates=1850, lr=2e-05, gnorm=1.014, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5891
2023-02-16 17:25:01 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.89, wpb=111.9, bsz=40, num_updates=1860, lr=2.01081e-05, gnorm=0.917, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5902
2023-02-16 17:25:13 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 11564 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.88, wpb=110.8, bsz=40, num_updates=1870, lr=2.02162e-05, gnorm=1.039, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5914
2023-02-16 17:25:24 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.91, wpb=111.2, bsz=40, num_updates=1880, lr=2.03243e-05, gnorm=1.148, clip=60, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=5925
2023-02-16 17:25:35 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.7, ups=0.91, wpb=114.5, bsz=40, num_updates=1890, lr=2.04324e-05, gnorm=0.948, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5936
2023-02-16 17:25:46 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 11564 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.3, ups=0.87, wpb=112.1, bsz=40, num_updates=1900, lr=2.05405e-05, gnorm=1.166, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=5947
2023-02-16 17:25:58 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=110.8, bsz=40, num_updates=1910, lr=2.06486e-05, gnorm=1.155, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5959
2023-02-16 17:26:09 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 11564 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.9, wpb=110.8, bsz=40, num_updates=1920, lr=2.07568e-05, gnorm=1.141, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5970
2023-02-16 17:26:20 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.1, ups=0.91, wpb=112.5, bsz=40, num_updates=1930, lr=2.08649e-05, gnorm=1.023, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5981
2023-02-16 17:26:31 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.88, wpb=112.2, bsz=40, num_updates=1940, lr=2.0973e-05, gnorm=1.076, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5992
2023-02-16 17:26:42 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.1, ups=0.92, wpb=112.3, bsz=40, num_updates=1950, lr=2.10811e-05, gnorm=0.888, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6003
2023-02-16 17:26:53 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.2, ups=0.92, wpb=113.4, bsz=40, num_updates=1960, lr=2.11892e-05, gnorm=1.088, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6014
2023-02-16 17:27:04 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.89, wpb=112, bsz=40, num_updates=1970, lr=2.12973e-05, gnorm=0.906, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6025
2023-02-16 17:27:15 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=111.5, bsz=40, num_updates=1980, lr=2.14054e-05, gnorm=0.816, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=6036
2023-02-16 17:27:27 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.88, wpb=111.7, bsz=40, num_updates=1990, lr=2.15135e-05, gnorm=0.863, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6048
2023-02-16 17:27:38 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103, ups=0.91, wpb=113.2, bsz=40, num_updates=2000, lr=2.16216e-05, gnorm=1.008, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6059
2023-02-16 17:27:38 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 17:27:39 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 17:27:39 - train.py[line:551] - INFO: load:0.88 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 17:29:41 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 17:29:41 - train.py[line:551] - INFO: load:0.91 valid_run:122.47 task_valid:118.82 collect_output:2.62
2023-02-16 17:31:41 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 17:31:41 - train.py[line:551] - INFO: load:0.93 valid_run:242.34 task_valid:234.41 collect_output:5.92
2023-02-16 17:33:44 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 17:33:44 - train.py[line:551] - INFO: load:0.95 valid_run:365.00 task_valid:350.80 collect_output:11.17
2023-02-16 17:35:46 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 17:35:46 - train.py[line:551] - INFO: load:0.98 valid_run:486.99 task_valid:464.44 collect_output:18.52
2023-02-16 17:37:46 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 17:37:46 - train.py[line:551] - INFO: load:1.00 valid_run:607.30 task_valid:581.65 collect_output:20.62
2023-02-16 17:39:49 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 17:39:49 - train.py[line:551] - INFO: load:1.03 valid_run:730.14 task_valid:700.34 collect_output:23.78
2023-02-16 17:41:52 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 17:41:52 - train.py[line:551] - INFO: load:1.05 valid_run:852.98 task_valid:818.39 collect_output:27.57
2023-02-16 17:43:54 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 17:43:54 - train.py[line:551] - INFO: load:1.08 valid_run:974.73 task_valid:934.78 collect_output:31.93
2023-02-16 17:45:58 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 17:45:58 - train.py[line:551] - INFO: load:1.10 valid_run:1098.42 task_valid:1051.98 collect_output:37.44
2023-02-16 17:48:00 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 17:48:00 - train.py[line:551] - INFO: load:1.13 valid_run:1220.08 task_valid:1164.53 collect_output:45.54
2023-02-16 17:50:00 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 17:50:00 - train.py[line:551] - INFO: load:1.15 valid_run:1340.20 task_valid:1280.06 collect_output:49.12
2023-02-16 17:52:01 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 17:52:01 - train.py[line:551] - INFO: load:1.18 valid_run:1461.68 task_valid:1396.92 collect_output:52.73
2023-02-16 17:54:00 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 17:54:00 - train.py[line:551] - INFO: load:1.20 valid_run:1580.38 task_valid:1510.40 collect_output:56.94
2023-02-16 17:56:01 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 17:56:01 - train.py[line:551] - INFO: load:1.23 valid_run:1701.27 task_valid:1628.01 collect_output:59.22
2023-02-16 17:58:02 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 17:58:02 - train.py[line:551] - INFO: load:1.25 valid_run:1822.08 task_valid:1744.06 collect_output:62.98
2023-02-16 18:00:03 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 18:00:03 - train.py[line:551] - INFO: load:1.28 valid_run:1943.03 task_valid:1857.87 collect_output:69.12
2023-02-16 18:02:04 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 18:02:04 - train.py[line:551] - INFO: load:1.30 valid_run:2064.21 task_valid:1973.85 collect_output:73.31
2023-02-16 18:04:05 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 18:04:05 - train.py[line:551] - INFO: load:1.33 valid_run:2184.63 task_valid:2091.50 collect_output:75.12
2023-02-16 18:06:06 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 18:06:06 - train.py[line:551] - INFO: load:1.35 valid_run:2305.90 task_valid:2208.54 collect_output:78.34
2023-02-16 18:08:06 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 18:08:06 - train.py[line:551] - INFO: load:1.37 valid_run:2426.19 task_valid:2325.13 collect_output:81.05
2023-02-16 18:10:08 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 18:10:08 - train.py[line:551] - INFO: load:1.40 valid_run:2547.89 task_valid:2441.68 collect_output:85.20
2023-02-16 18:12:10 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 18:12:10 - train.py[line:551] - INFO: load:1.42 valid_run:2669.65 task_valid:2560.38 collect_output:87.27
2023-02-16 18:14:10 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 18:14:10 - train.py[line:551] - INFO: load:1.45 valid_run:2789.97 task_valid:2674.49 collect_output:92.49
2023-02-16 18:16:10 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 18:16:10 - train.py[line:551] - INFO: load:1.47 valid_run:2909.54 task_valid:2790.36 collect_output:95.19
2023-02-16 18:18:11 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 18:18:11 - train.py[line:551] - INFO: load:1.50 valid_run:3031.11 task_valid:2906.50 collect_output:99.63
2023-02-16 18:20:14 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 18:20:14 - train.py[line:551] - INFO: load:1.52 valid_run:3153.82 task_valid:3022.34 collect_output:105.50
2023-02-16 18:22:14 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 18:22:14 - train.py[line:551] - INFO: load:1.55 valid_run:3273.30 task_valid:3136.27 collect_output:110.05
2023-02-16 18:24:16 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 18:24:16 - train.py[line:551] - INFO: load:1.57 valid_run:3394.99 task_valid:3255.56 collect_output:111.48
2023-02-16 18:26:17 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 18:26:17 - train.py[line:551] - INFO: load:1.60 valid_run:3516.57 task_valid:3370.88 collect_output:116.74
2023-02-16 18:28:19 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 18:28:19 - train.py[line:551] - INFO: load:1.62 valid_run:3638.42 task_valid:3489.18 collect_output:119.31
2023-02-16 18:30:20 - train.py[line:549] - INFO: 6200 / 6234
2023-02-16 18:30:20 - train.py[line:551] - INFO: load:1.65 valid_run:3759.44 task_valid:3607.65 collect_output:120.87

====================================================================================================
SGG eval:     R @ 50: 0.3334;     R @ 100: 0.3836;     R @ 500: 0.4557;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1455;    mR @ 100: 0.1991;    mR @ 500: 0.2420;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.1098) (covered in:0.0000) (covering:0.1429) (eating:0.4412) (flying in:0.5000) (growing on:0.1250) (hanging from:0.4968) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.2500) (playing:0.0000) (riding:0.4951) (says:0.0000) (sitting on:0.4705) (standing on:0.5008) (using:0.1500) (walking in:0.0000) (walking on:0.2162) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-02-16 18:30:50 - train.py[line:487] - INFO: 0.38356666666666667

====================================================================================================
SGG eval:     R @ 50: 0.3334;     R @ 100: 0.3836;     R @ 500: 0.4557;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1455;    mR @ 100: 0.1991;    mR @ 500: 0.2420;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.1098) (covered in:0.0000) (covering:0.1429) (eating:0.4412) (flying in:0.5000) (growing on:0.1250) (hanging from:0.4968) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.2500) (playing:0.0000) (riding:0.4951) (says:0.0000) (sitting on:0.4705) (standing on:0.5008) (using:0.1500) (walking in:0.0000) (walking on:0.2162) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-02-16 18:30:50 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-16 18:30:50 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.283 | loss_v1 0 | loss_v2 0 | nll_loss 0.12 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.383567 | ppl 1.09 | vqa_score 0.1892 | wps 118.3 | wpb 72 | bsz 24 | num_updates 2000 | best_R@100 0.383567
2023-02-16 18:30:50 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2023-02-16 18:30:50 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_2000.pt
2023-02-16 18:30:56 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_2000.pt
2023-02-16 18:31:01 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.38356666666666667) (writing took 10.239252764731646 seconds)
2023-02-16 18:31:12 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=0.3, ups=0, wpb=111, bsz=40, num_updates=2010, lr=2.17297e-05, gnorm=1.136, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9873
2023-02-16 18:31:23 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101, ups=0.9, wpb=112, bsz=40, num_updates=2020, lr=2.18378e-05, gnorm=1.065, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9884
2023-02-16 18:31:34 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.89, wpb=111.4, bsz=40, num_updates=2030, lr=2.19459e-05, gnorm=1.062, clip=60, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=9895
2023-02-16 18:31:46 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.87, wpb=113.2, bsz=40, num_updates=2040, lr=2.20541e-05, gnorm=0.876, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9907
2023-02-16 18:31:57 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.89, wpb=111.5, bsz=40, num_updates=2050, lr=2.21622e-05, gnorm=0.86, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9918
2023-02-16 18:32:08 - progress_bar.py[line:274] - INFO: epoch 001:   2061 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.9, wpb=110.4, bsz=40, num_updates=2060, lr=2.22703e-05, gnorm=1.104, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=9929
2023-02-16 18:32:20 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.88, wpb=111.1, bsz=40, num_updates=2070, lr=2.23784e-05, gnorm=1.079, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9941
2023-02-16 18:32:31 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.88, wpb=112.2, bsz=40, num_updates=2080, lr=2.24865e-05, gnorm=0.913, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9952
2023-02-16 18:32:42 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.89, wpb=112.9, bsz=40, num_updates=2090, lr=2.25946e-05, gnorm=0.95, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=9963
2023-02-16 18:32:53 - progress_bar.py[line:274] - INFO: epoch 001:   2101 / 11564 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.91, wpb=109.8, bsz=40, num_updates=2100, lr=2.27027e-05, gnorm=1.271, clip=70, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=9974
2023-02-16 18:33:04 - progress_bar.py[line:274] - INFO: epoch 001:   2111 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102, ups=0.91, wpb=112, bsz=40, num_updates=2110, lr=2.28108e-05, gnorm=1.105, clip=70, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=9985
2023-02-16 18:33:10 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 18:33:16 - progress_bar.py[line:274] - INFO: epoch 001:   2122 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=92.4, ups=0.82, wpb=112.8, bsz=40, num_updates=2120, lr=2.29189e-05, gnorm=1.093, clip=60, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=9997
2023-02-16 18:33:28 - progress_bar.py[line:274] - INFO: epoch 001:   2132 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.89, wpb=112.7, bsz=40, num_updates=2130, lr=2.3027e-05, gnorm=0.934, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10009
2023-02-16 18:33:39 - progress_bar.py[line:274] - INFO: epoch 001:   2142 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.91, wpb=111, bsz=40, num_updates=2140, lr=2.31351e-05, gnorm=0.941, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10020
2023-02-16 18:33:50 - progress_bar.py[line:274] - INFO: epoch 001:   2152 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.8, ups=0.92, wpb=112, bsz=40, num_updates=2150, lr=2.32432e-05, gnorm=1.058, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10031
2023-02-16 18:34:01 - progress_bar.py[line:274] - INFO: epoch 001:   2162 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.88, wpb=112.3, bsz=40, num_updates=2160, lr=2.33514e-05, gnorm=0.953, clip=20, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10042
2023-02-16 18:34:12 - progress_bar.py[line:274] - INFO: epoch 001:   2172 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.3, ups=0.93, wpb=112.3, bsz=40, num_updates=2170, lr=2.34595e-05, gnorm=1.01, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10053
2023-02-16 18:34:23 - progress_bar.py[line:274] - INFO: epoch 001:   2182 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.9, wpb=110.4, bsz=40, num_updates=2180, lr=2.35676e-05, gnorm=1.105, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10064
2023-02-16 18:34:34 - progress_bar.py[line:274] - INFO: epoch 001:   2192 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.9, wpb=112.1, bsz=40, num_updates=2190, lr=2.36757e-05, gnorm=0.695, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10075
2023-02-16 18:34:45 - progress_bar.py[line:274] - INFO: epoch 001:   2202 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.92, wpb=111.6, bsz=40, num_updates=2200, lr=2.37838e-05, gnorm=0.808, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=10086
2023-02-16 18:34:56 - progress_bar.py[line:274] - INFO: epoch 001:   2212 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.88, wpb=112.9, bsz=40, num_updates=2210, lr=2.38919e-05, gnorm=0.908, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10097
2023-02-16 18:35:07 - progress_bar.py[line:274] - INFO: epoch 001:   2222 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.9, wpb=111.4, bsz=40, num_updates=2220, lr=2.4e-05, gnorm=0.898, clip=40, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10108
2023-02-16 18:35:19 - progress_bar.py[line:274] - INFO: epoch 001:   2232 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=111.5, bsz=40, num_updates=2230, lr=2.41081e-05, gnorm=0.829, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10120
2023-02-16 18:35:30 - progress_bar.py[line:274] - INFO: epoch 001:   2242 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.91, wpb=110.4, bsz=40, num_updates=2240, lr=2.42162e-05, gnorm=0.776, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10131
2023-02-16 18:35:40 - progress_bar.py[line:274] - INFO: epoch 001:   2252 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=105.7, ups=0.93, wpb=113.2, bsz=40, num_updates=2250, lr=2.43243e-05, gnorm=0.862, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10141
2023-02-16 18:35:52 - progress_bar.py[line:274] - INFO: epoch 001:   2262 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.4, ups=0.9, wpb=111.9, bsz=40, num_updates=2260, lr=2.44324e-05, gnorm=0.778, clip=10, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=10152
2023-02-16 18:36:03 - progress_bar.py[line:274] - INFO: epoch 001:   2272 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=112.7, bsz=40, num_updates=2270, lr=2.45405e-05, gnorm=0.973, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10164
2023-02-16 18:36:14 - progress_bar.py[line:274] - INFO: epoch 001:   2282 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.91, wpb=110.1, bsz=40, num_updates=2280, lr=2.46486e-05, gnorm=0.97, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10175
2023-02-16 18:36:25 - progress_bar.py[line:274] - INFO: epoch 001:   2292 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97, ups=0.88, wpb=110.3, bsz=40, num_updates=2290, lr=2.47568e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10186
2023-02-16 18:36:36 - progress_bar.py[line:274] - INFO: epoch 001:   2302 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.92, wpb=111.8, bsz=40, num_updates=2300, lr=2.48649e-05, gnorm=0.795, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10197
2023-02-16 18:36:47 - progress_bar.py[line:274] - INFO: epoch 001:   2312 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.4, ups=0.9, wpb=112.1, bsz=40, num_updates=2310, lr=2.4973e-05, gnorm=0.819, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=10208
2023-02-16 18:36:58 - progress_bar.py[line:274] - INFO: epoch 001:   2322 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.92, wpb=110.9, bsz=40, num_updates=2320, lr=2.50811e-05, gnorm=0.802, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10219
2023-02-16 18:37:09 - progress_bar.py[line:274] - INFO: epoch 001:   2332 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.9, wpb=112.5, bsz=40, num_updates=2330, lr=2.51892e-05, gnorm=0.865, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10230
2023-02-16 18:37:21 - progress_bar.py[line:274] - INFO: epoch 001:   2342 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.89, wpb=111.3, bsz=40, num_updates=2340, lr=2.52973e-05, gnorm=0.889, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10242
2023-02-16 18:37:32 - progress_bar.py[line:274] - INFO: epoch 001:   2352 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.7, ups=0.91, wpb=112.4, bsz=40, num_updates=2350, lr=2.54054e-05, gnorm=0.924, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10253
2023-02-16 18:37:43 - progress_bar.py[line:274] - INFO: epoch 001:   2362 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.9, wpb=112.1, bsz=40, num_updates=2360, lr=2.55135e-05, gnorm=0.944, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10264
2023-02-16 18:37:54 - progress_bar.py[line:274] - INFO: epoch 001:   2372 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.92, wpb=111.3, bsz=40, num_updates=2370, lr=2.56216e-05, gnorm=0.836, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10275
2023-02-16 18:38:05 - progress_bar.py[line:274] - INFO: epoch 001:   2382 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.2, ups=0.93, wpb=112.5, bsz=40, num_updates=2380, lr=2.57297e-05, gnorm=0.884, clip=30, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=10285
2023-02-16 18:38:16 - progress_bar.py[line:274] - INFO: epoch 001:   2392 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.88, wpb=112.9, bsz=40, num_updates=2390, lr=2.58378e-05, gnorm=0.916, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10297
2023-02-16 18:38:27 - progress_bar.py[line:274] - INFO: epoch 001:   2402 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.88, wpb=112.2, bsz=40, num_updates=2400, lr=2.59459e-05, gnorm=0.951, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10308
2023-02-16 18:38:38 - progress_bar.py[line:274] - INFO: epoch 001:   2412 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.9, wpb=111.9, bsz=40, num_updates=2410, lr=2.60541e-05, gnorm=0.812, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10319
2023-02-16 18:38:49 - progress_bar.py[line:274] - INFO: epoch 001:   2422 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.8, ups=0.91, wpb=110.8, bsz=40, num_updates=2420, lr=2.61622e-05, gnorm=0.848, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10330
2023-02-16 18:39:01 - progress_bar.py[line:274] - INFO: epoch 001:   2432 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.3, ups=0.87, wpb=111.5, bsz=40, num_updates=2430, lr=2.62703e-05, gnorm=0.727, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10342
2023-02-16 18:39:12 - progress_bar.py[line:274] - INFO: epoch 001:   2442 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.91, wpb=111.8, bsz=40, num_updates=2440, lr=2.63784e-05, gnorm=0.737, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10353
2023-02-16 18:39:23 - progress_bar.py[line:274] - INFO: epoch 001:   2452 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.92, wpb=110.3, bsz=40, num_updates=2450, lr=2.64865e-05, gnorm=0.683, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10364
2023-02-16 18:39:34 - progress_bar.py[line:274] - INFO: epoch 001:   2462 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.91, wpb=111.9, bsz=40, num_updates=2460, lr=2.65946e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10375
2023-02-16 18:39:45 - progress_bar.py[line:274] - INFO: epoch 001:   2472 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.6, ups=0.93, wpb=112.4, bsz=40, num_updates=2470, lr=2.67027e-05, gnorm=0.869, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10386
2023-02-16 18:39:56 - progress_bar.py[line:274] - INFO: epoch 001:   2482 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.6, ups=0.87, wpb=110.8, bsz=40, num_updates=2480, lr=2.68108e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10397
2023-02-16 18:40:07 - progress_bar.py[line:274] - INFO: epoch 001:   2492 / 11564 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.1, ups=0.91, wpb=113.1, bsz=40, num_updates=2490, lr=2.69189e-05, gnorm=0.888, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10408
2023-02-16 18:40:18 - progress_bar.py[line:274] - INFO: epoch 001:   2502 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.91, wpb=112.2, bsz=40, num_updates=2500, lr=2.7027e-05, gnorm=0.883, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=10419
2023-02-16 18:40:29 - progress_bar.py[line:274] - INFO: epoch 001:   2512 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.88, wpb=112.7, bsz=40, num_updates=2510, lr=2.71351e-05, gnorm=0.748, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=10430
2023-02-16 18:40:41 - progress_bar.py[line:274] - INFO: epoch 001:   2522 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.6, ups=0.91, wpb=111, bsz=40, num_updates=2520, lr=2.72432e-05, gnorm=0.863, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10441
2023-02-16 18:40:52 - progress_bar.py[line:274] - INFO: epoch 001:   2532 / 11564 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.88, wpb=112.3, bsz=40, num_updates=2530, lr=2.73514e-05, gnorm=0.917, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10453
2023-02-16 18:41:03 - progress_bar.py[line:274] - INFO: epoch 001:   2542 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.89, wpb=111.6, bsz=40, num_updates=2540, lr=2.74595e-05, gnorm=0.849, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10464
2023-02-16 18:41:14 - progress_bar.py[line:274] - INFO: epoch 001:   2552 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.9, ups=0.92, wpb=110.9, bsz=40, num_updates=2550, lr=2.75676e-05, gnorm=0.846, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10475
2023-02-16 18:41:25 - progress_bar.py[line:274] - INFO: epoch 001:   2562 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103, ups=0.91, wpb=113.7, bsz=40, num_updates=2560, lr=2.76757e-05, gnorm=0.845, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10486
2023-02-16 18:41:36 - progress_bar.py[line:274] - INFO: epoch 001:   2572 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.92, wpb=110.6, bsz=40, num_updates=2570, lr=2.77838e-05, gnorm=0.696, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10497
2023-02-16 18:41:47 - progress_bar.py[line:274] - INFO: epoch 001:   2582 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.6, ups=0.92, wpb=112.6, bsz=40, num_updates=2580, lr=2.78919e-05, gnorm=0.728, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10508
2023-02-16 18:41:58 - progress_bar.py[line:274] - INFO: epoch 001:   2592 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.91, wpb=112.1, bsz=40, num_updates=2590, lr=2.8e-05, gnorm=0.695, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10519
2023-02-16 18:42:09 - progress_bar.py[line:274] - INFO: epoch 001:   2602 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.87, wpb=111.3, bsz=40, num_updates=2600, lr=2.81081e-05, gnorm=0.75, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10530
2023-02-16 18:42:20 - progress_bar.py[line:274] - INFO: epoch 001:   2612 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.89, wpb=111.4, bsz=40, num_updates=2610, lr=2.82162e-05, gnorm=0.819, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10541
2023-02-16 18:42:32 - progress_bar.py[line:274] - INFO: epoch 001:   2622 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.88, wpb=110.9, bsz=40, num_updates=2620, lr=2.83243e-05, gnorm=0.789, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10553
2023-02-16 18:42:43 - progress_bar.py[line:274] - INFO: epoch 001:   2632 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.89, wpb=111.1, bsz=40, num_updates=2630, lr=2.84324e-05, gnorm=0.91, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10564
2023-02-16 18:42:55 - progress_bar.py[line:274] - INFO: epoch 001:   2642 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.87, wpb=112.7, bsz=40, num_updates=2640, lr=2.85405e-05, gnorm=0.849, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10575
2023-02-16 18:43:06 - progress_bar.py[line:274] - INFO: epoch 001:   2652 / 11564 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.9, wpb=111.9, bsz=40, num_updates=2650, lr=2.86486e-05, gnorm=1.049, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10587
2023-02-16 18:43:17 - progress_bar.py[line:274] - INFO: epoch 001:   2662 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.87, wpb=111.2, bsz=40, num_updates=2660, lr=2.87568e-05, gnorm=0.834, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10598
2023-02-16 18:43:28 - progress_bar.py[line:274] - INFO: epoch 001:   2672 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.9, wpb=111.7, bsz=40, num_updates=2670, lr=2.88649e-05, gnorm=0.867, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10609
2023-02-16 18:43:39 - progress_bar.py[line:274] - INFO: epoch 001:   2682 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.92, wpb=111.4, bsz=40, num_updates=2680, lr=2.8973e-05, gnorm=0.588, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10620
2023-02-16 18:43:50 - progress_bar.py[line:274] - INFO: epoch 001:   2692 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.3, ups=0.92, wpb=112.8, bsz=40, num_updates=2690, lr=2.90811e-05, gnorm=0.873, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=10631
2023-02-16 18:44:01 - progress_bar.py[line:274] - INFO: epoch 001:   2702 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.87, wpb=112.6, bsz=40, num_updates=2700, lr=2.91892e-05, gnorm=0.635, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10642
2023-02-16 18:44:12 - progress_bar.py[line:274] - INFO: epoch 001:   2712 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.3, ups=0.92, wpb=112.5, bsz=40, num_updates=2710, lr=2.92973e-05, gnorm=0.936, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10653
2023-02-16 18:44:24 - progress_bar.py[line:274] - INFO: epoch 001:   2722 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.6, bsz=40, num_updates=2720, lr=2.94054e-05, gnorm=0.874, clip=40, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10664
2023-02-16 18:44:25 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 18:44:36 - progress_bar.py[line:274] - INFO: epoch 001:   2733 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=92.7, ups=0.84, wpb=110.9, bsz=40, num_updates=2730, lr=2.95135e-05, gnorm=0.797, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=10677
2023-02-16 18:44:47 - progress_bar.py[line:274] - INFO: epoch 001:   2743 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.9, ups=0.92, wpb=111, bsz=40, num_updates=2740, lr=2.96216e-05, gnorm=0.855, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10687
2023-02-16 18:44:58 - progress_bar.py[line:274] - INFO: epoch 001:   2753 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.88, wpb=113.2, bsz=40, num_updates=2750, lr=2.97297e-05, gnorm=0.741, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10699
2023-02-16 18:45:09 - progress_bar.py[line:274] - INFO: epoch 001:   2763 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.1, ups=0.88, wpb=112.4, bsz=40, num_updates=2760, lr=2.98378e-05, gnorm=1.067, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10710
2023-02-16 18:45:21 - progress_bar.py[line:274] - INFO: epoch 001:   2773 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.88, wpb=112.2, bsz=40, num_updates=2770, lr=2.99459e-05, gnorm=0.819, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10721
2023-02-16 18:45:31 - progress_bar.py[line:274] - INFO: epoch 001:   2783 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.9, ups=0.92, wpb=112.3, bsz=40, num_updates=2780, lr=3.00541e-05, gnorm=0.779, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10732
2023-02-16 18:45:43 - progress_bar.py[line:274] - INFO: epoch 001:   2793 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.88, wpb=111.9, bsz=40, num_updates=2790, lr=3.01622e-05, gnorm=0.807, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=10744
2023-02-16 18:45:54 - progress_bar.py[line:274] - INFO: epoch 001:   2803 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.89, wpb=110, bsz=40, num_updates=2800, lr=3.02703e-05, gnorm=0.935, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10755
2023-02-16 18:46:05 - progress_bar.py[line:274] - INFO: epoch 001:   2813 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.9, wpb=111.3, bsz=40, num_updates=2810, lr=3.03784e-05, gnorm=0.758, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10766
2023-02-16 18:46:16 - progress_bar.py[line:274] - INFO: epoch 001:   2823 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.2, ups=0.92, wpb=112, bsz=40, num_updates=2820, lr=3.04865e-05, gnorm=0.708, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10777
2023-02-16 18:46:27 - progress_bar.py[line:274] - INFO: epoch 001:   2833 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=112.4, bsz=40, num_updates=2830, lr=3.05946e-05, gnorm=1.091, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10788
2023-02-16 18:46:38 - progress_bar.py[line:274] - INFO: epoch 001:   2843 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.9, ups=0.93, wpb=112.4, bsz=40, num_updates=2840, lr=3.07027e-05, gnorm=0.801, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10799
2023-02-16 18:46:49 - progress_bar.py[line:274] - INFO: epoch 001:   2853 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.4, ups=0.91, wpb=111.9, bsz=40, num_updates=2850, lr=3.08108e-05, gnorm=0.992, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=10810
2023-02-16 18:47:00 - progress_bar.py[line:274] - INFO: epoch 001:   2863 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.9, wpb=111.5, bsz=40, num_updates=2860, lr=3.09189e-05, gnorm=0.776, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10821
2023-02-16 18:47:11 - progress_bar.py[line:274] - INFO: epoch 001:   2873 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.6, ups=0.93, wpb=111, bsz=40, num_updates=2870, lr=3.1027e-05, gnorm=1.06, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10832
2023-02-16 18:47:22 - progress_bar.py[line:274] - INFO: epoch 001:   2883 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.87, wpb=112.4, bsz=40, num_updates=2880, lr=3.11351e-05, gnorm=1.009, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10843
2023-02-16 18:47:33 - progress_bar.py[line:274] - INFO: epoch 001:   2893 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.9, wpb=112.2, bsz=40, num_updates=2890, lr=3.12432e-05, gnorm=0.82, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10854
2023-02-16 18:47:44 - progress_bar.py[line:274] - INFO: epoch 001:   2903 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104, ups=0.93, wpb=111.7, bsz=40, num_updates=2900, lr=3.13514e-05, gnorm=1.041, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10865
2023-02-16 18:47:55 - progress_bar.py[line:274] - INFO: epoch 001:   2913 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.88, wpb=112.9, bsz=40, num_updates=2910, lr=3.14595e-05, gnorm=0.674, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10876
2023-02-16 18:48:06 - progress_bar.py[line:274] - INFO: epoch 001:   2923 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.3, ups=0.93, wpb=111.9, bsz=40, num_updates=2920, lr=3.15676e-05, gnorm=0.863, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10887
2023-02-16 18:48:18 - progress_bar.py[line:274] - INFO: epoch 001:   2933 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.89, wpb=113, bsz=40, num_updates=2930, lr=3.16757e-05, gnorm=0.589, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10898
2023-02-16 18:48:29 - progress_bar.py[line:274] - INFO: epoch 001:   2943 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.88, wpb=112.6, bsz=40, num_updates=2940, lr=3.17838e-05, gnorm=0.779, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=10910
2023-02-16 18:48:40 - progress_bar.py[line:274] - INFO: epoch 001:   2953 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.8, ups=0.87, wpb=110.8, bsz=40, num_updates=2950, lr=3.18919e-05, gnorm=0.831, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10921
2023-02-16 18:48:51 - progress_bar.py[line:274] - INFO: epoch 001:   2963 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.91, wpb=112.7, bsz=40, num_updates=2960, lr=3.2e-05, gnorm=0.663, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=10932
2023-02-16 18:49:02 - progress_bar.py[line:274] - INFO: epoch 001:   2973 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.9, wpb=112.8, bsz=40, num_updates=2970, lr=3.21081e-05, gnorm=0.731, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=10943
2023-02-16 18:49:14 - progress_bar.py[line:274] - INFO: epoch 001:   2983 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.9, wpb=112.5, bsz=40, num_updates=2980, lr=3.22162e-05, gnorm=0.707, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=10955
2023-02-16 18:49:25 - progress_bar.py[line:274] - INFO: epoch 001:   2993 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.88, wpb=113.3, bsz=40, num_updates=2990, lr=3.23243e-05, gnorm=0.836, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10966
2023-02-16 18:49:36 - progress_bar.py[line:274] - INFO: epoch 001:   3003 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.9, wpb=110.7, bsz=40, num_updates=3000, lr=3.24324e-05, gnorm=0.805, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=10977
2023-02-16 18:49:36 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 18:49:37 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 18:49:37 - train.py[line:551] - INFO: load:0.95 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 18:51:40 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 18:51:40 - train.py[line:551] - INFO: load:0.97 valid_run:122.17 task_valid:119.10 collect_output:2.01
2023-02-16 18:53:40 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 18:53:40 - train.py[line:551] - INFO: load:1.00 valid_run:242.21 task_valid:235.00 collect_output:5.13
2023-02-16 18:55:42 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 18:55:42 - train.py[line:551] - INFO: load:1.02 valid_run:364.18 task_valid:351.42 collect_output:9.69
2023-02-16 18:57:44 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 18:57:44 - train.py[line:551] - INFO: load:1.05 valid_run:486.10 task_valid:465.09 collect_output:16.89
2023-02-16 18:59:44 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 18:59:44 - train.py[line:551] - INFO: load:1.08 valid_run:606.58 task_valid:582.40 collect_output:19.06
2023-02-16 19:01:47 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 19:01:47 - train.py[line:551] - INFO: load:1.10 valid_run:729.47 task_valid:701.10 collect_output:22.22
2023-02-16 19:03:50 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 19:03:50 - train.py[line:551] - INFO: load:1.13 valid_run:852.43 task_valid:819.05 collect_output:26.23
2023-02-16 19:05:52 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 19:05:52 - train.py[line:551] - INFO: load:1.16 valid_run:974.17 task_valid:935.46 collect_output:30.53
2023-02-16 19:07:56 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 19:07:56 - train.py[line:551] - INFO: load:1.18 valid_run:1097.97 task_valid:1052.58 collect_output:36.21
2023-02-16 19:09:58 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 19:09:58 - train.py[line:551] - INFO: load:1.21 valid_run:1219.61 task_valid:1165.19 collect_output:44.22
2023-02-16 19:11:58 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 19:11:58 - train.py[line:551] - INFO: load:1.23 valid_run:1339.80 task_valid:1280.74 collect_output:47.85
2023-02-16 19:13:59 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 19:13:59 - train.py[line:551] - INFO: load:1.26 valid_run:1461.34 task_valid:1397.57 collect_output:51.56
2023-02-16 19:15:58 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 19:15:58 - train.py[line:551] - INFO: load:1.28 valid_run:1580.10 task_valid:1511.21 collect_output:55.67
2023-02-16 19:17:59 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 19:17:59 - train.py[line:551] - INFO: load:1.31 valid_run:1700.89 task_valid:1628.78 collect_output:57.90
2023-02-16 19:20:00 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 19:20:00 - train.py[line:551] - INFO: load:1.34 valid_run:1821.65 task_valid:1744.73 collect_output:61.68
2023-02-16 19:22:01 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 19:22:01 - train.py[line:551] - INFO: load:1.36 valid_run:1942.64 task_valid:1858.56 collect_output:67.82
2023-02-16 19:24:02 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 19:24:02 - train.py[line:551] - INFO: load:1.39 valid_run:2063.85 task_valid:1974.49 collect_output:72.09
2023-02-16 19:26:03 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 19:26:03 - train.py[line:551] - INFO: load:1.41 valid_run:2184.42 task_valid:2092.30 collect_output:73.83
2023-02-16 19:28:04 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 19:28:04 - train.py[line:551] - INFO: load:1.44 valid_run:2305.65 task_valid:2209.19 collect_output:77.17
2023-02-16 19:30:04 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 19:30:04 - train.py[line:551] - INFO: load:1.47 valid_run:2425.84 task_valid:2325.68 collect_output:79.86
2023-02-16 19:32:06 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 19:32:06 - train.py[line:551] - INFO: load:1.49 valid_run:2547.40 task_valid:2442.15 collect_output:83.91
2023-02-16 19:34:08 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 19:34:08 - train.py[line:551] - INFO: load:1.52 valid_run:2669.34 task_valid:2560.90 collect_output:86.09
2023-02-16 19:36:08 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 19:36:08 - train.py[line:551] - INFO: load:1.55 valid_run:2789.50 task_valid:2675.02 collect_output:91.14
2023-02-16 19:38:08 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 19:38:08 - train.py[line:551] - INFO: load:1.57 valid_run:2909.09 task_valid:2791.07 collect_output:93.65
2023-02-16 19:40:09 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 19:40:09 - train.py[line:551] - INFO: load:1.60 valid_run:3030.57 task_valid:2907.10 collect_output:98.07
2023-02-16 19:42:12 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 19:42:12 - train.py[line:551] - INFO: load:1.63 valid_run:3153.29 task_valid:3022.88 collect_output:104.00
2023-02-16 19:44:12 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 19:44:12 - train.py[line:551] - INFO: load:1.65 valid_run:3272.67 task_valid:3136.75 collect_output:108.49
2023-02-16 19:46:13 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 19:46:13 - train.py[line:551] - INFO: load:1.68 valid_run:3394.42 task_valid:3256.04 collect_output:109.95
2023-02-16 19:48:15 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 19:48:15 - train.py[line:551] - INFO: load:1.71 valid_run:3516.06 task_valid:3371.39 collect_output:115.23
2023-02-16 19:50:17 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 19:50:17 - train.py[line:551] - INFO: load:1.73 valid_run:3638.00 task_valid:3489.81 collect_output:117.74
2023-02-16 19:52:18 - train.py[line:549] - INFO: 6200 / 6234
2023-02-16 19:52:18 - train.py[line:551] - INFO: load:1.76 valid_run:3758.99 task_valid:3608.17 collect_output:119.36

====================================================================================================
SGG eval:     R @ 50: 0.4577;     R @ 100: 0.5004;     R @ 500: 0.5679;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2269;    mR @ 100: 0.2988;    mR @ 500: 0.3655;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.3732) (covered in:0.0625) (covering:0.3714) (eating:0.6471) (flying in:0.6364) (growing on:0.1250) (hanging from:0.4516) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.5417) (playing:0.0000) (riding:0.7428) (says:0.0000) (sitting on:0.5686) (standing on:0.4800) (using:0.3000) (walking in:0.0000) (walking on:0.2838) (watching:0.1250) 
--------------------------------------------------------
====================================================================================================

2023-02-16 19:52:48 - train.py[line:487] - INFO: 0.5004359307359307
2023-02-16 19:52:49 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.4577;     R @ 100: 0.5004;     R @ 500: 0.5679;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2269;    mR @ 100: 0.2988;    mR @ 500: 0.3655;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.3732) (covered in:0.0625) (covering:0.3714) (eating:0.6471) (flying in:0.6364) (growing on:0.1250) (hanging from:0.4516) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.5417) (playing:0.0000) (riding:0.7428) (says:0.0000) (sitting on:0.5686) (standing on:0.4800) (using:0.3000) (walking in:0.0000) (walking on:0.2838) (watching:0.1250) 
--------------------------------------------------------
====================================================================================================

2023-02-16 19:52:49 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.288 | loss_v1 0 | loss_v2 0 | nll_loss 0.134 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.500436 | ppl 1.1 | vqa_score 0.2489 | wps 118.3 | wpb 72 | bsz 24 | num_updates 3000 | best_R@100 0.500436
2023-02-16 19:52:49 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 3000 updates
2023-02-16 19:52:49 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_3000.pt
2023-02-16 19:52:55 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_3000.pt
2023-02-16 19:53:00 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_3000.pt (epoch 1 @ 3000 updates, score 0.5004359307359307) (writing took 11.332041703164577 seconds)
2023-02-16 19:53:11 - progress_bar.py[line:274] - INFO: epoch 001:   3013 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=0.3, ups=0, wpb=111.4, bsz=40, num_updates=3010, lr=3.25405e-05, gnorm=0.77, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14792
2023-02-16 19:53:22 - progress_bar.py[line:274] - INFO: epoch 001:   3023 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=3020, lr=3.26486e-05, gnorm=0.842, clip=30, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=14803
2023-02-16 19:53:33 - progress_bar.py[line:274] - INFO: epoch 001:   3033 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.92, wpb=111.2, bsz=40, num_updates=3030, lr=3.27568e-05, gnorm=0.663, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14814
2023-02-16 19:53:45 - progress_bar.py[line:274] - INFO: epoch 001:   3043 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.88, wpb=112.9, bsz=40, num_updates=3040, lr=3.28649e-05, gnorm=0.903, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14825
2023-02-16 19:53:56 - progress_bar.py[line:274] - INFO: epoch 001:   3053 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.88, wpb=112, bsz=40, num_updates=3050, lr=3.2973e-05, gnorm=0.815, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14837
2023-02-16 19:54:07 - progress_bar.py[line:274] - INFO: epoch 001:   3063 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.9, wpb=111.6, bsz=40, num_updates=3060, lr=3.30811e-05, gnorm=0.934, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14848
2023-02-16 19:54:18 - progress_bar.py[line:274] - INFO: epoch 001:   3073 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.9, wpb=111, bsz=40, num_updates=3070, lr=3.31892e-05, gnorm=0.796, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=14859
2023-02-16 19:54:29 - progress_bar.py[line:274] - INFO: epoch 001:   3083 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=3080, lr=3.32973e-05, gnorm=0.823, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14870
2023-02-16 19:54:40 - progress_bar.py[line:274] - INFO: epoch 001:   3093 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.9, ups=0.92, wpb=112.3, bsz=40, num_updates=3090, lr=3.34054e-05, gnorm=0.693, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=14881
2023-02-16 19:54:51 - progress_bar.py[line:274] - INFO: epoch 001:   3103 / 11564 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.89, wpb=111.6, bsz=40, num_updates=3100, lr=3.35135e-05, gnorm=0.815, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14892
2023-02-16 19:55:03 - progress_bar.py[line:274] - INFO: epoch 001:   3113 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.9, wpb=113.3, bsz=40, num_updates=3110, lr=3.36216e-05, gnorm=0.635, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14903
2023-02-16 19:55:14 - progress_bar.py[line:274] - INFO: epoch 001:   3123 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.88, wpb=112.1, bsz=40, num_updates=3120, lr=3.37297e-05, gnorm=0.742, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14915
2023-02-16 19:55:25 - progress_bar.py[line:274] - INFO: epoch 001:   3133 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.91, wpb=112.3, bsz=40, num_updates=3130, lr=3.38378e-05, gnorm=0.736, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14926
2023-02-16 19:55:36 - progress_bar.py[line:274] - INFO: epoch 001:   3143 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.7, ups=0.92, wpb=111.7, bsz=40, num_updates=3140, lr=3.39459e-05, gnorm=0.835, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=14937
2023-02-16 19:55:47 - progress_bar.py[line:274] - INFO: epoch 001:   3153 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.5, ups=0.87, wpb=110.5, bsz=40, num_updates=3150, lr=3.40541e-05, gnorm=0.805, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14948
2023-02-16 19:55:58 - progress_bar.py[line:274] - INFO: epoch 001:   3163 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.91, wpb=112.5, bsz=40, num_updates=3160, lr=3.41622e-05, gnorm=0.812, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14959
2023-02-16 19:56:09 - progress_bar.py[line:274] - INFO: epoch 001:   3173 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.91, wpb=112.8, bsz=40, num_updates=3170, lr=3.42703e-05, gnorm=0.885, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=14970
2023-02-16 19:56:21 - progress_bar.py[line:274] - INFO: epoch 001:   3183 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.89, wpb=112.5, bsz=40, num_updates=3180, lr=3.43784e-05, gnorm=0.707, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=14981
2023-02-16 19:56:32 - progress_bar.py[line:274] - INFO: epoch 001:   3193 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.87, wpb=112.6, bsz=40, num_updates=3190, lr=3.44865e-05, gnorm=0.805, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14993
2023-02-16 19:56:43 - progress_bar.py[line:274] - INFO: epoch 001:   3203 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.7, ups=0.92, wpb=112.7, bsz=40, num_updates=3200, lr=3.45946e-05, gnorm=0.717, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15004
2023-02-16 19:56:54 - progress_bar.py[line:274] - INFO: epoch 001:   3213 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.89, wpb=113, bsz=40, num_updates=3210, lr=3.47027e-05, gnorm=0.727, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15015
2023-02-16 19:57:05 - progress_bar.py[line:274] - INFO: epoch 001:   3223 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.9, wpb=110.7, bsz=40, num_updates=3220, lr=3.48108e-05, gnorm=0.687, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15026
2023-02-16 19:57:17 - progress_bar.py[line:274] - INFO: epoch 001:   3233 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.2, ups=0.87, wpb=111.4, bsz=40, num_updates=3230, lr=3.49189e-05, gnorm=0.809, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15038
2023-02-16 19:57:28 - progress_bar.py[line:274] - INFO: epoch 001:   3243 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.87, wpb=112.8, bsz=40, num_updates=3240, lr=3.5027e-05, gnorm=0.58, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15049
2023-02-16 19:57:40 - progress_bar.py[line:274] - INFO: epoch 001:   3253 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.88, wpb=110.5, bsz=40, num_updates=3250, lr=3.51351e-05, gnorm=0.865, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15060
2023-02-16 19:57:51 - progress_bar.py[line:274] - INFO: epoch 001:   3263 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.91, wpb=110.1, bsz=40, num_updates=3260, lr=3.52432e-05, gnorm=0.833, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15071
2023-02-16 19:58:01 - progress_bar.py[line:274] - INFO: epoch 001:   3273 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.2, ups=0.93, wpb=111.9, bsz=40, num_updates=3270, lr=3.53514e-05, gnorm=0.67, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15082
2023-02-16 19:58:12 - progress_bar.py[line:274] - INFO: epoch 001:   3283 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.89, wpb=110.1, bsz=40, num_updates=3280, lr=3.54595e-05, gnorm=0.813, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15093
2023-02-16 19:58:23 - progress_bar.py[line:274] - INFO: epoch 001:   3293 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.8, ups=0.92, wpb=111.9, bsz=40, num_updates=3290, lr=3.55676e-05, gnorm=0.937, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15104
2023-02-16 19:58:35 - progress_bar.py[line:274] - INFO: epoch 001:   3303 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.87, wpb=111.3, bsz=40, num_updates=3300, lr=3.56757e-05, gnorm=0.677, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15116
2023-02-16 19:58:46 - progress_bar.py[line:274] - INFO: epoch 001:   3313 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.91, wpb=112.9, bsz=40, num_updates=3310, lr=3.57838e-05, gnorm=0.822, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15127
2023-02-16 19:58:57 - progress_bar.py[line:274] - INFO: epoch 001:   3323 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.3, ups=0.92, wpb=112.1, bsz=40, num_updates=3320, lr=3.58919e-05, gnorm=0.644, clip=20, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15138
2023-02-16 19:59:08 - progress_bar.py[line:274] - INFO: epoch 001:   3333 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.88, wpb=110.8, bsz=40, num_updates=3330, lr=3.6e-05, gnorm=0.741, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15149
2023-02-16 19:59:19 - progress_bar.py[line:274] - INFO: epoch 001:   3343 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=111.4, bsz=40, num_updates=3340, lr=3.61081e-05, gnorm=0.811, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15160
2023-02-16 19:59:30 - progress_bar.py[line:274] - INFO: epoch 001:   3353 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.91, wpb=112.4, bsz=40, num_updates=3350, lr=3.62162e-05, gnorm=0.779, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15171
2023-02-16 19:59:41 - progress_bar.py[line:274] - INFO: epoch 001:   3363 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.2, ups=0.93, wpb=110.8, bsz=40, num_updates=3360, lr=3.63243e-05, gnorm=0.762, clip=20, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=15182
2023-02-16 19:59:52 - progress_bar.py[line:274] - INFO: epoch 001:   3373 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.91, wpb=112.1, bsz=40, num_updates=3370, lr=3.64324e-05, gnorm=0.712, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15193
2023-02-16 20:00:03 - progress_bar.py[line:274] - INFO: epoch 001:   3383 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.88, wpb=112.3, bsz=40, num_updates=3380, lr=3.65405e-05, gnorm=0.768, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15204
2023-02-16 20:00:15 - progress_bar.py[line:274] - INFO: epoch 001:   3393 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.89, wpb=112.6, bsz=40, num_updates=3390, lr=3.66486e-05, gnorm=0.789, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15216
2023-02-16 20:00:26 - progress_bar.py[line:274] - INFO: epoch 001:   3403 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.89, wpb=112.1, bsz=40, num_updates=3400, lr=3.67568e-05, gnorm=0.714, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15227
2023-02-16 20:00:37 - progress_bar.py[line:274] - INFO: epoch 001:   3413 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.89, wpb=111, bsz=40, num_updates=3410, lr=3.68649e-05, gnorm=0.688, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15238
2023-02-16 20:00:48 - progress_bar.py[line:274] - INFO: epoch 001:   3423 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.91, wpb=110.9, bsz=40, num_updates=3420, lr=3.6973e-05, gnorm=0.752, clip=20, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=15249
2023-02-16 20:00:56 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 20:01:00 - progress_bar.py[line:274] - INFO: epoch 001:   3434 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.2, ups=0.85, wpb=111.9, bsz=40, num_updates=3430, lr=3.70811e-05, gnorm=0.61, clip=20, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=15261
2023-02-16 20:01:11 - progress_bar.py[line:274] - INFO: epoch 001:   3444 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.88, wpb=113.1, bsz=40, num_updates=3440, lr=3.71892e-05, gnorm=0.516, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=15272
2023-02-16 20:01:23 - progress_bar.py[line:274] - INFO: epoch 001:   3454 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.5, bsz=40, num_updates=3450, lr=3.72973e-05, gnorm=0.661, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15283
2023-02-16 20:01:34 - progress_bar.py[line:274] - INFO: epoch 001:   3464 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.89, wpb=112.2, bsz=40, num_updates=3460, lr=3.74054e-05, gnorm=0.825, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=15295
2023-02-16 20:01:45 - progress_bar.py[line:274] - INFO: epoch 001:   3474 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111, bsz=40, num_updates=3470, lr=3.75135e-05, gnorm=0.717, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15306
2023-02-16 20:01:56 - progress_bar.py[line:274] - INFO: epoch 001:   3484 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=111.4, bsz=40, num_updates=3480, lr=3.76216e-05, gnorm=0.795, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15317
2023-02-16 20:02:07 - progress_bar.py[line:274] - INFO: epoch 001:   3494 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.2, ups=0.91, wpb=112.8, bsz=40, num_updates=3490, lr=3.77297e-05, gnorm=0.741, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=15328
2023-02-16 20:02:18 - progress_bar.py[line:274] - INFO: epoch 001:   3504 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=110.7, bsz=40, num_updates=3500, lr=3.78378e-05, gnorm=0.823, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15339
2023-02-16 20:02:29 - progress_bar.py[line:274] - INFO: epoch 001:   3514 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.1, ups=0.92, wpb=111.1, bsz=40, num_updates=3510, lr=3.79459e-05, gnorm=0.775, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15350
2023-02-16 20:02:40 - progress_bar.py[line:274] - INFO: epoch 001:   3524 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.8, ups=0.95, wpb=111.8, bsz=40, num_updates=3520, lr=3.80541e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15361
2023-02-16 20:02:51 - progress_bar.py[line:274] - INFO: epoch 001:   3534 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.88, wpb=113, bsz=40, num_updates=3530, lr=3.81622e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=11, gb_free=11.4, ema_decay=0.9999, wall=15372
2023-02-16 20:03:02 - progress_bar.py[line:274] - INFO: epoch 001:   3544 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.89, wpb=111.3, bsz=40, num_updates=3540, lr=3.82703e-05, gnorm=0.782, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15383
2023-02-16 20:03:14 - progress_bar.py[line:274] - INFO: epoch 001:   3554 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.88, wpb=111.6, bsz=40, num_updates=3550, lr=3.83784e-05, gnorm=0.728, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15395
2023-02-16 20:03:25 - progress_bar.py[line:274] - INFO: epoch 001:   3564 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.89, wpb=111.9, bsz=40, num_updates=3560, lr=3.84865e-05, gnorm=0.768, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15406
2023-02-16 20:03:36 - progress_bar.py[line:274] - INFO: epoch 001:   3574 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.7, ups=0.91, wpb=111.7, bsz=40, num_updates=3570, lr=3.85946e-05, gnorm=0.778, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15417
2023-02-16 20:03:47 - progress_bar.py[line:274] - INFO: epoch 001:   3584 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.91, wpb=111.3, bsz=40, num_updates=3580, lr=3.87027e-05, gnorm=0.785, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15428
2023-02-16 20:03:58 - progress_bar.py[line:274] - INFO: epoch 001:   3594 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.88, wpb=112.8, bsz=40, num_updates=3590, lr=3.88108e-05, gnorm=0.635, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15439
2023-02-16 20:04:09 - progress_bar.py[line:274] - INFO: epoch 001:   3604 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.92, wpb=111.4, bsz=40, num_updates=3600, lr=3.89189e-05, gnorm=0.8, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=15450
2023-02-16 20:04:20 - progress_bar.py[line:274] - INFO: epoch 001:   3614 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.9, wpb=112.5, bsz=40, num_updates=3610, lr=3.9027e-05, gnorm=0.691, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15461
2023-02-16 20:04:32 - progress_bar.py[line:274] - INFO: epoch 001:   3624 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.88, wpb=112.8, bsz=40, num_updates=3620, lr=3.91351e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15473
2023-02-16 20:04:43 - progress_bar.py[line:274] - INFO: epoch 001:   3634 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.92, wpb=111.5, bsz=40, num_updates=3630, lr=3.92432e-05, gnorm=0.714, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15484
2023-02-16 20:04:54 - progress_bar.py[line:274] - INFO: epoch 001:   3644 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.3, ups=0.92, wpb=112.3, bsz=40, num_updates=3640, lr=3.93514e-05, gnorm=0.807, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15494
2023-02-16 20:05:04 - progress_bar.py[line:274] - INFO: epoch 001:   3654 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.4, ups=0.92, wpb=111.2, bsz=40, num_updates=3650, lr=3.94595e-05, gnorm=0.84, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15505
2023-02-16 20:05:15 - progress_bar.py[line:274] - INFO: epoch 001:   3664 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.3, ups=0.92, wpb=112.5, bsz=40, num_updates=3660, lr=3.95676e-05, gnorm=0.787, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15516
2023-02-16 20:05:27 - progress_bar.py[line:274] - INFO: epoch 001:   3674 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.88, wpb=111.4, bsz=40, num_updates=3670, lr=3.96757e-05, gnorm=0.718, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15528
2023-02-16 20:05:38 - progress_bar.py[line:274] - INFO: epoch 001:   3684 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.5, ups=0.92, wpb=113.7, bsz=40, num_updates=3680, lr=3.97838e-05, gnorm=0.594, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15539
2023-02-16 20:05:49 - progress_bar.py[line:274] - INFO: epoch 001:   3694 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.7, ups=0.87, wpb=111.1, bsz=40, num_updates=3690, lr=3.98919e-05, gnorm=0.708, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15550
2023-02-16 20:06:00 - progress_bar.py[line:274] - INFO: epoch 001:   3704 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.92, wpb=111.5, bsz=40, num_updates=3700, lr=4e-05, gnorm=0.639, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15561
2023-02-16 20:06:11 - progress_bar.py[line:274] - INFO: epoch 001:   3714 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=111.9, bsz=40, num_updates=3710, lr=4.01081e-05, gnorm=0.74, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15572
2023-02-16 20:06:22 - progress_bar.py[line:274] - INFO: epoch 001:   3724 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.91, wpb=112.7, bsz=40, num_updates=3720, lr=4.02162e-05, gnorm=0.846, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15583
2023-02-16 20:06:34 - progress_bar.py[line:274] - INFO: epoch 001:   3734 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.8, ups=0.88, wpb=110.9, bsz=40, num_updates=3730, lr=4.03243e-05, gnorm=0.942, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15594
2023-02-16 20:06:45 - progress_bar.py[line:274] - INFO: epoch 001:   3744 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.91, wpb=111.6, bsz=40, num_updates=3740, lr=4.04324e-05, gnorm=0.793, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15606
2023-02-16 20:06:56 - progress_bar.py[line:274] - INFO: epoch 001:   3754 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.7, bsz=40, num_updates=3750, lr=4.05405e-05, gnorm=0.569, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15617
2023-02-16 20:07:07 - progress_bar.py[line:274] - INFO: epoch 001:   3764 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.87, wpb=114, bsz=40, num_updates=3760, lr=4.06486e-05, gnorm=0.536, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=15628
2023-02-16 20:07:18 - progress_bar.py[line:274] - INFO: epoch 001:   3774 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.9, wpb=112.6, bsz=40, num_updates=3770, lr=4.07568e-05, gnorm=0.73, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15639
2023-02-16 20:07:30 - progress_bar.py[line:274] - INFO: epoch 001:   3784 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.2, bsz=40, num_updates=3780, lr=4.08649e-05, gnorm=0.655, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15651
2023-02-16 20:07:41 - progress_bar.py[line:274] - INFO: epoch 001:   3794 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.9, wpb=112.7, bsz=40, num_updates=3790, lr=4.0973e-05, gnorm=0.617, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15662
2023-02-16 20:07:52 - progress_bar.py[line:274] - INFO: epoch 001:   3804 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.2, ups=0.91, wpb=112.4, bsz=40, num_updates=3800, lr=4.10811e-05, gnorm=0.67, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15673
2023-02-16 20:08:03 - progress_bar.py[line:274] - INFO: epoch 001:   3814 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.87, wpb=111.6, bsz=40, num_updates=3810, lr=4.11892e-05, gnorm=0.656, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15684
2023-02-16 20:08:14 - progress_bar.py[line:274] - INFO: epoch 001:   3824 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.5, ups=0.92, wpb=112.7, bsz=40, num_updates=3820, lr=4.12973e-05, gnorm=0.638, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15695
2023-02-16 20:08:25 - progress_bar.py[line:274] - INFO: epoch 001:   3834 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.9, wpb=111.4, bsz=40, num_updates=3830, lr=4.14054e-05, gnorm=0.749, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15706
2023-02-16 20:08:37 - progress_bar.py[line:274] - INFO: epoch 001:   3844 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.88, wpb=111.7, bsz=40, num_updates=3840, lr=4.15135e-05, gnorm=0.661, clip=20, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=15718
2023-02-16 20:08:48 - progress_bar.py[line:274] - INFO: epoch 001:   3854 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.92, wpb=111.3, bsz=40, num_updates=3850, lr=4.16216e-05, gnorm=0.653, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15729
2023-02-16 20:08:58 - progress_bar.py[line:274] - INFO: epoch 001:   3864 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.7, ups=0.92, wpb=112, bsz=40, num_updates=3860, lr=4.17297e-05, gnorm=0.659, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15739
2023-02-16 20:09:10 - progress_bar.py[line:274] - INFO: epoch 001:   3874 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=3870, lr=4.18378e-05, gnorm=0.709, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15751
2023-02-16 20:09:21 - progress_bar.py[line:274] - INFO: epoch 001:   3884 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.9, wpb=111.2, bsz=40, num_updates=3880, lr=4.19459e-05, gnorm=0.535, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15762
2023-02-16 20:09:32 - progress_bar.py[line:274] - INFO: epoch 001:   3894 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.87, wpb=113.1, bsz=40, num_updates=3890, lr=4.20541e-05, gnorm=0.622, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15773
2023-02-16 20:09:44 - progress_bar.py[line:274] - INFO: epoch 001:   3904 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.9, wpb=111.6, bsz=40, num_updates=3900, lr=4.21622e-05, gnorm=0.751, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=15785
2023-02-16 20:09:55 - progress_bar.py[line:274] - INFO: epoch 001:   3914 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=3910, lr=4.22703e-05, gnorm=0.827, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15796
2023-02-16 20:10:06 - progress_bar.py[line:274] - INFO: epoch 001:   3924 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.87, wpb=112.8, bsz=40, num_updates=3920, lr=4.23784e-05, gnorm=0.769, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15807
2023-02-16 20:10:17 - progress_bar.py[line:274] - INFO: epoch 001:   3934 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.89, wpb=110.7, bsz=40, num_updates=3930, lr=4.24865e-05, gnorm=0.613, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=15818
2023-02-16 20:10:28 - progress_bar.py[line:274] - INFO: epoch 001:   3944 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.9, wpb=111.2, bsz=40, num_updates=3940, lr=4.25946e-05, gnorm=0.698, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15829
2023-02-16 20:10:40 - progress_bar.py[line:274] - INFO: epoch 001:   3954 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=111.9, bsz=40, num_updates=3950, lr=4.27027e-05, gnorm=0.563, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15840
2023-02-16 20:10:50 - progress_bar.py[line:274] - INFO: epoch 001:   3964 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.5, ups=0.92, wpb=113.4, bsz=40, num_updates=3960, lr=4.28108e-05, gnorm=0.72, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=15851
2023-02-16 20:11:01 - progress_bar.py[line:274] - INFO: epoch 001:   3974 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.3, ups=0.93, wpb=113, bsz=40, num_updates=3970, lr=4.29189e-05, gnorm=0.723, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15862
2023-02-16 20:11:12 - progress_bar.py[line:274] - INFO: epoch 001:   3984 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.7, ups=0.93, wpb=112.4, bsz=40, num_updates=3980, lr=4.3027e-05, gnorm=0.543, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15873
2023-02-16 20:11:23 - progress_bar.py[line:274] - INFO: epoch 001:   3994 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.9, wpb=110.7, bsz=40, num_updates=3990, lr=4.31351e-05, gnorm=0.749, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=15884
2023-02-16 20:11:34 - progress_bar.py[line:274] - INFO: epoch 001:   4004 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.92, wpb=112.5, bsz=40, num_updates=4000, lr=4.32432e-05, gnorm=0.457, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15895
2023-02-16 20:11:34 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 20:11:35 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 20:11:35 - train.py[line:551] - INFO: load:0.88 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 20:13:37 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 20:13:37 - train.py[line:551] - INFO: load:0.90 valid_run:121.95 task_valid:118.89 collect_output:2.04
2023-02-16 20:15:37 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 20:15:37 - train.py[line:551] - INFO: load:0.93 valid_run:241.67 task_valid:234.46 collect_output:5.19
2023-02-16 20:17:39 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 20:17:39 - train.py[line:551] - INFO: load:0.95 valid_run:363.45 task_valid:350.83 collect_output:9.58
2023-02-16 20:19:41 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 20:19:41 - train.py[line:551] - INFO: load:0.98 valid_run:485.30 task_valid:464.51 collect_output:16.73
2023-02-16 20:21:41 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 20:21:41 - train.py[line:551] - INFO: load:1.00 valid_run:605.57 task_valid:581.73 collect_output:18.79
2023-02-16 20:23:44 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 20:23:44 - train.py[line:551] - INFO: load:1.03 valid_run:728.37 task_valid:700.36 collect_output:21.94
2023-02-16 20:25:47 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 20:25:47 - train.py[line:551] - INFO: load:1.05 valid_run:851.13 task_valid:818.24 collect_output:25.83
2023-02-16 20:27:49 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 20:27:49 - train.py[line:551] - INFO: load:1.08 valid_run:973.07 task_valid:934.66 collect_output:30.30
2023-02-16 20:29:52 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 20:29:52 - train.py[line:551] - INFO: load:1.10 valid_run:1096.80 task_valid:1051.71 collect_output:35.97
2023-02-16 20:31:54 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 20:31:54 - train.py[line:551] - INFO: load:1.13 valid_run:1218.33 task_valid:1164.13 collect_output:44.08
2023-02-16 20:33:54 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 20:33:54 - train.py[line:551] - INFO: load:1.15 valid_run:1338.38 task_valid:1279.62 collect_output:47.62
2023-02-16 20:35:55 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 20:35:55 - train.py[line:551] - INFO: load:1.18 valid_run:1459.66 task_valid:1396.23 collect_output:51.30
2023-02-16 20:37:54 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 20:37:54 - train.py[line:551] - INFO: load:1.20 valid_run:1578.24 task_valid:1509.77 collect_output:55.34
2023-02-16 20:39:55 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 20:39:55 - train.py[line:551] - INFO: load:1.23 valid_run:1698.99 task_valid:1627.42 collect_output:57.45
2023-02-16 20:41:56 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 20:41:56 - train.py[line:551] - INFO: load:1.26 valid_run:1819.64 task_valid:1743.37 collect_output:61.13
2023-02-16 20:43:56 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 20:43:56 - train.py[line:551] - INFO: load:1.28 valid_run:1940.49 task_valid:1857.10 collect_output:67.25
2023-02-16 20:45:58 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 20:45:58 - train.py[line:551] - INFO: load:1.31 valid_run:2061.57 task_valid:1972.95 collect_output:71.49
2023-02-16 20:47:58 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 20:47:58 - train.py[line:551] - INFO: load:1.33 valid_run:2182.17 task_valid:2090.69 collect_output:73.36
2023-02-16 20:49:59 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 20:49:59 - train.py[line:551] - INFO: load:1.36 valid_run:2303.15 task_valid:2207.37 collect_output:76.67
2023-02-16 20:52:00 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 20:52:00 - train.py[line:551] - INFO: load:1.39 valid_run:2423.35 task_valid:2323.80 collect_output:79.44
2023-02-16 20:54:01 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 20:54:01 - train.py[line:551] - INFO: load:1.41 valid_run:2544.92 task_valid:2440.19 collect_output:83.62
2023-02-16 20:56:03 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 20:56:03 - train.py[line:551] - INFO: load:1.44 valid_run:2666.72 task_valid:2558.98 collect_output:85.62
2023-02-16 20:58:03 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 20:58:03 - train.py[line:551] - INFO: load:1.46 valid_run:2786.71 task_valid:2672.96 collect_output:90.63
2023-02-16 21:00:03 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 21:00:03 - train.py[line:551] - INFO: load:1.49 valid_run:2906.20 task_valid:2788.85 collect_output:93.20
2023-02-16 21:02:04 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 21:02:04 - train.py[line:551] - INFO: load:1.51 valid_run:3027.55 task_valid:2904.77 collect_output:97.61
2023-02-16 21:04:07 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 21:04:07 - train.py[line:551] - INFO: load:1.54 valid_run:3150.12 task_valid:3020.53 collect_output:103.43
2023-02-16 21:06:06 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 21:06:06 - train.py[line:551] - INFO: load:1.56 valid_run:3269.57 task_valid:3134.47 collect_output:107.93
2023-02-16 21:08:08 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 21:08:08 - train.py[line:551] - INFO: load:1.59 valid_run:3391.29 task_valid:3253.78 collect_output:109.33
2023-02-16 21:10:10 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 21:10:10 - train.py[line:551] - INFO: load:1.62 valid_run:3512.91 task_valid:3369.11 collect_output:114.60
2023-02-16 21:12:11 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 21:12:11 - train.py[line:551] - INFO: load:1.64 valid_run:3634.71 task_valid:3487.49 collect_output:117.00
2023-02-16 21:14:12 - train.py[line:549] - INFO: 6200 / 6234
2023-02-16 21:14:12 - train.py[line:551] - INFO: load:1.67 valid_run:3755.64 task_valid:3605.90 collect_output:118.52

====================================================================================================
SGG eval:     R @ 50: 0.5619;     R @ 100: 0.6058;     R @ 500: 0.6523;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3495;    mR @ 100: 0.4057;    mR @ 500: 0.4673;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5976) (covered in:0.2500) (covering:0.3714) (eating:0.7059) (flying in:0.8636) (growing on:0.2500) (hanging from:0.5452) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.8750) (playing:0.0000) (riding:0.9438) (says:0.0000) (sitting on:0.6307) (standing on:0.4403) (using:0.3000) (walking in:0.0000) (walking on:0.5676) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-02-16 21:14:43 - train.py[line:487] - INFO: 0.6057817163228928

====================================================================================================
SGG eval:     R @ 50: 0.5619;     R @ 100: 0.6058;     R @ 500: 0.6523;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3495;    mR @ 100: 0.4057;    mR @ 500: 0.4673;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5976) (covered in:0.2500) (covering:0.3714) (eating:0.7059) (flying in:0.8636) (growing on:0.2500) (hanging from:0.5452) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.8750) (playing:0.0000) (riding:0.9438) (says:0.0000) (sitting on:0.6307) (standing on:0.4403) (using:0.3000) (walking in:0.0000) (walking on:0.5676) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-02-16 21:14:43 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-16 21:14:43 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.266 | loss_v1 0 | loss_v2 0 | nll_loss 0.099 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.605782 | ppl 1.07 | vqa_score 0.3446 | wps 118.4 | wpb 72 | bsz 24 | num_updates 4000 | best_R@100 0.605782
2023-02-16 21:14:43 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4000 updates
2023-02-16 21:14:43 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_4000.pt
2023-02-16 21:14:48 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_4000.pt
2023-02-16 21:14:54 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 0.6057817163228928) (writing took 11.052510743960738 seconds)
2023-02-16 21:15:05 - progress_bar.py[line:274] - INFO: epoch 001:   4014 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=110.9, bsz=40, num_updates=4010, lr=4.33514e-05, gnorm=0.791, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19706
2023-02-16 21:15:16 - progress_bar.py[line:274] - INFO: epoch 001:   4024 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.9, wpb=113.4, bsz=40, num_updates=4020, lr=4.34595e-05, gnorm=0.816, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19717
2023-02-16 21:15:27 - progress_bar.py[line:274] - INFO: epoch 001:   4034 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.9, ups=0.93, wpb=112.4, bsz=40, num_updates=4030, lr=4.35676e-05, gnorm=0.786, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19728
2023-02-16 21:15:38 - progress_bar.py[line:274] - INFO: epoch 001:   4044 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.91, wpb=110.9, bsz=40, num_updates=4040, lr=4.36757e-05, gnorm=0.615, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19739
2023-02-16 21:15:49 - progress_bar.py[line:274] - INFO: epoch 001:   4054 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=4050, lr=4.37838e-05, gnorm=0.575, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19750
2023-02-16 21:16:00 - progress_bar.py[line:274] - INFO: epoch 001:   4064 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.9, wpb=112.8, bsz=40, num_updates=4060, lr=4.38919e-05, gnorm=0.848, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19761
2023-02-16 21:16:12 - progress_bar.py[line:274] - INFO: epoch 001:   4074 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=111.4, bsz=40, num_updates=4070, lr=4.4e-05, gnorm=0.593, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19773
2023-02-16 21:16:23 - progress_bar.py[line:274] - INFO: epoch 001:   4084 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=112.5, bsz=40, num_updates=4080, lr=4.41081e-05, gnorm=0.781, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19784
2023-02-16 21:16:34 - progress_bar.py[line:274] - INFO: epoch 001:   4094 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.91, wpb=112.7, bsz=40, num_updates=4090, lr=4.42162e-05, gnorm=0.719, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19795
2023-02-16 21:16:45 - progress_bar.py[line:274] - INFO: epoch 001:   4104 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.88, wpb=112.8, bsz=40, num_updates=4100, lr=4.43243e-05, gnorm=0.506, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19806
2023-02-16 21:16:56 - progress_bar.py[line:274] - INFO: epoch 001:   4114 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.9, wpb=113.3, bsz=40, num_updates=4110, lr=4.44324e-05, gnorm=0.583, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19817
2023-02-16 21:17:07 - progress_bar.py[line:274] - INFO: epoch 001:   4124 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.1, ups=0.92, wpb=112.2, bsz=40, num_updates=4120, lr=4.45405e-05, gnorm=0.571, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19828
2023-02-16 21:17:19 - progress_bar.py[line:274] - INFO: epoch 001:   4134 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.88, wpb=112.3, bsz=40, num_updates=4130, lr=4.46486e-05, gnorm=0.671, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19839
2023-02-16 21:17:30 - progress_bar.py[line:274] - INFO: epoch 001:   4144 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.9, wpb=111.7, bsz=40, num_updates=4140, lr=4.47568e-05, gnorm=0.563, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19851
2023-02-16 21:17:41 - progress_bar.py[line:274] - INFO: epoch 001:   4154 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=112, bsz=40, num_updates=4150, lr=4.48649e-05, gnorm=0.49, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19862
2023-02-16 21:17:52 - progress_bar.py[line:274] - INFO: epoch 001:   4164 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.9, wpb=113.1, bsz=40, num_updates=4160, lr=4.4973e-05, gnorm=0.594, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19873
2023-02-16 21:18:03 - progress_bar.py[line:274] - INFO: epoch 001:   4174 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=112.5, bsz=40, num_updates=4170, lr=4.50811e-05, gnorm=0.375, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19884
2023-02-16 21:18:14 - progress_bar.py[line:274] - INFO: epoch 001:   4184 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.1, ups=0.91, wpb=113.5, bsz=40, num_updates=4180, lr=4.51892e-05, gnorm=0.631, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19895
2023-02-16 21:18:25 - progress_bar.py[line:274] - INFO: epoch 001:   4194 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=111.3, bsz=40, num_updates=4190, lr=4.52973e-05, gnorm=0.618, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19906
2023-02-16 21:18:32 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 21:18:37 - progress_bar.py[line:274] - INFO: epoch 001:   4205 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=93.4, ups=0.84, wpb=111, bsz=40, num_updates=4200, lr=4.54054e-05, gnorm=0.635, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=19918
2023-02-16 21:18:49 - progress_bar.py[line:274] - INFO: epoch 001:   4215 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.89, wpb=112.3, bsz=40, num_updates=4210, lr=4.55135e-05, gnorm=0.626, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19930
2023-02-16 21:19:00 - progress_bar.py[line:274] - INFO: epoch 001:   4225 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=112.2, bsz=40, num_updates=4220, lr=4.56216e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19941
2023-02-16 21:19:11 - progress_bar.py[line:274] - INFO: epoch 001:   4235 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=112.7, bsz=40, num_updates=4230, lr=4.57297e-05, gnorm=0.57, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19952
2023-02-16 21:19:22 - progress_bar.py[line:274] - INFO: epoch 001:   4245 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.1, ups=0.92, wpb=112.4, bsz=40, num_updates=4240, lr=4.58378e-05, gnorm=0.565, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19963
2023-02-16 21:19:33 - progress_bar.py[line:274] - INFO: epoch 001:   4255 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.9, ups=0.92, wpb=111.8, bsz=40, num_updates=4250, lr=4.59459e-05, gnorm=0.582, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19974
2023-02-16 21:19:44 - progress_bar.py[line:274] - INFO: epoch 001:   4265 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.91, wpb=110.7, bsz=40, num_updates=4260, lr=4.60541e-05, gnorm=0.718, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=19985
2023-02-16 21:19:55 - progress_bar.py[line:274] - INFO: epoch 001:   4275 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.91, wpb=111.1, bsz=40, num_updates=4270, lr=4.61622e-05, gnorm=0.665, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19996
2023-02-16 21:20:06 - progress_bar.py[line:274] - INFO: epoch 001:   4285 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=4280, lr=4.62703e-05, gnorm=0.659, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20007
2023-02-16 21:20:17 - progress_bar.py[line:274] - INFO: epoch 001:   4295 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.91, wpb=113, bsz=40, num_updates=4290, lr=4.63784e-05, gnorm=0.578, clip=10, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=20018
2023-02-16 21:20:28 - progress_bar.py[line:274] - INFO: epoch 001:   4305 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.9, wpb=111.7, bsz=40, num_updates=4300, lr=4.64865e-05, gnorm=0.495, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20029
2023-02-16 21:20:39 - progress_bar.py[line:274] - INFO: epoch 001:   4315 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.91, wpb=111.9, bsz=40, num_updates=4310, lr=4.65946e-05, gnorm=0.671, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20040
2023-02-16 21:20:50 - progress_bar.py[line:274] - INFO: epoch 001:   4325 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=111.5, bsz=40, num_updates=4320, lr=4.67027e-05, gnorm=0.503, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20051
2023-02-16 21:21:01 - progress_bar.py[line:274] - INFO: epoch 001:   4335 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104, ups=0.92, wpb=113, bsz=40, num_updates=4330, lr=4.68108e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20062
2023-02-16 21:21:12 - progress_bar.py[line:274] - INFO: epoch 001:   4345 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.88, wpb=112.8, bsz=40, num_updates=4340, lr=4.69189e-05, gnorm=0.478, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20073
2023-02-16 21:21:23 - progress_bar.py[line:274] - INFO: epoch 001:   4355 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.92, wpb=112.7, bsz=40, num_updates=4350, lr=4.7027e-05, gnorm=0.649, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20084
2023-02-16 21:21:34 - progress_bar.py[line:274] - INFO: epoch 001:   4365 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=112.1, bsz=40, num_updates=4360, lr=4.71351e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20095
2023-02-16 21:21:46 - progress_bar.py[line:274] - INFO: epoch 001:   4375 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.88, wpb=112.5, bsz=40, num_updates=4370, lr=4.72432e-05, gnorm=0.742, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20107
2023-02-16 21:21:57 - progress_bar.py[line:274] - INFO: epoch 001:   4385 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.6, ups=0.9, wpb=111.8, bsz=40, num_updates=4380, lr=4.73514e-05, gnorm=0.622, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20118
2023-02-16 21:22:08 - progress_bar.py[line:274] - INFO: epoch 001:   4395 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.92, wpb=111.7, bsz=40, num_updates=4390, lr=4.74595e-05, gnorm=0.481, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20129
2023-02-16 21:22:19 - progress_bar.py[line:274] - INFO: epoch 001:   4405 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.8, ups=0.92, wpb=112, bsz=40, num_updates=4400, lr=4.75676e-05, gnorm=0.548, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20139
2023-02-16 21:22:30 - progress_bar.py[line:274] - INFO: epoch 001:   4415 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.87, wpb=113.6, bsz=40, num_updates=4410, lr=4.76757e-05, gnorm=0.604, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20151
2023-02-16 21:22:41 - progress_bar.py[line:274] - INFO: epoch 001:   4425 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.3, ups=0.92, wpb=112.7, bsz=40, num_updates=4420, lr=4.77838e-05, gnorm=0.576, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20162
2023-02-16 21:22:52 - progress_bar.py[line:274] - INFO: epoch 001:   4435 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.89, wpb=113, bsz=40, num_updates=4430, lr=4.78919e-05, gnorm=0.712, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20173
2023-02-16 21:23:03 - progress_bar.py[line:274] - INFO: epoch 001:   4445 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.91, wpb=110.1, bsz=40, num_updates=4440, lr=4.8e-05, gnorm=0.556, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20184
2023-02-16 21:23:15 - progress_bar.py[line:274] - INFO: epoch 001:   4455 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=111.3, bsz=40, num_updates=4450, lr=4.81081e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20195
2023-02-16 21:23:25 - progress_bar.py[line:274] - INFO: epoch 001:   4465 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.4, ups=0.92, wpb=111.8, bsz=40, num_updates=4460, lr=4.82162e-05, gnorm=0.716, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20206
2023-02-16 21:23:37 - progress_bar.py[line:274] - INFO: epoch 001:   4475 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.9, wpb=111.7, bsz=40, num_updates=4470, lr=4.83243e-05, gnorm=0.682, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20217
2023-02-16 21:23:48 - progress_bar.py[line:274] - INFO: epoch 001:   4485 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=4480, lr=4.84324e-05, gnorm=0.695, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20229
2023-02-16 21:23:59 - progress_bar.py[line:274] - INFO: epoch 001:   4495 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.9, wpb=111.6, bsz=40, num_updates=4490, lr=4.85405e-05, gnorm=0.695, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20240
2023-02-16 21:24:10 - progress_bar.py[line:274] - INFO: epoch 001:   4505 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.9, wpb=110.8, bsz=40, num_updates=4500, lr=4.86486e-05, gnorm=0.673, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=20251
2023-02-16 21:24:21 - progress_bar.py[line:274] - INFO: epoch 001:   4515 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.87, wpb=112.5, bsz=40, num_updates=4510, lr=4.87568e-05, gnorm=0.646, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20262
2023-02-16 21:24:32 - progress_bar.py[line:274] - INFO: epoch 001:   4525 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.6, ups=0.94, wpb=111.8, bsz=40, num_updates=4520, lr=4.88649e-05, gnorm=0.566, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20273
2023-02-16 21:24:43 - progress_bar.py[line:274] - INFO: epoch 001:   4535 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.89, wpb=110.4, bsz=40, num_updates=4530, lr=4.8973e-05, gnorm=0.521, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20284
2023-02-16 21:24:54 - progress_bar.py[line:274] - INFO: epoch 001:   4545 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.7, ups=0.91, wpb=112.1, bsz=40, num_updates=4540, lr=4.90811e-05, gnorm=0.65, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20295
2023-02-16 21:25:06 - progress_bar.py[line:274] - INFO: epoch 001:   4555 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.6, ups=0.91, wpb=113, bsz=40, num_updates=4550, lr=4.91892e-05, gnorm=0.592, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20306
2023-02-16 21:25:17 - progress_bar.py[line:274] - INFO: epoch 001:   4565 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.92, wpb=112, bsz=40, num_updates=4560, lr=4.92973e-05, gnorm=0.611, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20317
2023-02-16 21:25:27 - progress_bar.py[line:274] - INFO: epoch 001:   4575 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.3, ups=0.93, wpb=111.8, bsz=40, num_updates=4570, lr=4.94054e-05, gnorm=0.733, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20328
2023-02-16 21:25:39 - progress_bar.py[line:274] - INFO: epoch 001:   4585 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.88, wpb=111.7, bsz=40, num_updates=4580, lr=4.95135e-05, gnorm=0.411, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20340
2023-02-16 21:25:50 - progress_bar.py[line:274] - INFO: epoch 001:   4595 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112.5, bsz=40, num_updates=4590, lr=4.96216e-05, gnorm=0.548, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20351
2023-02-16 21:26:01 - progress_bar.py[line:274] - INFO: epoch 001:   4605 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=111.6, bsz=40, num_updates=4600, lr=4.97297e-05, gnorm=0.656, clip=10, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=20362
2023-02-16 21:26:12 - progress_bar.py[line:274] - INFO: epoch 001:   4615 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.3, ups=0.93, wpb=112, bsz=40, num_updates=4610, lr=4.98378e-05, gnorm=0.6, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20373
2023-02-16 21:26:23 - progress_bar.py[line:274] - INFO: epoch 001:   4625 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.89, wpb=111.9, bsz=40, num_updates=4620, lr=4.99459e-05, gnorm=0.615, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=20384
2023-02-16 21:26:34 - progress_bar.py[line:274] - INFO: epoch 001:   4635 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.89, wpb=112.5, bsz=40, num_updates=4630, lr=4.99977e-05, gnorm=0.678, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20395
2023-02-16 21:26:46 - progress_bar.py[line:274] - INFO: epoch 001:   4645 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.87, wpb=111.7, bsz=40, num_updates=4640, lr=4.99932e-05, gnorm=0.569, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20407
2023-02-16 21:26:57 - progress_bar.py[line:274] - INFO: epoch 001:   4655 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.9, wpb=112, bsz=40, num_updates=4650, lr=4.99887e-05, gnorm=0.605, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20418
2023-02-16 21:27:08 - progress_bar.py[line:274] - INFO: epoch 001:   4665 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.9, ups=0.91, wpb=112.4, bsz=40, num_updates=4660, lr=4.99842e-05, gnorm=0.917, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20429
2023-02-16 21:27:19 - progress_bar.py[line:274] - INFO: epoch 001:   4675 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103, ups=0.92, wpb=112, bsz=40, num_updates=4670, lr=4.99797e-05, gnorm=0.574, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20440
2023-02-16 21:27:30 - progress_bar.py[line:274] - INFO: epoch 001:   4685 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.9, wpb=110.7, bsz=40, num_updates=4680, lr=4.99752e-05, gnorm=0.568, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20451
2023-02-16 21:27:41 - progress_bar.py[line:274] - INFO: epoch 001:   4695 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.91, wpb=111.7, bsz=40, num_updates=4690, lr=4.99707e-05, gnorm=0.603, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20462
2023-02-16 21:27:52 - progress_bar.py[line:274] - INFO: epoch 001:   4705 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.3, ups=0.91, wpb=112.7, bsz=40, num_updates=4700, lr=4.99662e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20473
2023-02-16 21:28:03 - progress_bar.py[line:274] - INFO: epoch 001:   4715 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.9, wpb=113.1, bsz=40, num_updates=4710, lr=4.99617e-05, gnorm=0.555, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20484
2023-02-16 21:28:14 - progress_bar.py[line:274] - INFO: epoch 001:   4725 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.9, wpb=111.4, bsz=40, num_updates=4720, lr=4.99572e-05, gnorm=0.371, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20495
2023-02-16 21:28:26 - progress_bar.py[line:274] - INFO: epoch 001:   4735 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=112.5, bsz=40, num_updates=4730, lr=4.99527e-05, gnorm=0.637, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=20506
2023-02-16 21:28:37 - progress_bar.py[line:274] - INFO: epoch 001:   4745 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.9, wpb=112.4, bsz=40, num_updates=4740, lr=4.99482e-05, gnorm=0.598, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20518
2023-02-16 21:28:47 - progress_bar.py[line:274] - INFO: epoch 001:   4755 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.6, ups=0.92, wpb=112.4, bsz=40, num_updates=4750, lr=4.99437e-05, gnorm=0.461, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20528
2023-02-16 21:28:59 - progress_bar.py[line:274] - INFO: epoch 001:   4765 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.9, wpb=111.6, bsz=40, num_updates=4760, lr=4.99392e-05, gnorm=0.492, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20539
2023-02-16 21:29:10 - progress_bar.py[line:274] - INFO: epoch 001:   4775 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.91, wpb=110.9, bsz=40, num_updates=4770, lr=4.99347e-05, gnorm=0.587, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20551
2023-02-16 21:29:15 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-16 21:29:22 - progress_bar.py[line:274] - INFO: epoch 001:   4786 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=94.2, ups=0.84, wpb=112.1, bsz=40, num_updates=4780, lr=4.99302e-05, gnorm=0.583, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=20562
2023-02-16 21:29:33 - progress_bar.py[line:274] - INFO: epoch 001:   4796 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.91, wpb=111.4, bsz=40, num_updates=4790, lr=4.99257e-05, gnorm=0.468, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20573
2023-02-16 21:29:44 - progress_bar.py[line:274] - INFO: epoch 001:   4806 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.89, wpb=112.9, bsz=40, num_updates=4800, lr=4.99212e-05, gnorm=0.55, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20585
2023-02-16 21:29:55 - progress_bar.py[line:274] - INFO: epoch 001:   4816 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.3, ups=0.87, wpb=112.1, bsz=40, num_updates=4810, lr=4.99167e-05, gnorm=0.479, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20596
2023-02-16 21:30:07 - progress_bar.py[line:274] - INFO: epoch 001:   4826 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.87, wpb=113, bsz=40, num_updates=4820, lr=4.99122e-05, gnorm=0.524, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20608
2023-02-16 21:30:18 - progress_bar.py[line:274] - INFO: epoch 001:   4836 / 11564 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=112.7, bsz=40, num_updates=4830, lr=4.99077e-05, gnorm=0.419, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20619
2023-02-16 21:30:29 - progress_bar.py[line:274] - INFO: epoch 001:   4846 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.92, wpb=111.3, bsz=40, num_updates=4840, lr=4.99032e-05, gnorm=0.644, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20630
2023-02-16 21:30:40 - progress_bar.py[line:274] - INFO: epoch 001:   4856 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.2, bsz=40, num_updates=4850, lr=4.98987e-05, gnorm=0.545, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20641
2023-02-16 21:30:51 - progress_bar.py[line:274] - INFO: epoch 001:   4866 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.88, wpb=113.1, bsz=40, num_updates=4860, lr=4.98942e-05, gnorm=0.628, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20652
2023-02-16 21:31:02 - progress_bar.py[line:274] - INFO: epoch 001:   4876 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=111.4, bsz=40, num_updates=4870, lr=4.98897e-05, gnorm=0.493, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20663
2023-02-16 21:31:14 - progress_bar.py[line:274] - INFO: epoch 001:   4886 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.88, wpb=111.2, bsz=40, num_updates=4880, lr=4.98852e-05, gnorm=0.568, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=20675
2023-02-16 21:31:25 - progress_bar.py[line:274] - INFO: epoch 001:   4896 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.91, wpb=112.4, bsz=40, num_updates=4890, lr=4.98806e-05, gnorm=0.638, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20686
2023-02-16 21:31:36 - progress_bar.py[line:274] - INFO: epoch 001:   4906 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.92, wpb=111.4, bsz=40, num_updates=4900, lr=4.98761e-05, gnorm=0.582, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20696
2023-02-16 21:31:47 - progress_bar.py[line:274] - INFO: epoch 001:   4916 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.91, wpb=112.7, bsz=40, num_updates=4910, lr=4.98716e-05, gnorm=0.597, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=20708
2023-02-16 21:31:58 - progress_bar.py[line:274] - INFO: epoch 001:   4926 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.87, wpb=111.8, bsz=40, num_updates=4920, lr=4.98671e-05, gnorm=0.586, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20719
2023-02-16 21:32:09 - progress_bar.py[line:274] - INFO: epoch 001:   4936 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.89, wpb=112.2, bsz=40, num_updates=4930, lr=4.98626e-05, gnorm=0.593, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20730
2023-02-16 21:32:20 - progress_bar.py[line:274] - INFO: epoch 001:   4946 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.9, bsz=40, num_updates=4940, lr=4.98581e-05, gnorm=0.441, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=20741
2023-02-16 21:32:32 - progress_bar.py[line:274] - INFO: epoch 001:   4956 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.91, wpb=113, bsz=40, num_updates=4950, lr=4.98536e-05, gnorm=0.492, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20752
2023-02-16 21:32:43 - progress_bar.py[line:274] - INFO: epoch 001:   4966 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=111.4, bsz=40, num_updates=4960, lr=4.98491e-05, gnorm=0.477, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=20764
2023-02-16 21:32:54 - progress_bar.py[line:274] - INFO: epoch 001:   4976 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.89, wpb=113.5, bsz=40, num_updates=4970, lr=4.98446e-05, gnorm=0.503, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20775
2023-02-16 21:33:05 - progress_bar.py[line:274] - INFO: epoch 001:   4986 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.92, wpb=112.8, bsz=40, num_updates=4980, lr=4.98401e-05, gnorm=0.412, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20786
2023-02-16 21:33:15 - progress_bar.py[line:274] - INFO: epoch 001:   4996 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.6, ups=0.92, wpb=111.2, bsz=40, num_updates=4990, lr=4.98356e-05, gnorm=0.474, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20796
2023-02-16 21:33:26 - progress_bar.py[line:274] - INFO: epoch 001:   5006 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=108.2, ups=0.97, wpb=111.5, bsz=40, num_updates=5000, lr=4.98311e-05, gnorm=0.712, clip=20, loss_scale=512, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=20807
2023-02-16 21:33:26 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 21:33:27 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 21:33:27 - train.py[line:551] - INFO: load:0.93 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 21:35:29 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 21:35:29 - train.py[line:551] - INFO: load:0.96 valid_run:122.05 task_valid:119.07 collect_output:1.89
2023-02-16 21:37:29 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 21:37:29 - train.py[line:551] - INFO: load:0.98 valid_run:242.21 task_valid:235.08 collect_output:4.99
2023-02-16 21:39:31 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 21:39:31 - train.py[line:551] - INFO: load:1.01 valid_run:364.19 task_valid:351.62 collect_output:9.41
2023-02-16 21:41:33 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 21:41:33 - train.py[line:551] - INFO: load:1.03 valid_run:486.06 task_valid:465.36 collect_output:16.53
2023-02-16 21:43:34 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 21:43:34 - train.py[line:551] - INFO: load:1.06 valid_run:606.55 task_valid:582.71 collect_output:18.66
2023-02-16 21:45:37 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 21:45:37 - train.py[line:551] - INFO: load:1.08 valid_run:729.50 task_valid:701.56 collect_output:21.74
2023-02-16 21:47:40 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 21:47:40 - train.py[line:551] - INFO: load:1.11 valid_run:852.54 task_valid:819.80 collect_output:25.52
2023-02-16 21:49:42 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 21:49:42 - train.py[line:551] - INFO: load:1.13 valid_run:974.38 task_valid:936.38 collect_output:29.76
2023-02-16 21:51:46 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 21:51:46 - train.py[line:551] - INFO: load:1.16 valid_run:1098.05 task_valid:1053.69 collect_output:35.10
2023-02-16 21:53:47 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 21:53:47 - train.py[line:551] - INFO: load:1.18 valid_run:1219.73 task_valid:1166.41 collect_output:43.04
2023-02-16 21:55:48 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 21:55:48 - train.py[line:551] - INFO: load:1.21 valid_run:1339.88 task_valid:1282.12 collect_output:46.46
2023-02-16 21:57:49 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 21:57:49 - train.py[line:551] - INFO: load:1.23 valid_run:1461.52 task_valid:1399.12 collect_output:50.10
2023-02-16 21:59:48 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 21:59:48 - train.py[line:551] - INFO: load:1.26 valid_run:1580.45 task_valid:1512.95 collect_output:54.17
2023-02-16 22:01:49 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 22:01:49 - train.py[line:551] - INFO: load:1.28 valid_run:1701.41 task_valid:1630.68 collect_output:56.41
2023-02-16 22:03:50 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 22:03:50 - train.py[line:551] - INFO: load:1.31 valid_run:1822.33 task_valid:1746.80 collect_output:60.19
2023-02-16 22:05:51 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 22:05:51 - train.py[line:551] - INFO: load:1.33 valid_run:1943.47 task_valid:1860.69 collect_output:66.43
2023-02-16 22:07:53 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 22:07:53 - train.py[line:551] - INFO: load:1.36 valid_run:2064.83 task_valid:1976.81 collect_output:70.67
2023-02-16 22:09:54 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 22:09:54 - train.py[line:551] - INFO: load:1.39 valid_run:2185.53 task_valid:2094.78 collect_output:72.38
2023-02-16 22:11:55 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 22:11:55 - train.py[line:551] - INFO: load:1.41 valid_run:2306.99 task_valid:2212.00 collect_output:75.56
2023-02-16 22:13:56 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 22:13:56 - train.py[line:551] - INFO: load:1.44 valid_run:2427.35 task_valid:2328.56 collect_output:78.36
2023-02-16 22:15:57 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 22:15:57 - train.py[line:551] - INFO: load:1.46 valid_run:2549.16 task_valid:2445.29 collect_output:82.43
2023-02-16 22:18:00 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 22:18:00 - train.py[line:551] - INFO: load:1.49 valid_run:2671.17 task_valid:2564.28 collect_output:84.43
2023-02-16 22:20:00 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 22:20:00 - train.py[line:551] - INFO: load:1.52 valid_run:2791.52 task_valid:2678.56 collect_output:89.50
2023-02-16 22:22:00 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 22:22:00 - train.py[line:551] - INFO: load:1.54 valid_run:2911.36 task_valid:2794.70 collect_output:92.18
2023-02-16 22:24:01 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 22:24:01 - train.py[line:551] - INFO: load:1.57 valid_run:3032.92 task_valid:2910.87 collect_output:96.56
2023-02-16 22:26:04 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 22:26:04 - train.py[line:551] - INFO: load:1.59 valid_run:3155.77 task_valid:3026.87 collect_output:102.40
2023-02-16 22:28:04 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 22:28:04 - train.py[line:551] - INFO: load:1.62 valid_run:3275.36 task_valid:3140.99 collect_output:106.86
2023-02-16 22:30:06 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 22:30:06 - train.py[line:551] - INFO: load:1.64 valid_run:3397.15 task_valid:3260.25 collect_output:108.39
2023-02-16 22:32:08 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 22:32:08 - train.py[line:551] - INFO: load:1.67 valid_run:3519.02 task_valid:3375.93 collect_output:113.55
2023-02-16 22:34:10 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 22:34:10 - train.py[line:551] - INFO: load:1.70 valid_run:3640.87 task_valid:3494.44 collect_output:115.89
2023-02-16 22:36:11 - train.py[line:549] - INFO: 6200 / 6234
2023-02-16 22:36:11 - train.py[line:551] - INFO: load:1.73 valid_run:3762.00 task_valid:3612.96 collect_output:117.50

====================================================================================================
SGG eval:     R @ 50: 0.6115;     R @ 100: 0.6562;     R @ 500: 0.6921;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4088;    mR @ 100: 0.4504;    mR @ 500: 0.4919;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7073) (covered in:0.3125) (covering:0.3714) (eating:0.7059) (flying in:0.9545) (growing on:0.3750) (hanging from:0.5161) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9712) (says:0.0000) (sitting on:0.6851) (standing on:0.4279) (using:0.4500) (walking in:0.0000) (walking on:0.6757) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6115;     R @ 100: 0.6562;     R @ 500: 0.6921;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4088;    mR @ 100: 0.4504;    mR @ 500: 0.4919;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7073) (covered in:0.3125) (covering:0.3714) (eating:0.7059) (flying in:0.9545) (growing on:0.3750) (hanging from:0.5161) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9712) (says:0.0000) (sitting on:0.6851) (standing on:0.4279) (using:0.4500) (walking in:0.0000) (walking on:0.6757) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-02-16 22:36:42 - train.py[line:487] - INFO: 0.6562282658517952
2023-02-16 22:36:42 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-16 22:36:42 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.277 | loss_v1 0 | loss_v2 0 | nll_loss 0.102 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.656228 | ppl 1.07 | vqa_score 0.4257 | wps 118.2 | wpb 72 | bsz 24 | num_updates 5000 | best_R@100 0.656228
2023-02-16 22:36:42 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 5000 updates
2023-02-16 22:36:42 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_5000.pt
2023-02-16 22:36:48 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_5000.pt
2023-02-16 22:36:53 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_5000.pt (epoch 1 @ 5000 updates, score 0.6562282658517952) (writing took 10.643551344051957 seconds)
2023-02-16 22:37:04 - progress_bar.py[line:274] - INFO: epoch 001:   5016 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=111.7, bsz=40, num_updates=5010, lr=4.98266e-05, gnorm=0.628, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24625
2023-02-16 22:37:15 - progress_bar.py[line:274] - INFO: epoch 001:   5026 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.89, wpb=111.2, bsz=40, num_updates=5020, lr=4.98221e-05, gnorm=0.585, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24636
2023-02-16 22:37:26 - progress_bar.py[line:274] - INFO: epoch 001:   5036 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.89, wpb=112.7, bsz=40, num_updates=5030, lr=4.98176e-05, gnorm=0.517, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24647
2023-02-16 22:37:37 - progress_bar.py[line:274] - INFO: epoch 001:   5046 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.9, wpb=111.6, bsz=40, num_updates=5040, lr=4.98131e-05, gnorm=0.542, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24658
2023-02-16 22:37:48 - progress_bar.py[line:274] - INFO: epoch 001:   5056 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.9, wpb=112.8, bsz=40, num_updates=5050, lr=4.98086e-05, gnorm=0.516, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=24669
2023-02-16 22:38:00 - progress_bar.py[line:274] - INFO: epoch 001:   5066 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=113.6, bsz=40, num_updates=5060, lr=4.98041e-05, gnorm=0.585, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24681
2023-02-16 22:38:11 - progress_bar.py[line:274] - INFO: epoch 001:   5076 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.91, wpb=111.1, bsz=40, num_updates=5070, lr=4.97996e-05, gnorm=0.644, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24692
2023-02-16 22:38:22 - progress_bar.py[line:274] - INFO: epoch 001:   5086 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.5, ups=0.92, wpb=112.1, bsz=40, num_updates=5080, lr=4.97951e-05, gnorm=0.788, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=24702
2023-02-16 22:38:32 - progress_bar.py[line:274] - INFO: epoch 001:   5096 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=105.9, ups=0.93, wpb=113.8, bsz=40, num_updates=5090, lr=4.97906e-05, gnorm=0.563, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24713
2023-02-16 22:38:44 - progress_bar.py[line:274] - INFO: epoch 001:   5106 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=5100, lr=4.97861e-05, gnorm=0.539, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24724
2023-02-16 22:38:54 - progress_bar.py[line:274] - INFO: epoch 001:   5116 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.1, ups=0.92, wpb=112.4, bsz=40, num_updates=5110, lr=4.97816e-05, gnorm=0.536, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24735
2023-02-16 22:39:05 - progress_bar.py[line:274] - INFO: epoch 001:   5126 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.91, wpb=111.1, bsz=40, num_updates=5120, lr=4.97771e-05, gnorm=0.544, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24746
2023-02-16 22:39:16 - progress_bar.py[line:274] - INFO: epoch 001:   5136 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.92, wpb=111.6, bsz=40, num_updates=5130, lr=4.97726e-05, gnorm=0.653, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=24757
2023-02-16 22:39:28 - progress_bar.py[line:274] - INFO: epoch 001:   5146 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=111.5, bsz=40, num_updates=5140, lr=4.9768e-05, gnorm=0.595, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24768
2023-02-16 22:39:39 - progress_bar.py[line:274] - INFO: epoch 001:   5156 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.87, wpb=112.7, bsz=40, num_updates=5150, lr=4.97635e-05, gnorm=0.516, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24780
2023-02-16 22:39:50 - progress_bar.py[line:274] - INFO: epoch 001:   5166 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.91, wpb=112.7, bsz=40, num_updates=5160, lr=4.9759e-05, gnorm=0.491, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24791
2023-02-16 22:40:02 - progress_bar.py[line:274] - INFO: epoch 001:   5176 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.87, wpb=112.5, bsz=40, num_updates=5170, lr=4.97545e-05, gnorm=0.49, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24802
2023-02-16 22:40:12 - progress_bar.py[line:274] - INFO: epoch 001:   5186 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.92, wpb=111.1, bsz=40, num_updates=5180, lr=4.975e-05, gnorm=0.692, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24813
2023-02-16 22:40:23 - progress_bar.py[line:274] - INFO: epoch 001:   5196 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.2, ups=0.92, wpb=112.5, bsz=40, num_updates=5190, lr=4.97455e-05, gnorm=0.543, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24824
2023-02-16 22:40:35 - progress_bar.py[line:274] - INFO: epoch 001:   5206 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=111.5, bsz=40, num_updates=5200, lr=4.9741e-05, gnorm=0.493, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24835
2023-02-16 22:40:46 - progress_bar.py[line:274] - INFO: epoch 001:   5216 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=111.5, bsz=40, num_updates=5210, lr=4.97365e-05, gnorm=0.481, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24847
2023-02-16 22:40:57 - progress_bar.py[line:274] - INFO: epoch 001:   5226 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.88, wpb=112.8, bsz=40, num_updates=5220, lr=4.9732e-05, gnorm=0.662, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=24858
2023-02-16 22:41:08 - progress_bar.py[line:274] - INFO: epoch 001:   5236 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=112.7, bsz=40, num_updates=5230, lr=4.97275e-05, gnorm=0.407, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24869
2023-02-16 22:41:19 - progress_bar.py[line:274] - INFO: epoch 001:   5246 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=111.5, bsz=40, num_updates=5240, lr=4.9723e-05, gnorm=0.42, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24880
2023-02-16 22:41:30 - progress_bar.py[line:274] - INFO: epoch 001:   5256 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.92, wpb=110.5, bsz=40, num_updates=5250, lr=4.97185e-05, gnorm=0.613, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24891
2023-02-16 22:41:41 - progress_bar.py[line:274] - INFO: epoch 001:   5266 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.9, wpb=113, bsz=40, num_updates=5260, lr=4.9714e-05, gnorm=0.662, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24902
2023-02-16 22:41:52 - progress_bar.py[line:274] - INFO: epoch 001:   5276 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.91, wpb=112.7, bsz=40, num_updates=5270, lr=4.97095e-05, gnorm=0.422, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=24913
2023-02-16 22:42:04 - progress_bar.py[line:274] - INFO: epoch 001:   5286 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.88, wpb=111.8, bsz=40, num_updates=5280, lr=4.9705e-05, gnorm=0.529, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24925
2023-02-16 22:42:14 - progress_bar.py[line:274] - INFO: epoch 001:   5296 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.2, ups=0.93, wpb=111.8, bsz=40, num_updates=5290, lr=4.97005e-05, gnorm=0.575, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=24935
2023-02-16 22:42:26 - progress_bar.py[line:274] - INFO: epoch 001:   5306 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.88, wpb=111.2, bsz=40, num_updates=5300, lr=4.9696e-05, gnorm=0.467, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=24947
2023-02-16 22:42:37 - progress_bar.py[line:274] - INFO: epoch 001:   5316 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.91, wpb=111.5, bsz=40, num_updates=5310, lr=4.96915e-05, gnorm=0.65, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=24958
2023-02-16 22:42:48 - progress_bar.py[line:274] - INFO: epoch 001:   5326 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=111.8, bsz=40, num_updates=5320, lr=4.9687e-05, gnorm=0.472, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24969
2023-02-16 22:42:59 - progress_bar.py[line:274] - INFO: epoch 001:   5336 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.89, wpb=111.2, bsz=40, num_updates=5330, lr=4.96825e-05, gnorm=0.448, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24980
2023-02-16 22:43:10 - progress_bar.py[line:274] - INFO: epoch 001:   5346 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.9, wpb=113.5, bsz=40, num_updates=5340, lr=4.9678e-05, gnorm=0.456, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=24991
2023-02-16 22:43:21 - progress_bar.py[line:274] - INFO: epoch 001:   5356 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.1, ups=0.92, wpb=112.1, bsz=40, num_updates=5350, lr=4.96735e-05, gnorm=0.608, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25002
2023-02-16 22:43:32 - progress_bar.py[line:274] - INFO: epoch 001:   5366 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.1, ups=0.92, wpb=112.2, bsz=40, num_updates=5360, lr=4.9669e-05, gnorm=0.534, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25013
2023-02-16 22:43:43 - progress_bar.py[line:274] - INFO: epoch 001:   5376 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.87, wpb=112.3, bsz=40, num_updates=5370, lr=4.96645e-05, gnorm=0.597, clip=10, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=25024
2023-02-16 22:43:54 - progress_bar.py[line:274] - INFO: epoch 001:   5386 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.3, ups=0.92, wpb=112.2, bsz=40, num_updates=5380, lr=4.966e-05, gnorm=0.501, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25035
2023-02-16 22:44:06 - progress_bar.py[line:274] - INFO: epoch 001:   5396 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.87, wpb=112.2, bsz=40, num_updates=5390, lr=4.96555e-05, gnorm=0.505, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25047
2023-02-16 22:44:17 - progress_bar.py[line:274] - INFO: epoch 001:   5406 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.88, wpb=110.8, bsz=40, num_updates=5400, lr=4.96509e-05, gnorm=0.546, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25058
2023-02-16 22:44:28 - progress_bar.py[line:274] - INFO: epoch 001:   5416 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.89, wpb=112.4, bsz=40, num_updates=5410, lr=4.96464e-05, gnorm=0.618, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25069
2023-02-16 22:44:39 - progress_bar.py[line:274] - INFO: epoch 001:   5426 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.6, ups=0.93, wpb=112.7, bsz=40, num_updates=5420, lr=4.96419e-05, gnorm=0.528, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25080
2023-02-16 22:44:50 - progress_bar.py[line:274] - INFO: epoch 001:   5436 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.2, ups=0.9, wpb=113.1, bsz=40, num_updates=5430, lr=4.96374e-05, gnorm=0.589, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25091
2023-02-16 22:45:01 - progress_bar.py[line:274] - INFO: epoch 001:   5446 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.2, ups=0.91, wpb=112.7, bsz=40, num_updates=5440, lr=4.96329e-05, gnorm=0.538, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25102
2023-02-16 22:45:12 - progress_bar.py[line:274] - INFO: epoch 001:   5456 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=5450, lr=4.96284e-05, gnorm=0.564, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25113
2023-02-16 22:45:24 - progress_bar.py[line:274] - INFO: epoch 001:   5466 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.88, wpb=112.7, bsz=40, num_updates=5460, lr=4.96239e-05, gnorm=0.372, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25125
2023-02-16 22:45:35 - progress_bar.py[line:274] - INFO: epoch 001:   5476 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.6, bsz=40, num_updates=5470, lr=4.96194e-05, gnorm=0.552, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25136
2023-02-16 22:45:46 - progress_bar.py[line:274] - INFO: epoch 001:   5486 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=112.4, bsz=40, num_updates=5480, lr=4.96149e-05, gnorm=0.554, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25147
2023-02-16 22:45:57 - progress_bar.py[line:274] - INFO: epoch 001:   5496 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105, ups=0.93, wpb=112.7, bsz=40, num_updates=5490, lr=4.96104e-05, gnorm=0.523, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25158
2023-02-16 22:46:08 - progress_bar.py[line:274] - INFO: epoch 001:   5506 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.1, bsz=40, num_updates=5500, lr=4.96059e-05, gnorm=0.56, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25169
2023-02-16 22:46:19 - progress_bar.py[line:274] - INFO: epoch 001:   5516 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.9, wpb=113, bsz=40, num_updates=5510, lr=4.96014e-05, gnorm=0.561, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25180
2023-02-16 22:46:30 - progress_bar.py[line:274] - INFO: epoch 001:   5526 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.88, wpb=112.3, bsz=40, num_updates=5520, lr=4.95969e-05, gnorm=0.554, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25191
2023-02-16 22:46:41 - progress_bar.py[line:274] - INFO: epoch 001:   5536 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.3, ups=0.92, wpb=112.7, bsz=40, num_updates=5530, lr=4.95924e-05, gnorm=0.474, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25202
2023-02-16 22:46:52 - progress_bar.py[line:274] - INFO: epoch 001:   5546 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.89, wpb=113.1, bsz=40, num_updates=5540, lr=4.95879e-05, gnorm=0.488, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25213
2023-02-16 22:47:04 - progress_bar.py[line:274] - INFO: epoch 001:   5556 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=111.3, bsz=40, num_updates=5550, lr=4.95834e-05, gnorm=0.616, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25225
2023-02-16 22:47:14 - progress_bar.py[line:274] - INFO: epoch 001:   5566 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.92, wpb=112.3, bsz=40, num_updates=5560, lr=4.95789e-05, gnorm=0.49, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=25235
2023-02-16 22:47:26 - progress_bar.py[line:274] - INFO: epoch 001:   5576 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=112, bsz=40, num_updates=5570, lr=4.95744e-05, gnorm=0.497, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25247
2023-02-16 22:47:37 - progress_bar.py[line:274] - INFO: epoch 001:   5586 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.91, wpb=112.6, bsz=40, num_updates=5580, lr=4.95699e-05, gnorm=0.447, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25258
2023-02-16 22:47:48 - progress_bar.py[line:274] - INFO: epoch 001:   5596 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.9, wpb=110.9, bsz=40, num_updates=5590, lr=4.95654e-05, gnorm=0.458, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25269
2023-02-16 22:47:59 - progress_bar.py[line:274] - INFO: epoch 001:   5606 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.91, wpb=111.7, bsz=40, num_updates=5600, lr=4.95609e-05, gnorm=0.545, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25280
2023-02-16 22:48:10 - progress_bar.py[line:274] - INFO: epoch 001:   5616 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.89, wpb=112.2, bsz=40, num_updates=5610, lr=4.95564e-05, gnorm=0.53, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25291
2023-02-16 22:48:21 - progress_bar.py[line:274] - INFO: epoch 001:   5626 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=113.6, bsz=40, num_updates=5620, lr=4.95519e-05, gnorm=0.491, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25302
2023-02-16 22:48:33 - progress_bar.py[line:274] - INFO: epoch 001:   5636 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.9, wpb=111.5, bsz=40, num_updates=5630, lr=4.95474e-05, gnorm=0.55, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25313
2023-02-16 22:48:44 - progress_bar.py[line:274] - INFO: epoch 001:   5646 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=112, bsz=40, num_updates=5640, lr=4.95429e-05, gnorm=0.453, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25325
2023-02-16 22:48:55 - progress_bar.py[line:274] - INFO: epoch 001:   5656 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.9, wpb=112, bsz=40, num_updates=5650, lr=4.95384e-05, gnorm=0.47, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25336
2023-02-16 22:49:06 - progress_bar.py[line:274] - INFO: epoch 001:   5666 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.92, wpb=109.9, bsz=40, num_updates=5660, lr=4.95338e-05, gnorm=0.569, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25347
2023-02-16 22:49:17 - progress_bar.py[line:274] - INFO: epoch 001:   5676 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=112.5, bsz=40, num_updates=5670, lr=4.95293e-05, gnorm=0.454, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25358
2023-02-16 22:49:28 - progress_bar.py[line:274] - INFO: epoch 001:   5686 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.4, ups=0.93, wpb=112.2, bsz=40, num_updates=5680, lr=4.95248e-05, gnorm=0.547, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=25369
2023-02-16 22:49:39 - progress_bar.py[line:274] - INFO: epoch 001:   5696 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.87, wpb=112.2, bsz=40, num_updates=5690, lr=4.95203e-05, gnorm=0.499, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=25380
2023-02-16 22:49:51 - progress_bar.py[line:274] - INFO: epoch 001:   5706 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.87, wpb=113.6, bsz=40, num_updates=5700, lr=4.95158e-05, gnorm=0.506, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25392
2023-02-16 22:50:02 - progress_bar.py[line:274] - INFO: epoch 001:   5716 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.87, wpb=112.3, bsz=40, num_updates=5710, lr=4.95113e-05, gnorm=0.578, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25403
2023-02-16 22:50:14 - progress_bar.py[line:274] - INFO: epoch 001:   5726 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.88, wpb=111.4, bsz=40, num_updates=5720, lr=4.95068e-05, gnorm=0.626, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25414
2023-02-16 22:50:25 - progress_bar.py[line:274] - INFO: epoch 001:   5736 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.9, wpb=111.3, bsz=40, num_updates=5730, lr=4.95023e-05, gnorm=0.506, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25426
2023-02-16 22:50:36 - progress_bar.py[line:274] - INFO: epoch 001:   5746 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.88, wpb=111.9, bsz=40, num_updates=5740, lr=4.94978e-05, gnorm=0.438, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25437
2023-02-16 22:50:47 - progress_bar.py[line:274] - INFO: epoch 001:   5756 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.88, wpb=112.5, bsz=40, num_updates=5750, lr=4.94933e-05, gnorm=0.684, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25448
2023-02-16 22:50:58 - progress_bar.py[line:274] - INFO: epoch 001:   5766 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.92, wpb=111.6, bsz=40, num_updates=5760, lr=4.94888e-05, gnorm=0.536, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25459
2023-02-16 22:51:10 - progress_bar.py[line:274] - INFO: epoch 001:   5776 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.88, wpb=112.7, bsz=40, num_updates=5770, lr=4.94843e-05, gnorm=0.479, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25471
2023-02-16 22:51:21 - progress_bar.py[line:274] - INFO: epoch 001:   5786 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.9, wpb=111.3, bsz=40, num_updates=5780, lr=4.94798e-05, gnorm=0.492, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25482
2023-02-16 22:51:32 - progress_bar.py[line:274] - INFO: epoch 001:   5796 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.91, wpb=112.6, bsz=40, num_updates=5790, lr=4.94753e-05, gnorm=0.434, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25493
2023-02-16 22:51:43 - progress_bar.py[line:274] - INFO: epoch 001:   5806 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.87, wpb=112.8, bsz=40, num_updates=5800, lr=4.94708e-05, gnorm=0.514, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25504
2023-02-16 22:51:54 - progress_bar.py[line:274] - INFO: epoch 001:   5816 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.91, wpb=112.2, bsz=40, num_updates=5810, lr=4.94663e-05, gnorm=0.567, clip=10, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25515
2023-02-16 22:52:06 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-16 22:52:07 - progress_bar.py[line:274] - INFO: epoch 001:   5827 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=89.4, ups=0.8, wpb=111.2, bsz=40, num_updates=5820, lr=4.94618e-05, gnorm=0.354, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=25528
2023-02-16 22:52:18 - progress_bar.py[line:274] - INFO: epoch 001:   5837 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.88, wpb=113.4, bsz=40, num_updates=5830, lr=4.94573e-05, gnorm=0.497, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25539
2023-02-16 22:52:29 - progress_bar.py[line:274] - INFO: epoch 001:   5847 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.91, wpb=112.4, bsz=40, num_updates=5840, lr=4.94528e-05, gnorm=0.475, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=25550
2023-02-16 22:52:40 - progress_bar.py[line:274] - INFO: epoch 001:   5857 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.9, wpb=111.4, bsz=40, num_updates=5850, lr=4.94483e-05, gnorm=0.564, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25561
2023-02-16 22:52:51 - progress_bar.py[line:274] - INFO: epoch 001:   5867 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.92, wpb=110.8, bsz=40, num_updates=5860, lr=4.94438e-05, gnorm=0.487, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25572
2023-02-16 22:53:02 - progress_bar.py[line:274] - INFO: epoch 001:   5877 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.3, ups=0.92, wpb=112.3, bsz=40, num_updates=5870, lr=4.94393e-05, gnorm=0.504, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25583
2023-02-16 22:53:13 - progress_bar.py[line:274] - INFO: epoch 001:   5887 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.9, wpb=114, bsz=40, num_updates=5880, lr=4.94348e-05, gnorm=0.45, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=25594
2023-02-16 22:53:24 - progress_bar.py[line:274] - INFO: epoch 001:   5897 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=112.7, bsz=40, num_updates=5890, lr=4.94303e-05, gnorm=0.603, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25605
2023-02-16 22:53:35 - progress_bar.py[line:274] - INFO: epoch 001:   5907 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.91, wpb=111, bsz=40, num_updates=5900, lr=4.94258e-05, gnorm=0.458, clip=0, loss_scale=1024, train_wall=11, gb_free=11.4, ema_decay=0.9999, wall=25616
2023-02-16 22:53:46 - progress_bar.py[line:274] - INFO: epoch 001:   5917 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.6, ups=0.92, wpb=111.8, bsz=40, num_updates=5910, lr=4.94212e-05, gnorm=0.47, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=25627
2023-02-16 22:53:57 - progress_bar.py[line:274] - INFO: epoch 001:   5927 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.91, wpb=112.3, bsz=40, num_updates=5920, lr=4.94167e-05, gnorm=0.405, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25638
2023-02-16 22:54:09 - progress_bar.py[line:274] - INFO: epoch 001:   5937 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.9, ups=0.87, wpb=110.8, bsz=40, num_updates=5930, lr=4.94122e-05, gnorm=0.408, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25650
2023-02-16 22:54:20 - progress_bar.py[line:274] - INFO: epoch 001:   5947 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=5940, lr=4.94077e-05, gnorm=0.395, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25661
2023-02-16 22:54:31 - progress_bar.py[line:274] - INFO: epoch 001:   5957 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.88, wpb=112.2, bsz=40, num_updates=5950, lr=4.94032e-05, gnorm=0.622, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25672
2023-02-16 22:54:42 - progress_bar.py[line:274] - INFO: epoch 001:   5967 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112.2, bsz=40, num_updates=5960, lr=4.93987e-05, gnorm=0.478, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25683
2023-02-16 22:54:53 - progress_bar.py[line:274] - INFO: epoch 001:   5977 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.9, wpb=109.9, bsz=40, num_updates=5970, lr=4.93942e-05, gnorm=0.521, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25694
2023-02-16 22:55:05 - progress_bar.py[line:274] - INFO: epoch 001:   5987 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.9, wpb=110.9, bsz=40, num_updates=5980, lr=4.93897e-05, gnorm=0.477, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25706
2023-02-16 22:55:16 - progress_bar.py[line:274] - INFO: epoch 001:   5997 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.91, wpb=111.8, bsz=40, num_updates=5990, lr=4.93852e-05, gnorm=0.488, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=25717
2023-02-16 22:55:27 - progress_bar.py[line:274] - INFO: epoch 001:   6007 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.89, wpb=111.9, bsz=40, num_updates=6000, lr=4.93807e-05, gnorm=0.486, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=25728
2023-02-16 22:55:27 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-16 22:55:28 - train.py[line:549] - INFO: 0 / 6234
2023-02-16 22:55:28 - train.py[line:551] - INFO: load:0.89 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-16 22:57:30 - train.py[line:549] - INFO: 200 / 6234
2023-02-16 22:57:30 - train.py[line:551] - INFO: load:0.91 valid_run:121.82 task_valid:118.94 collect_output:1.83
2023-02-16 22:59:30 - train.py[line:549] - INFO: 400 / 6234
2023-02-16 22:59:30 - train.py[line:551] - INFO: load:0.94 valid_run:241.69 task_valid:234.68 collect_output:4.89
2023-02-16 23:01:32 - train.py[line:549] - INFO: 600 / 6234
2023-02-16 23:01:32 - train.py[line:551] - INFO: load:0.96 valid_run:363.53 task_valid:351.03 collect_output:9.35
2023-02-16 23:03:34 - train.py[line:549] - INFO: 800 / 6234
2023-02-16 23:03:34 - train.py[line:551] - INFO: load:0.99 valid_run:485.25 task_valid:464.54 collect_output:16.57
2023-02-16 23:05:34 - train.py[line:549] - INFO: 1000 / 6234
2023-02-16 23:05:34 - train.py[line:551] - INFO: load:1.01 valid_run:605.59 task_valid:581.73 collect_output:18.74
2023-02-16 23:07:37 - train.py[line:549] - INFO: 1200 / 6234
2023-02-16 23:07:37 - train.py[line:551] - INFO: load:1.04 valid_run:728.31 task_valid:700.20 collect_output:21.96
2023-02-16 23:09:40 - train.py[line:549] - INFO: 1400 / 6234
2023-02-16 23:09:40 - train.py[line:551] - INFO: load:1.06 valid_run:851.25 task_valid:818.14 collect_output:25.95
2023-02-16 23:11:41 - train.py[line:549] - INFO: 1600 / 6234
2023-02-16 23:11:41 - train.py[line:551] - INFO: load:1.09 valid_run:972.91 task_valid:934.40 collect_output:30.34
2023-02-16 23:13:45 - train.py[line:549] - INFO: 1800 / 6234
2023-02-16 23:13:45 - train.py[line:551] - INFO: load:1.11 valid_run:1096.55 task_valid:1051.38 collect_output:36.01
2023-02-16 23:15:47 - train.py[line:549] - INFO: 2000 / 6234
2023-02-16 23:15:47 - train.py[line:551] - INFO: load:1.14 valid_run:1218.14 task_valid:1163.73 collect_output:44.25
2023-02-16 23:17:47 - train.py[line:549] - INFO: 2200 / 6234
2023-02-16 23:17:47 - train.py[line:551] - INFO: load:1.17 valid_run:1338.00 task_valid:1279.04 collect_output:47.79
2023-02-16 23:19:48 - train.py[line:549] - INFO: 2400 / 6234
2023-02-16 23:19:48 - train.py[line:551] - INFO: load:1.19 valid_run:1459.45 task_valid:1395.91 collect_output:51.37
2023-02-16 23:21:47 - train.py[line:549] - INFO: 2600 / 6234
2023-02-16 23:21:47 - train.py[line:551] - INFO: load:1.21 valid_run:1578.16 task_valid:1509.40 collect_output:55.59
2023-02-16 23:23:48 - train.py[line:549] - INFO: 2800 / 6234
2023-02-16 23:23:48 - train.py[line:551] - INFO: load:1.24 valid_run:1698.98 task_valid:1626.89 collect_output:57.92
2023-02-16 23:25:49 - train.py[line:549] - INFO: 3000 / 6234
2023-02-16 23:25:49 - train.py[line:551] - INFO: load:1.26 valid_run:1820.00 task_valid:1742.81 collect_output:62.03
2023-02-16 23:27:50 - train.py[line:549] - INFO: 3200 / 6234
2023-02-16 23:27:50 - train.py[line:551] - INFO: load:1.29 valid_run:1940.95 task_valid:1856.47 collect_output:68.32
2023-02-16 23:29:51 - train.py[line:549] - INFO: 3400 / 6234
2023-02-16 23:29:51 - train.py[line:551] - INFO: load:1.31 valid_run:2062.19 task_valid:1972.30 collect_output:72.73
2023-02-16 23:31:52 - train.py[line:549] - INFO: 3600 / 6234
2023-02-16 23:31:52 - train.py[line:551] - INFO: load:1.34 valid_run:2182.66 task_valid:2089.99 collect_output:74.53
2023-02-16 23:33:53 - train.py[line:549] - INFO: 3800 / 6234
2023-02-16 23:33:53 - train.py[line:551] - INFO: load:1.36 valid_run:2303.66 task_valid:2206.63 collect_output:77.90
2023-02-16 23:35:53 - train.py[line:549] - INFO: 4000 / 6234
2023-02-16 23:35:53 - train.py[line:551] - INFO: load:1.39 valid_run:2423.73 task_valid:2322.85 collect_output:80.77
2023-02-16 23:37:55 - train.py[line:549] - INFO: 4200 / 6234
2023-02-16 23:37:55 - train.py[line:551] - INFO: load:1.41 valid_run:2545.19 task_valid:2439.15 collect_output:84.93
2023-02-16 23:39:56 - train.py[line:549] - INFO: 4400 / 6234
2023-02-16 23:39:56 - train.py[line:551] - INFO: load:1.44 valid_run:2667.05 task_valid:2557.86 collect_output:87.09
2023-02-16 23:41:57 - train.py[line:549] - INFO: 4600 / 6234
2023-02-16 23:41:57 - train.py[line:551] - INFO: load:1.46 valid_run:2787.25 task_valid:2671.93 collect_output:92.22
2023-02-16 23:43:56 - train.py[line:549] - INFO: 4800 / 6234
2023-02-16 23:43:56 - train.py[line:551] - INFO: load:1.48 valid_run:2906.98 task_valid:2787.84 collect_output:95.06
2023-02-16 23:45:58 - train.py[line:549] - INFO: 5000 / 6234
2023-02-16 23:45:58 - train.py[line:551] - INFO: load:1.51 valid_run:3028.39 task_valid:2903.77 collect_output:99.55
2023-02-16 23:48:01 - train.py[line:549] - INFO: 5200 / 6234
2023-02-16 23:48:01 - train.py[line:551] - INFO: load:1.53 valid_run:3151.18 task_valid:3019.57 collect_output:105.53
2023-02-16 23:50:00 - train.py[line:549] - INFO: 5400 / 6234
2023-02-16 23:50:00 - train.py[line:551] - INFO: load:1.56 valid_run:3270.57 task_valid:3133.41 collect_output:110.08
2023-02-16 23:52:02 - train.py[line:549] - INFO: 5600 / 6234
2023-02-16 23:52:02 - train.py[line:551] - INFO: load:1.58 valid_run:3392.13 task_valid:3252.53 collect_output:111.52
2023-02-16 23:54:03 - train.py[line:549] - INFO: 5800 / 6234
2023-02-16 23:54:03 - train.py[line:551] - INFO: load:1.61 valid_run:3513.64 task_valid:3367.83 collect_output:116.74
2023-02-16 23:56:05 - train.py[line:549] - INFO: 6000 / 6234
2023-02-16 23:56:05 - train.py[line:551] - INFO: load:1.63 valid_run:3635.40 task_valid:3486.15 collect_output:119.19
2023-02-16 23:58:06 - train.py[line:549] - INFO: 6200 / 6234
2023-02-16 23:58:06 - train.py[line:551] - INFO: load:1.66 valid_run:3756.29 task_valid:3604.39 collect_output:120.83

====================================================================================================
SGG eval:     R @ 50: 0.6396;     R @ 100: 0.6811;     R @ 500: 0.7068;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4374;    mR @ 100: 0.4862;    mR @ 500: 0.5278;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.5625) (covering:0.3714) (eating:0.8235) (flying in:0.9545) (growing on:0.5000) (hanging from:0.5161) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9712) (says:0.0000) (sitting on:0.7132) (standing on:0.3943) (using:0.5500) (walking in:0.0000) (walking on:0.6892) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-02-16 23:58:37 - train.py[line:487] - INFO: 0.6810806468041761

====================================================================================================
SGG eval:     R @ 50: 0.6396;     R @ 100: 0.6811;     R @ 500: 0.7068;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4374;    mR @ 100: 0.4862;    mR @ 500: 0.5278;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7805) (covered in:0.5625) (covering:0.3714) (eating:0.8235) (flying in:0.9545) (growing on:0.5000) (hanging from:0.5161) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9712) (says:0.0000) (sitting on:0.7132) (standing on:0.3943) (using:0.5500) (walking in:0.0000) (walking on:0.6892) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-02-16 23:58:37 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-16 23:58:37 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.251 | loss_v1 0 | loss_v2 0 | nll_loss 0.085 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.681081 | ppl 1.06 | vqa_score 0.4955 | wps 118.4 | wpb 72 | bsz 24 | num_updates 6000 | best_R@100 0.681081
2023-02-16 23:58:37 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6000 updates
2023-02-16 23:58:37 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_6000.pt
2023-02-16 23:58:42 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_6000.pt
2023-02-16 23:58:47 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 0.6810806468041761) (writing took 10.046708703041077 seconds)
2023-02-16 23:58:58 - progress_bar.py[line:274] - INFO: epoch 001:   6017 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=111.8, bsz=40, num_updates=6010, lr=4.93762e-05, gnorm=0.409, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29539
2023-02-16 23:59:09 - progress_bar.py[line:274] - INFO: epoch 001:   6027 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.91, wpb=113.7, bsz=40, num_updates=6020, lr=4.93717e-05, gnorm=0.358, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29550
2023-02-16 23:59:20 - progress_bar.py[line:274] - INFO: epoch 001:   6037 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.91, wpb=113.9, bsz=40, num_updates=6030, lr=4.93672e-05, gnorm=0.446, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29561
2023-02-16 23:59:32 - progress_bar.py[line:274] - INFO: epoch 001:   6047 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=6040, lr=4.93627e-05, gnorm=0.385, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29572
2023-02-16 23:59:43 - progress_bar.py[line:274] - INFO: epoch 001:   6057 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.89, wpb=111.9, bsz=40, num_updates=6050, lr=4.93582e-05, gnorm=0.514, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29584
2023-02-16 23:59:54 - progress_bar.py[line:274] - INFO: epoch 001:   6067 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=112.4, bsz=40, num_updates=6060, lr=4.93537e-05, gnorm=0.541, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29595
2023-02-17 00:00:05 - progress_bar.py[line:274] - INFO: epoch 001:   6077 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.2, ups=0.88, wpb=110, bsz=40, num_updates=6070, lr=4.93492e-05, gnorm=0.431, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29606
2023-02-17 00:00:16 - progress_bar.py[line:274] - INFO: epoch 001:   6087 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.91, wpb=111.8, bsz=40, num_updates=6080, lr=4.93447e-05, gnorm=0.632, clip=10, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=29617
2023-02-17 00:00:27 - progress_bar.py[line:274] - INFO: epoch 001:   6097 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.91, wpb=111.3, bsz=40, num_updates=6090, lr=4.93402e-05, gnorm=0.514, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29628
2023-02-17 00:00:38 - progress_bar.py[line:274] - INFO: epoch 001:   6107 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.89, wpb=112.5, bsz=40, num_updates=6100, lr=4.93357e-05, gnorm=0.469, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29639
2023-02-17 00:00:49 - progress_bar.py[line:274] - INFO: epoch 001:   6117 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.7, ups=0.93, wpb=112.4, bsz=40, num_updates=6110, lr=4.93312e-05, gnorm=0.419, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29650
2023-02-17 00:01:00 - progress_bar.py[line:274] - INFO: epoch 001:   6127 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.91, wpb=113.5, bsz=40, num_updates=6120, lr=4.93267e-05, gnorm=0.497, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29661
2023-02-17 00:01:11 - progress_bar.py[line:274] - INFO: epoch 001:   6137 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.92, wpb=110.7, bsz=40, num_updates=6130, lr=4.93222e-05, gnorm=0.624, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29672
2023-02-17 00:01:22 - progress_bar.py[line:274] - INFO: epoch 001:   6147 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.9, ups=0.92, wpb=111.9, bsz=40, num_updates=6140, lr=4.93177e-05, gnorm=0.559, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29683
2023-02-17 00:01:33 - progress_bar.py[line:274] - INFO: epoch 001:   6157 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.89, wpb=111.4, bsz=40, num_updates=6150, lr=4.93132e-05, gnorm=0.437, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29694
2023-02-17 00:01:45 - progress_bar.py[line:274] - INFO: epoch 001:   6167 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=112.3, bsz=40, num_updates=6160, lr=4.93087e-05, gnorm=0.528, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29705
2023-02-17 00:01:56 - progress_bar.py[line:274] - INFO: epoch 001:   6177 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112.7, bsz=40, num_updates=6170, lr=4.93041e-05, gnorm=0.382, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29717
2023-02-17 00:02:07 - progress_bar.py[line:274] - INFO: epoch 001:   6187 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.91, wpb=111.2, bsz=40, num_updates=6180, lr=4.92996e-05, gnorm=0.553, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29728
2023-02-17 00:02:18 - progress_bar.py[line:274] - INFO: epoch 001:   6197 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.91, wpb=111.9, bsz=40, num_updates=6190, lr=4.92951e-05, gnorm=0.441, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=29739
2023-02-17 00:02:29 - progress_bar.py[line:274] - INFO: epoch 001:   6207 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.88, wpb=111.9, bsz=40, num_updates=6200, lr=4.92906e-05, gnorm=0.409, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29750
2023-02-17 00:02:40 - progress_bar.py[line:274] - INFO: epoch 001:   6217 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.89, wpb=111.8, bsz=40, num_updates=6210, lr=4.92861e-05, gnorm=0.563, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29761
2023-02-17 00:02:52 - progress_bar.py[line:274] - INFO: epoch 001:   6227 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.9, wpb=111.9, bsz=40, num_updates=6220, lr=4.92816e-05, gnorm=0.584, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29773
2023-02-17 00:03:03 - progress_bar.py[line:274] - INFO: epoch 001:   6237 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.91, wpb=112.3, bsz=40, num_updates=6230, lr=4.92771e-05, gnorm=0.476, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=29784
2023-02-17 00:03:14 - progress_bar.py[line:274] - INFO: epoch 001:   6247 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.88, wpb=111.9, bsz=40, num_updates=6240, lr=4.92726e-05, gnorm=0.473, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29795
2023-02-17 00:03:25 - progress_bar.py[line:274] - INFO: epoch 001:   6257 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.88, wpb=113.3, bsz=40, num_updates=6250, lr=4.92681e-05, gnorm=0.421, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29806
2023-02-17 00:03:36 - progress_bar.py[line:274] - INFO: epoch 001:   6267 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.6, ups=0.92, wpb=112.8, bsz=40, num_updates=6260, lr=4.92636e-05, gnorm=0.481, clip=0, loss_scale=1024, train_wall=11, gb_free=11.4, ema_decay=0.9999, wall=29817
2023-02-17 00:03:47 - progress_bar.py[line:274] - INFO: epoch 001:   6277 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=111.6, bsz=40, num_updates=6270, lr=4.92591e-05, gnorm=0.464, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29828
2023-02-17 00:03:58 - progress_bar.py[line:274] - INFO: epoch 001:   6287 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.91, wpb=110.8, bsz=40, num_updates=6280, lr=4.92546e-05, gnorm=0.471, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29839
2023-02-17 00:04:10 - progress_bar.py[line:274] - INFO: epoch 001:   6297 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=111.5, bsz=40, num_updates=6290, lr=4.92501e-05, gnorm=0.615, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29851
2023-02-17 00:04:21 - progress_bar.py[line:274] - INFO: epoch 001:   6307 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.5, ups=0.92, wpb=112.7, bsz=40, num_updates=6300, lr=4.92456e-05, gnorm=0.464, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29862
2023-02-17 00:04:32 - progress_bar.py[line:274] - INFO: epoch 001:   6317 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.88, wpb=112.5, bsz=40, num_updates=6310, lr=4.92411e-05, gnorm=0.387, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29873
2023-02-17 00:04:43 - progress_bar.py[line:274] - INFO: epoch 001:   6327 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.87, wpb=112.8, bsz=40, num_updates=6320, lr=4.92366e-05, gnorm=0.441, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=29884
2023-02-17 00:04:54 - progress_bar.py[line:274] - INFO: epoch 001:   6337 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.6, ups=0.91, wpb=112.9, bsz=40, num_updates=6330, lr=4.92321e-05, gnorm=0.423, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29895
2023-02-17 00:05:05 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-17 00:05:06 - progress_bar.py[line:274] - INFO: epoch 001:   6348 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=92.7, ups=0.83, wpb=111.8, bsz=40, num_updates=6340, lr=4.92276e-05, gnorm=0.368, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29907
2023-02-17 00:05:17 - progress_bar.py[line:274] - INFO: epoch 001:   6358 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.91, wpb=112.7, bsz=40, num_updates=6350, lr=4.92231e-05, gnorm=0.414, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=29918
2023-02-17 00:05:28 - progress_bar.py[line:274] - INFO: epoch 001:   6368 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.1, ups=0.93, wpb=112.7, bsz=40, num_updates=6360, lr=4.92186e-05, gnorm=0.485, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29929
2023-02-17 00:05:40 - progress_bar.py[line:274] - INFO: epoch 001:   6378 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=6370, lr=4.92141e-05, gnorm=0.614, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29940
2023-02-17 00:05:51 - progress_bar.py[line:274] - INFO: epoch 001:   6388 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.88, wpb=111.3, bsz=40, num_updates=6380, lr=4.92096e-05, gnorm=0.569, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=29952
2023-02-17 00:06:02 - progress_bar.py[line:274] - INFO: epoch 001:   6398 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.2, ups=0.93, wpb=111.9, bsz=40, num_updates=6390, lr=4.92051e-05, gnorm=0.476, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29963
2023-02-17 00:06:13 - progress_bar.py[line:274] - INFO: epoch 001:   6408 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=112.7, bsz=40, num_updates=6400, lr=4.92006e-05, gnorm=0.476, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=29974
2023-02-17 00:06:24 - progress_bar.py[line:274] - INFO: epoch 001:   6418 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.88, wpb=111.2, bsz=40, num_updates=6410, lr=4.91961e-05, gnorm=0.342, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29985
2023-02-17 00:06:36 - progress_bar.py[line:274] - INFO: epoch 001:   6428 / 11564 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=112, bsz=40, num_updates=6420, lr=4.91916e-05, gnorm=0.342, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=29996
2023-02-17 00:06:47 - progress_bar.py[line:274] - INFO: epoch 001:   6438 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=111.7, bsz=40, num_updates=6430, lr=4.9187e-05, gnorm=0.453, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=30007
2023-02-17 00:06:57 - progress_bar.py[line:274] - INFO: epoch 001:   6448 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.8, ups=0.92, wpb=111.9, bsz=40, num_updates=6440, lr=4.91825e-05, gnorm=0.519, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30018
2023-02-17 00:07:09 - progress_bar.py[line:274] - INFO: epoch 001:   6458 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.87, wpb=111.9, bsz=40, num_updates=6450, lr=4.9178e-05, gnorm=0.568, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30030
2023-02-17 00:07:20 - progress_bar.py[line:274] - INFO: epoch 001:   6468 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=112.3, bsz=40, num_updates=6460, lr=4.91735e-05, gnorm=0.533, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30041
2023-02-17 00:07:31 - progress_bar.py[line:274] - INFO: epoch 001:   6478 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.6, ups=0.92, wpb=111.8, bsz=40, num_updates=6470, lr=4.9169e-05, gnorm=0.555, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30052
2023-02-17 00:07:42 - progress_bar.py[line:274] - INFO: epoch 001:   6488 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.1, ups=0.93, wpb=110.9, bsz=40, num_updates=6480, lr=4.91645e-05, gnorm=0.486, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30062
2023-02-17 00:07:53 - progress_bar.py[line:274] - INFO: epoch 001:   6498 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.89, wpb=113.3, bsz=40, num_updates=6490, lr=4.916e-05, gnorm=0.43, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30074
2023-02-17 00:08:04 - progress_bar.py[line:274] - INFO: epoch 001:   6508 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=111.7, bsz=40, num_updates=6500, lr=4.91555e-05, gnorm=0.459, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30085
2023-02-17 00:08:15 - progress_bar.py[line:274] - INFO: epoch 001:   6518 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.91, wpb=112.2, bsz=40, num_updates=6510, lr=4.9151e-05, gnorm=0.503, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30096
2023-02-17 00:08:26 - progress_bar.py[line:274] - INFO: epoch 001:   6528 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.89, wpb=110.8, bsz=40, num_updates=6520, lr=4.91465e-05, gnorm=0.48, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30107
2023-02-17 00:08:37 - progress_bar.py[line:274] - INFO: epoch 001:   6538 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.9, wpb=113.5, bsz=40, num_updates=6530, lr=4.9142e-05, gnorm=0.504, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30118
2023-02-17 00:08:48 - progress_bar.py[line:274] - INFO: epoch 001:   6548 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=107.2, ups=0.94, wpb=113.4, bsz=40, num_updates=6540, lr=4.91375e-05, gnorm=0.505, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30129
2023-02-17 00:08:59 - progress_bar.py[line:274] - INFO: epoch 001:   6558 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.88, wpb=112, bsz=40, num_updates=6550, lr=4.9133e-05, gnorm=0.554, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30140
2023-02-17 00:09:10 - progress_bar.py[line:274] - INFO: epoch 001:   6568 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=112, bsz=40, num_updates=6560, lr=4.91285e-05, gnorm=0.432, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30151
2023-02-17 00:09:21 - progress_bar.py[line:274] - INFO: epoch 001:   6578 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.92, wpb=111.7, bsz=40, num_updates=6570, lr=4.9124e-05, gnorm=0.475, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=30162
2023-02-17 00:09:32 - progress_bar.py[line:274] - INFO: epoch 001:   6588 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.88, wpb=112.7, bsz=40, num_updates=6580, lr=4.91195e-05, gnorm=0.425, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30173
2023-02-17 00:09:44 - progress_bar.py[line:274] - INFO: epoch 001:   6598 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.89, wpb=110.4, bsz=40, num_updates=6590, lr=4.9115e-05, gnorm=0.462, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30185
2023-02-17 00:09:55 - progress_bar.py[line:274] - INFO: epoch 001:   6608 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.9, wpb=111.8, bsz=40, num_updates=6600, lr=4.91105e-05, gnorm=0.56, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30196
2023-02-17 00:10:06 - progress_bar.py[line:274] - INFO: epoch 001:   6618 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.1, ups=0.87, wpb=111.3, bsz=40, num_updates=6610, lr=4.9106e-05, gnorm=0.446, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=30207
2023-02-17 00:10:18 - progress_bar.py[line:274] - INFO: epoch 001:   6628 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=112.2, bsz=40, num_updates=6620, lr=4.91015e-05, gnorm=0.468, clip=10, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=30218
2023-02-17 00:10:29 - progress_bar.py[line:274] - INFO: epoch 001:   6638 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=111.7, bsz=40, num_updates=6630, lr=4.9097e-05, gnorm=0.411, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30230
2023-02-17 00:10:40 - progress_bar.py[line:274] - INFO: epoch 001:   6648 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.92, wpb=112.6, bsz=40, num_updates=6640, lr=4.90925e-05, gnorm=0.511, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30241
2023-02-17 00:10:51 - progress_bar.py[line:274] - INFO: epoch 001:   6658 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=110.5, bsz=40, num_updates=6650, lr=4.9088e-05, gnorm=0.562, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30252
2023-02-17 00:11:02 - progress_bar.py[line:274] - INFO: epoch 001:   6668 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.88, wpb=113.3, bsz=40, num_updates=6660, lr=4.90835e-05, gnorm=0.489, clip=10, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=30263
2023-02-17 00:11:14 - progress_bar.py[line:274] - INFO: epoch 001:   6678 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.87, wpb=111.9, bsz=40, num_updates=6670, lr=4.9079e-05, gnorm=0.59, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30275
2023-02-17 00:11:25 - progress_bar.py[line:274] - INFO: epoch 001:   6688 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.7, ups=0.88, wpb=111.7, bsz=40, num_updates=6680, lr=4.90744e-05, gnorm=0.511, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30286
2023-02-17 00:11:36 - progress_bar.py[line:274] - INFO: epoch 001:   6698 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.9, ups=0.93, wpb=111.6, bsz=40, num_updates=6690, lr=4.90699e-05, gnorm=0.52, clip=0, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=30297
2023-02-17 00:11:47 - progress_bar.py[line:274] - INFO: epoch 001:   6708 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.2, ups=0.92, wpb=111.1, bsz=40, num_updates=6700, lr=4.90654e-05, gnorm=0.47, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30308
2023-02-17 00:11:58 - progress_bar.py[line:274] - INFO: epoch 001:   6718 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.89, wpb=111.7, bsz=40, num_updates=6710, lr=4.90609e-05, gnorm=0.507, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=30319
2023-02-17 00:12:09 - progress_bar.py[line:274] - INFO: epoch 001:   6728 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.91, wpb=111.4, bsz=40, num_updates=6720, lr=4.90564e-05, gnorm=0.501, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30330
2023-02-17 00:12:20 - progress_bar.py[line:274] - INFO: epoch 001:   6738 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=6730, lr=4.90519e-05, gnorm=0.396, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30341
2023-02-17 00:12:31 - progress_bar.py[line:274] - INFO: epoch 001:   6748 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.9, ups=0.91, wpb=110.6, bsz=40, num_updates=6740, lr=4.90474e-05, gnorm=0.528, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=30352
2023-02-17 00:12:43 - progress_bar.py[line:274] - INFO: epoch 001:   6758 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.88, wpb=112.1, bsz=40, num_updates=6750, lr=4.90429e-05, gnorm=0.489, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30363
2023-02-17 00:12:54 - progress_bar.py[line:274] - INFO: epoch 001:   6768 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=112, bsz=40, num_updates=6760, lr=4.90384e-05, gnorm=0.472, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30375
2023-02-17 00:13:05 - progress_bar.py[line:274] - INFO: epoch 001:   6778 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.89, wpb=112.5, bsz=40, num_updates=6770, lr=4.90339e-05, gnorm=0.39, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30386
2023-02-17 00:13:16 - progress_bar.py[line:274] - INFO: epoch 001:   6788 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.91, wpb=112.7, bsz=40, num_updates=6780, lr=4.90294e-05, gnorm=0.355, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30397
2023-02-17 00:13:27 - progress_bar.py[line:274] - INFO: epoch 001:   6798 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=112.5, bsz=40, num_updates=6790, lr=4.90249e-05, gnorm=0.435, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30408
2023-02-17 00:13:39 - progress_bar.py[line:274] - INFO: epoch 001:   6808 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.87, wpb=113, bsz=40, num_updates=6800, lr=4.90204e-05, gnorm=0.487, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30419
2023-02-17 00:13:49 - progress_bar.py[line:274] - INFO: epoch 001:   6818 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.8, ups=0.92, wpb=112, bsz=40, num_updates=6810, lr=4.90159e-05, gnorm=0.429, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30430
2023-02-17 00:14:01 - progress_bar.py[line:274] - INFO: epoch 001:   6828 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=112.1, bsz=40, num_updates=6820, lr=4.90114e-05, gnorm=0.421, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30441
2023-02-17 00:14:11 - progress_bar.py[line:274] - INFO: epoch 001:   6838 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.8, ups=0.96, wpb=110, bsz=40, num_updates=6830, lr=4.90069e-05, gnorm=0.499, clip=0, loss_scale=1024, train_wall=10, gb_free=11, ema_decay=0.9999, wall=30452
2023-02-17 00:14:22 - progress_bar.py[line:274] - INFO: epoch 001:   6848 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=112.4, bsz=40, num_updates=6840, lr=4.90024e-05, gnorm=0.357, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30463
2023-02-17 00:14:33 - progress_bar.py[line:274] - INFO: epoch 001:   6858 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.9, ups=0.92, wpb=110.6, bsz=40, num_updates=6850, lr=4.89979e-05, gnorm=0.572, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30474
2023-02-17 00:14:44 - progress_bar.py[line:274] - INFO: epoch 001:   6868 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.89, wpb=112.1, bsz=40, num_updates=6860, lr=4.89934e-05, gnorm=0.478, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30485
2023-02-17 00:14:56 - progress_bar.py[line:274] - INFO: epoch 001:   6878 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.87, wpb=112.1, bsz=40, num_updates=6870, lr=4.89889e-05, gnorm=0.562, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30497
2023-02-17 00:15:07 - progress_bar.py[line:274] - INFO: epoch 001:   6888 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=112.1, bsz=40, num_updates=6880, lr=4.89844e-05, gnorm=0.423, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30508
2023-02-17 00:15:18 - progress_bar.py[line:274] - INFO: epoch 001:   6898 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=112.2, bsz=40, num_updates=6890, lr=4.89799e-05, gnorm=0.407, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30519
2023-02-17 00:15:29 - progress_bar.py[line:274] - INFO: epoch 001:   6908 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=112, bsz=40, num_updates=6900, lr=4.89754e-05, gnorm=0.386, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30530
2023-02-17 00:15:40 - progress_bar.py[line:274] - INFO: epoch 001:   6918 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.2, bsz=40, num_updates=6910, lr=4.89709e-05, gnorm=0.367, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30541
2023-02-17 00:15:51 - progress_bar.py[line:274] - INFO: epoch 001:   6928 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=112, bsz=40, num_updates=6920, lr=4.89664e-05, gnorm=0.378, clip=0, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30552
2023-02-17 00:16:02 - progress_bar.py[line:274] - INFO: epoch 001:   6938 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.92, wpb=111.5, bsz=40, num_updates=6930, lr=4.89619e-05, gnorm=0.491, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30563
2023-02-17 00:16:13 - progress_bar.py[line:274] - INFO: epoch 001:   6948 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=112.3, bsz=40, num_updates=6940, lr=4.89573e-05, gnorm=0.454, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30574
2023-02-17 00:16:24 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-17 00:16:26 - progress_bar.py[line:274] - INFO: epoch 001:   6959 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=94, ups=0.83, wpb=113.1, bsz=40, num_updates=6950, lr=4.89528e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30586
2023-02-17 00:16:36 - progress_bar.py[line:274] - INFO: epoch 001:   6969 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.91, wpb=113.1, bsz=40, num_updates=6960, lr=4.89483e-05, gnorm=0.353, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30597
2023-02-17 00:16:48 - progress_bar.py[line:274] - INFO: epoch 001:   6979 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.91, wpb=111.1, bsz=40, num_updates=6970, lr=4.89438e-05, gnorm=0.48, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30608
2023-02-17 00:16:59 - progress_bar.py[line:274] - INFO: epoch 001:   6989 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=111.8, bsz=40, num_updates=6980, lr=4.89393e-05, gnorm=0.324, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30619
2023-02-17 00:17:10 - progress_bar.py[line:274] - INFO: epoch 001:   6999 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=112.7, bsz=40, num_updates=6990, lr=4.89348e-05, gnorm=0.44, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30631
2023-02-17 00:17:21 - progress_bar.py[line:274] - INFO: epoch 001:   7009 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.2, ups=0.91, wpb=112.5, bsz=40, num_updates=7000, lr=4.89303e-05, gnorm=0.398, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30642
2023-02-17 00:17:21 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 00:17:22 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 00:17:22 - train.py[line:551] - INFO: load:0.88 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 00:19:24 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 00:19:24 - train.py[line:551] - INFO: load:0.91 valid_run:122.25 task_valid:119.03 collect_output:2.14
2023-02-17 00:21:24 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 00:21:24 - train.py[line:551] - INFO: load:0.93 valid_run:242.06 task_valid:234.74 collect_output:5.22
2023-02-17 00:23:26 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 00:23:26 - train.py[line:551] - INFO: load:0.96 valid_run:363.79 task_valid:351.05 collect_output:9.63
2023-02-17 00:25:28 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 00:25:28 - train.py[line:551] - INFO: load:0.98 valid_run:485.44 task_valid:464.55 collect_output:16.77
2023-02-17 00:27:28 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 00:27:28 - train.py[line:551] - INFO: load:1.01 valid_run:605.94 task_valid:581.74 collect_output:19.06
2023-02-17 00:29:31 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 00:29:31 - train.py[line:551] - INFO: load:1.04 valid_run:728.86 task_valid:700.49 collect_output:22.21
2023-02-17 00:31:34 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 00:31:34 - train.py[line:551] - INFO: load:1.06 valid_run:851.77 task_valid:818.49 collect_output:26.11
2023-02-17 00:33:36 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 00:33:36 - train.py[line:551] - INFO: load:1.09 valid_run:973.45 task_valid:934.87 collect_output:30.39
2023-02-17 00:35:39 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 00:35:39 - train.py[line:551] - INFO: load:1.11 valid_run:1097.02 task_valid:1052.02 collect_output:35.80
2023-02-17 00:37:41 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 00:37:41 - train.py[line:551] - INFO: load:1.14 valid_run:1218.68 task_valid:1164.67 collect_output:43.77
2023-02-17 00:39:41 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 00:39:41 - train.py[line:551] - INFO: load:1.16 valid_run:1338.71 task_valid:1280.13 collect_output:47.32
2023-02-17 00:41:43 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 00:41:43 - train.py[line:551] - INFO: load:1.19 valid_run:1460.37 task_valid:1397.16 collect_output:50.93
2023-02-17 00:43:42 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 00:43:42 - train.py[line:551] - INFO: load:1.22 valid_run:1579.19 task_valid:1510.78 collect_output:55.12
2023-02-17 00:45:43 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 00:45:43 - train.py[line:551] - INFO: load:1.24 valid_run:1700.13 task_valid:1628.45 collect_output:57.35
2023-02-17 00:47:44 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 00:47:44 - train.py[line:551] - INFO: load:1.27 valid_run:1821.02 task_valid:1744.39 collect_output:61.27
2023-02-17 00:49:45 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 00:49:45 - train.py[line:551] - INFO: load:1.29 valid_run:1941.93 task_valid:1858.25 collect_output:67.31
2023-02-17 00:51:46 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 00:51:46 - train.py[line:551] - INFO: load:1.32 valid_run:2063.19 task_valid:1974.11 collect_output:71.70
2023-02-17 00:53:47 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 00:53:47 - train.py[line:551] - INFO: load:1.34 valid_run:2183.66 task_valid:2091.80 collect_output:73.45
2023-02-17 00:55:48 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 00:55:48 - train.py[line:551] - INFO: load:1.37 valid_run:2304.83 task_valid:2208.71 collect_output:76.71
2023-02-17 00:57:48 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 00:57:48 - train.py[line:551] - INFO: load:1.40 valid_run:2425.11 task_valid:2325.11 collect_output:79.59
2023-02-17 00:59:50 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 00:59:50 - train.py[line:551] - INFO: load:1.42 valid_run:2546.70 task_valid:2441.58 collect_output:83.71
2023-02-17 01:01:52 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 01:01:52 - train.py[line:551] - INFO: load:1.45 valid_run:2668.58 task_valid:2560.27 collect_output:85.87
2023-02-17 01:03:52 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 01:03:52 - train.py[line:551] - INFO: load:1.48 valid_run:2789.05 task_valid:2674.52 collect_output:91.06
2023-02-17 01:05:52 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 01:05:52 - train.py[line:551] - INFO: load:1.50 valid_run:2908.87 task_valid:2790.53 collect_output:93.83
2023-02-17 01:07:54 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 01:07:54 - train.py[line:551] - INFO: load:1.53 valid_run:3030.55 task_valid:2906.66 collect_output:98.34
2023-02-17 01:09:57 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 01:09:57 - train.py[line:551] - INFO: load:1.55 valid_run:3153.43 task_valid:3022.49 collect_output:104.39
2023-02-17 01:11:57 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 01:11:57 - train.py[line:551] - INFO: load:1.58 valid_run:3273.09 task_valid:3136.57 collect_output:108.96
2023-02-17 01:13:58 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 01:13:58 - train.py[line:551] - INFO: load:1.61 valid_run:3394.91 task_valid:3255.87 collect_output:110.45
2023-02-17 01:16:00 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 01:16:00 - train.py[line:551] - INFO: load:1.63 valid_run:3516.51 task_valid:3371.14 collect_output:115.76
2023-02-17 01:18:02 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 01:18:02 - train.py[line:551] - INFO: load:1.66 valid_run:3638.30 task_valid:3489.56 collect_output:118.10
2023-02-17 01:20:03 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 01:20:03 - train.py[line:551] - INFO: load:1.69 valid_run:3759.25 task_valid:3607.88 collect_output:119.70

====================================================================================================
SGG eval:     R @ 50: 0.6547;     R @ 100: 0.6920;     R @ 500: 0.7183;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4513;    mR @ 100: 0.4892;    mR @ 500: 0.5407;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.5625) (covering:0.3714) (eating:0.8235) (flying in:0.7727) (growing on:0.3750) (hanging from:0.5484) (lying on:0.4000) (mounted on:0.0000) (painted on:0.2500) (parked on:1.0000) (playing:0.0000) (riding:0.9663) (says:0.0000) (sitting on:0.7313) (standing on:0.3893) (using:0.6000) (walking in:0.0000) (walking on:0.7703) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-02-17 01:20:33 - train.py[line:487] - INFO: 0.6920200407435702

====================================================================================================
SGG eval:     R @ 50: 0.6547;     R @ 100: 0.6920;     R @ 500: 0.7183;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4513;    mR @ 100: 0.4892;    mR @ 500: 0.5407;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.5625) (covering:0.3714) (eating:0.8235) (flying in:0.7727) (growing on:0.3750) (hanging from:0.5484) (lying on:0.4000) (mounted on:0.0000) (painted on:0.2500) (parked on:1.0000) (playing:0.0000) (riding:0.9663) (says:0.0000) (sitting on:0.7313) (standing on:0.3893) (using:0.6000) (walking in:0.0000) (walking on:0.7703) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-02-17 01:20:34 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 01:20:34 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.261 | loss_v1 0 | loss_v2 0 | nll_loss 0.098 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.69202 | ppl 1.07 | vqa_score 0.5259 | wps 118.3 | wpb 72 | bsz 24 | num_updates 7000 | best_R@100 0.69202
2023-02-17 01:20:34 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 7000 updates
2023-02-17 01:20:34 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_7000.pt
2023-02-17 01:20:39 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_7000.pt
2023-02-17 01:20:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_7000.pt (epoch 1 @ 7000 updates, score 0.6920200407435702) (writing took 10.717084476724267 seconds)
2023-02-17 01:20:55 - progress_bar.py[line:274] - INFO: epoch 001:   7019 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=0.3, ups=0, wpb=112.5, bsz=40, num_updates=7010, lr=4.89258e-05, gnorm=0.596, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34456
2023-02-17 01:21:06 - progress_bar.py[line:274] - INFO: epoch 001:   7029 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=112.4, bsz=40, num_updates=7020, lr=4.89213e-05, gnorm=0.443, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34467
2023-02-17 01:21:18 - progress_bar.py[line:274] - INFO: epoch 001:   7039 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.88, wpb=112.3, bsz=40, num_updates=7030, lr=4.89168e-05, gnorm=0.603, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34479
2023-02-17 01:21:29 - progress_bar.py[line:274] - INFO: epoch 001:   7049 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=111.1, bsz=40, num_updates=7040, lr=4.89123e-05, gnorm=0.428, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34490
2023-02-17 01:21:40 - progress_bar.py[line:274] - INFO: epoch 001:   7059 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.92, wpb=111.3, bsz=40, num_updates=7050, lr=4.89078e-05, gnorm=0.353, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34501
2023-02-17 01:21:48 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 01:21:52 - progress_bar.py[line:274] - INFO: epoch 001:   7070 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=89.1, ups=0.8, wpb=111, bsz=40, num_updates=7060, lr=4.89033e-05, gnorm=0.482, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=34513
2023-02-17 01:22:03 - progress_bar.py[line:274] - INFO: epoch 001:   7080 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.6, ups=0.92, wpb=112.9, bsz=40, num_updates=7070, lr=4.88988e-05, gnorm=0.481, clip=0, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=34524
2023-02-17 01:22:14 - progress_bar.py[line:274] - INFO: epoch 001:   7090 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.91, wpb=112.9, bsz=40, num_updates=7080, lr=4.88943e-05, gnorm=0.413, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34535
2023-02-17 01:22:25 - progress_bar.py[line:274] - INFO: epoch 001:   7100 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.92, wpb=111.2, bsz=40, num_updates=7090, lr=4.88898e-05, gnorm=0.341, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34546
2023-02-17 01:22:36 - progress_bar.py[line:274] - INFO: epoch 001:   7110 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=111.8, bsz=40, num_updates=7100, lr=4.88853e-05, gnorm=0.421, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34557
2023-02-17 01:22:47 - progress_bar.py[line:274] - INFO: epoch 001:   7120 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.89, wpb=112.6, bsz=40, num_updates=7110, lr=4.88808e-05, gnorm=0.397, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34568
2023-02-17 01:22:58 - progress_bar.py[line:274] - INFO: epoch 001:   7130 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.9, wpb=111.1, bsz=40, num_updates=7120, lr=4.88763e-05, gnorm=0.406, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34579
2023-02-17 01:23:09 - progress_bar.py[line:274] - INFO: epoch 001:   7140 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.4, ups=0.92, wpb=112.4, bsz=40, num_updates=7130, lr=4.88718e-05, gnorm=0.49, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=34590
2023-02-17 01:23:20 - progress_bar.py[line:274] - INFO: epoch 001:   7150 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.91, wpb=113.9, bsz=40, num_updates=7140, lr=4.88673e-05, gnorm=0.442, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=34601
2023-02-17 01:23:31 - progress_bar.py[line:274] - INFO: epoch 001:   7160 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.8, ups=0.92, wpb=111.8, bsz=40, num_updates=7150, lr=4.88628e-05, gnorm=0.424, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34612
2023-02-17 01:23:42 - progress_bar.py[line:274] - INFO: epoch 001:   7170 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.2, ups=0.92, wpb=113.1, bsz=40, num_updates=7160, lr=4.88583e-05, gnorm=0.45, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34623
2023-02-17 01:23:53 - progress_bar.py[line:274] - INFO: epoch 001:   7180 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=107.4, ups=0.94, wpb=113.7, bsz=40, num_updates=7170, lr=4.88538e-05, gnorm=0.472, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=34634
2023-02-17 01:24:04 - progress_bar.py[line:274] - INFO: epoch 001:   7190 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=112.8, bsz=40, num_updates=7180, lr=4.88493e-05, gnorm=0.48, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34645
2023-02-17 01:24:15 - progress_bar.py[line:274] - INFO: epoch 001:   7200 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.1, ups=0.92, wpb=111.1, bsz=40, num_updates=7190, lr=4.88448e-05, gnorm=0.469, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34656
2023-02-17 01:24:26 - progress_bar.py[line:274] - INFO: epoch 001:   7210 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.89, wpb=114, bsz=40, num_updates=7200, lr=4.88402e-05, gnorm=0.384, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34667
2023-02-17 01:24:37 - progress_bar.py[line:274] - INFO: epoch 001:   7220 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.91, wpb=111.6, bsz=40, num_updates=7210, lr=4.88357e-05, gnorm=0.573, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34678
2023-02-17 01:24:48 - progress_bar.py[line:274] - INFO: epoch 001:   7230 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.91, wpb=111.8, bsz=40, num_updates=7220, lr=4.88312e-05, gnorm=0.484, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34689
2023-02-17 01:25:00 - progress_bar.py[line:274] - INFO: epoch 001:   7240 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.2, ups=0.88, wpb=111.1, bsz=40, num_updates=7230, lr=4.88267e-05, gnorm=0.401, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34700
2023-02-17 01:25:11 - progress_bar.py[line:274] - INFO: epoch 001:   7250 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.91, wpb=112.6, bsz=40, num_updates=7240, lr=4.88222e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34711
2023-02-17 01:25:21 - progress_bar.py[line:274] - INFO: epoch 001:   7260 / 11564 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.92, wpb=111.6, bsz=40, num_updates=7250, lr=4.88177e-05, gnorm=0.345, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34722
2023-02-17 01:25:32 - progress_bar.py[line:274] - INFO: epoch 001:   7270 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.92, wpb=110.8, bsz=40, num_updates=7260, lr=4.88132e-05, gnorm=0.522, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34733
2023-02-17 01:25:43 - progress_bar.py[line:274] - INFO: epoch 001:   7280 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=111.2, bsz=40, num_updates=7270, lr=4.88087e-05, gnorm=0.369, clip=0, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=34744
2023-02-17 01:25:55 - progress_bar.py[line:274] - INFO: epoch 001:   7290 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.9, wpb=111.1, bsz=40, num_updates=7280, lr=4.88042e-05, gnorm=0.463, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=34755
2023-02-17 01:26:06 - progress_bar.py[line:274] - INFO: epoch 001:   7300 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.89, wpb=110, bsz=40, num_updates=7290, lr=4.87997e-05, gnorm=0.591, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34767
2023-02-17 01:26:17 - progress_bar.py[line:274] - INFO: epoch 001:   7310 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=111.7, bsz=40, num_updates=7300, lr=4.87952e-05, gnorm=0.469, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34778
2023-02-17 01:26:28 - progress_bar.py[line:274] - INFO: epoch 001:   7320 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.91, wpb=110.5, bsz=40, num_updates=7310, lr=4.87907e-05, gnorm=0.517, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34789
2023-02-17 01:26:39 - progress_bar.py[line:274] - INFO: epoch 001:   7330 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.91, wpb=110.8, bsz=40, num_updates=7320, lr=4.87862e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=34800
2023-02-17 01:26:50 - progress_bar.py[line:274] - INFO: epoch 001:   7340 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=111.8, bsz=40, num_updates=7330, lr=4.87817e-05, gnorm=0.324, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34811
2023-02-17 01:27:01 - progress_bar.py[line:274] - INFO: epoch 001:   7350 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.91, wpb=113.9, bsz=40, num_updates=7340, lr=4.87772e-05, gnorm=0.347, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34822
2023-02-17 01:27:12 - progress_bar.py[line:274] - INFO: epoch 001:   7360 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=112.2, bsz=40, num_updates=7350, lr=4.87727e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34833
2023-02-17 01:27:23 - progress_bar.py[line:274] - INFO: epoch 001:   7370 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.91, wpb=112.2, bsz=40, num_updates=7360, lr=4.87682e-05, gnorm=0.441, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34844
2023-02-17 01:27:35 - progress_bar.py[line:274] - INFO: epoch 001:   7380 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.88, wpb=113.3, bsz=40, num_updates=7370, lr=4.87637e-05, gnorm=0.507, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=34856
2023-02-17 01:27:46 - progress_bar.py[line:274] - INFO: epoch 001:   7390 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=112.4, bsz=40, num_updates=7380, lr=4.87592e-05, gnorm=0.334, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=34867
2023-02-17 01:27:57 - progress_bar.py[line:274] - INFO: epoch 001:   7400 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=111.4, bsz=40, num_updates=7390, lr=4.87547e-05, gnorm=0.374, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34878
2023-02-17 01:28:08 - progress_bar.py[line:274] - INFO: epoch 001:   7410 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.89, wpb=112.5, bsz=40, num_updates=7400, lr=4.87502e-05, gnorm=0.432, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34889
2023-02-17 01:28:19 - progress_bar.py[line:274] - INFO: epoch 001:   7420 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.2, ups=0.92, wpb=113.4, bsz=40, num_updates=7410, lr=4.87457e-05, gnorm=0.349, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34900
2023-02-17 01:28:31 - progress_bar.py[line:274] - INFO: epoch 001:   7430 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.87, wpb=111.8, bsz=40, num_updates=7420, lr=4.87412e-05, gnorm=0.469, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34912
2023-02-17 01:28:42 - progress_bar.py[line:274] - INFO: epoch 001:   7440 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.9, wpb=112.9, bsz=40, num_updates=7430, lr=4.87367e-05, gnorm=0.434, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34923
2023-02-17 01:28:53 - progress_bar.py[line:274] - INFO: epoch 001:   7450 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.89, wpb=112, bsz=40, num_updates=7440, lr=4.87322e-05, gnorm=0.405, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=34934
2023-02-17 01:29:04 - progress_bar.py[line:274] - INFO: epoch 001:   7460 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=112.4, bsz=40, num_updates=7450, lr=4.87276e-05, gnorm=0.444, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=34945
2023-02-17 01:29:15 - progress_bar.py[line:274] - INFO: epoch 001:   7470 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.5, ups=0.91, wpb=112, bsz=40, num_updates=7460, lr=4.87231e-05, gnorm=0.607, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=34956
2023-02-17 01:29:26 - progress_bar.py[line:274] - INFO: epoch 001:   7480 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.88, wpb=110.3, bsz=40, num_updates=7470, lr=4.87186e-05, gnorm=0.385, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=34967
2023-02-17 01:29:38 - progress_bar.py[line:274] - INFO: epoch 001:   7490 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=112, bsz=40, num_updates=7480, lr=4.87141e-05, gnorm=0.452, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34979
2023-02-17 01:29:49 - progress_bar.py[line:274] - INFO: epoch 001:   7500 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=111.5, bsz=40, num_updates=7490, lr=4.87096e-05, gnorm=0.386, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=34990
2023-02-17 01:30:00 - progress_bar.py[line:274] - INFO: epoch 001:   7510 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.92, wpb=111.1, bsz=40, num_updates=7500, lr=4.87051e-05, gnorm=0.42, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=35001
2023-02-17 01:30:11 - progress_bar.py[line:274] - INFO: epoch 001:   7520 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.91, wpb=112, bsz=40, num_updates=7510, lr=4.87006e-05, gnorm=0.477, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35012
2023-02-17 01:30:22 - progress_bar.py[line:274] - INFO: epoch 001:   7530 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.89, wpb=112.1, bsz=40, num_updates=7520, lr=4.86961e-05, gnorm=0.53, clip=0, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=35023
2023-02-17 01:30:33 - progress_bar.py[line:274] - INFO: epoch 001:   7540 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.92, wpb=112.5, bsz=40, num_updates=7530, lr=4.86916e-05, gnorm=0.467, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=35034
2023-02-17 01:30:44 - progress_bar.py[line:274] - INFO: epoch 001:   7550 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.87, wpb=112.8, bsz=40, num_updates=7540, lr=4.86871e-05, gnorm=0.534, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35045
2023-02-17 01:30:56 - progress_bar.py[line:274] - INFO: epoch 001:   7560 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=110.9, bsz=40, num_updates=7550, lr=4.86826e-05, gnorm=0.39, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35056
2023-02-17 01:31:07 - progress_bar.py[line:274] - INFO: epoch 001:   7570 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.88, wpb=111.4, bsz=40, num_updates=7560, lr=4.86781e-05, gnorm=0.392, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=35068
2023-02-17 01:31:18 - progress_bar.py[line:274] - INFO: epoch 001:   7580 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=112.7, bsz=40, num_updates=7570, lr=4.86736e-05, gnorm=0.437, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35079
2023-02-17 01:31:29 - progress_bar.py[line:274] - INFO: epoch 001:   7590 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.9, wpb=113.1, bsz=40, num_updates=7580, lr=4.86691e-05, gnorm=0.433, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35090
2023-02-17 01:31:41 - progress_bar.py[line:274] - INFO: epoch 001:   7600 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=112, bsz=40, num_updates=7590, lr=4.86646e-05, gnorm=0.342, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35101
2023-02-17 01:31:52 - progress_bar.py[line:274] - INFO: epoch 001:   7610 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=112, bsz=40, num_updates=7600, lr=4.86601e-05, gnorm=0.467, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35113
2023-02-17 01:32:03 - progress_bar.py[line:274] - INFO: epoch 001:   7620 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=7610, lr=4.86556e-05, gnorm=0.492, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35124
2023-02-17 01:32:14 - progress_bar.py[line:274] - INFO: epoch 001:   7630 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.87, wpb=112.5, bsz=40, num_updates=7620, lr=4.86511e-05, gnorm=0.449, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35135
2023-02-17 01:32:26 - progress_bar.py[line:274] - INFO: epoch 001:   7640 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.9, wpb=111.5, bsz=40, num_updates=7630, lr=4.86466e-05, gnorm=0.323, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35146
2023-02-17 01:32:36 - progress_bar.py[line:274] - INFO: epoch 001:   7650 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.92, wpb=112.8, bsz=40, num_updates=7640, lr=4.86421e-05, gnorm=0.377, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35157
2023-02-17 01:32:48 - progress_bar.py[line:274] - INFO: epoch 001:   7660 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.87, wpb=111.9, bsz=40, num_updates=7650, lr=4.86376e-05, gnorm=0.437, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=35169
2023-02-17 01:32:59 - progress_bar.py[line:274] - INFO: epoch 001:   7670 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.88, wpb=113, bsz=40, num_updates=7660, lr=4.86331e-05, gnorm=0.434, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35180
2023-02-17 01:33:10 - progress_bar.py[line:274] - INFO: epoch 001:   7680 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=111.1, bsz=40, num_updates=7670, lr=4.86286e-05, gnorm=0.412, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35191
2023-02-17 01:33:22 - progress_bar.py[line:274] - INFO: epoch 001:   7690 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=111, bsz=40, num_updates=7680, lr=4.86241e-05, gnorm=0.351, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=35202
2023-02-17 01:33:32 - progress_bar.py[line:274] - INFO: epoch 001:   7700 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.92, wpb=111.2, bsz=40, num_updates=7690, lr=4.86196e-05, gnorm=0.507, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35213
2023-02-17 01:33:44 - progress_bar.py[line:274] - INFO: epoch 001:   7710 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.9, wpb=113.4, bsz=40, num_updates=7700, lr=4.86151e-05, gnorm=0.465, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=35225
2023-02-17 01:33:55 - progress_bar.py[line:274] - INFO: epoch 001:   7720 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=111.9, bsz=40, num_updates=7710, lr=4.86105e-05, gnorm=0.386, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35236
2023-02-17 01:34:05 - progress_bar.py[line:274] - INFO: epoch 001:   7730 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.9, ups=0.93, wpb=112.4, bsz=40, num_updates=7720, lr=4.8606e-05, gnorm=0.481, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35246
2023-02-17 01:34:16 - progress_bar.py[line:274] - INFO: epoch 001:   7740 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.9, wpb=113.1, bsz=40, num_updates=7730, lr=4.86015e-05, gnorm=0.456, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35257
2023-02-17 01:34:28 - progress_bar.py[line:274] - INFO: epoch 001:   7750 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=113.5, bsz=40, num_updates=7740, lr=4.8597e-05, gnorm=0.315, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35269
2023-02-17 01:34:39 - progress_bar.py[line:274] - INFO: epoch 001:   7760 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.88, wpb=112.9, bsz=40, num_updates=7750, lr=4.85925e-05, gnorm=0.423, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=35280
2023-02-17 01:34:50 - progress_bar.py[line:274] - INFO: epoch 001:   7770 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.91, wpb=111.8, bsz=40, num_updates=7760, lr=4.8588e-05, gnorm=0.436, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=35291
2023-02-17 01:35:01 - progress_bar.py[line:274] - INFO: epoch 001:   7780 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.88, wpb=112.6, bsz=40, num_updates=7770, lr=4.85835e-05, gnorm=0.396, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35302
2023-02-17 01:35:12 - progress_bar.py[line:274] - INFO: epoch 001:   7790 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.9, wpb=111.2, bsz=40, num_updates=7780, lr=4.8579e-05, gnorm=0.428, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35313
2023-02-17 01:35:23 - progress_bar.py[line:274] - INFO: epoch 001:   7800 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.9, wpb=113, bsz=40, num_updates=7790, lr=4.85745e-05, gnorm=0.307, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=35324
2023-02-17 01:35:35 - progress_bar.py[line:274] - INFO: epoch 001:   7810 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.9, bsz=40, num_updates=7800, lr=4.857e-05, gnorm=0.467, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35336
2023-02-17 01:35:46 - progress_bar.py[line:274] - INFO: epoch 001:   7820 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.9, ups=0.92, wpb=112.9, bsz=40, num_updates=7810, lr=4.85655e-05, gnorm=0.392, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35347
2023-02-17 01:35:57 - progress_bar.py[line:274] - INFO: epoch 001:   7830 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.9, wpb=111.5, bsz=40, num_updates=7820, lr=4.8561e-05, gnorm=0.483, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35358
2023-02-17 01:36:08 - progress_bar.py[line:274] - INFO: epoch 001:   7840 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.89, wpb=112.4, bsz=40, num_updates=7830, lr=4.85565e-05, gnorm=0.361, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35369
2023-02-17 01:36:19 - progress_bar.py[line:274] - INFO: epoch 001:   7850 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.89, wpb=113, bsz=40, num_updates=7840, lr=4.8552e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35380
2023-02-17 01:36:31 - progress_bar.py[line:274] - INFO: epoch 001:   7860 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.2, ups=0.87, wpb=110.7, bsz=40, num_updates=7850, lr=4.85475e-05, gnorm=0.521, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=35392
2023-02-17 01:36:42 - progress_bar.py[line:274] - INFO: epoch 001:   7870 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.88, wpb=112.7, bsz=40, num_updates=7860, lr=4.8543e-05, gnorm=0.419, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35403
2023-02-17 01:36:53 - progress_bar.py[line:274] - INFO: epoch 001:   7880 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.9, ups=0.88, wpb=110.9, bsz=40, num_updates=7870, lr=4.85385e-05, gnorm=0.485, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35414
2023-02-17 01:37:04 - progress_bar.py[line:274] - INFO: epoch 001:   7890 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.2, ups=0.94, wpb=111.1, bsz=40, num_updates=7880, lr=4.8534e-05, gnorm=0.349, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=35425
2023-02-17 01:37:15 - progress_bar.py[line:274] - INFO: epoch 001:   7900 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.4, ups=0.89, wpb=112.2, bsz=40, num_updates=7890, lr=4.85295e-05, gnorm=0.559, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35436
2023-02-17 01:37:26 - progress_bar.py[line:274] - INFO: epoch 001:   7910 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.92, wpb=111.6, bsz=40, num_updates=7900, lr=4.8525e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=35447
2023-02-17 01:37:37 - progress_bar.py[line:274] - INFO: epoch 001:   7920 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.1, ups=0.93, wpb=110.5, bsz=40, num_updates=7910, lr=4.85205e-05, gnorm=0.559, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35458
2023-02-17 01:37:48 - progress_bar.py[line:274] - INFO: epoch 001:   7930 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.91, wpb=111.5, bsz=40, num_updates=7920, lr=4.8516e-05, gnorm=0.419, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35469
2023-02-17 01:37:59 - progress_bar.py[line:274] - INFO: epoch 001:   7940 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.88, wpb=111.9, bsz=40, num_updates=7930, lr=4.85115e-05, gnorm=0.352, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=35480
2023-02-17 01:38:11 - progress_bar.py[line:274] - INFO: epoch 001:   7950 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.88, wpb=110.6, bsz=40, num_updates=7940, lr=4.8507e-05, gnorm=0.418, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35491
2023-02-17 01:38:22 - progress_bar.py[line:274] - INFO: epoch 001:   7960 / 11564 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.88, wpb=111.3, bsz=40, num_updates=7950, lr=4.85025e-05, gnorm=0.423, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=35503
2023-02-17 01:38:33 - progress_bar.py[line:274] - INFO: epoch 001:   7970 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.89, wpb=112.6, bsz=40, num_updates=7960, lr=4.8498e-05, gnorm=0.486, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35514
2023-02-17 01:38:44 - progress_bar.py[line:274] - INFO: epoch 001:   7980 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=111.9, bsz=40, num_updates=7970, lr=4.84934e-05, gnorm=0.365, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=35525
2023-02-17 01:38:56 - progress_bar.py[line:274] - INFO: epoch 001:   7990 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=111.4, bsz=40, num_updates=7980, lr=4.84889e-05, gnorm=0.452, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=35536
2023-02-17 01:39:06 - progress_bar.py[line:274] - INFO: epoch 001:   8000 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.93, wpb=110.4, bsz=40, num_updates=7990, lr=4.84844e-05, gnorm=0.509, clip=0, loss_scale=1024, train_wall=11, gb_free=11.5, ema_decay=0.9999, wall=35547
2023-02-17 01:39:17 - progress_bar.py[line:274] - INFO: epoch 001:   8010 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112.9, bsz=40, num_updates=8000, lr=4.84799e-05, gnorm=0.406, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=35558
2023-02-17 01:39:17 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 01:39:19 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 01:39:19 - train.py[line:551] - INFO: load:0.88 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 01:41:21 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 01:41:21 - train.py[line:551] - INFO: load:0.91 valid_run:121.98 task_valid:119.11 collect_output:1.85
2023-02-17 01:43:20 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 01:43:20 - train.py[line:551] - INFO: load:0.93 valid_run:241.62 task_valid:234.67 collect_output:4.91
2023-02-17 01:45:22 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 01:45:22 - train.py[line:551] - INFO: load:0.96 valid_run:363.62 task_valid:351.08 collect_output:9.47
2023-02-17 01:47:24 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 01:47:24 - train.py[line:551] - INFO: load:0.99 valid_run:485.32 task_valid:464.41 collect_output:16.80
2023-02-17 01:49:25 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 01:49:25 - train.py[line:551] - INFO: load:1.01 valid_run:605.63 task_valid:581.53 collect_output:18.96
2023-02-17 01:51:27 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 01:51:27 - train.py[line:551] - INFO: load:1.04 valid_run:728.31 task_valid:700.06 collect_output:22.12
2023-02-17 01:53:30 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 01:53:30 - train.py[line:551] - INFO: load:1.06 valid_run:851.18 task_valid:818.02 collect_output:26.05
2023-02-17 01:55:32 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 01:55:32 - train.py[line:551] - INFO: load:1.09 valid_run:972.93 task_valid:934.45 collect_output:30.38
2023-02-17 01:57:36 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 01:57:36 - train.py[line:551] - INFO: load:1.11 valid_run:1096.57 task_valid:1051.45 collect_output:36.02
2023-02-17 01:59:37 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 01:59:37 - train.py[line:551] - INFO: load:1.14 valid_run:1218.26 task_valid:1164.01 collect_output:44.16
2023-02-17 02:01:38 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 02:01:38 - train.py[line:551] - INFO: load:1.16 valid_run:1338.39 task_valid:1279.47 collect_output:47.80
2023-02-17 02:03:39 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 02:03:39 - train.py[line:551] - INFO: load:1.19 valid_run:1459.85 task_valid:1396.16 collect_output:51.58
2023-02-17 02:05:38 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 02:05:38 - train.py[line:551] - INFO: load:1.21 valid_run:1578.62 task_valid:1509.75 collect_output:55.75
2023-02-17 02:07:39 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 02:07:39 - train.py[line:551] - INFO: load:1.24 valid_run:1699.67 task_valid:1627.44 collect_output:58.10
2023-02-17 02:09:40 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 02:09:40 - train.py[line:551] - INFO: load:1.27 valid_run:1820.56 task_valid:1743.47 collect_output:61.95
2023-02-17 02:11:41 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 02:11:41 - train.py[line:551] - INFO: load:1.29 valid_run:1941.59 task_valid:1857.34 collect_output:68.12
2023-02-17 02:13:42 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 02:13:42 - train.py[line:551] - INFO: load:1.32 valid_run:2062.82 task_valid:1973.28 collect_output:72.41
2023-02-17 02:15:43 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 02:15:43 - train.py[line:551] - INFO: load:1.34 valid_run:2183.26 task_valid:2090.95 collect_output:74.18
2023-02-17 02:17:44 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 02:17:44 - train.py[line:551] - INFO: load:1.37 valid_run:2304.29 task_valid:2207.65 collect_output:77.51
2023-02-17 02:19:44 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 02:19:44 - train.py[line:551] - INFO: load:1.39 valid_run:2424.57 task_valid:2324.08 collect_output:80.35
2023-02-17 02:21:46 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 02:21:46 - train.py[line:551] - INFO: load:1.42 valid_run:2546.10 task_valid:2440.38 collect_output:84.56
2023-02-17 02:23:48 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 02:23:48 - train.py[line:551] - INFO: load:1.44 valid_run:2667.86 task_valid:2558.98 collect_output:86.69
2023-02-17 02:25:48 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 02:25:48 - train.py[line:551] - INFO: load:1.47 valid_run:2788.01 task_valid:2673.10 collect_output:91.71
2023-02-17 02:27:48 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 02:27:48 - train.py[line:551] - INFO: load:1.49 valid_run:2907.66 task_valid:2789.07 collect_output:94.39
2023-02-17 02:29:49 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 02:29:49 - train.py[line:551] - INFO: load:1.52 valid_run:3029.19 task_valid:2905.13 collect_output:98.87
2023-02-17 02:31:52 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 02:31:52 - train.py[line:551] - INFO: load:1.55 valid_run:3151.96 task_valid:3020.86 collect_output:104.89
2023-02-17 02:33:52 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 02:33:52 - train.py[line:551] - INFO: load:1.57 valid_run:3271.46 task_valid:3134.79 collect_output:109.44
2023-02-17 02:35:53 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 02:35:53 - train.py[line:551] - INFO: load:1.60 valid_run:3393.16 task_valid:3253.98 collect_output:110.97
2023-02-17 02:37:55 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 02:37:55 - train.py[line:551] - INFO: load:1.62 valid_run:3514.87 task_valid:3369.45 collect_output:116.19
2023-02-17 02:39:57 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 02:39:57 - train.py[line:551] - INFO: load:1.65 valid_run:3636.54 task_valid:3487.65 collect_output:118.68
2023-02-17 02:41:58 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 02:41:58 - train.py[line:551] - INFO: load:1.67 valid_run:3757.39 task_valid:3605.83 collect_output:120.36

====================================================================================================
SGG eval:     R @ 50: 0.6557;     R @ 100: 0.6892;     R @ 500: 0.7152;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4454;    mR @ 100: 0.4790;    mR @ 500: 0.5373;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.5625) (covering:0.3714) (eating:0.7647) (flying in:0.6818) (growing on:0.5000) (hanging from:0.5806) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9663) (says:0.0000) (sitting on:0.7409) (standing on:0.3793) (using:0.6000) (walking in:0.0000) (walking on:0.7432) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6557;     R @ 100: 0.6892;     R @ 500: 0.7152;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4454;    mR @ 100: 0.4790;    mR @ 500: 0.5373;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.5625) (covering:0.3714) (eating:0.7647) (flying in:0.6818) (growing on:0.5000) (hanging from:0.5806) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9663) (says:0.0000) (sitting on:0.7409) (standing on:0.3793) (using:0.6000) (walking in:0.0000) (walking on:0.7432) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-02-17 02:42:28 - train.py[line:487] - INFO: 0.6891564043799338
2023-02-17 02:42:28 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 02:42:28 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.259 | loss_v1 0 | loss_v2 0 | nll_loss 0.096 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.689156 | ppl 1.07 | vqa_score 0.5349 | wps 118.4 | wpb 72 | bsz 24 | num_updates 8000 | best_R@100 0.69202
2023-02-17 02:42:28 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 8000 updates
2023-02-17 02:42:28 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_8000.pt
2023-02-17 02:42:34 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_8000.pt
2023-02-17 02:42:36 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_8000.pt (epoch 1 @ 8000 updates, score 0.6891564043799338) (writing took 8.033961825072765 seconds)
2023-02-17 02:42:48 - progress_bar.py[line:274] - INFO: epoch 001:   8020 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=112.9, bsz=40, num_updates=8010, lr=4.84754e-05, gnorm=0.419, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39368
2023-02-17 02:42:59 - progress_bar.py[line:274] - INFO: epoch 001:   8030 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=112.7, bsz=40, num_updates=8020, lr=4.84709e-05, gnorm=0.481, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39380
2023-02-17 02:43:10 - progress_bar.py[line:274] - INFO: epoch 001:   8040 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=112, bsz=40, num_updates=8030, lr=4.84664e-05, gnorm=0.327, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39391
2023-02-17 02:43:21 - progress_bar.py[line:274] - INFO: epoch 001:   8050 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.9, wpb=112.4, bsz=40, num_updates=8040, lr=4.84619e-05, gnorm=0.369, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39402
2023-02-17 02:43:32 - progress_bar.py[line:274] - INFO: epoch 001:   8060 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.88, wpb=113, bsz=40, num_updates=8050, lr=4.84574e-05, gnorm=0.436, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39413
2023-02-17 02:43:44 - progress_bar.py[line:274] - INFO: epoch 001:   8070 / 11564 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114.9, nsentences=40, sample_size=114.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.88, wpb=114.9, bsz=40, num_updates=8060, lr=4.84529e-05, gnorm=0.296, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39425
2023-02-17 02:43:55 - progress_bar.py[line:274] - INFO: epoch 001:   8080 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=113, bsz=40, num_updates=8070, lr=4.84484e-05, gnorm=0.445, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39436
2023-02-17 02:44:06 - progress_bar.py[line:274] - INFO: epoch 001:   8090 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.92, wpb=111.3, bsz=40, num_updates=8080, lr=4.84439e-05, gnorm=0.462, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39447
2023-02-17 02:44:17 - progress_bar.py[line:274] - INFO: epoch 001:   8100 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.92, wpb=112.3, bsz=40, num_updates=8090, lr=4.84394e-05, gnorm=0.446, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39458
2023-02-17 02:44:22 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-17 02:44:29 - progress_bar.py[line:274] - INFO: epoch 001:   8111 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=90.4, ups=0.81, wpb=111.8, bsz=40, num_updates=8100, lr=4.84349e-05, gnorm=0.468, clip=0, loss_scale=1024, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=39470
2023-02-17 02:44:41 - progress_bar.py[line:274] - INFO: epoch 001:   8121 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.87, wpb=111.2, bsz=40, num_updates=8110, lr=4.84304e-05, gnorm=0.418, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39482
2023-02-17 02:44:52 - progress_bar.py[line:274] - INFO: epoch 001:   8131 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.5, ups=0.87, wpb=110.7, bsz=40, num_updates=8120, lr=4.84259e-05, gnorm=0.406, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39493
2023-02-17 02:45:03 - progress_bar.py[line:274] - INFO: epoch 001:   8141 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.89, wpb=113.5, bsz=40, num_updates=8130, lr=4.84214e-05, gnorm=0.424, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39504
2023-02-17 02:45:14 - progress_bar.py[line:274] - INFO: epoch 001:   8151 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.91, wpb=111.2, bsz=40, num_updates=8140, lr=4.84169e-05, gnorm=0.381, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39515
2023-02-17 02:45:25 - progress_bar.py[line:274] - INFO: epoch 001:   8161 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.91, wpb=112.4, bsz=40, num_updates=8150, lr=4.84124e-05, gnorm=0.366, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39526
2023-02-17 02:45:36 - progress_bar.py[line:274] - INFO: epoch 001:   8171 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=111.5, bsz=40, num_updates=8160, lr=4.84079e-05, gnorm=0.36, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=39537
2023-02-17 02:45:47 - progress_bar.py[line:274] - INFO: epoch 001:   8181 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.91, wpb=112.2, bsz=40, num_updates=8170, lr=4.84034e-05, gnorm=0.573, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39548
2023-02-17 02:45:59 - progress_bar.py[line:274] - INFO: epoch 001:   8191 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.88, wpb=112.6, bsz=40, num_updates=8180, lr=4.83989e-05, gnorm=0.507, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39560
2023-02-17 02:46:10 - progress_bar.py[line:274] - INFO: epoch 001:   8201 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=112.5, bsz=40, num_updates=8190, lr=4.83944e-05, gnorm=0.444, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=39571
2023-02-17 02:46:21 - progress_bar.py[line:274] - INFO: epoch 001:   8211 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.9, wpb=112.6, bsz=40, num_updates=8200, lr=4.83899e-05, gnorm=0.433, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39582
2023-02-17 02:46:32 - progress_bar.py[line:274] - INFO: epoch 001:   8221 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.9, wpb=112.4, bsz=40, num_updates=8210, lr=4.83854e-05, gnorm=0.466, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=39593
2023-02-17 02:46:43 - progress_bar.py[line:274] - INFO: epoch 001:   8231 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.91, wpb=111.2, bsz=40, num_updates=8220, lr=4.83808e-05, gnorm=0.402, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39604
2023-02-17 02:46:55 - progress_bar.py[line:274] - INFO: epoch 001:   8241 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.89, wpb=112.1, bsz=40, num_updates=8230, lr=4.83763e-05, gnorm=0.48, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39616
2023-02-17 02:47:06 - progress_bar.py[line:274] - INFO: epoch 001:   8251 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=8240, lr=4.83718e-05, gnorm=0.386, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39627
2023-02-17 02:47:17 - progress_bar.py[line:274] - INFO: epoch 001:   8261 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=111.5, bsz=40, num_updates=8250, lr=4.83673e-05, gnorm=0.397, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39638
2023-02-17 02:47:28 - progress_bar.py[line:274] - INFO: epoch 001:   8271 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.91, wpb=112.7, bsz=40, num_updates=8260, lr=4.83628e-05, gnorm=0.483, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39649
2023-02-17 02:47:39 - progress_bar.py[line:274] - INFO: epoch 001:   8281 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.89, wpb=112.3, bsz=40, num_updates=8270, lr=4.83583e-05, gnorm=0.362, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39660
2023-02-17 02:47:51 - progress_bar.py[line:274] - INFO: epoch 001:   8291 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=112, bsz=40, num_updates=8280, lr=4.83538e-05, gnorm=0.466, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39672
2023-02-17 02:48:01 - progress_bar.py[line:274] - INFO: epoch 001:   8301 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=106.8, ups=0.95, wpb=112.4, bsz=40, num_updates=8290, lr=4.83493e-05, gnorm=0.39, clip=0, loss_scale=1024, train_wall=10, gb_free=10.5, ema_decay=0.9999, wall=39682
2023-02-17 02:48:12 - progress_bar.py[line:274] - INFO: epoch 001:   8311 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=111.7, bsz=40, num_updates=8300, lr=4.83448e-05, gnorm=0.471, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39693
2023-02-17 02:48:23 - progress_bar.py[line:274] - INFO: epoch 001:   8321 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=8310, lr=4.83403e-05, gnorm=0.438, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39704
2023-02-17 02:48:34 - progress_bar.py[line:274] - INFO: epoch 001:   8331 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=111.9, bsz=40, num_updates=8320, lr=4.83358e-05, gnorm=0.36, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39715
2023-02-17 02:48:45 - progress_bar.py[line:274] - INFO: epoch 001:   8341 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.91, wpb=111.2, bsz=40, num_updates=8330, lr=4.83313e-05, gnorm=0.308, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39726
2023-02-17 02:48:56 - progress_bar.py[line:274] - INFO: epoch 001:   8351 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=112.7, bsz=40, num_updates=8340, lr=4.83268e-05, gnorm=0.405, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39737
2023-02-17 02:49:08 - progress_bar.py[line:274] - INFO: epoch 001:   8361 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.88, wpb=112.5, bsz=40, num_updates=8350, lr=4.83223e-05, gnorm=0.558, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39748
2023-02-17 02:49:18 - progress_bar.py[line:274] - INFO: epoch 001:   8371 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.92, wpb=112.5, bsz=40, num_updates=8360, lr=4.83178e-05, gnorm=0.308, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39759
2023-02-17 02:49:29 - progress_bar.py[line:274] - INFO: epoch 001:   8381 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.92, wpb=111.6, bsz=40, num_updates=8370, lr=4.83133e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39770
2023-02-17 02:49:40 - progress_bar.py[line:274] - INFO: epoch 001:   8391 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.9, wpb=110.6, bsz=40, num_updates=8380, lr=4.83088e-05, gnorm=0.445, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39781
2023-02-17 02:49:52 - progress_bar.py[line:274] - INFO: epoch 001:   8401 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.88, wpb=113.5, bsz=40, num_updates=8390, lr=4.83043e-05, gnorm=0.363, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39793
2023-02-17 02:50:03 - progress_bar.py[line:274] - INFO: epoch 001:   8411 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=112.9, bsz=40, num_updates=8400, lr=4.82998e-05, gnorm=0.396, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=39804
2023-02-17 02:50:14 - progress_bar.py[line:274] - INFO: epoch 001:   8421 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.91, wpb=112.5, bsz=40, num_updates=8410, lr=4.82953e-05, gnorm=0.377, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39815
2023-02-17 02:50:25 - progress_bar.py[line:274] - INFO: epoch 001:   8431 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.89, wpb=111.6, bsz=40, num_updates=8420, lr=4.82908e-05, gnorm=0.411, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39826
2023-02-17 02:50:37 - progress_bar.py[line:274] - INFO: epoch 001:   8441 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.1, ups=0.88, wpb=110.9, bsz=40, num_updates=8430, lr=4.82863e-05, gnorm=0.417, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39838
2023-02-17 02:50:48 - progress_bar.py[line:274] - INFO: epoch 001:   8451 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=112.6, bsz=40, num_updates=8440, lr=4.82818e-05, gnorm=0.374, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39849
2023-02-17 02:50:59 - progress_bar.py[line:274] - INFO: epoch 001:   8461 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.88, wpb=111, bsz=40, num_updates=8450, lr=4.82773e-05, gnorm=0.416, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39860
2023-02-17 02:51:10 - progress_bar.py[line:274] - INFO: epoch 001:   8471 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.91, wpb=112.5, bsz=40, num_updates=8460, lr=4.82728e-05, gnorm=0.411, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39871
2023-02-17 02:51:21 - progress_bar.py[line:274] - INFO: epoch 001:   8481 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.92, wpb=112, bsz=40, num_updates=8470, lr=4.82683e-05, gnorm=0.314, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39882
2023-02-17 02:51:32 - progress_bar.py[line:274] - INFO: epoch 001:   8491 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.88, wpb=110.4, bsz=40, num_updates=8480, lr=4.82637e-05, gnorm=0.458, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39893
2023-02-17 02:51:44 - progress_bar.py[line:274] - INFO: epoch 001:   8501 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.89, wpb=112.9, bsz=40, num_updates=8490, lr=4.82592e-05, gnorm=0.447, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39905
2023-02-17 02:51:55 - progress_bar.py[line:274] - INFO: epoch 001:   8511 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=111.3, bsz=40, num_updates=8500, lr=4.82547e-05, gnorm=0.428, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=39916
2023-02-17 02:52:06 - progress_bar.py[line:274] - INFO: epoch 001:   8521 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.9, wpb=112, bsz=40, num_updates=8510, lr=4.82502e-05, gnorm=0.496, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39927
2023-02-17 02:52:17 - progress_bar.py[line:274] - INFO: epoch 001:   8531 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=111.4, bsz=40, num_updates=8520, lr=4.82457e-05, gnorm=0.492, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39938
2023-02-17 02:52:29 - progress_bar.py[line:274] - INFO: epoch 001:   8541 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.88, wpb=112.4, bsz=40, num_updates=8530, lr=4.82412e-05, gnorm=0.58, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39950
2023-02-17 02:52:40 - progress_bar.py[line:274] - INFO: epoch 001:   8551 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.2, ups=0.92, wpb=112.1, bsz=40, num_updates=8540, lr=4.82367e-05, gnorm=0.495, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39961
2023-02-17 02:52:51 - progress_bar.py[line:274] - INFO: epoch 001:   8561 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.91, wpb=111, bsz=40, num_updates=8550, lr=4.82322e-05, gnorm=0.342, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=39972
2023-02-17 02:53:02 - progress_bar.py[line:274] - INFO: epoch 001:   8571 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=111.3, bsz=40, num_updates=8560, lr=4.82277e-05, gnorm=0.559, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39983
2023-02-17 02:53:13 - progress_bar.py[line:274] - INFO: epoch 001:   8581 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112.9, bsz=40, num_updates=8570, lr=4.82232e-05, gnorm=0.39, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39994
2023-02-17 02:53:24 - progress_bar.py[line:274] - INFO: epoch 001:   8591 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=111.5, bsz=40, num_updates=8580, lr=4.82187e-05, gnorm=0.375, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40005
2023-02-17 02:53:35 - progress_bar.py[line:274] - INFO: epoch 001:   8601 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=112.6, bsz=40, num_updates=8590, lr=4.82142e-05, gnorm=0.386, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40016
2023-02-17 02:53:47 - progress_bar.py[line:274] - INFO: epoch 001:   8611 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.88, wpb=111.7, bsz=40, num_updates=8600, lr=4.82097e-05, gnorm=0.494, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40028
2023-02-17 02:53:58 - progress_bar.py[line:274] - INFO: epoch 001:   8621 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.91, wpb=111.6, bsz=40, num_updates=8610, lr=4.82052e-05, gnorm=0.35, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40039
2023-02-17 02:54:09 - progress_bar.py[line:274] - INFO: epoch 001:   8631 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=112, bsz=40, num_updates=8620, lr=4.82007e-05, gnorm=0.335, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40050
2023-02-17 02:54:20 - progress_bar.py[line:274] - INFO: epoch 001:   8641 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.92, wpb=112.5, bsz=40, num_updates=8630, lr=4.81962e-05, gnorm=0.384, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40061
2023-02-17 02:54:31 - progress_bar.py[line:274] - INFO: epoch 001:   8651 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=112.8, bsz=40, num_updates=8640, lr=4.81917e-05, gnorm=0.276, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40072
2023-02-17 02:54:41 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-17 02:54:43 - progress_bar.py[line:274] - INFO: epoch 001:   8662 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=92.3, ups=0.83, wpb=111.2, bsz=40, num_updates=8650, lr=4.81872e-05, gnorm=0.456, clip=10, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=40084
2023-02-17 02:54:54 - progress_bar.py[line:274] - INFO: epoch 001:   8672 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.92, wpb=111.9, bsz=40, num_updates=8660, lr=4.81827e-05, gnorm=0.413, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40095
2023-02-17 02:55:05 - progress_bar.py[line:274] - INFO: epoch 001:   8682 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97, ups=0.87, wpb=111.2, bsz=40, num_updates=8670, lr=4.81782e-05, gnorm=0.46, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=40106
2023-02-17 02:55:16 - progress_bar.py[line:274] - INFO: epoch 001:   8692 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.3, ups=0.92, wpb=112.4, bsz=40, num_updates=8680, lr=4.81737e-05, gnorm=0.798, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40117
2023-02-17 02:55:27 - progress_bar.py[line:274] - INFO: epoch 001:   8702 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=112.9, bsz=40, num_updates=8690, lr=4.81692e-05, gnorm=0.42, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40128
2023-02-17 02:55:39 - progress_bar.py[line:274] - INFO: epoch 001:   8712 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.88, wpb=112.6, bsz=40, num_updates=8700, lr=4.81647e-05, gnorm=0.399, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40140
2023-02-17 02:55:50 - progress_bar.py[line:274] - INFO: epoch 001:   8722 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.87, wpb=112.1, bsz=40, num_updates=8710, lr=4.81602e-05, gnorm=0.458, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40151
2023-02-17 02:56:01 - progress_bar.py[line:274] - INFO: epoch 001:   8732 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.9, wpb=110.8, bsz=40, num_updates=8720, lr=4.81557e-05, gnorm=0.4, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40162
2023-02-17 02:56:12 - progress_bar.py[line:274] - INFO: epoch 001:   8742 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.92, wpb=111.5, bsz=40, num_updates=8730, lr=4.81512e-05, gnorm=0.498, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=40173
2023-02-17 02:56:23 - progress_bar.py[line:274] - INFO: epoch 001:   8752 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.9, wpb=110.6, bsz=40, num_updates=8740, lr=4.81466e-05, gnorm=0.403, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40184
2023-02-17 02:56:35 - progress_bar.py[line:274] - INFO: epoch 001:   8762 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.89, wpb=112.4, bsz=40, num_updates=8750, lr=4.81421e-05, gnorm=0.528, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40196
2023-02-17 02:56:46 - progress_bar.py[line:274] - INFO: epoch 001:   8772 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=112.2, bsz=40, num_updates=8760, lr=4.81376e-05, gnorm=0.371, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40207
2023-02-17 02:56:57 - progress_bar.py[line:274] - INFO: epoch 001:   8782 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.88, wpb=111.4, bsz=40, num_updates=8770, lr=4.81331e-05, gnorm=0.524, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=40218
2023-02-17 02:57:08 - progress_bar.py[line:274] - INFO: epoch 001:   8792 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.88, wpb=112.2, bsz=40, num_updates=8780, lr=4.81286e-05, gnorm=0.374, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=40229
2023-02-17 02:57:19 - progress_bar.py[line:274] - INFO: epoch 001:   8802 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.91, wpb=110.6, bsz=40, num_updates=8790, lr=4.81241e-05, gnorm=0.363, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40240
2023-02-17 02:57:30 - progress_bar.py[line:274] - INFO: epoch 001:   8812 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.9, ups=0.92, wpb=112.8, bsz=40, num_updates=8800, lr=4.81196e-05, gnorm=0.391, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=40251
2023-02-17 02:57:42 - progress_bar.py[line:274] - INFO: epoch 001:   8822 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=113.2, bsz=40, num_updates=8810, lr=4.81151e-05, gnorm=0.316, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40262
2023-02-17 02:57:52 - progress_bar.py[line:274] - INFO: epoch 001:   8832 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.92, wpb=111, bsz=40, num_updates=8820, lr=4.81106e-05, gnorm=0.411, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40273
2023-02-17 02:58:03 - progress_bar.py[line:274] - INFO: epoch 001:   8842 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.91, wpb=111.7, bsz=40, num_updates=8830, lr=4.81061e-05, gnorm=0.424, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=40284
2023-02-17 02:58:14 - progress_bar.py[line:274] - INFO: epoch 001:   8852 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.2, ups=0.92, wpb=111, bsz=40, num_updates=8840, lr=4.81016e-05, gnorm=0.409, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40295
2023-02-17 02:58:25 - progress_bar.py[line:274] - INFO: epoch 001:   8862 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.3, ups=0.91, wpb=113.7, bsz=40, num_updates=8850, lr=4.80971e-05, gnorm=0.381, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40306
2023-02-17 02:58:36 - progress_bar.py[line:274] - INFO: epoch 001:   8872 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.3, ups=0.92, wpb=113.6, bsz=40, num_updates=8860, lr=4.80926e-05, gnorm=0.346, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=40317
2023-02-17 02:58:47 - progress_bar.py[line:274] - INFO: epoch 001:   8882 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=112.6, bsz=40, num_updates=8870, lr=4.80881e-05, gnorm=0.469, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40328
2023-02-17 02:58:58 - progress_bar.py[line:274] - INFO: epoch 001:   8892 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.4, ups=0.93, wpb=112, bsz=40, num_updates=8880, lr=4.80836e-05, gnorm=0.456, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40339
2023-02-17 02:59:09 - progress_bar.py[line:274] - INFO: epoch 001:   8902 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112.8, bsz=40, num_updates=8890, lr=4.80791e-05, gnorm=0.468, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40350
2023-02-17 02:59:20 - progress_bar.py[line:274] - INFO: epoch 001:   8912 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.92, wpb=110.9, bsz=40, num_updates=8900, lr=4.80746e-05, gnorm=0.377, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40361
2023-02-17 02:59:31 - progress_bar.py[line:274] - INFO: epoch 001:   8922 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.9, wpb=110.4, bsz=40, num_updates=8910, lr=4.80701e-05, gnorm=0.399, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40372
2023-02-17 02:59:42 - progress_bar.py[line:274] - INFO: epoch 001:   8932 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.92, wpb=110.9, bsz=40, num_updates=8920, lr=4.80656e-05, gnorm=0.442, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40383
2023-02-17 02:59:53 - progress_bar.py[line:274] - INFO: epoch 001:   8942 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.91, wpb=110.6, bsz=40, num_updates=8930, lr=4.80611e-05, gnorm=0.426, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40394
2023-02-17 03:00:04 - progress_bar.py[line:274] - INFO: epoch 001:   8952 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=111.9, bsz=40, num_updates=8940, lr=4.80566e-05, gnorm=0.339, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40405
2023-02-17 03:00:15 - progress_bar.py[line:274] - INFO: epoch 001:   8962 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.9, wpb=110.7, bsz=40, num_updates=8950, lr=4.80521e-05, gnorm=0.356, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40416
2023-02-17 03:00:26 - progress_bar.py[line:274] - INFO: epoch 001:   8972 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.91, wpb=111.4, bsz=40, num_updates=8960, lr=4.80476e-05, gnorm=0.46, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40427
2023-02-17 03:00:37 - progress_bar.py[line:274] - INFO: epoch 001:   8982 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.8, ups=0.92, wpb=111.8, bsz=40, num_updates=8970, lr=4.80431e-05, gnorm=0.462, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40438
2023-02-17 03:00:49 - progress_bar.py[line:274] - INFO: epoch 001:   8992 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.1, ups=0.87, wpb=111.1, bsz=40, num_updates=8980, lr=4.80386e-05, gnorm=0.345, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40450
2023-02-17 03:01:00 - progress_bar.py[line:274] - INFO: epoch 001:   9002 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.1, bsz=40, num_updates=8990, lr=4.8034e-05, gnorm=0.313, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=40461
2023-02-17 03:01:11 - progress_bar.py[line:274] - INFO: epoch 001:   9012 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.92, wpb=113.1, bsz=40, num_updates=9000, lr=4.80295e-05, gnorm=0.335, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=40472
2023-02-17 03:01:11 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 03:01:12 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 03:01:12 - train.py[line:551] - INFO: load:0.91 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 03:03:14 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 03:03:14 - train.py[line:551] - INFO: load:0.94 valid_run:122.15 task_valid:119.00 collect_output:2.08
2023-02-17 03:05:14 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 03:05:14 - train.py[line:551] - INFO: load:0.96 valid_run:242.07 task_valid:234.70 collect_output:5.27
2023-02-17 03:07:16 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 03:07:16 - train.py[line:551] - INFO: load:0.98 valid_run:363.82 task_valid:351.05 collect_output:9.67
2023-02-17 03:09:18 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 03:09:18 - train.py[line:551] - INFO: load:1.01 valid_run:485.57 task_valid:464.56 collect_output:16.90
2023-02-17 03:11:18 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 03:11:18 - train.py[line:551] - INFO: load:1.03 valid_run:605.87 task_valid:581.75 collect_output:19.01
2023-02-17 03:13:21 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 03:13:21 - train.py[line:551] - INFO: load:1.05 valid_run:728.58 task_valid:700.28 collect_output:22.19
2023-02-17 03:15:24 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 03:15:24 - train.py[line:551] - INFO: load:1.08 valid_run:851.31 task_valid:818.13 collect_output:26.09
2023-02-17 03:17:25 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 03:17:25 - train.py[line:551] - INFO: load:1.10 valid_run:972.92 task_valid:934.45 collect_output:30.38
2023-02-17 03:19:29 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 03:19:29 - train.py[line:551] - INFO: load:1.13 valid_run:1096.49 task_valid:1051.47 collect_output:35.94
2023-02-17 03:21:31 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 03:21:31 - train.py[line:551] - INFO: load:1.15 valid_run:1218.09 task_valid:1163.90 collect_output:44.10
2023-02-17 03:23:31 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 03:23:31 - train.py[line:551] - INFO: load:1.17 valid_run:1337.99 task_valid:1279.29 collect_output:47.61
2023-02-17 03:25:32 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 03:25:32 - train.py[line:551] - INFO: load:1.20 valid_run:1459.53 task_valid:1396.13 collect_output:51.32
2023-02-17 03:27:31 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 03:27:31 - train.py[line:551] - INFO: load:1.22 valid_run:1578.33 task_valid:1509.78 collect_output:55.48
2023-02-17 03:29:32 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 03:29:32 - train.py[line:551] - INFO: load:1.25 valid_run:1699.16 task_valid:1627.35 collect_output:57.74
2023-02-17 03:31:33 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 03:31:33 - train.py[line:551] - INFO: load:1.27 valid_run:1819.83 task_valid:1743.18 collect_output:61.57
2023-02-17 03:33:34 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 03:33:34 - train.py[line:551] - INFO: load:1.29 valid_run:1940.68 task_valid:1856.90 collect_output:67.69
2023-02-17 03:35:35 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 03:35:35 - train.py[line:551] - INFO: load:1.32 valid_run:2061.86 task_valid:1972.78 collect_output:72.00
2023-02-17 03:37:35 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 03:37:35 - train.py[line:551] - INFO: load:1.34 valid_run:2182.39 task_valid:2090.52 collect_output:73.81
2023-02-17 03:39:37 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 03:39:37 - train.py[line:551] - INFO: load:1.37 valid_run:2303.47 task_valid:2207.23 collect_output:77.17
2023-02-17 03:41:37 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 03:41:37 - train.py[line:551] - INFO: load:1.39 valid_run:2423.75 task_valid:2323.67 collect_output:80.00
2023-02-17 03:43:38 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 03:43:38 - train.py[line:551] - INFO: load:1.42 valid_run:2545.23 task_valid:2439.98 collect_output:84.19
2023-02-17 03:45:40 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 03:45:40 - train.py[line:551] - INFO: load:1.44 valid_run:2666.95 task_valid:2558.60 collect_output:86.29
2023-02-17 03:47:40 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 03:47:40 - train.py[line:551] - INFO: load:1.46 valid_run:2787.10 task_valid:2672.76 collect_output:91.29
2023-02-17 03:49:40 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 03:49:40 - train.py[line:551] - INFO: load:1.49 valid_run:2906.86 task_valid:2788.79 collect_output:94.02
2023-02-17 03:51:42 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 03:51:42 - train.py[line:551] - INFO: load:1.51 valid_run:3028.35 task_valid:2904.82 collect_output:98.49
2023-02-17 03:53:45 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 03:53:45 - train.py[line:551] - INFO: load:1.54 valid_run:3151.03 task_valid:3020.53 collect_output:104.48
2023-02-17 03:55:44 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 03:55:44 - train.py[line:551] - INFO: load:1.56 valid_run:3270.51 task_valid:3134.35 collect_output:109.13
2023-02-17 03:57:46 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 03:57:46 - train.py[line:551] - INFO: load:1.59 valid_run:3392.11 task_valid:3253.38 collect_output:110.70
2023-02-17 03:59:47 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 03:59:47 - train.py[line:551] - INFO: load:1.61 valid_run:3513.75 task_valid:3368.65 collect_output:116.08
2023-02-17 04:01:49 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 04:01:49 - train.py[line:551] - INFO: load:1.63 valid_run:3635.45 task_valid:3486.94 collect_output:118.50
2023-02-17 04:03:50 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 04:03:50 - train.py[line:551] - INFO: load:1.66 valid_run:3756.49 task_valid:3605.33 collect_output:120.17

====================================================================================================
SGG eval:     R @ 50: 0.6487;     R @ 100: 0.6880;     R @ 500: 0.7129;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4440;    mR @ 100: 0.4831;    mR @ 500: 0.5362;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6875) (covering:0.3714) (eating:0.8235) (flying in:0.6364) (growing on:0.5000) (hanging from:0.5484) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9663) (says:0.0000) (sitting on:0.7375) (standing on:0.3843) (using:0.6000) (walking in:0.0000) (walking on:0.7162) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6487;     R @ 100: 0.6880;     R @ 500: 0.7129;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4440;    mR @ 100: 0.4831;    mR @ 500: 0.5362;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6875) (covering:0.3714) (eating:0.8235) (flying in:0.6364) (growing on:0.5000) (hanging from:0.5484) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9663) (says:0.0000) (sitting on:0.7375) (standing on:0.3843) (using:0.6000) (walking in:0.0000) (walking on:0.7162) (watching:0.4306) 
--------------------------------------------------------
====================================================================================================

2023-02-17 04:04:21 - train.py[line:487] - INFO: 0.6879745861981155
2023-02-17 04:04:21 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 04:04:21 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.266 | loss_v1 0 | loss_v2 0 | nll_loss 0.101 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.687975 | ppl 1.07 | vqa_score 0.545 | wps 118.4 | wpb 72 | bsz 24 | num_updates 9000 | best_R@100 0.69202
2023-02-17 04:04:21 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 9000 updates
2023-02-17 04:04:21 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_9000.pt
2023-02-17 04:04:26 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_9000.pt
2023-02-17 04:04:29 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_9000.pt (epoch 1 @ 9000 updates, score 0.6879745861981155) (writing took 7.836298970505595 seconds)
2023-02-17 04:04:40 - progress_bar.py[line:274] - INFO: epoch 001:   9022 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=112.2, bsz=40, num_updates=9010, lr=4.8025e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44281
2023-02-17 04:04:51 - progress_bar.py[line:274] - INFO: epoch 001:   9032 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=112.4, bsz=40, num_updates=9020, lr=4.80205e-05, gnorm=0.408, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44292
2023-02-17 04:05:02 - progress_bar.py[line:274] - INFO: epoch 001:   9042 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.91, wpb=110.3, bsz=40, num_updates=9030, lr=4.8016e-05, gnorm=0.375, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44303
2023-02-17 04:05:13 - progress_bar.py[line:274] - INFO: epoch 001:   9052 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=112, bsz=40, num_updates=9040, lr=4.80115e-05, gnorm=0.403, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44314
2023-02-17 04:05:25 - progress_bar.py[line:274] - INFO: epoch 001:   9062 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=112.5, bsz=40, num_updates=9050, lr=4.8007e-05, gnorm=0.356, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44325
2023-02-17 04:05:36 - progress_bar.py[line:274] - INFO: epoch 001:   9072 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=9060, lr=4.80025e-05, gnorm=0.403, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44337
2023-02-17 04:05:47 - progress_bar.py[line:274] - INFO: epoch 001:   9082 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=112.3, bsz=40, num_updates=9070, lr=4.7998e-05, gnorm=0.378, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44348
2023-02-17 04:05:58 - progress_bar.py[line:274] - INFO: epoch 001:   9092 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.89, wpb=112.1, bsz=40, num_updates=9080, lr=4.79935e-05, gnorm=0.598, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44359
2023-02-17 04:06:09 - progress_bar.py[line:274] - INFO: epoch 001:   9102 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.6, ups=0.92, wpb=111.4, bsz=40, num_updates=9090, lr=4.7989e-05, gnorm=0.537, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44370
2023-02-17 04:06:20 - progress_bar.py[line:274] - INFO: epoch 001:   9112 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=111.9, bsz=40, num_updates=9100, lr=4.79845e-05, gnorm=0.369, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44381
2023-02-17 04:06:31 - progress_bar.py[line:274] - INFO: epoch 001:   9122 / 11564 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=108.3, ups=0.96, wpb=113.3, bsz=40, num_updates=9110, lr=4.798e-05, gnorm=0.328, clip=10, loss_scale=1024, train_wall=10, gb_free=11.1, ema_decay=0.9999, wall=44392
2023-02-17 04:06:42 - progress_bar.py[line:274] - INFO: epoch 001:   9132 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.1, ups=0.87, wpb=111.2, bsz=40, num_updates=9120, lr=4.79755e-05, gnorm=0.423, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44403
2023-02-17 04:06:53 - progress_bar.py[line:274] - INFO: epoch 001:   9142 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.91, wpb=111.3, bsz=40, num_updates=9130, lr=4.7971e-05, gnorm=0.448, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44414
2023-02-17 04:07:04 - progress_bar.py[line:274] - INFO: epoch 001:   9152 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.3, ups=0.92, wpb=112.3, bsz=40, num_updates=9140, lr=4.79665e-05, gnorm=0.446, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44425
2023-02-17 04:07:15 - progress_bar.py[line:274] - INFO: epoch 001:   9162 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.91, wpb=112.6, bsz=40, num_updates=9150, lr=4.7962e-05, gnorm=0.48, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44436
2023-02-17 04:07:27 - progress_bar.py[line:274] - INFO: epoch 001:   9172 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.88, wpb=112.1, bsz=40, num_updates=9160, lr=4.79575e-05, gnorm=0.412, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44448
2023-02-17 04:07:33 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-17 04:07:39 - progress_bar.py[line:274] - INFO: epoch 001:   9183 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=94.5, ups=0.84, wpb=112.4, bsz=40, num_updates=9170, lr=4.7953e-05, gnorm=0.28, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=44460
2023-02-17 04:07:43 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 04:07:51 - progress_bar.py[line:274] - INFO: epoch 001:   9194 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=91.2, ups=0.8, wpb=113.6, bsz=40, num_updates=9180, lr=4.79485e-05, gnorm=0.426, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=44472
2023-02-17 04:08:02 - progress_bar.py[line:274] - INFO: epoch 001:   9204 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=112.2, bsz=40, num_updates=9190, lr=4.7944e-05, gnorm=0.461, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44483
2023-02-17 04:08:13 - progress_bar.py[line:274] - INFO: epoch 001:   9214 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.91, wpb=112.1, bsz=40, num_updates=9200, lr=4.79395e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44494
2023-02-17 04:08:25 - progress_bar.py[line:274] - INFO: epoch 001:   9224 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=111.6, bsz=40, num_updates=9210, lr=4.7935e-05, gnorm=0.333, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44505
2023-02-17 04:08:35 - progress_bar.py[line:274] - INFO: epoch 001:   9234 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103, ups=0.92, wpb=111.8, bsz=40, num_updates=9220, lr=4.79305e-05, gnorm=0.465, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44516
2023-02-17 04:08:47 - progress_bar.py[line:274] - INFO: epoch 001:   9244 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.89, wpb=112.5, bsz=40, num_updates=9230, lr=4.7926e-05, gnorm=0.47, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44528
2023-02-17 04:08:58 - progress_bar.py[line:274] - INFO: epoch 001:   9254 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.91, wpb=111.9, bsz=40, num_updates=9240, lr=4.79215e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44539
2023-02-17 04:09:09 - progress_bar.py[line:274] - INFO: epoch 001:   9264 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.87, wpb=112.4, bsz=40, num_updates=9250, lr=4.79169e-05, gnorm=0.42, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44550
2023-02-17 04:09:20 - progress_bar.py[line:274] - INFO: epoch 001:   9274 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=106.5, ups=0.95, wpb=112.3, bsz=40, num_updates=9260, lr=4.79124e-05, gnorm=0.363, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=44561
2023-02-17 04:09:31 - progress_bar.py[line:274] - INFO: epoch 001:   9284 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.9, wpb=111, bsz=40, num_updates=9270, lr=4.79079e-05, gnorm=0.385, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=44572
2023-02-17 04:09:42 - progress_bar.py[line:274] - INFO: epoch 001:   9294 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.2, ups=0.93, wpb=112.6, bsz=40, num_updates=9280, lr=4.79034e-05, gnorm=0.477, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44583
2023-02-17 04:09:53 - progress_bar.py[line:274] - INFO: epoch 001:   9304 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=112, bsz=40, num_updates=9290, lr=4.78989e-05, gnorm=0.325, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44594
2023-02-17 04:10:04 - progress_bar.py[line:274] - INFO: epoch 001:   9314 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=112.3, bsz=40, num_updates=9300, lr=4.78944e-05, gnorm=0.411, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44605
2023-02-17 04:10:15 - progress_bar.py[line:274] - INFO: epoch 001:   9324 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.9, wpb=111.1, bsz=40, num_updates=9310, lr=4.78899e-05, gnorm=0.516, clip=10, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=44616
2023-02-17 04:10:26 - progress_bar.py[line:274] - INFO: epoch 001:   9334 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.87, wpb=112.7, bsz=40, num_updates=9320, lr=4.78854e-05, gnorm=0.487, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44627
2023-02-17 04:10:38 - progress_bar.py[line:274] - INFO: epoch 001:   9344 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.5, bsz=40, num_updates=9330, lr=4.78809e-05, gnorm=0.393, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44639
2023-02-17 04:10:48 - progress_bar.py[line:274] - INFO: epoch 001:   9354 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.92, wpb=111.9, bsz=40, num_updates=9340, lr=4.78764e-05, gnorm=0.279, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44649
2023-02-17 04:10:59 - progress_bar.py[line:274] - INFO: epoch 001:   9364 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114.2, nsentences=40, sample_size=114.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.5, ups=0.91, wpb=114.2, bsz=40, num_updates=9350, lr=4.78719e-05, gnorm=0.321, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44660
2023-02-17 04:11:10 - progress_bar.py[line:274] - INFO: epoch 001:   9374 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.92, wpb=112.5, bsz=40, num_updates=9360, lr=4.78674e-05, gnorm=0.38, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44671
2023-02-17 04:11:21 - progress_bar.py[line:274] - INFO: epoch 001:   9384 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.9, wpb=110.7, bsz=40, num_updates=9370, lr=4.78629e-05, gnorm=0.474, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44682
2023-02-17 04:11:33 - progress_bar.py[line:274] - INFO: epoch 001:   9394 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.91, wpb=111.4, bsz=40, num_updates=9380, lr=4.78584e-05, gnorm=0.336, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44693
2023-02-17 04:11:44 - progress_bar.py[line:274] - INFO: epoch 001:   9404 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=112.5, bsz=40, num_updates=9390, lr=4.78539e-05, gnorm=0.339, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44704
2023-02-17 04:11:55 - progress_bar.py[line:274] - INFO: epoch 001:   9414 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.9, wpb=112.9, bsz=40, num_updates=9400, lr=4.78494e-05, gnorm=0.286, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44716
2023-02-17 04:12:06 - progress_bar.py[line:274] - INFO: epoch 001:   9424 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.88, wpb=113.2, bsz=40, num_updates=9410, lr=4.78449e-05, gnorm=0.407, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44727
2023-02-17 04:12:17 - progress_bar.py[line:274] - INFO: epoch 001:   9434 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.9, ups=0.93, wpb=111.7, bsz=40, num_updates=9420, lr=4.78404e-05, gnorm=0.411, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44738
2023-02-17 04:12:28 - progress_bar.py[line:274] - INFO: epoch 001:   9444 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.9, bsz=40, num_updates=9430, lr=4.78359e-05, gnorm=0.441, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44749
2023-02-17 04:12:39 - progress_bar.py[line:274] - INFO: epoch 001:   9454 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.94, wpb=110.8, bsz=40, num_updates=9440, lr=4.78314e-05, gnorm=0.347, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44760
2023-02-17 04:12:50 - progress_bar.py[line:274] - INFO: epoch 001:   9464 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.91, wpb=112.4, bsz=40, num_updates=9450, lr=4.78269e-05, gnorm=0.35, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44771
2023-02-17 04:13:01 - progress_bar.py[line:274] - INFO: epoch 001:   9474 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.9, wpb=112.1, bsz=40, num_updates=9460, lr=4.78224e-05, gnorm=0.372, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44782
2023-02-17 04:13:12 - progress_bar.py[line:274] - INFO: epoch 001:   9484 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.93, wpb=111.4, bsz=40, num_updates=9470, lr=4.78179e-05, gnorm=0.374, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44793
2023-02-17 04:13:23 - progress_bar.py[line:274] - INFO: epoch 001:   9494 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.9, wpb=113.1, bsz=40, num_updates=9480, lr=4.78134e-05, gnorm=0.479, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44804
2023-02-17 04:13:34 - progress_bar.py[line:274] - INFO: epoch 001:   9504 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.92, wpb=111.9, bsz=40, num_updates=9490, lr=4.78089e-05, gnorm=0.352, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=44815
2023-02-17 04:13:45 - progress_bar.py[line:274] - INFO: epoch 001:   9514 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.88, wpb=111.8, bsz=40, num_updates=9500, lr=4.78044e-05, gnorm=0.396, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44826
2023-02-17 04:13:57 - progress_bar.py[line:274] - INFO: epoch 001:   9524 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.86, wpb=112.3, bsz=40, num_updates=9510, lr=4.77998e-05, gnorm=0.421, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=44837
2023-02-17 04:14:08 - progress_bar.py[line:274] - INFO: epoch 001:   9534 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.4, ups=0.87, wpb=110.4, bsz=40, num_updates=9520, lr=4.77953e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=44849
2023-02-17 04:14:19 - progress_bar.py[line:274] - INFO: epoch 001:   9544 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.92, wpb=111.7, bsz=40, num_updates=9530, lr=4.77908e-05, gnorm=0.353, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44860
2023-02-17 04:14:30 - progress_bar.py[line:274] - INFO: epoch 001:   9554 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.91, wpb=111.5, bsz=40, num_updates=9540, lr=4.77863e-05, gnorm=0.454, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44871
2023-02-17 04:14:41 - progress_bar.py[line:274] - INFO: epoch 001:   9564 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.87, wpb=113.5, bsz=40, num_updates=9550, lr=4.77818e-05, gnorm=0.407, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44882
2023-02-17 04:14:52 - progress_bar.py[line:274] - INFO: epoch 001:   9574 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.91, wpb=111.8, bsz=40, num_updates=9560, lr=4.77773e-05, gnorm=0.431, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44893
2023-02-17 04:15:04 - progress_bar.py[line:274] - INFO: epoch 001:   9584 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=112.3, bsz=40, num_updates=9570, lr=4.77728e-05, gnorm=0.421, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44905
2023-02-17 04:15:15 - progress_bar.py[line:274] - INFO: epoch 001:   9594 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=111.5, bsz=40, num_updates=9580, lr=4.77683e-05, gnorm=0.666, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44916
2023-02-17 04:15:26 - progress_bar.py[line:274] - INFO: epoch 001:   9604 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.9, bsz=40, num_updates=9590, lr=4.77638e-05, gnorm=0.599, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44927
2023-02-17 04:15:37 - progress_bar.py[line:274] - INFO: epoch 001:   9614 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.91, wpb=113.3, bsz=40, num_updates=9600, lr=4.77593e-05, gnorm=0.358, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44938
2023-02-17 04:15:48 - progress_bar.py[line:274] - INFO: epoch 001:   9624 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=112.5, bsz=40, num_updates=9610, lr=4.77548e-05, gnorm=0.413, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44949
2023-02-17 04:16:00 - progress_bar.py[line:274] - INFO: epoch 001:   9634 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=111.6, bsz=40, num_updates=9620, lr=4.77503e-05, gnorm=0.401, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44961
2023-02-17 04:16:11 - progress_bar.py[line:274] - INFO: epoch 001:   9644 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=112.5, bsz=40, num_updates=9630, lr=4.77458e-05, gnorm=0.37, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44972
2023-02-17 04:16:22 - progress_bar.py[line:274] - INFO: epoch 001:   9654 / 11564 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=114.3, nsentences=40, sample_size=114.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.8, ups=0.9, wpb=114.3, bsz=40, num_updates=9640, lr=4.77413e-05, gnorm=0.345, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44983
2023-02-17 04:16:33 - progress_bar.py[line:274] - INFO: epoch 001:   9664 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.88, wpb=112.4, bsz=40, num_updates=9650, lr=4.77368e-05, gnorm=0.47, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44994
2023-02-17 04:16:45 - progress_bar.py[line:274] - INFO: epoch 001:   9674 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.88, wpb=111.8, bsz=40, num_updates=9660, lr=4.77323e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=45006
2023-02-17 04:16:56 - progress_bar.py[line:274] - INFO: epoch 001:   9684 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=9670, lr=4.77278e-05, gnorm=0.368, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45017
2023-02-17 04:17:07 - progress_bar.py[line:274] - INFO: epoch 001:   9694 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=113.1, bsz=40, num_updates=9680, lr=4.77233e-05, gnorm=0.411, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45028
2023-02-17 04:17:18 - progress_bar.py[line:274] - INFO: epoch 001:   9704 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.91, wpb=112.5, bsz=40, num_updates=9690, lr=4.77188e-05, gnorm=0.347, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45039
2023-02-17 04:17:29 - progress_bar.py[line:274] - INFO: epoch 001:   9714 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.92, wpb=112.3, bsz=40, num_updates=9700, lr=4.77143e-05, gnorm=0.432, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45050
2023-02-17 04:17:40 - progress_bar.py[line:274] - INFO: epoch 001:   9724 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.89, wpb=111.7, bsz=40, num_updates=9710, lr=4.77098e-05, gnorm=0.36, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=45061
2023-02-17 04:17:51 - progress_bar.py[line:274] - INFO: epoch 001:   9734 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=112.3, bsz=40, num_updates=9720, lr=4.77053e-05, gnorm=0.433, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=45072
2023-02-17 04:18:02 - progress_bar.py[line:274] - INFO: epoch 001:   9744 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.9, wpb=110.7, bsz=40, num_updates=9730, lr=4.77008e-05, gnorm=0.389, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45083
2023-02-17 04:18:13 - progress_bar.py[line:274] - INFO: epoch 001:   9754 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.9, wpb=111.3, bsz=40, num_updates=9740, lr=4.76963e-05, gnorm=0.388, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45094
2023-02-17 04:18:24 - progress_bar.py[line:274] - INFO: epoch 001:   9764 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.93, wpb=111.7, bsz=40, num_updates=9750, lr=4.76918e-05, gnorm=0.435, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=45105
2023-02-17 04:18:35 - progress_bar.py[line:274] - INFO: epoch 001:   9774 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111.7, bsz=40, num_updates=9760, lr=4.76872e-05, gnorm=0.445, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45116
2023-02-17 04:18:46 - progress_bar.py[line:274] - INFO: epoch 001:   9784 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.92, wpb=110.3, bsz=40, num_updates=9770, lr=4.76827e-05, gnorm=0.369, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45127
2023-02-17 04:18:57 - progress_bar.py[line:274] - INFO: epoch 001:   9794 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.8, ups=0.92, wpb=112.9, bsz=40, num_updates=9780, lr=4.76782e-05, gnorm=0.445, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45138
2023-02-17 04:19:08 - progress_bar.py[line:274] - INFO: epoch 001:   9804 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.9, wpb=113.2, bsz=40, num_updates=9790, lr=4.76737e-05, gnorm=0.371, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45149
2023-02-17 04:19:20 - progress_bar.py[line:274] - INFO: epoch 001:   9814 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.6, ups=0.88, wpb=110.3, bsz=40, num_updates=9800, lr=4.76692e-05, gnorm=0.361, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=45160
2023-02-17 04:19:30 - progress_bar.py[line:274] - INFO: epoch 001:   9824 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.6, ups=0.92, wpb=113.1, bsz=40, num_updates=9810, lr=4.76647e-05, gnorm=0.436, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45171
2023-02-17 04:19:41 - progress_bar.py[line:274] - INFO: epoch 001:   9834 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.92, wpb=112.2, bsz=40, num_updates=9820, lr=4.76602e-05, gnorm=0.351, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45182
2023-02-17 04:19:52 - progress_bar.py[line:274] - INFO: epoch 001:   9844 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.91, wpb=111.5, bsz=40, num_updates=9830, lr=4.76557e-05, gnorm=0.398, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=45193
2023-02-17 04:20:03 - progress_bar.py[line:274] - INFO: epoch 001:   9854 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=112.1, bsz=40, num_updates=9840, lr=4.76512e-05, gnorm=0.303, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=45204
2023-02-17 04:20:14 - progress_bar.py[line:274] - INFO: epoch 001:   9864 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.92, wpb=112.6, bsz=40, num_updates=9850, lr=4.76467e-05, gnorm=0.414, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45215
2023-02-17 04:20:26 - progress_bar.py[line:274] - INFO: epoch 001:   9874 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=112.1, bsz=40, num_updates=9860, lr=4.76422e-05, gnorm=0.38, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45227
2023-02-17 04:20:36 - progress_bar.py[line:274] - INFO: epoch 001:   9884 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.93, wpb=110.2, bsz=40, num_updates=9870, lr=4.76377e-05, gnorm=0.378, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45237
2023-02-17 04:20:47 - progress_bar.py[line:274] - INFO: epoch 001:   9894 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.91, wpb=113.8, bsz=40, num_updates=9880, lr=4.76332e-05, gnorm=0.35, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45248
2023-02-17 04:20:58 - progress_bar.py[line:274] - INFO: epoch 001:   9904 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.92, wpb=112.7, bsz=40, num_updates=9890, lr=4.76287e-05, gnorm=0.387, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45259
2023-02-17 04:21:09 - progress_bar.py[line:274] - INFO: epoch 001:   9914 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.91, wpb=111.7, bsz=40, num_updates=9900, lr=4.76242e-05, gnorm=0.421, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45270
2023-02-17 04:21:20 - progress_bar.py[line:274] - INFO: epoch 001:   9924 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=108.5, ups=0.96, wpb=112.9, bsz=40, num_updates=9910, lr=4.76197e-05, gnorm=0.431, clip=0, loss_scale=1024, train_wall=10, gb_free=10.8, ema_decay=0.9999, wall=45281
2023-02-17 04:21:31 - progress_bar.py[line:274] - INFO: epoch 001:   9934 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.89, wpb=112.3, bsz=40, num_updates=9920, lr=4.76152e-05, gnorm=0.522, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=45292
2023-02-17 04:21:42 - progress_bar.py[line:274] - INFO: epoch 001:   9944 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.89, wpb=113, bsz=40, num_updates=9930, lr=4.76107e-05, gnorm=0.377, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45303
2023-02-17 04:21:53 - progress_bar.py[line:274] - INFO: epoch 001:   9954 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=111.9, bsz=40, num_updates=9940, lr=4.76062e-05, gnorm=0.423, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45314
2023-02-17 04:22:04 - progress_bar.py[line:274] - INFO: epoch 001:   9964 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.2, ups=0.94, wpb=112.5, bsz=40, num_updates=9950, lr=4.76017e-05, gnorm=0.41, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45325
2023-02-17 04:22:15 - progress_bar.py[line:274] - INFO: epoch 001:   9974 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.89, wpb=111.2, bsz=40, num_updates=9960, lr=4.75972e-05, gnorm=0.395, clip=0, loss_scale=1024, train_wall=11, gb_free=11.4, ema_decay=0.9999, wall=45336
2023-02-17 04:22:26 - progress_bar.py[line:274] - INFO: epoch 001:   9984 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.9, wpb=113.9, bsz=40, num_updates=9970, lr=4.75927e-05, gnorm=0.405, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45347
2023-02-17 04:22:38 - progress_bar.py[line:274] - INFO: epoch 001:   9994 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=112.5, bsz=40, num_updates=9980, lr=4.75882e-05, gnorm=0.34, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45358
2023-02-17 04:22:49 - progress_bar.py[line:274] - INFO: epoch 001:  10004 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.88, wpb=111.3, bsz=40, num_updates=9990, lr=4.75837e-05, gnorm=0.278, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=45370
2023-02-17 04:23:00 - progress_bar.py[line:274] - INFO: epoch 001:  10014 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.8, ups=0.93, wpb=112.6, bsz=40, num_updates=10000, lr=4.75792e-05, gnorm=0.396, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45381
2023-02-17 04:23:00 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 04:23:01 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 04:23:01 - train.py[line:551] - INFO: load:0.92 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 04:25:03 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 04:25:03 - train.py[line:551] - INFO: load:0.95 valid_run:122.15 task_valid:118.92 collect_output:2.16
2023-02-17 04:27:03 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 04:27:03 - train.py[line:551] - INFO: load:0.97 valid_run:242.35 task_valid:234.57 collect_output:5.67
2023-02-17 04:29:05 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 04:29:05 - train.py[line:551] - INFO: load:1.00 valid_run:364.30 task_valid:350.96 collect_output:10.20
2023-02-17 04:31:07 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 04:31:07 - train.py[line:551] - INFO: load:1.02 valid_run:486.25 task_valid:464.47 collect_output:17.62
2023-02-17 04:33:08 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 04:33:08 - train.py[line:551] - INFO: load:1.05 valid_run:606.56 task_valid:581.49 collect_output:19.90
2023-02-17 04:35:11 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 04:35:11 - train.py[line:551] - INFO: load:1.08 valid_run:729.47 task_valid:700.15 collect_output:23.13
2023-02-17 04:37:14 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 04:37:14 - train.py[line:551] - INFO: load:1.10 valid_run:852.37 task_valid:818.00 collect_output:27.16
2023-02-17 04:39:15 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 04:39:15 - train.py[line:551] - INFO: load:1.13 valid_run:974.05 task_valid:934.21 collect_output:31.59
2023-02-17 04:41:19 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 04:41:19 - train.py[line:551] - INFO: load:1.16 valid_run:1097.66 task_valid:1051.23 collect_output:37.16
2023-02-17 04:43:21 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 04:43:21 - train.py[line:551] - INFO: load:1.18 valid_run:1219.25 task_valid:1163.71 collect_output:45.27
2023-02-17 04:45:21 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 04:45:21 - train.py[line:551] - INFO: load:1.21 valid_run:1339.30 task_valid:1279.10 collect_output:48.92
2023-02-17 04:47:22 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 04:47:22 - train.py[line:551] - INFO: load:1.23 valid_run:1460.77 task_valid:1395.98 collect_output:52.50
2023-02-17 04:49:21 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 04:49:21 - train.py[line:551] - INFO: load:1.26 valid_run:1579.65 task_valid:1509.64 collect_output:56.72
2023-02-17 04:51:22 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 04:51:22 - train.py[line:551] - INFO: load:1.29 valid_run:1700.46 task_valid:1627.09 collect_output:59.07
2023-02-17 04:53:23 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 04:53:23 - train.py[line:551] - INFO: load:1.31 valid_run:1821.34 task_valid:1742.92 collect_output:63.04
2023-02-17 04:55:24 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 04:55:24 - train.py[line:551] - INFO: load:1.34 valid_run:1942.26 task_valid:1856.64 collect_output:69.16
2023-02-17 04:57:25 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 04:57:25 - train.py[line:551] - INFO: load:1.37 valid_run:2063.48 task_valid:1972.49 collect_output:73.52
2023-02-17 04:59:26 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 04:59:26 - train.py[line:551] - INFO: load:1.39 valid_run:2183.92 task_valid:2090.15 collect_output:75.31
2023-02-17 05:01:27 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 05:01:27 - train.py[line:551] - INFO: load:1.42 valid_run:2305.07 task_valid:2206.91 collect_output:78.68
2023-02-17 05:03:27 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 05:03:27 - train.py[line:551] - INFO: load:1.45 valid_run:2425.27 task_valid:2323.25 collect_output:81.54
2023-02-17 05:05:29 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 05:05:29 - train.py[line:551] - INFO: load:1.47 valid_run:2546.89 task_valid:2439.68 collect_output:85.71
2023-02-17 05:07:31 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 05:07:31 - train.py[line:551] - INFO: load:1.50 valid_run:2668.62 task_valid:2558.30 collect_output:87.83
2023-02-17 05:09:31 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 05:09:31 - train.py[line:551] - INFO: load:1.53 valid_run:2789.00 task_valid:2672.43 collect_output:93.05
2023-02-17 05:11:31 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 05:11:31 - train.py[line:551] - INFO: load:1.55 valid_run:2908.75 task_valid:2788.44 collect_output:95.76
2023-02-17 05:13:33 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 05:13:33 - train.py[line:551] - INFO: load:1.58 valid_run:3030.52 task_valid:2904.74 collect_output:100.23
2023-02-17 05:15:36 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 05:15:36 - train.py[line:551] - INFO: load:1.60 valid_run:3153.30 task_valid:3020.52 collect_output:106.21
2023-02-17 05:17:35 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 05:17:35 - train.py[line:551] - INFO: load:1.63 valid_run:3272.90 task_valid:3134.54 collect_output:110.78
2023-02-17 05:19:37 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 05:19:37 - train.py[line:551] - INFO: load:1.66 valid_run:3394.65 task_valid:3253.73 collect_output:112.34
2023-02-17 05:21:39 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 05:21:39 - train.py[line:551] - INFO: load:1.68 valid_run:3516.35 task_valid:3369.09 collect_output:117.67
2023-02-17 05:23:41 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 05:23:41 - train.py[line:551] - INFO: load:1.71 valid_run:3638.08 task_valid:3487.33 collect_output:120.15
2023-02-17 05:25:42 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 05:25:42 - train.py[line:551] - INFO: load:1.74 valid_run:3759.08 task_valid:3605.68 collect_output:121.79

====================================================================================================
SGG eval:     R @ 50: 0.6374;     R @ 100: 0.6768;     R @ 500: 0.6992;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4323;    mR @ 100: 0.4688;    mR @ 500: 0.5233;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6875) (covering:0.2286) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.5290) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9663) (says:0.0000) (sitting on:0.7324) (standing on:0.3793) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.4028) 
--------------------------------------------------------
====================================================================================================

2023-02-17 05:26:12 - train.py[line:487] - INFO: 0.6767594346829642
2023-02-17 05:26:12 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 05:26:12 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.258 | loss_v1 0 | loss_v2 0 | nll_loss 0.096 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.676759 | ppl 1.07 | vqa_score 0.5405 | wps 118.3 | wpb 72 | bsz 24 | num_updates 10000 | best_R@100 0.69202
2023-02-17 05:26:12 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 10000 updates
2023-02-17 05:26:12 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_10000.pt

====================================================================================================
SGG eval:     R @ 50: 0.6374;     R @ 100: 0.6768;     R @ 500: 0.6992;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4323;    mR @ 100: 0.4688;    mR @ 500: 0.5233;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6875) (covering:0.2286) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.5290) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.9663) (says:0.0000) (sitting on:0.7324) (standing on:0.3793) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.4028) 
--------------------------------------------------------
====================================================================================================

2023-02-17 05:26:18 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_10000.pt
2023-02-17 05:26:21 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_10000.pt (epoch 1 @ 10000 updates, score 0.6767594346829642) (writing took 8.259153343737125 seconds)
2023-02-17 05:26:32 - progress_bar.py[line:274] - INFO: epoch 001:  10024 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=110.8, bsz=40, num_updates=10010, lr=4.75747e-05, gnorm=0.375, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49193
2023-02-17 05:26:43 - progress_bar.py[line:274] - INFO: epoch 001:  10034 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.91, wpb=111.9, bsz=40, num_updates=10020, lr=4.75701e-05, gnorm=0.441, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49204
2023-02-17 05:26:54 - progress_bar.py[line:274] - INFO: epoch 001:  10044 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.91, wpb=110.6, bsz=40, num_updates=10030, lr=4.75656e-05, gnorm=0.425, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49215
2023-02-17 05:27:05 - progress_bar.py[line:274] - INFO: epoch 001:  10054 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=111.6, bsz=40, num_updates=10040, lr=4.75611e-05, gnorm=0.324, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49226
2023-02-17 05:27:15 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 05:27:18 - progress_bar.py[line:274] - INFO: epoch 001:  10065 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=91.3, ups=0.82, wpb=111.6, bsz=40, num_updates=10050, lr=4.75566e-05, gnorm=0.422, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=49239
2023-02-17 05:27:29 - progress_bar.py[line:274] - INFO: epoch 001:  10075 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=112, bsz=40, num_updates=10060, lr=4.75521e-05, gnorm=0.375, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49250
2023-02-17 05:27:40 - progress_bar.py[line:274] - INFO: epoch 001:  10085 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.9, ups=0.93, wpb=112.4, bsz=40, num_updates=10070, lr=4.75476e-05, gnorm=0.459, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49261
2023-02-17 05:27:51 - progress_bar.py[line:274] - INFO: epoch 001:  10095 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.88, wpb=113.4, bsz=40, num_updates=10080, lr=4.75431e-05, gnorm=0.341, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=49272
2023-02-17 05:28:03 - progress_bar.py[line:274] - INFO: epoch 001:  10105 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.87, wpb=112, bsz=40, num_updates=10090, lr=4.75386e-05, gnorm=0.324, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=49284
2023-02-17 05:28:14 - progress_bar.py[line:274] - INFO: epoch 001:  10115 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=112.4, bsz=40, num_updates=10100, lr=4.75341e-05, gnorm=0.368, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49295
2023-02-17 05:28:25 - progress_bar.py[line:274] - INFO: epoch 001:  10125 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.91, wpb=111.7, bsz=40, num_updates=10110, lr=4.75296e-05, gnorm=0.424, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49306
2023-02-17 05:28:36 - progress_bar.py[line:274] - INFO: epoch 001:  10135 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.7, ups=0.92, wpb=112.7, bsz=40, num_updates=10120, lr=4.75251e-05, gnorm=0.405, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49317
2023-02-17 05:28:47 - progress_bar.py[line:274] - INFO: epoch 001:  10145 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.2, ups=0.93, wpb=112.8, bsz=40, num_updates=10130, lr=4.75206e-05, gnorm=0.286, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49328
2023-02-17 05:28:58 - progress_bar.py[line:274] - INFO: epoch 001:  10155 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=112, bsz=40, num_updates=10140, lr=4.75161e-05, gnorm=0.348, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49339
2023-02-17 05:29:09 - progress_bar.py[line:274] - INFO: epoch 001:  10165 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.7, ups=0.93, wpb=113.4, bsz=40, num_updates=10150, lr=4.75116e-05, gnorm=0.403, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49349
2023-02-17 05:29:20 - progress_bar.py[line:274] - INFO: epoch 001:  10175 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.91, wpb=113.5, bsz=40, num_updates=10160, lr=4.75071e-05, gnorm=0.408, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49360
2023-02-17 05:29:31 - progress_bar.py[line:274] - INFO: epoch 001:  10185 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.91, wpb=112, bsz=40, num_updates=10170, lr=4.75026e-05, gnorm=0.476, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49371
2023-02-17 05:29:42 - progress_bar.py[line:274] - INFO: epoch 001:  10195 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.91, wpb=112.8, bsz=40, num_updates=10180, lr=4.74981e-05, gnorm=0.317, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49383
2023-02-17 05:29:53 - progress_bar.py[line:274] - INFO: epoch 001:  10205 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=112.5, bsz=40, num_updates=10190, lr=4.74936e-05, gnorm=0.283, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49394
2023-02-17 05:30:04 - progress_bar.py[line:274] - INFO: epoch 001:  10215 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.88, wpb=110.6, bsz=40, num_updates=10200, lr=4.74891e-05, gnorm=0.443, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49405
2023-02-17 05:30:15 - progress_bar.py[line:274] - INFO: epoch 001:  10225 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=112.6, bsz=40, num_updates=10210, lr=4.74846e-05, gnorm=0.438, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49416
2023-02-17 05:30:27 - progress_bar.py[line:274] - INFO: epoch 001:  10235 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=10220, lr=4.74801e-05, gnorm=0.385, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49427
2023-02-17 05:30:37 - progress_bar.py[line:274] - INFO: epoch 001:  10245 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.92, wpb=112.9, bsz=40, num_updates=10230, lr=4.74756e-05, gnorm=0.372, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49438
2023-02-17 05:30:49 - progress_bar.py[line:274] - INFO: epoch 001:  10255 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=111.2, bsz=40, num_updates=10240, lr=4.74711e-05, gnorm=0.329, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49450
2023-02-17 05:31:00 - progress_bar.py[line:274] - INFO: epoch 001:  10265 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.91, wpb=113.7, bsz=40, num_updates=10250, lr=4.74666e-05, gnorm=0.331, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49461
2023-02-17 05:31:11 - progress_bar.py[line:274] - INFO: epoch 001:  10275 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.88, wpb=112.1, bsz=40, num_updates=10260, lr=4.74621e-05, gnorm=0.361, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49472
2023-02-17 05:31:22 - progress_bar.py[line:274] - INFO: epoch 001:  10285 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.93, wpb=111.5, bsz=40, num_updates=10270, lr=4.74576e-05, gnorm=0.403, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49483
2023-02-17 05:31:33 - progress_bar.py[line:274] - INFO: epoch 001:  10295 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.9, wpb=110.5, bsz=40, num_updates=10280, lr=4.7453e-05, gnorm=0.536, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49494
2023-02-17 05:31:44 - progress_bar.py[line:274] - INFO: epoch 001:  10305 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.9, wpb=113.7, bsz=40, num_updates=10290, lr=4.74485e-05, gnorm=0.379, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49505
2023-02-17 05:31:55 - progress_bar.py[line:274] - INFO: epoch 001:  10315 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.3, bsz=40, num_updates=10300, lr=4.7444e-05, gnorm=0.329, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49516
2023-02-17 05:32:06 - progress_bar.py[line:274] - INFO: epoch 001:  10325 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.91, wpb=111.8, bsz=40, num_updates=10310, lr=4.74395e-05, gnorm=0.409, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49527
2023-02-17 05:32:18 - progress_bar.py[line:274] - INFO: epoch 001:  10335 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111.6, bsz=40, num_updates=10320, lr=4.7435e-05, gnorm=0.346, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49538
2023-02-17 05:32:29 - progress_bar.py[line:274] - INFO: epoch 001:  10345 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.88, wpb=112.9, bsz=40, num_updates=10330, lr=4.74305e-05, gnorm=0.336, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=49550
2023-02-17 05:32:40 - progress_bar.py[line:274] - INFO: epoch 001:  10355 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.9, wpb=112.8, bsz=40, num_updates=10340, lr=4.7426e-05, gnorm=0.372, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49561
2023-02-17 05:32:51 - progress_bar.py[line:274] - INFO: epoch 001:  10365 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.87, wpb=111.4, bsz=40, num_updates=10350, lr=4.74215e-05, gnorm=0.28, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49572
2023-02-17 05:33:02 - progress_bar.py[line:274] - INFO: epoch 001:  10375 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.91, wpb=111.4, bsz=40, num_updates=10360, lr=4.7417e-05, gnorm=0.392, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=49583
2023-02-17 05:33:14 - progress_bar.py[line:274] - INFO: epoch 001:  10385 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.9, wpb=112.3, bsz=40, num_updates=10370, lr=4.74125e-05, gnorm=0.333, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49595
2023-02-17 05:33:24 - progress_bar.py[line:274] - INFO: epoch 001:  10395 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.4, ups=0.93, wpb=112.1, bsz=40, num_updates=10380, lr=4.7408e-05, gnorm=0.33, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49605
2023-02-17 05:33:35 - progress_bar.py[line:274] - INFO: epoch 001:  10405 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=112.3, bsz=40, num_updates=10390, lr=4.74035e-05, gnorm=0.362, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49616
2023-02-17 05:33:46 - progress_bar.py[line:274] - INFO: epoch 001:  10415 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.91, wpb=110.9, bsz=40, num_updates=10400, lr=4.7399e-05, gnorm=0.352, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=49627
2023-02-17 05:33:57 - progress_bar.py[line:274] - INFO: epoch 001:  10425 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.91, wpb=112.7, bsz=40, num_updates=10410, lr=4.73945e-05, gnorm=0.397, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49638
2023-02-17 05:34:08 - progress_bar.py[line:274] - INFO: epoch 001:  10435 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.92, wpb=112.4, bsz=40, num_updates=10420, lr=4.739e-05, gnorm=0.303, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49649
2023-02-17 05:34:19 - progress_bar.py[line:274] - INFO: epoch 001:  10445 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=111.7, bsz=40, num_updates=10430, lr=4.73855e-05, gnorm=0.403, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49660
2023-02-17 05:34:31 - progress_bar.py[line:274] - INFO: epoch 001:  10455 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=111.4, bsz=40, num_updates=10440, lr=4.7381e-05, gnorm=0.414, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49672
2023-02-17 05:34:42 - progress_bar.py[line:274] - INFO: epoch 001:  10465 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.88, wpb=112.2, bsz=40, num_updates=10450, lr=4.73765e-05, gnorm=0.339, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=49683
2023-02-17 05:34:53 - progress_bar.py[line:274] - INFO: epoch 001:  10475 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.91, wpb=110.8, bsz=40, num_updates=10460, lr=4.7372e-05, gnorm=0.333, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49694
2023-02-17 05:35:04 - progress_bar.py[line:274] - INFO: epoch 001:  10485 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.91, wpb=111.6, bsz=40, num_updates=10470, lr=4.73675e-05, gnorm=0.556, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49705
2023-02-17 05:35:15 - progress_bar.py[line:274] - INFO: epoch 001:  10495 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.88, wpb=112.2, bsz=40, num_updates=10480, lr=4.7363e-05, gnorm=0.455, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49716
2023-02-17 05:35:26 - progress_bar.py[line:274] - INFO: epoch 001:  10505 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.4, bsz=40, num_updates=10490, lr=4.73585e-05, gnorm=0.393, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49727
2023-02-17 05:35:37 - progress_bar.py[line:274] - INFO: epoch 001:  10515 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.91, wpb=112.2, bsz=40, num_updates=10500, lr=4.7354e-05, gnorm=0.413, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49738
2023-02-17 05:35:49 - progress_bar.py[line:274] - INFO: epoch 001:  10525 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112, bsz=40, num_updates=10510, lr=4.73495e-05, gnorm=0.344, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=49750
2023-02-17 05:36:00 - progress_bar.py[line:274] - INFO: epoch 001:  10535 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=112.5, bsz=40, num_updates=10520, lr=4.7345e-05, gnorm=0.359, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49761
2023-02-17 05:36:11 - progress_bar.py[line:274] - INFO: epoch 001:  10545 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.91, wpb=109.9, bsz=40, num_updates=10530, lr=4.73404e-05, gnorm=0.442, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49772
2023-02-17 05:36:22 - progress_bar.py[line:274] - INFO: epoch 001:  10555 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.91, wpb=111.2, bsz=40, num_updates=10540, lr=4.73359e-05, gnorm=0.356, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=49783
2023-02-17 05:36:33 - progress_bar.py[line:274] - INFO: epoch 001:  10565 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.9, wpb=111.1, bsz=40, num_updates=10550, lr=4.73314e-05, gnorm=0.352, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49794
2023-02-17 05:36:44 - progress_bar.py[line:274] - INFO: epoch 001:  10575 / 11564 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.88, wpb=112.7, bsz=40, num_updates=10560, lr=4.73269e-05, gnorm=0.347, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49805
2023-02-17 05:36:55 - progress_bar.py[line:274] - INFO: epoch 001:  10585 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=112.7, bsz=40, num_updates=10570, lr=4.73224e-05, gnorm=0.335, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=49816
2023-02-17 05:37:06 - progress_bar.py[line:274] - INFO: epoch 001:  10595 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.91, wpb=112.8, bsz=40, num_updates=10580, lr=4.73179e-05, gnorm=0.349, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49827
2023-02-17 05:37:18 - progress_bar.py[line:274] - INFO: epoch 001:  10605 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.3, ups=0.87, wpb=111.3, bsz=40, num_updates=10590, lr=4.73134e-05, gnorm=0.409, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49839
2023-02-17 05:37:29 - progress_bar.py[line:274] - INFO: epoch 001:  10615 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.9, wpb=111.6, bsz=40, num_updates=10600, lr=4.73089e-05, gnorm=0.272, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49850
2023-02-17 05:37:40 - progress_bar.py[line:274] - INFO: epoch 001:  10625 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.1, ups=0.89, wpb=112, bsz=40, num_updates=10610, lr=4.73044e-05, gnorm=0.279, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49861
2023-02-17 05:37:51 - progress_bar.py[line:274] - INFO: epoch 001:  10635 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.91, wpb=110.7, bsz=40, num_updates=10620, lr=4.72999e-05, gnorm=0.328, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49872
2023-02-17 05:38:02 - progress_bar.py[line:274] - INFO: epoch 001:  10645 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.9, wpb=112.4, bsz=40, num_updates=10630, lr=4.72954e-05, gnorm=0.386, clip=0, loss_scale=1024, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=49883
2023-02-17 05:38:13 - progress_bar.py[line:274] - INFO: epoch 001:  10655 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.91, wpb=113.2, bsz=40, num_updates=10640, lr=4.72909e-05, gnorm=0.316, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49894
2023-02-17 05:38:24 - progress_bar.py[line:274] - INFO: epoch 001:  10665 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.91, wpb=112.9, bsz=40, num_updates=10650, lr=4.72864e-05, gnorm=0.306, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49905
2023-02-17 05:38:36 - progress_bar.py[line:274] - INFO: epoch 001:  10675 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.88, wpb=111.4, bsz=40, num_updates=10660, lr=4.72819e-05, gnorm=0.359, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49917
2023-02-17 05:38:47 - progress_bar.py[line:274] - INFO: epoch 001:  10685 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.89, wpb=112, bsz=40, num_updates=10670, lr=4.72774e-05, gnorm=0.4, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=49928
2023-02-17 05:38:58 - progress_bar.py[line:274] - INFO: epoch 001:  10695 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.8, ups=0.92, wpb=112.9, bsz=40, num_updates=10680, lr=4.72729e-05, gnorm=0.355, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49939
2023-02-17 05:39:09 - progress_bar.py[line:274] - INFO: epoch 001:  10705 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.9, wpb=111.1, bsz=40, num_updates=10690, lr=4.72684e-05, gnorm=0.321, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49950
2023-02-17 05:39:20 - progress_bar.py[line:274] - INFO: epoch 001:  10715 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.9, wpb=112.3, bsz=40, num_updates=10700, lr=4.72639e-05, gnorm=0.359, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=49961
2023-02-17 05:39:31 - progress_bar.py[line:274] - INFO: epoch 001:  10725 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=113.3, bsz=40, num_updates=10710, lr=4.72594e-05, gnorm=0.346, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49972
2023-02-17 05:39:43 - progress_bar.py[line:274] - INFO: epoch 001:  10735 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.9, wpb=111.9, bsz=40, num_updates=10720, lr=4.72549e-05, gnorm=0.485, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=49984
2023-02-17 05:39:54 - progress_bar.py[line:274] - INFO: epoch 001:  10745 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.88, wpb=112.8, bsz=40, num_updates=10730, lr=4.72504e-05, gnorm=0.345, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=49995
2023-02-17 05:40:05 - progress_bar.py[line:274] - INFO: epoch 001:  10755 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.87, wpb=113.6, bsz=40, num_updates=10740, lr=4.72459e-05, gnorm=0.675, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50006
2023-02-17 05:40:16 - progress_bar.py[line:274] - INFO: epoch 001:  10765 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.91, wpb=111.5, bsz=40, num_updates=10750, lr=4.72414e-05, gnorm=0.371, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50017
2023-02-17 05:40:28 - progress_bar.py[line:274] - INFO: epoch 001:  10775 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.88, wpb=111.4, bsz=40, num_updates=10760, lr=4.72369e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50029
2023-02-17 05:40:39 - progress_bar.py[line:274] - INFO: epoch 001:  10785 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.87, wpb=111.7, bsz=40, num_updates=10770, lr=4.72324e-05, gnorm=0.293, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50040
2023-02-17 05:40:50 - progress_bar.py[line:274] - INFO: epoch 001:  10795 / 11564 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.9, wpb=112.7, bsz=40, num_updates=10780, lr=4.72279e-05, gnorm=0.295, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=50051
2023-02-17 05:41:01 - progress_bar.py[line:274] - INFO: epoch 001:  10805 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.91, wpb=112.8, bsz=40, num_updates=10790, lr=4.72233e-05, gnorm=0.382, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50062
2023-02-17 05:41:12 - progress_bar.py[line:274] - INFO: epoch 001:  10815 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.89, wpb=112.4, bsz=40, num_updates=10800, lr=4.72188e-05, gnorm=0.418, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=50073
2023-02-17 05:41:24 - progress_bar.py[line:274] - INFO: epoch 001:  10825 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.9, bsz=40, num_updates=10810, lr=4.72143e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50085
2023-02-17 05:41:35 - progress_bar.py[line:274] - INFO: epoch 001:  10835 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.91, wpb=112.9, bsz=40, num_updates=10820, lr=4.72098e-05, gnorm=0.46, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50096
2023-02-17 05:41:46 - progress_bar.py[line:274] - INFO: epoch 001:  10845 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.92, wpb=111.3, bsz=40, num_updates=10830, lr=4.72053e-05, gnorm=0.339, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50106
2023-02-17 05:41:56 - progress_bar.py[line:274] - INFO: epoch 001:  10855 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.6, ups=0.92, wpb=112.5, bsz=40, num_updates=10840, lr=4.72008e-05, gnorm=0.405, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=50117
2023-02-17 05:42:07 - progress_bar.py[line:274] - INFO: epoch 001:  10865 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105, ups=0.93, wpb=112.8, bsz=40, num_updates=10850, lr=4.71963e-05, gnorm=0.32, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=50128
2023-02-17 05:42:18 - progress_bar.py[line:274] - INFO: epoch 001:  10875 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.89, wpb=113, bsz=40, num_updates=10860, lr=4.71918e-05, gnorm=0.369, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=50139
2023-02-17 05:42:29 - progress_bar.py[line:274] - INFO: epoch 001:  10885 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.91, wpb=112.2, bsz=40, num_updates=10870, lr=4.71873e-05, gnorm=0.405, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=50150
2023-02-17 05:42:41 - progress_bar.py[line:274] - INFO: epoch 001:  10895 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=111.9, bsz=40, num_updates=10880, lr=4.71828e-05, gnorm=0.415, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50162
2023-02-17 05:42:52 - progress_bar.py[line:274] - INFO: epoch 001:  10905 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.4, ups=0.87, wpb=111.5, bsz=40, num_updates=10890, lr=4.71783e-05, gnorm=0.317, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50173
2023-02-17 05:43:04 - progress_bar.py[line:274] - INFO: epoch 001:  10915 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.3, ups=0.87, wpb=111.6, bsz=40, num_updates=10900, lr=4.71738e-05, gnorm=0.329, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50185
2023-02-17 05:43:15 - progress_bar.py[line:274] - INFO: epoch 001:  10925 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.92, wpb=112, bsz=40, num_updates=10910, lr=4.71693e-05, gnorm=0.266, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=50195
2023-02-17 05:43:26 - progress_bar.py[line:274] - INFO: epoch 001:  10935 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=112.3, bsz=40, num_updates=10920, lr=4.71648e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50206
2023-02-17 05:43:37 - progress_bar.py[line:274] - INFO: epoch 001:  10945 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.9, wpb=113.7, bsz=40, num_updates=10930, lr=4.71603e-05, gnorm=0.449, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50218
2023-02-17 05:43:47 - progress_bar.py[line:274] - INFO: epoch 001:  10955 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.1, ups=0.94, wpb=111.2, bsz=40, num_updates=10940, lr=4.71558e-05, gnorm=0.387, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50228
2023-02-17 05:43:58 - progress_bar.py[line:274] - INFO: epoch 001:  10965 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.91, wpb=113.1, bsz=40, num_updates=10950, lr=4.71513e-05, gnorm=0.305, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50239
2023-02-17 05:44:09 - progress_bar.py[line:274] - INFO: epoch 001:  10975 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.3, ups=0.93, wpb=112.2, bsz=40, num_updates=10960, lr=4.71468e-05, gnorm=0.359, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50250
2023-02-17 05:44:20 - progress_bar.py[line:274] - INFO: epoch 001:  10985 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=111.1, bsz=40, num_updates=10970, lr=4.71423e-05, gnorm=0.34, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=50261
2023-02-17 05:44:32 - progress_bar.py[line:274] - INFO: epoch 001:  10995 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=112.6, bsz=40, num_updates=10980, lr=4.71378e-05, gnorm=0.399, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50272
2023-02-17 05:44:43 - progress_bar.py[line:274] - INFO: epoch 001:  11005 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=111.4, bsz=40, num_updates=10990, lr=4.71333e-05, gnorm=0.441, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=50284
2023-02-17 05:44:44 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 05:44:54 - progress_bar.py[line:274] - INFO: epoch 001:  11016 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95.9, ups=0.85, wpb=112.7, bsz=40, num_updates=11000, lr=4.71288e-05, gnorm=0.315, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=50295
2023-02-17 05:44:54 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 05:44:55 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 05:44:55 - train.py[line:551] - INFO: load:0.87 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 05:46:58 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 05:46:58 - train.py[line:551] - INFO: load:0.90 valid_run:122.31 task_valid:119.27 collect_output:1.91
2023-02-17 05:48:58 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 05:48:58 - train.py[line:551] - INFO: load:0.93 valid_run:242.35 task_valid:235.28 collect_output:4.85
2023-02-17 05:51:00 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 05:51:00 - train.py[line:551] - INFO: load:0.95 valid_run:364.23 task_valid:351.86 collect_output:9.12
2023-02-17 05:53:02 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 05:53:02 - train.py[line:551] - INFO: load:0.98 valid_run:486.13 task_valid:465.58 collect_output:16.29
2023-02-17 05:55:02 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 05:55:02 - train.py[line:551] - INFO: load:1.00 valid_run:606.53 task_valid:582.85 collect_output:18.42
2023-02-17 05:57:05 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 05:57:05 - train.py[line:551] - INFO: load:1.03 valid_run:729.38 task_valid:701.54 collect_output:21.58
2023-02-17 05:59:08 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 05:59:08 - train.py[line:551] - INFO: load:1.05 valid_run:852.33 task_valid:819.64 collect_output:25.42
2023-02-17 06:01:10 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 06:01:10 - train.py[line:551] - INFO: load:1.08 valid_run:974.14 task_valid:936.19 collect_output:29.65
2023-02-17 06:03:14 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 06:03:14 - train.py[line:551] - INFO: load:1.10 valid_run:1097.75 task_valid:1053.22 collect_output:35.24
2023-02-17 06:05:15 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 06:05:15 - train.py[line:551] - INFO: load:1.13 valid_run:1219.31 task_valid:1165.71 collect_output:43.32
2023-02-17 06:07:15 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 06:07:15 - train.py[line:551] - INFO: load:1.15 valid_run:1339.34 task_valid:1281.18 collect_output:46.86
2023-02-17 06:09:17 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 06:09:17 - train.py[line:551] - INFO: load:1.18 valid_run:1460.83 task_valid:1397.94 collect_output:50.60
2023-02-17 06:11:16 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 06:11:16 - train.py[line:551] - INFO: load:1.20 valid_run:1579.66 task_valid:1511.57 collect_output:54.79
2023-02-17 06:13:17 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 06:13:17 - train.py[line:551] - INFO: load:1.23 valid_run:1700.57 task_valid:1629.24 collect_output:57.05
2023-02-17 06:15:18 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 06:15:18 - train.py[line:551] - INFO: load:1.25 valid_run:1821.47 task_valid:1745.16 collect_output:61.04
2023-02-17 06:17:19 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 06:17:19 - train.py[line:551] - INFO: load:1.28 valid_run:1942.53 task_valid:1859.10 collect_output:67.15
2023-02-17 06:19:20 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 06:19:20 - train.py[line:551] - INFO: load:1.30 valid_run:2063.84 task_valid:1975.08 collect_output:71.48
2023-02-17 06:21:21 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 06:21:21 - train.py[line:551] - INFO: load:1.33 valid_run:2184.50 task_valid:2092.98 collect_output:73.21
2023-02-17 06:23:22 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 06:23:22 - train.py[line:551] - INFO: load:1.35 valid_run:2305.79 task_valid:2209.97 collect_output:76.51
2023-02-17 06:25:23 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 06:25:23 - train.py[line:551] - INFO: load:1.38 valid_run:2426.02 task_valid:2326.51 collect_output:79.18
2023-02-17 06:27:24 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 06:27:24 - train.py[line:551] - INFO: load:1.40 valid_run:2547.52 task_valid:2442.89 collect_output:83.31
2023-02-17 06:29:26 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 06:29:26 - train.py[line:551] - INFO: load:1.43 valid_run:2669.37 task_valid:2561.63 collect_output:85.44
2023-02-17 06:31:26 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 06:31:26 - train.py[line:551] - INFO: load:1.45 valid_run:2789.65 task_valid:2675.88 collect_output:90.48
2023-02-17 06:33:26 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 06:33:26 - train.py[line:551] - INFO: load:1.48 valid_run:2909.41 task_valid:2791.90 collect_output:93.22
2023-02-17 06:35:28 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 06:35:28 - train.py[line:551] - INFO: load:1.50 valid_run:3030.86 task_valid:2907.97 collect_output:97.60
2023-02-17 06:37:31 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 06:37:31 - train.py[line:551] - INFO: load:1.53 valid_run:3153.65 task_valid:3023.77 collect_output:103.58
2023-02-17 06:39:30 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 06:39:30 - train.py[line:551] - INFO: load:1.55 valid_run:3273.10 task_valid:3137.75 collect_output:108.03
2023-02-17 06:41:32 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 06:41:32 - train.py[line:551] - INFO: load:1.58 valid_run:3394.90 task_valid:3257.09 collect_output:109.47
2023-02-17 06:43:34 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 06:43:34 - train.py[line:551] - INFO: load:1.60 valid_run:3516.47 task_valid:3372.54 collect_output:114.59
2023-02-17 06:45:36 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 06:45:36 - train.py[line:551] - INFO: load:1.63 valid_run:3638.35 task_valid:3490.98 collect_output:117.03
2023-02-17 06:47:37 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 06:47:37 - train.py[line:551] - INFO: load:1.65 valid_run:3759.39 task_valid:3609.34 collect_output:118.70

====================================================================================================
SGG eval:     R @ 50: 0.6344;     R @ 100: 0.6686;     R @ 500: 0.6934;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4343;    mR @ 100: 0.4625;    mR @ 500: 0.5187;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6875) (covering:0.2286) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9644) (says:0.0000) (sitting on:0.7290) (standing on:0.3793) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.4028) 
--------------------------------------------------------
====================================================================================================

2023-02-17 06:48:07 - train.py[line:487] - INFO: 0.6685594346829642

====================================================================================================
SGG eval:     R @ 50: 0.6344;     R @ 100: 0.6686;     R @ 500: 0.6934;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4343;    mR @ 100: 0.4625;    mR @ 500: 0.5187;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6875) (covering:0.2286) (eating:0.8235) (flying in:0.5909) (growing on:0.5000) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9644) (says:0.0000) (sitting on:0.7290) (standing on:0.3793) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.4028) 
--------------------------------------------------------
====================================================================================================

2023-02-17 06:48:07 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 06:48:07 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.278 | loss_v1 0 | loss_v2 0 | nll_loss 0.11 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.668559 | ppl 1.08 | vqa_score 0.5417 | wps 118.3 | wpb 72 | bsz 24 | num_updates 11000 | best_R@100 0.69202
2023-02-17 06:48:07 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 11000 updates
2023-02-17 06:48:07 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_11000.pt
2023-02-17 06:48:13 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_11000.pt
2023-02-17 06:48:15 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_1_11000.pt (epoch 1 @ 11000 updates, score 0.6685594346829642) (writing took 8.152509603649378 seconds)
2023-02-17 06:48:27 - progress_bar.py[line:274] - INFO: epoch 001:  11026 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=112.1, bsz=40, num_updates=11010, lr=4.71243e-05, gnorm=0.407, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54107
2023-02-17 06:48:38 - progress_bar.py[line:274] - INFO: epoch 001:  11036 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.9, wpb=110.6, bsz=40, num_updates=11020, lr=4.71198e-05, gnorm=0.446, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54119
2023-02-17 06:48:49 - progress_bar.py[line:274] - INFO: epoch 001:  11046 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.9, wpb=113.5, bsz=40, num_updates=11030, lr=4.71153e-05, gnorm=0.439, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54130
2023-02-17 06:49:00 - progress_bar.py[line:274] - INFO: epoch 001:  11056 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.7, bsz=40, num_updates=11040, lr=4.71108e-05, gnorm=0.359, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54141
2023-02-17 06:49:11 - progress_bar.py[line:274] - INFO: epoch 001:  11066 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=111.4, bsz=40, num_updates=11050, lr=4.71062e-05, gnorm=0.358, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54152
2023-02-17 06:49:23 - progress_bar.py[line:274] - INFO: epoch 001:  11076 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.89, wpb=111.5, bsz=40, num_updates=11060, lr=4.71017e-05, gnorm=0.305, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54164
2023-02-17 06:49:34 - progress_bar.py[line:274] - INFO: epoch 001:  11086 / 11564 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.91, wpb=111.1, bsz=40, num_updates=11070, lr=4.70972e-05, gnorm=0.295, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54175
2023-02-17 06:49:45 - progress_bar.py[line:274] - INFO: epoch 001:  11096 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=110.5, bsz=40, num_updates=11080, lr=4.70927e-05, gnorm=0.377, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54186
2023-02-17 06:49:56 - progress_bar.py[line:274] - INFO: epoch 001:  11106 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=111.6, bsz=40, num_updates=11090, lr=4.70882e-05, gnorm=0.328, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54197
2023-02-17 06:50:07 - progress_bar.py[line:274] - INFO: epoch 001:  11116 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.9, wpb=112.7, bsz=40, num_updates=11100, lr=4.70837e-05, gnorm=0.318, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54208
2023-02-17 06:50:18 - progress_bar.py[line:274] - INFO: epoch 001:  11126 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.9, ups=0.94, wpb=112, bsz=40, num_updates=11110, lr=4.70792e-05, gnorm=0.238, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54219
2023-02-17 06:50:29 - progress_bar.py[line:274] - INFO: epoch 001:  11136 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.88, wpb=112.1, bsz=40, num_updates=11120, lr=4.70747e-05, gnorm=0.41, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54230
2023-02-17 06:50:41 - progress_bar.py[line:274] - INFO: epoch 001:  11146 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.1, ups=0.88, wpb=110.8, bsz=40, num_updates=11130, lr=4.70702e-05, gnorm=0.31, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54242
2023-02-17 06:50:52 - progress_bar.py[line:274] - INFO: epoch 001:  11156 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=112.2, bsz=40, num_updates=11140, lr=4.70657e-05, gnorm=0.342, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54253
2023-02-17 06:51:03 - progress_bar.py[line:274] - INFO: epoch 001:  11166 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.91, wpb=112.8, bsz=40, num_updates=11150, lr=4.70612e-05, gnorm=0.385, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54264
2023-02-17 06:51:14 - progress_bar.py[line:274] - INFO: epoch 001:  11176 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=112.7, bsz=40, num_updates=11160, lr=4.70567e-05, gnorm=0.354, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54275
2023-02-17 06:51:25 - progress_bar.py[line:274] - INFO: epoch 001:  11186 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.88, wpb=112.6, bsz=40, num_updates=11170, lr=4.70522e-05, gnorm=0.488, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=54286
2023-02-17 06:51:37 - progress_bar.py[line:274] - INFO: epoch 001:  11196 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.88, wpb=113.2, bsz=40, num_updates=11180, lr=4.70477e-05, gnorm=0.38, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54298
2023-02-17 06:51:48 - progress_bar.py[line:274] - INFO: epoch 001:  11206 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=11190, lr=4.70432e-05, gnorm=0.423, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54309
2023-02-17 06:51:59 - progress_bar.py[line:274] - INFO: epoch 001:  11216 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.9, wpb=111.5, bsz=40, num_updates=11200, lr=4.70387e-05, gnorm=0.385, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=54320
2023-02-17 06:52:10 - progress_bar.py[line:274] - INFO: epoch 001:  11226 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.91, wpb=112.2, bsz=40, num_updates=11210, lr=4.70342e-05, gnorm=0.379, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54331
2023-02-17 06:52:22 - progress_bar.py[line:274] - INFO: epoch 001:  11236 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.88, wpb=112.4, bsz=40, num_updates=11220, lr=4.70297e-05, gnorm=0.364, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54342
2023-02-17 06:52:33 - progress_bar.py[line:274] - INFO: epoch 001:  11246 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=111.9, bsz=40, num_updates=11230, lr=4.70252e-05, gnorm=0.396, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54353
2023-02-17 06:52:44 - progress_bar.py[line:274] - INFO: epoch 001:  11256 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.88, wpb=112, bsz=40, num_updates=11240, lr=4.70207e-05, gnorm=0.371, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54365
2023-02-17 06:52:55 - progress_bar.py[line:274] - INFO: epoch 001:  11266 / 11564 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.1, ups=0.88, wpb=112.2, bsz=40, num_updates=11250, lr=4.70162e-05, gnorm=0.244, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54376
2023-02-17 06:53:06 - progress_bar.py[line:274] - INFO: epoch 001:  11276 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.3, bsz=40, num_updates=11260, lr=4.70117e-05, gnorm=0.338, clip=0, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=54387
2023-02-17 06:53:17 - progress_bar.py[line:274] - INFO: epoch 001:  11286 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.8, ups=0.93, wpb=113.6, bsz=40, num_updates=11270, lr=4.70072e-05, gnorm=0.321, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54398
2023-02-17 06:53:29 - progress_bar.py[line:274] - INFO: epoch 001:  11296 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.87, wpb=112.2, bsz=40, num_updates=11280, lr=4.70027e-05, gnorm=0.38, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=54410
2023-02-17 06:53:40 - progress_bar.py[line:274] - INFO: epoch 001:  11306 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.92, wpb=110.8, bsz=40, num_updates=11290, lr=4.69982e-05, gnorm=0.313, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54421
2023-02-17 06:53:51 - progress_bar.py[line:274] - INFO: epoch 001:  11316 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.88, wpb=110.8, bsz=40, num_updates=11300, lr=4.69936e-05, gnorm=0.27, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54432
2023-02-17 06:54:02 - progress_bar.py[line:274] - INFO: epoch 001:  11326 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.9, ups=0.87, wpb=112.4, bsz=40, num_updates=11310, lr=4.69891e-05, gnorm=0.427, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54443
2023-02-17 06:54:14 - progress_bar.py[line:274] - INFO: epoch 001:  11336 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.89, wpb=111.6, bsz=40, num_updates=11320, lr=4.69846e-05, gnorm=0.501, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54455
2023-02-17 06:54:25 - progress_bar.py[line:274] - INFO: epoch 001:  11346 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=110.8, bsz=40, num_updates=11330, lr=4.69801e-05, gnorm=0.36, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54466
2023-02-17 06:54:36 - progress_bar.py[line:274] - INFO: epoch 001:  11356 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.89, wpb=110.4, bsz=40, num_updates=11340, lr=4.69756e-05, gnorm=0.379, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54477
2023-02-17 06:54:48 - progress_bar.py[line:274] - INFO: epoch 001:  11366 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.88, wpb=111.3, bsz=40, num_updates=11350, lr=4.69711e-05, gnorm=0.362, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54489
2023-02-17 06:54:59 - progress_bar.py[line:274] - INFO: epoch 001:  11376 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=112, bsz=40, num_updates=11360, lr=4.69666e-05, gnorm=0.361, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54500
2023-02-17 06:55:10 - progress_bar.py[line:274] - INFO: epoch 001:  11386 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=11370, lr=4.69621e-05, gnorm=0.32, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54511
2023-02-17 06:55:21 - progress_bar.py[line:274] - INFO: epoch 001:  11396 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.93, wpb=110.3, bsz=40, num_updates=11380, lr=4.69576e-05, gnorm=0.309, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54522
2023-02-17 06:55:32 - progress_bar.py[line:274] - INFO: epoch 001:  11406 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.89, wpb=113.4, bsz=40, num_updates=11390, lr=4.69531e-05, gnorm=0.371, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=54533
2023-02-17 06:55:43 - progress_bar.py[line:274] - INFO: epoch 001:  11416 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=111.7, bsz=40, num_updates=11400, lr=4.69486e-05, gnorm=0.355, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54544
2023-02-17 06:55:54 - progress_bar.py[line:274] - INFO: epoch 001:  11426 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.91, wpb=111.9, bsz=40, num_updates=11410, lr=4.69441e-05, gnorm=0.265, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54555
2023-02-17 06:56:05 - progress_bar.py[line:274] - INFO: epoch 001:  11436 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.89, wpb=110, bsz=40, num_updates=11420, lr=4.69396e-05, gnorm=0.275, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54566
2023-02-17 06:56:16 - progress_bar.py[line:274] - INFO: epoch 001:  11446 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=112.1, bsz=40, num_updates=11430, lr=4.69351e-05, gnorm=0.365, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54577
2023-02-17 06:56:27 - progress_bar.py[line:274] - INFO: epoch 001:  11456 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.88, wpb=113.3, bsz=40, num_updates=11440, lr=4.69306e-05, gnorm=0.372, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=54588
2023-02-17 06:56:38 - progress_bar.py[line:274] - INFO: epoch 001:  11466 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.8, ups=0.92, wpb=113.4, bsz=40, num_updates=11450, lr=4.69261e-05, gnorm=0.403, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54599
2023-02-17 06:56:49 - progress_bar.py[line:274] - INFO: epoch 001:  11476 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=111, bsz=40, num_updates=11460, lr=4.69216e-05, gnorm=0.352, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54610
2023-02-17 06:57:00 - progress_bar.py[line:274] - INFO: epoch 001:  11486 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=111.9, bsz=40, num_updates=11470, lr=4.69171e-05, gnorm=0.282, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54621
2023-02-17 06:57:12 - progress_bar.py[line:274] - INFO: epoch 001:  11496 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.3, ups=0.87, wpb=112.6, bsz=40, num_updates=11480, lr=4.69126e-05, gnorm=0.37, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54633
2023-02-17 06:57:23 - progress_bar.py[line:274] - INFO: epoch 001:  11506 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.93, wpb=111.2, bsz=40, num_updates=11490, lr=4.69081e-05, gnorm=0.382, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54644
2023-02-17 06:57:34 - progress_bar.py[line:274] - INFO: epoch 001:  11516 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.91, wpb=110.4, bsz=40, num_updates=11500, lr=4.69036e-05, gnorm=0.423, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54655
2023-02-17 06:57:45 - progress_bar.py[line:274] - INFO: epoch 001:  11526 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=112.2, bsz=40, num_updates=11510, lr=4.68991e-05, gnorm=0.435, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54666
2023-02-17 06:57:56 - progress_bar.py[line:274] - INFO: epoch 001:  11536 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=112.8, bsz=40, num_updates=11520, lr=4.68946e-05, gnorm=0.315, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54677
2023-02-17 06:58:07 - progress_bar.py[line:274] - INFO: epoch 001:  11546 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.88, wpb=112.1, bsz=40, num_updates=11530, lr=4.68901e-05, gnorm=0.379, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54688
2023-02-17 06:58:18 - progress_bar.py[line:274] - INFO: epoch 001:  11556 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.92, wpb=112.3, bsz=40, num_updates=11540, lr=4.68856e-05, gnorm=0.327, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54699
2023-02-17 06:58:27 - train.py[line:339] - INFO: end of epoch 1 (average epoch stats below)
2023-02-17 06:58:27 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.225 | loss_v1 0 | loss_v2 0 | nll_loss 0.093 | ntokens 112.007 | nsentences 40 | sample_size 112.007 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.07 | wps 23.6 | ups 0.21 | wpb 112 | bsz 40 | num_updates 11548 | lr 4.6882e-05 | gnorm 0.746 | clip 18.8 | loss_scale 1024 | train_wall 12805 | gb_free 10.8 | ema_decay 0.9999 | wall 54708
2023-02-17 06:58:27 - trainer.py[line:694] - INFO: loading train data for epoch 2
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E1.tsv slice_id 0 row count 231280 total row count 462560
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E1.tsv slice_id 1 row count 231280 total row count 462560
2023-02-17 06:58:29 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-17 06:58:29 - trainer.py[line:758] - INFO: begin training epoch 2
2023-02-17 06:58:29 - train.py[line:312] - INFO: Start iterating over samples
2023-02-17 06:58:33 - progress_bar.py[line:274] - INFO: epoch 002:      2 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=73.8, ups=0.67, wpb=110.6, bsz=40, num_updates=11550, lr=4.68811e-05, gnorm=0.243, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=54714
2023-02-17 06:58:44 - progress_bar.py[line:274] - INFO: epoch 002:     12 / 11564 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=112, bsz=40, num_updates=11560, lr=4.68765e-05, gnorm=0.239, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54725
2023-02-17 06:58:56 - progress_bar.py[line:274] - INFO: epoch 002:     22 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.7, ups=0.87, wpb=111.1, bsz=40, num_updates=11570, lr=4.6872e-05, gnorm=0.315, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54737
2023-02-17 06:59:07 - progress_bar.py[line:274] - INFO: epoch 002:     32 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.5, ups=0.87, wpb=110.7, bsz=40, num_updates=11580, lr=4.68675e-05, gnorm=0.439, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54748
2023-02-17 06:59:18 - progress_bar.py[line:274] - INFO: epoch 002:     42 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.93, wpb=111.4, bsz=40, num_updates=11590, lr=4.6863e-05, gnorm=0.444, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54759
2023-02-17 06:59:30 - progress_bar.py[line:274] - INFO: epoch 002:     52 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.89, wpb=110.9, bsz=40, num_updates=11600, lr=4.68585e-05, gnorm=0.326, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54770
2023-02-17 06:59:41 - progress_bar.py[line:274] - INFO: epoch 002:     62 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.9, wpb=111, bsz=40, num_updates=11610, lr=4.6854e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54782
2023-02-17 06:59:52 - progress_bar.py[line:274] - INFO: epoch 002:     72 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=112, bsz=40, num_updates=11620, lr=4.68495e-05, gnorm=0.405, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54793
2023-02-17 07:00:03 - progress_bar.py[line:274] - INFO: epoch 002:     82 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.7, bsz=40, num_updates=11630, lr=4.6845e-05, gnorm=0.36, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54804
2023-02-17 07:00:14 - progress_bar.py[line:274] - INFO: epoch 002:     92 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.5, ups=0.93, wpb=111.3, bsz=40, num_updates=11640, lr=4.68405e-05, gnorm=0.4, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54815
2023-02-17 07:00:25 - progress_bar.py[line:274] - INFO: epoch 002:    102 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.91, wpb=113, bsz=40, num_updates=11650, lr=4.6836e-05, gnorm=0.309, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54826
2023-02-17 07:00:36 - progress_bar.py[line:274] - INFO: epoch 002:    112 / 11564 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.9, wpb=112.6, bsz=40, num_updates=11660, lr=4.68315e-05, gnorm=0.24, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54837
2023-02-17 07:00:47 - progress_bar.py[line:274] - INFO: epoch 002:    122 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.6, ups=0.89, wpb=112, bsz=40, num_updates=11670, lr=4.6827e-05, gnorm=0.422, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54848
2023-02-17 07:00:58 - progress_bar.py[line:274] - INFO: epoch 002:    132 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.89, wpb=112.8, bsz=40, num_updates=11680, lr=4.68225e-05, gnorm=0.293, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54859
2023-02-17 07:01:09 - progress_bar.py[line:274] - INFO: epoch 002:    142 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.92, wpb=111.8, bsz=40, num_updates=11690, lr=4.6818e-05, gnorm=0.352, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54870
2023-02-17 07:01:20 - progress_bar.py[line:274] - INFO: epoch 002:    152 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=112.3, bsz=40, num_updates=11700, lr=4.68135e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54881
2023-02-17 07:01:32 - progress_bar.py[line:274] - INFO: epoch 002:    162 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=114.4, nsentences=40, sample_size=114.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.88, wpb=114.4, bsz=40, num_updates=11710, lr=4.6809e-05, gnorm=0.317, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54893
2023-02-17 07:01:43 - progress_bar.py[line:274] - INFO: epoch 002:    172 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.9, wpb=112.2, bsz=40, num_updates=11720, lr=4.68045e-05, gnorm=0.42, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54904
2023-02-17 07:01:54 - progress_bar.py[line:274] - INFO: epoch 002:    182 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.9, wpb=110.9, bsz=40, num_updates=11730, lr=4.68e-05, gnorm=0.324, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54915
2023-02-17 07:02:05 - progress_bar.py[line:274] - INFO: epoch 002:    192 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.7, ups=0.92, wpb=113.3, bsz=40, num_updates=11740, lr=4.67955e-05, gnorm=0.298, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54926
2023-02-17 07:02:16 - progress_bar.py[line:274] - INFO: epoch 002:    202 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.91, wpb=111.1, bsz=40, num_updates=11750, lr=4.6791e-05, gnorm=0.425, clip=0, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=54936
2023-02-17 07:02:27 - progress_bar.py[line:274] - INFO: epoch 002:    212 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.88, wpb=113.3, bsz=40, num_updates=11760, lr=4.67865e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54948
2023-02-17 07:02:38 - progress_bar.py[line:274] - INFO: epoch 002:    222 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.88, wpb=112.7, bsz=40, num_updates=11770, lr=4.6782e-05, gnorm=0.455, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54959
2023-02-17 07:02:49 - progress_bar.py[line:274] - INFO: epoch 002:    232 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=111.8, bsz=40, num_updates=11780, lr=4.67775e-05, gnorm=0.318, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54970
2023-02-17 07:03:00 - progress_bar.py[line:274] - INFO: epoch 002:    242 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.92, wpb=112.8, bsz=40, num_updates=11790, lr=4.6773e-05, gnorm=0.375, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54981
2023-02-17 07:03:11 - progress_bar.py[line:274] - INFO: epoch 002:    252 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.2, ups=0.93, wpb=112.3, bsz=40, num_updates=11800, lr=4.67685e-05, gnorm=0.357, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54992
2023-02-17 07:03:22 - progress_bar.py[line:274] - INFO: epoch 002:    262 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.3, ups=0.92, wpb=112.8, bsz=40, num_updates=11810, lr=4.6764e-05, gnorm=0.295, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55003
2023-02-17 07:03:33 - progress_bar.py[line:274] - INFO: epoch 002:    272 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.93, wpb=111.5, bsz=40, num_updates=11820, lr=4.67594e-05, gnorm=0.439, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55014
2023-02-17 07:03:44 - progress_bar.py[line:274] - INFO: epoch 002:    282 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.92, wpb=111.5, bsz=40, num_updates=11830, lr=4.67549e-05, gnorm=0.503, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55025
2023-02-17 07:03:55 - progress_bar.py[line:274] - INFO: epoch 002:    292 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=11840, lr=4.67504e-05, gnorm=0.256, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55036
2023-02-17 07:04:07 - progress_bar.py[line:274] - INFO: epoch 002:    302 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.87, wpb=113, bsz=40, num_updates=11850, lr=4.67459e-05, gnorm=0.491, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55048
2023-02-17 07:04:18 - progress_bar.py[line:274] - INFO: epoch 002:    312 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.9, wpb=112.7, bsz=40, num_updates=11860, lr=4.67414e-05, gnorm=0.303, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55059
2023-02-17 07:04:29 - progress_bar.py[line:274] - INFO: epoch 002:    322 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.88, wpb=112.8, bsz=40, num_updates=11870, lr=4.67369e-05, gnorm=0.36, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55070
2023-02-17 07:04:40 - progress_bar.py[line:274] - INFO: epoch 002:    332 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.91, wpb=110.8, bsz=40, num_updates=11880, lr=4.67324e-05, gnorm=0.328, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55081
2023-02-17 07:04:51 - progress_bar.py[line:274] - INFO: epoch 002:    342 / 11564 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.89, wpb=112.3, bsz=40, num_updates=11890, lr=4.67279e-05, gnorm=0.275, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55092
2023-02-17 07:04:58 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 07:05:03 - progress_bar.py[line:274] - INFO: epoch 002:    353 / 11564 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=95, ups=0.85, wpb=112.3, bsz=40, num_updates=11900, lr=4.67234e-05, gnorm=0.278, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=55104
2023-02-17 07:05:14 - progress_bar.py[line:274] - INFO: epoch 002:    363 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.9, wpb=112.5, bsz=40, num_updates=11910, lr=4.67189e-05, gnorm=0.279, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55115
2023-02-17 07:05:26 - progress_bar.py[line:274] - INFO: epoch 002:    373 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.6, ups=0.87, wpb=111.1, bsz=40, num_updates=11920, lr=4.67144e-05, gnorm=0.483, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55127
2023-02-17 07:05:36 - progress_bar.py[line:274] - INFO: epoch 002:    383 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.92, wpb=110.5, bsz=40, num_updates=11930, lr=4.67099e-05, gnorm=0.417, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55137
2023-02-17 07:05:47 - progress_bar.py[line:274] - INFO: epoch 002:    393 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.93, wpb=111.4, bsz=40, num_updates=11940, lr=4.67054e-05, gnorm=0.312, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55148
2023-02-17 07:05:58 - progress_bar.py[line:274] - INFO: epoch 002:    403 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.9, wpb=111.8, bsz=40, num_updates=11950, lr=4.67009e-05, gnorm=0.44, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55159
2023-02-17 07:06:09 - progress_bar.py[line:274] - INFO: epoch 002:    413 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.91, wpb=112.1, bsz=40, num_updates=11960, lr=4.66964e-05, gnorm=0.297, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55170
2023-02-17 07:06:21 - progress_bar.py[line:274] - INFO: epoch 002:    423 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.9, wpb=112.6, bsz=40, num_updates=11970, lr=4.66919e-05, gnorm=0.414, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55181
2023-02-17 07:06:31 - progress_bar.py[line:274] - INFO: epoch 002:    433 / 11564 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.3, ups=0.93, wpb=112.3, bsz=40, num_updates=11980, lr=4.66874e-05, gnorm=0.243, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55192
2023-02-17 07:06:42 - progress_bar.py[line:274] - INFO: epoch 002:    443 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.91, wpb=113.1, bsz=40, num_updates=11990, lr=4.66829e-05, gnorm=0.322, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=55203
2023-02-17 07:06:53 - progress_bar.py[line:274] - INFO: epoch 002:    453 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=112.7, bsz=40, num_updates=12000, lr=4.66784e-05, gnorm=0.267, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55214
2023-02-17 07:06:53 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 07:06:54 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 07:06:54 - train.py[line:551] - INFO: load:0.85 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 07:08:57 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 07:08:57 - train.py[line:551] - INFO: load:0.87 valid_run:122.06 task_valid:118.86 collect_output:2.16
2023-02-17 07:10:56 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 07:10:56 - train.py[line:551] - INFO: load:0.90 valid_run:241.79 task_valid:234.48 collect_output:5.26
2023-02-17 07:12:58 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 07:12:58 - train.py[line:551] - INFO: load:0.92 valid_run:363.52 task_valid:350.84 collect_output:9.60
2023-02-17 07:15:00 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 07:15:00 - train.py[line:551] - INFO: load:0.95 valid_run:485.23 task_valid:464.30 collect_output:16.84
2023-02-17 07:17:00 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 07:17:00 - train.py[line:551] - INFO: load:0.97 valid_run:605.54 task_valid:581.49 collect_output:18.95
2023-02-17 07:19:03 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 07:19:03 - train.py[line:551] - INFO: load:1.00 valid_run:728.30 task_valid:700.12 collect_output:22.06
2023-02-17 07:21:06 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 07:21:06 - train.py[line:551] - INFO: load:1.02 valid_run:851.29 task_valid:818.26 collect_output:25.90
2023-02-17 07:23:08 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 07:23:08 - train.py[line:551] - INFO: load:1.05 valid_run:972.90 task_valid:934.60 collect_output:30.17
2023-02-17 07:25:11 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 07:25:11 - train.py[line:551] - INFO: load:1.07 valid_run:1096.40 task_valid:1051.52 collect_output:35.75
2023-02-17 07:27:13 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 07:27:13 - train.py[line:551] - INFO: load:1.10 valid_run:1217.93 task_valid:1163.96 collect_output:43.83
2023-02-17 07:29:13 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 07:29:13 - train.py[line:551] - INFO: load:1.12 valid_run:1337.81 task_valid:1279.31 collect_output:47.38
2023-02-17 07:31:14 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 07:31:14 - train.py[line:551] - INFO: load:1.14 valid_run:1459.22 task_valid:1396.00 collect_output:51.12
2023-02-17 07:33:13 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 07:33:13 - train.py[line:551] - INFO: load:1.17 valid_run:1577.80 task_valid:1509.52 collect_output:55.20
2023-02-17 07:35:14 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 07:35:14 - train.py[line:551] - INFO: load:1.19 valid_run:1698.58 task_valid:1627.06 collect_output:57.46
2023-02-17 07:37:15 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 07:37:15 - train.py[line:551] - INFO: load:1.22 valid_run:1819.35 task_valid:1743.05 collect_output:61.23
2023-02-17 07:39:16 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 07:39:16 - train.py[line:551] - INFO: load:1.24 valid_run:1940.30 task_valid:1856.80 collect_output:67.44
2023-02-17 07:41:17 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 07:41:17 - train.py[line:551] - INFO: load:1.27 valid_run:2061.53 task_valid:1972.71 collect_output:71.73
2023-02-17 07:43:18 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 07:43:18 - train.py[line:551] - INFO: load:1.29 valid_run:2182.08 task_valid:2090.55 collect_output:73.43
2023-02-17 07:45:19 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 07:45:19 - train.py[line:551] - INFO: load:1.32 valid_run:2303.17 task_valid:2207.39 collect_output:76.67
2023-02-17 07:47:19 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 07:47:19 - train.py[line:551] - INFO: load:1.34 valid_run:2423.39 task_valid:2323.74 collect_output:79.55
2023-02-17 07:49:21 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 07:49:21 - train.py[line:551] - INFO: load:1.37 valid_run:2544.99 task_valid:2440.29 collect_output:83.58
2023-02-17 07:51:23 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 07:51:23 - train.py[line:551] - INFO: load:1.39 valid_run:2666.78 task_valid:2559.04 collect_output:85.59
2023-02-17 07:53:23 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 07:53:23 - train.py[line:551] - INFO: load:1.42 valid_run:2786.92 task_valid:2673.20 collect_output:90.57
2023-02-17 07:55:22 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 07:55:22 - train.py[line:551] - INFO: load:1.44 valid_run:2906.58 task_valid:2789.21 collect_output:93.20
2023-02-17 07:57:24 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 07:57:24 - train.py[line:551] - INFO: load:1.47 valid_run:3027.97 task_valid:2905.21 collect_output:97.60
2023-02-17 07:59:27 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 07:59:27 - train.py[line:551] - INFO: load:1.49 valid_run:3150.77 task_valid:3021.05 collect_output:103.54
2023-02-17 08:01:26 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 08:01:26 - train.py[line:551] - INFO: load:1.52 valid_run:3270.33 task_valid:3135.07 collect_output:108.08
2023-02-17 08:03:28 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 08:03:28 - train.py[line:551] - INFO: load:1.54 valid_run:3392.09 task_valid:3254.35 collect_output:109.53
2023-02-17 08:05:30 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 08:05:30 - train.py[line:551] - INFO: load:1.57 valid_run:3513.55 task_valid:3369.64 collect_output:114.67
2023-02-17 08:07:31 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 08:07:31 - train.py[line:551] - INFO: load:1.59 valid_run:3635.22 task_valid:3487.85 collect_output:117.13
2023-02-17 08:09:32 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 08:09:32 - train.py[line:551] - INFO: load:1.62 valid_run:3756.24 task_valid:3606.17 collect_output:118.79

====================================================================================================
SGG eval:     R @ 50: 0.6291;     R @ 100: 0.6656;     R @ 500: 0.6905;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4353;    mR @ 100: 0.4764;    mR @ 500: 0.5217;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.6875) (covering:0.2286) (eating:0.7647) (flying in:0.8182) (growing on:0.5000) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9742) (says:0.0000) (sitting on:0.7205) (standing on:0.3643) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.4444) 
--------------------------------------------------------
====================================================================================================

2023-02-17 08:10:03 - train.py[line:487] - INFO: 0.6656351922587217

====================================================================================================
SGG eval:     R @ 50: 0.6291;     R @ 100: 0.6656;     R @ 500: 0.6905;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4353;    mR @ 100: 0.4764;    mR @ 500: 0.5217;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.6875) (covering:0.2286) (eating:0.7647) (flying in:0.8182) (growing on:0.5000) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9742) (says:0.0000) (sitting on:0.7205) (standing on:0.3643) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.4444) 
--------------------------------------------------------
====================================================================================================

2023-02-17 08:10:03 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 08:10:03 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.251 | loss_v1 0 | loss_v2 0 | nll_loss 0.081 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.665635 | ppl 1.06 | vqa_score 0.536 | wps 118.4 | wpb 72 | bsz 24 | num_updates 12000 | best_R@100 0.69202
2023-02-17 08:10:03 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 12000 updates
2023-02-17 08:10:03 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_12000.pt
2023-02-17 08:10:09 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_12000.pt
2023-02-17 08:10:11 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_12000.pt (epoch 2 @ 12000 updates, score 0.6656351922587217) (writing took 7.829490585252643 seconds)
2023-02-17 08:10:22 - progress_bar.py[line:274] - INFO: epoch 002:    463 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=112.8, bsz=40, num_updates=12010, lr=4.66739e-05, gnorm=0.287, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59023
2023-02-17 08:10:33 - progress_bar.py[line:274] - INFO: epoch 002:    473 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.9, wpb=112.8, bsz=40, num_updates=12020, lr=4.66694e-05, gnorm=0.358, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59034
2023-02-17 08:10:44 - progress_bar.py[line:274] - INFO: epoch 002:    483 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.89, wpb=114, bsz=40, num_updates=12030, lr=4.66649e-05, gnorm=0.309, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59045
2023-02-17 08:10:55 - progress_bar.py[line:274] - INFO: epoch 002:    493 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.7, bsz=40, num_updates=12040, lr=4.66604e-05, gnorm=0.34, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59056
2023-02-17 08:11:06 - progress_bar.py[line:274] - INFO: epoch 002:    503 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.5, ups=0.91, wpb=112.1, bsz=40, num_updates=12050, lr=4.66559e-05, gnorm=0.317, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59067
2023-02-17 08:11:18 - progress_bar.py[line:274] - INFO: epoch 002:    513 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.89, wpb=112.4, bsz=40, num_updates=12060, lr=4.66514e-05, gnorm=0.317, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59078
2023-02-17 08:11:28 - progress_bar.py[line:274] - INFO: epoch 002:    523 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.92, wpb=112.4, bsz=40, num_updates=12070, lr=4.66468e-05, gnorm=0.437, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59089
2023-02-17 08:11:40 - progress_bar.py[line:274] - INFO: epoch 002:    533 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=111.4, bsz=40, num_updates=12080, lr=4.66423e-05, gnorm=0.581, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59101
2023-02-17 08:11:51 - progress_bar.py[line:274] - INFO: epoch 002:    543 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.88, wpb=111.2, bsz=40, num_updates=12090, lr=4.66378e-05, gnorm=0.356, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59112
2023-02-17 08:12:02 - progress_bar.py[line:274] - INFO: epoch 002:    553 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.91, wpb=113.6, bsz=40, num_updates=12100, lr=4.66333e-05, gnorm=0.389, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59123
2023-02-17 08:12:13 - progress_bar.py[line:274] - INFO: epoch 002:    563 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.89, wpb=113, bsz=40, num_updates=12110, lr=4.66288e-05, gnorm=0.426, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59134
2023-02-17 08:12:24 - progress_bar.py[line:274] - INFO: epoch 002:    573 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.92, wpb=112.3, bsz=40, num_updates=12120, lr=4.66243e-05, gnorm=0.324, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59145
2023-02-17 08:12:35 - progress_bar.py[line:274] - INFO: epoch 002:    583 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=112.3, bsz=40, num_updates=12130, lr=4.66198e-05, gnorm=0.316, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=59156
2023-02-17 08:12:46 - progress_bar.py[line:274] - INFO: epoch 002:    593 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.1, ups=0.9, wpb=112.6, bsz=40, num_updates=12140, lr=4.66153e-05, gnorm=0.285, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59167
2023-02-17 08:12:58 - progress_bar.py[line:274] - INFO: epoch 002:    603 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=113.2, bsz=40, num_updates=12150, lr=4.66108e-05, gnorm=0.447, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59179
2023-02-17 08:13:09 - progress_bar.py[line:274] - INFO: epoch 002:    613 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=112.3, bsz=40, num_updates=12160, lr=4.66063e-05, gnorm=0.331, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59190
2023-02-17 08:13:20 - progress_bar.py[line:274] - INFO: epoch 002:    623 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=113, bsz=40, num_updates=12170, lr=4.66018e-05, gnorm=0.346, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59201
2023-02-17 08:13:31 - progress_bar.py[line:274] - INFO: epoch 002:    633 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.91, wpb=111.1, bsz=40, num_updates=12180, lr=4.65973e-05, gnorm=0.264, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59212
2023-02-17 08:13:42 - progress_bar.py[line:274] - INFO: epoch 002:    643 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.1, ups=0.87, wpb=111.4, bsz=40, num_updates=12190, lr=4.65928e-05, gnorm=0.332, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59223
2023-02-17 08:13:53 - progress_bar.py[line:274] - INFO: epoch 002:    653 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.92, wpb=112.8, bsz=40, num_updates=12200, lr=4.65883e-05, gnorm=0.371, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59234
2023-02-17 08:14:04 - progress_bar.py[line:274] - INFO: epoch 002:    663 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.92, wpb=112.5, bsz=40, num_updates=12210, lr=4.65838e-05, gnorm=0.413, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59245
2023-02-17 08:14:15 - progress_bar.py[line:274] - INFO: epoch 002:    673 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.5, ups=0.87, wpb=110.6, bsz=40, num_updates=12220, lr=4.65793e-05, gnorm=0.426, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59256
2023-02-17 08:14:27 - progress_bar.py[line:274] - INFO: epoch 002:    683 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.89, wpb=112.3, bsz=40, num_updates=12230, lr=4.65748e-05, gnorm=0.344, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59268
2023-02-17 08:14:38 - progress_bar.py[line:274] - INFO: epoch 002:    693 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.3, ups=0.92, wpb=113.4, bsz=40, num_updates=12240, lr=4.65703e-05, gnorm=0.354, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59279
2023-02-17 08:14:49 - progress_bar.py[line:274] - INFO: epoch 002:    703 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.91, wpb=110.4, bsz=40, num_updates=12250, lr=4.65658e-05, gnorm=0.342, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59290
2023-02-17 08:15:00 - progress_bar.py[line:274] - INFO: epoch 002:    713 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=111.9, bsz=40, num_updates=12260, lr=4.65613e-05, gnorm=0.356, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59301
2023-02-17 08:15:10 - progress_bar.py[line:274] - INFO: epoch 002:    723 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=105.4, ups=0.93, wpb=113.1, bsz=40, num_updates=12270, lr=4.65568e-05, gnorm=0.311, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59311
2023-02-17 08:15:22 - progress_bar.py[line:274] - INFO: epoch 002:    733 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.9, wpb=111.7, bsz=40, num_updates=12280, lr=4.65523e-05, gnorm=0.333, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59322
2023-02-17 08:15:33 - progress_bar.py[line:274] - INFO: epoch 002:    743 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.87, wpb=112.1, bsz=40, num_updates=12290, lr=4.65478e-05, gnorm=0.334, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59334
2023-02-17 08:15:44 - progress_bar.py[line:274] - INFO: epoch 002:    753 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=112.1, bsz=40, num_updates=12300, lr=4.65433e-05, gnorm=0.389, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59345
2023-02-17 08:15:55 - progress_bar.py[line:274] - INFO: epoch 002:    763 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.9, wpb=111.3, bsz=40, num_updates=12310, lr=4.65388e-05, gnorm=0.393, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=59356
2023-02-17 08:16:07 - progress_bar.py[line:274] - INFO: epoch 002:    773 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.8, nsentences=40, sample_size=113.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.88, wpb=113.8, bsz=40, num_updates=12320, lr=4.65343e-05, gnorm=0.337, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59368
2023-02-17 08:16:18 - progress_bar.py[line:274] - INFO: epoch 002:    783 / 11564 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.9, wpb=111.2, bsz=40, num_updates=12330, lr=4.65297e-05, gnorm=0.299, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59379
2023-02-17 08:16:29 - progress_bar.py[line:274] - INFO: epoch 002:    793 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.3, ups=0.87, wpb=111.4, bsz=40, num_updates=12340, lr=4.65252e-05, gnorm=0.262, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59390
2023-02-17 08:16:40 - progress_bar.py[line:274] - INFO: epoch 002:    803 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=112.1, bsz=40, num_updates=12350, lr=4.65207e-05, gnorm=0.304, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59401
2023-02-17 08:16:52 - progress_bar.py[line:274] - INFO: epoch 002:    813 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=112, bsz=40, num_updates=12360, lr=4.65162e-05, gnorm=0.325, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=59412
2023-02-17 08:17:03 - progress_bar.py[line:274] - INFO: epoch 002:    823 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=111.9, bsz=40, num_updates=12370, lr=4.65117e-05, gnorm=0.264, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59424
2023-02-17 08:17:14 - progress_bar.py[line:274] - INFO: epoch 002:    833 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112.1, bsz=40, num_updates=12380, lr=4.65072e-05, gnorm=0.278, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59435
2023-02-17 08:17:25 - progress_bar.py[line:274] - INFO: epoch 002:    843 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.88, wpb=113.6, bsz=40, num_updates=12390, lr=4.65027e-05, gnorm=0.392, clip=0, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=59446
2023-02-17 08:17:37 - progress_bar.py[line:274] - INFO: epoch 002:    853 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.88, wpb=112.1, bsz=40, num_updates=12400, lr=4.64982e-05, gnorm=0.418, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59458
2023-02-17 08:17:48 - progress_bar.py[line:274] - INFO: epoch 002:    863 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.91, wpb=112.4, bsz=40, num_updates=12410, lr=4.64937e-05, gnorm=0.446, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59469
2023-02-17 08:17:59 - progress_bar.py[line:274] - INFO: epoch 002:    873 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.91, wpb=112, bsz=40, num_updates=12420, lr=4.64892e-05, gnorm=0.326, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59480
2023-02-17 08:18:10 - progress_bar.py[line:274] - INFO: epoch 002:    883 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=112.4, bsz=40, num_updates=12430, lr=4.64847e-05, gnorm=0.394, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59491
2023-02-17 08:18:21 - progress_bar.py[line:274] - INFO: epoch 002:    893 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.91, wpb=110.7, bsz=40, num_updates=12440, lr=4.64802e-05, gnorm=0.451, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59502
2023-02-17 08:18:32 - progress_bar.py[line:274] - INFO: epoch 002:    903 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.89, wpb=110.8, bsz=40, num_updates=12450, lr=4.64757e-05, gnorm=0.307, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=59513
2023-02-17 08:18:44 - progress_bar.py[line:274] - INFO: epoch 002:    913 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.88, wpb=112.6, bsz=40, num_updates=12460, lr=4.64712e-05, gnorm=0.296, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59525
2023-02-17 08:18:55 - progress_bar.py[line:274] - INFO: epoch 002:    923 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.88, wpb=113.1, bsz=40, num_updates=12470, lr=4.64667e-05, gnorm=0.362, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59536
2023-02-17 08:19:06 - progress_bar.py[line:274] - INFO: epoch 002:    933 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.88, wpb=111.9, bsz=40, num_updates=12480, lr=4.64622e-05, gnorm=0.337, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59547
2023-02-17 08:19:17 - progress_bar.py[line:274] - INFO: epoch 002:    943 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.91, wpb=112.8, bsz=40, num_updates=12490, lr=4.64577e-05, gnorm=0.416, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59558
2023-02-17 08:19:28 - progress_bar.py[line:274] - INFO: epoch 002:    953 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.92, wpb=111.9, bsz=40, num_updates=12500, lr=4.64532e-05, gnorm=0.294, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59569
2023-02-17 08:19:39 - progress_bar.py[line:274] - INFO: epoch 002:    963 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.91, wpb=113.3, bsz=40, num_updates=12510, lr=4.64487e-05, gnorm=0.302, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59580
2023-02-17 08:19:50 - progress_bar.py[line:274] - INFO: epoch 002:    973 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.92, wpb=111.5, bsz=40, num_updates=12520, lr=4.64442e-05, gnorm=0.312, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59591
2023-02-17 08:20:01 - progress_bar.py[line:274] - INFO: epoch 002:    983 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.91, wpb=112.1, bsz=40, num_updates=12530, lr=4.64397e-05, gnorm=0.42, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59602
2023-02-17 08:20:12 - progress_bar.py[line:274] - INFO: epoch 002:    993 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=111, bsz=40, num_updates=12540, lr=4.64352e-05, gnorm=0.478, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=59613
2023-02-17 08:20:24 - progress_bar.py[line:274] - INFO: epoch 002:   1003 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.87, wpb=112.4, bsz=40, num_updates=12550, lr=4.64307e-05, gnorm=0.444, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59625
2023-02-17 08:20:35 - progress_bar.py[line:274] - INFO: epoch 002:   1013 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.9, wpb=112.4, bsz=40, num_updates=12560, lr=4.64262e-05, gnorm=0.37, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59636
2023-02-17 08:20:46 - progress_bar.py[line:274] - INFO: epoch 002:   1023 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.88, wpb=113.6, bsz=40, num_updates=12570, lr=4.64217e-05, gnorm=0.313, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59647
2023-02-17 08:20:57 - progress_bar.py[line:274] - INFO: epoch 002:   1033 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.5, bsz=40, num_updates=12580, lr=4.64172e-05, gnorm=0.34, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59658
2023-02-17 08:21:09 - progress_bar.py[line:274] - INFO: epoch 002:   1043 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=111.6, bsz=40, num_updates=12590, lr=4.64126e-05, gnorm=0.316, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=59670
2023-02-17 08:21:20 - progress_bar.py[line:274] - INFO: epoch 002:   1053 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.91, wpb=111.6, bsz=40, num_updates=12600, lr=4.64081e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59681
2023-02-17 08:21:31 - progress_bar.py[line:274] - INFO: epoch 002:   1063 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=112.7, bsz=40, num_updates=12610, lr=4.64036e-05, gnorm=0.453, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59692
2023-02-17 08:21:42 - progress_bar.py[line:274] - INFO: epoch 002:   1073 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.92, wpb=112.5, bsz=40, num_updates=12620, lr=4.63991e-05, gnorm=0.321, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59703
2023-02-17 08:21:53 - progress_bar.py[line:274] - INFO: epoch 002:   1083 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.89, wpb=112.6, bsz=40, num_updates=12630, lr=4.63946e-05, gnorm=0.393, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59714
2023-02-17 08:22:04 - progress_bar.py[line:274] - INFO: epoch 002:   1093 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.91, wpb=112.4, bsz=40, num_updates=12640, lr=4.63901e-05, gnorm=0.442, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=59725
2023-02-17 08:22:15 - progress_bar.py[line:274] - INFO: epoch 002:   1103 / 11564 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=111.5, bsz=40, num_updates=12650, lr=4.63856e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59736
2023-02-17 08:22:26 - progress_bar.py[line:274] - INFO: epoch 002:   1113 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.2, ups=0.91, wpb=113.9, bsz=40, num_updates=12660, lr=4.63811e-05, gnorm=0.374, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59747
2023-02-17 08:22:37 - progress_bar.py[line:274] - INFO: epoch 002:   1123 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.8, ups=0.93, wpb=111.8, bsz=40, num_updates=12670, lr=4.63766e-05, gnorm=0.392, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59758
2023-02-17 08:22:48 - progress_bar.py[line:274] - INFO: epoch 002:   1133 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.9, wpb=111.5, bsz=40, num_updates=12680, lr=4.63721e-05, gnorm=0.421, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59769
2023-02-17 08:22:59 - progress_bar.py[line:274] - INFO: epoch 002:   1143 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112.4, bsz=40, num_updates=12690, lr=4.63676e-05, gnorm=0.382, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59780
2023-02-17 08:23:10 - progress_bar.py[line:274] - INFO: epoch 002:   1153 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.92, wpb=112.3, bsz=40, num_updates=12700, lr=4.63631e-05, gnorm=0.452, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59791
2023-02-17 08:23:21 - progress_bar.py[line:274] - INFO: epoch 002:   1163 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.5, ups=0.91, wpb=112.6, bsz=40, num_updates=12710, lr=4.63586e-05, gnorm=0.466, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59802
2023-02-17 08:23:33 - progress_bar.py[line:274] - INFO: epoch 002:   1173 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.88, wpb=111.7, bsz=40, num_updates=12720, lr=4.63541e-05, gnorm=0.265, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59813
2023-02-17 08:23:43 - progress_bar.py[line:274] - INFO: epoch 002:   1183 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.92, wpb=110.6, bsz=40, num_updates=12730, lr=4.63496e-05, gnorm=0.293, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59824
2023-02-17 08:23:55 - progress_bar.py[line:274] - INFO: epoch 002:   1193 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=112.3, bsz=40, num_updates=12740, lr=4.63451e-05, gnorm=0.462, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59836
2023-02-17 08:24:06 - progress_bar.py[line:274] - INFO: epoch 002:   1203 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.6, ups=0.92, wpb=111.6, bsz=40, num_updates=12750, lr=4.63406e-05, gnorm=0.297, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59846
2023-02-17 08:24:17 - progress_bar.py[line:274] - INFO: epoch 002:   1213 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.9, wpb=110.9, bsz=40, num_updates=12760, lr=4.63361e-05, gnorm=0.349, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59858
2023-02-17 08:24:28 - progress_bar.py[line:274] - INFO: epoch 002:   1223 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.92, wpb=112.2, bsz=40, num_updates=12770, lr=4.63316e-05, gnorm=0.486, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59868
2023-02-17 08:24:39 - progress_bar.py[line:274] - INFO: epoch 002:   1233 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.9, wpb=111.3, bsz=40, num_updates=12780, lr=4.63271e-05, gnorm=0.384, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59880
2023-02-17 08:24:50 - progress_bar.py[line:274] - INFO: epoch 002:   1243 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.9, wpb=113.2, bsz=40, num_updates=12790, lr=4.63226e-05, gnorm=0.41, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59891
2023-02-17 08:25:01 - progress_bar.py[line:274] - INFO: epoch 002:   1253 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.87, wpb=113.2, bsz=40, num_updates=12800, lr=4.63181e-05, gnorm=0.367, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59902
2023-02-17 08:25:12 - progress_bar.py[line:274] - INFO: epoch 002:   1263 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.9, wpb=112.5, bsz=40, num_updates=12810, lr=4.63136e-05, gnorm=0.493, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59913
2023-02-17 08:25:24 - progress_bar.py[line:274] - INFO: epoch 002:   1273 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.89, wpb=113.4, bsz=40, num_updates=12820, lr=4.63091e-05, gnorm=0.504, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59924
2023-02-17 08:25:35 - progress_bar.py[line:274] - INFO: epoch 002:   1283 / 11564 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.2, ups=0.91, wpb=111.6, bsz=40, num_updates=12830, lr=4.63046e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59935
2023-02-17 08:25:46 - progress_bar.py[line:274] - INFO: epoch 002:   1293 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.8, bsz=40, num_updates=12840, lr=4.63e-05, gnorm=0.358, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59947
2023-02-17 08:25:56 - progress_bar.py[line:274] - INFO: epoch 002:   1303 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.6, ups=0.94, wpb=111, bsz=40, num_updates=12850, lr=4.62955e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59957
2023-02-17 08:26:07 - progress_bar.py[line:274] - INFO: epoch 002:   1313 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111.9, bsz=40, num_updates=12860, lr=4.6291e-05, gnorm=0.319, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59968
2023-02-17 08:26:19 - progress_bar.py[line:274] - INFO: epoch 002:   1323 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.2, ups=0.91, wpb=110.5, bsz=40, num_updates=12870, lr=4.62865e-05, gnorm=0.413, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59979
2023-02-17 08:26:29 - progress_bar.py[line:274] - INFO: epoch 002:   1333 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.92, wpb=112, bsz=40, num_updates=12880, lr=4.6282e-05, gnorm=0.253, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59990
2023-02-17 08:26:40 - progress_bar.py[line:274] - INFO: epoch 002:   1343 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.91, wpb=111.4, bsz=40, num_updates=12890, lr=4.62775e-05, gnorm=0.41, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=60001
2023-02-17 08:26:52 - progress_bar.py[line:274] - INFO: epoch 002:   1353 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.89, wpb=111.7, bsz=40, num_updates=12900, lr=4.6273e-05, gnorm=0.372, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=60013
2023-02-17 08:27:03 - progress_bar.py[line:274] - INFO: epoch 002:   1363 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=112.8, bsz=40, num_updates=12910, lr=4.62685e-05, gnorm=0.364, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=60024
2023-02-17 08:27:14 - progress_bar.py[line:274] - INFO: epoch 002:   1373 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.92, wpb=111.2, bsz=40, num_updates=12920, lr=4.6264e-05, gnorm=0.401, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=60035
2023-02-17 08:27:25 - progress_bar.py[line:274] - INFO: epoch 002:   1383 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.91, wpb=112, bsz=40, num_updates=12930, lr=4.62595e-05, gnorm=0.327, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=60046
2023-02-17 08:27:36 - progress_bar.py[line:274] - INFO: epoch 002:   1393 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104, ups=0.93, wpb=111.8, bsz=40, num_updates=12940, lr=4.6255e-05, gnorm=0.368, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=60057
2023-02-17 08:27:47 - progress_bar.py[line:274] - INFO: epoch 002:   1403 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=112.8, bsz=40, num_updates=12950, lr=4.62505e-05, gnorm=0.338, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=60068
2023-02-17 08:27:58 - progress_bar.py[line:274] - INFO: epoch 002:   1413 / 11564 loss=0.18, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.3, ups=0.9, wpb=111.9, bsz=40, num_updates=12960, lr=4.6246e-05, gnorm=0.216, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=60079
2023-02-17 08:28:09 - progress_bar.py[line:274] - INFO: epoch 002:   1423 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=111.6, bsz=40, num_updates=12970, lr=4.62415e-05, gnorm=0.3, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=60090
2023-02-17 08:28:20 - progress_bar.py[line:274] - INFO: epoch 002:   1433 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=112.1, bsz=40, num_updates=12980, lr=4.6237e-05, gnorm=0.402, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=60101
2023-02-17 08:28:31 - progress_bar.py[line:274] - INFO: epoch 002:   1443 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.91, wpb=111.6, bsz=40, num_updates=12990, lr=4.62325e-05, gnorm=0.332, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=60112
2023-02-17 08:28:42 - progress_bar.py[line:274] - INFO: epoch 002:   1453 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.91, wpb=113.4, bsz=40, num_updates=13000, lr=4.6228e-05, gnorm=0.323, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=60123
2023-02-17 08:28:42 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 08:28:44 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 08:28:44 - train.py[line:551] - INFO: load:1.04 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 08:30:46 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 08:30:46 - train.py[line:551] - INFO: load:1.07 valid_run:121.97 task_valid:119.17 collect_output:1.74
2023-02-17 08:32:45 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 08:32:45 - train.py[line:551] - INFO: load:1.09 valid_run:241.68 task_valid:234.89 collect_output:4.73
2023-02-17 08:34:47 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 08:34:47 - train.py[line:551] - INFO: load:1.12 valid_run:363.54 task_valid:351.28 collect_output:9.17
2023-02-17 08:36:49 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 08:36:49 - train.py[line:551] - INFO: load:1.14 valid_run:485.28 task_valid:464.93 collect_output:16.26
2023-02-17 08:38:49 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 08:38:49 - train.py[line:551] - INFO: load:1.16 valid_run:605.48 task_valid:582.02 collect_output:18.39
2023-02-17 08:40:52 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 08:40:52 - train.py[line:551] - INFO: load:1.19 valid_run:728.25 task_valid:700.52 collect_output:21.66
2023-02-17 08:42:55 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 08:42:55 - train.py[line:551] - INFO: load:1.21 valid_run:851.04 task_valid:818.33 collect_output:25.65
2023-02-17 08:44:57 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 08:44:57 - train.py[line:551] - INFO: load:1.24 valid_run:972.80 task_valid:934.74 collect_output:29.99
2023-02-17 08:47:00 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 08:47:00 - train.py[line:551] - INFO: load:1.26 valid_run:1096.36 task_valid:1051.74 collect_output:35.56
2023-02-17 08:49:02 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 08:49:02 - train.py[line:551] - INFO: load:1.29 valid_run:1218.10 task_valid:1164.38 collect_output:43.65
2023-02-17 08:51:02 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 08:51:02 - train.py[line:551] - INFO: load:1.31 valid_run:1337.99 task_valid:1279.80 collect_output:47.12
2023-02-17 08:53:04 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 08:53:04 - train.py[line:551] - INFO: load:1.34 valid_run:1459.69 task_valid:1396.76 collect_output:50.83
2023-02-17 08:55:03 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 08:55:03 - train.py[line:551] - INFO: load:1.37 valid_run:1578.54 task_valid:1510.40 collect_output:55.02
2023-02-17 08:57:04 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 08:57:04 - train.py[line:551] - INFO: load:1.39 valid_run:1699.39 task_valid:1627.93 collect_output:57.34
2023-02-17 08:59:05 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 08:59:05 - train.py[line:551] - INFO: load:1.42 valid_run:1820.21 task_valid:1743.78 collect_output:61.29
2023-02-17 09:01:06 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 09:01:06 - train.py[line:551] - INFO: load:1.44 valid_run:1941.06 task_valid:1857.51 collect_output:67.41
2023-02-17 09:03:07 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 09:03:07 - train.py[line:551] - INFO: load:1.47 valid_run:2062.27 task_valid:1973.37 collect_output:71.74
2023-02-17 09:05:07 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 09:05:07 - train.py[line:551] - INFO: load:1.49 valid_run:2182.81 task_valid:2091.12 collect_output:73.52
2023-02-17 09:07:09 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 09:07:09 - train.py[line:551] - INFO: load:1.52 valid_run:2303.98 task_valid:2207.92 collect_output:76.88
2023-02-17 09:09:09 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 09:09:09 - train.py[line:551] - INFO: load:1.54 valid_run:2424.23 task_valid:2324.28 collect_output:79.77
2023-02-17 09:11:11 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 09:11:11 - train.py[line:551] - INFO: load:1.57 valid_run:2545.81 task_valid:2440.79 collect_output:83.84
2023-02-17 09:13:13 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 09:13:13 - train.py[line:551] - INFO: load:1.60 valid_run:2667.69 task_valid:2559.55 collect_output:85.95
2023-02-17 09:15:13 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 09:15:13 - train.py[line:551] - INFO: load:1.62 valid_run:2788.03 task_valid:2673.73 collect_output:91.11
2023-02-17 09:17:13 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 09:17:13 - train.py[line:551] - INFO: load:1.65 valid_run:2907.81 task_valid:2789.73 collect_output:93.88
2023-02-17 09:19:14 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 09:19:14 - train.py[line:551] - INFO: load:1.67 valid_run:3029.41 task_valid:2905.91 collect_output:98.30
2023-02-17 09:21:17 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 09:21:17 - train.py[line:551] - INFO: load:1.70 valid_run:3152.25 task_valid:3021.80 collect_output:104.23
2023-02-17 09:23:17 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 09:23:17 - train.py[line:551] - INFO: load:1.72 valid_run:3271.88 task_valid:3135.88 collect_output:108.78
2023-02-17 09:25:19 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 09:25:19 - train.py[line:551] - INFO: load:1.75 valid_run:3393.81 task_valid:3255.33 collect_output:110.27
2023-02-17 09:27:21 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 09:27:21 - train.py[line:551] - INFO: load:1.77 valid_run:3515.45 task_valid:3370.84 collect_output:115.36
2023-02-17 09:29:23 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 09:29:23 - train.py[line:551] - INFO: load:1.80 valid_run:3637.27 task_valid:3489.17 collect_output:117.85
2023-02-17 09:31:24 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 09:31:24 - train.py[line:551] - INFO: load:1.82 valid_run:3758.38 task_valid:3607.64 collect_output:119.49

====================================================================================================
SGG eval:     R @ 50: 0.6218;     R @ 100: 0.6631;     R @ 500: 0.6860;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4241;    mR @ 100: 0.4685;    mR @ 500: 0.5135;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.6875) (covering:0.2286) (eating:0.7647) (flying in:0.8182) (growing on:0.3750) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9742) (says:0.0000) (sitting on:0.7188) (standing on:0.3743) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.4028) 
--------------------------------------------------------
====================================================================================================

2023-02-17 09:31:54 - train.py[line:487] - INFO: 0.6631351922587216

====================================================================================================
SGG eval:     R @ 50: 0.6218;     R @ 100: 0.6631;     R @ 500: 0.6860;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4241;    mR @ 100: 0.4685;    mR @ 500: 0.5135;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.6875) (covering:0.2286) (eating:0.7647) (flying in:0.8182) (growing on:0.3750) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9742) (says:0.0000) (sitting on:0.7188) (standing on:0.3743) (using:0.6000) (walking in:0.0000) (walking on:0.6486) (watching:0.4028) 
--------------------------------------------------------
====================================================================================================

2023-02-17 09:31:54 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 09:31:54 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.261 | loss_v1 0 | loss_v2 0 | nll_loss 0.099 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.663135 | ppl 1.07 | vqa_score 0.5394 | wps 118.3 | wpb 72 | bsz 24 | num_updates 13000 | best_R@100 0.69202
2023-02-17 09:31:55 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 13000 updates
2023-02-17 09:31:55 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_13000.pt
2023-02-17 09:32:00 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_13000.pt
2023-02-17 09:32:02 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_13000.pt (epoch 2 @ 13000 updates, score 0.6631351922587216) (writing took 7.778195489197969 seconds)
2023-02-17 09:32:13 - progress_bar.py[line:274] - INFO: epoch 002:   1463 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=111.5, bsz=40, num_updates=13010, lr=4.62235e-05, gnorm=0.384, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=63934
2023-02-17 09:32:16 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-17 09:32:25 - progress_bar.py[line:274] - INFO: epoch 002:   1474 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96, ups=0.85, wpb=112.8, bsz=40, num_updates=13020, lr=4.6219e-05, gnorm=0.404, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=63946
2023-02-17 09:32:36 - progress_bar.py[line:274] - INFO: epoch 002:   1484 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.8, ups=0.92, wpb=113, bsz=40, num_updates=13030, lr=4.62145e-05, gnorm=0.364, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=63957
2023-02-17 09:32:47 - progress_bar.py[line:274] - INFO: epoch 002:   1494 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.92, wpb=112.7, bsz=40, num_updates=13040, lr=4.621e-05, gnorm=0.386, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=63968
2023-02-17 09:32:58 - progress_bar.py[line:274] - INFO: epoch 002:   1504 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.89, wpb=112.4, bsz=40, num_updates=13050, lr=4.62055e-05, gnorm=0.388, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=63979
2023-02-17 09:33:09 - progress_bar.py[line:274] - INFO: epoch 002:   1514 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.88, wpb=112.7, bsz=40, num_updates=13060, lr=4.6201e-05, gnorm=0.402, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=63990
2023-02-17 09:33:21 - progress_bar.py[line:274] - INFO: epoch 002:   1524 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=111.6, bsz=40, num_updates=13070, lr=4.61965e-05, gnorm=0.392, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64002
2023-02-17 09:33:32 - progress_bar.py[line:274] - INFO: epoch 002:   1534 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.92, wpb=111.8, bsz=40, num_updates=13080, lr=4.6192e-05, gnorm=0.242, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64013
2023-02-17 09:33:43 - progress_bar.py[line:274] - INFO: epoch 002:   1544 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.91, wpb=111.2, bsz=40, num_updates=13090, lr=4.61875e-05, gnorm=0.381, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=64024
2023-02-17 09:33:54 - progress_bar.py[line:274] - INFO: epoch 002:   1554 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.89, wpb=112.2, bsz=40, num_updates=13100, lr=4.61829e-05, gnorm=0.388, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64035
2023-02-17 09:34:05 - progress_bar.py[line:274] - INFO: epoch 002:   1564 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.91, wpb=114.1, bsz=40, num_updates=13110, lr=4.61784e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=64046
2023-02-17 09:34:16 - progress_bar.py[line:274] - INFO: epoch 002:   1574 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=111.9, bsz=40, num_updates=13120, lr=4.61739e-05, gnorm=0.371, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64057
2023-02-17 09:34:28 - progress_bar.py[line:274] - INFO: epoch 002:   1584 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=111.5, bsz=40, num_updates=13130, lr=4.61694e-05, gnorm=0.392, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=64069
2023-02-17 09:34:38 - progress_bar.py[line:274] - INFO: epoch 002:   1594 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.2, ups=0.93, wpb=112.8, bsz=40, num_updates=13140, lr=4.61649e-05, gnorm=0.448, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=64079
2023-02-17 09:34:50 - progress_bar.py[line:274] - INFO: epoch 002:   1604 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.87, wpb=113.9, bsz=40, num_updates=13150, lr=4.61604e-05, gnorm=0.329, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64091
2023-02-17 09:35:01 - progress_bar.py[line:274] - INFO: epoch 002:   1614 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.7, ups=0.93, wpb=112.4, bsz=40, num_updates=13160, lr=4.61559e-05, gnorm=0.387, clip=0, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=64102
2023-02-17 09:35:12 - progress_bar.py[line:274] - INFO: epoch 002:   1624 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=111.6, bsz=40, num_updates=13170, lr=4.61514e-05, gnorm=0.347, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=64113
2023-02-17 09:35:23 - progress_bar.py[line:274] - INFO: epoch 002:   1634 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.88, wpb=112.3, bsz=40, num_updates=13180, lr=4.61469e-05, gnorm=0.394, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64124
2023-02-17 09:35:35 - progress_bar.py[line:274] - INFO: epoch 002:   1644 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.89, wpb=112.7, bsz=40, num_updates=13190, lr=4.61424e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=64135
2023-02-17 09:35:46 - progress_bar.py[line:274] - INFO: epoch 002:   1654 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.9, wpb=110.4, bsz=40, num_updates=13200, lr=4.61379e-05, gnorm=0.348, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64147
2023-02-17 09:35:57 - progress_bar.py[line:274] - INFO: epoch 002:   1664 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=112.4, bsz=40, num_updates=13210, lr=4.61334e-05, gnorm=0.324, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64158
2023-02-17 09:36:08 - progress_bar.py[line:274] - INFO: epoch 002:   1674 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=112.3, bsz=40, num_updates=13220, lr=4.61289e-05, gnorm=0.392, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64169
2023-02-17 09:36:19 - progress_bar.py[line:274] - INFO: epoch 002:   1684 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.9, wpb=111.6, bsz=40, num_updates=13230, lr=4.61244e-05, gnorm=0.444, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64180
2023-02-17 09:36:30 - progress_bar.py[line:274] - INFO: epoch 002:   1694 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.9, wpb=113.5, bsz=40, num_updates=13240, lr=4.61199e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64191
2023-02-17 09:36:41 - progress_bar.py[line:274] - INFO: epoch 002:   1704 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.3, ups=0.88, wpb=110.3, bsz=40, num_updates=13250, lr=4.61154e-05, gnorm=0.335, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64202
2023-02-17 09:36:52 - progress_bar.py[line:274] - INFO: epoch 002:   1714 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.91, wpb=111.7, bsz=40, num_updates=13260, lr=4.61109e-05, gnorm=0.364, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64213
2023-02-17 09:37:04 - progress_bar.py[line:274] - INFO: epoch 002:   1724 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.89, wpb=110.2, bsz=40, num_updates=13270, lr=4.61064e-05, gnorm=0.297, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=64225
2023-02-17 09:37:15 - progress_bar.py[line:274] - INFO: epoch 002:   1734 / 11564 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.88, wpb=112.8, bsz=40, num_updates=13280, lr=4.61019e-05, gnorm=0.337, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64236
2023-02-17 09:37:26 - progress_bar.py[line:274] - INFO: epoch 002:   1744 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.92, wpb=112.3, bsz=40, num_updates=13290, lr=4.60974e-05, gnorm=0.384, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64247
2023-02-17 09:37:37 - progress_bar.py[line:274] - INFO: epoch 002:   1754 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.4, ups=0.91, wpb=113, bsz=40, num_updates=13300, lr=4.60929e-05, gnorm=0.387, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64258
2023-02-17 09:37:48 - progress_bar.py[line:274] - INFO: epoch 002:   1764 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=112.6, bsz=40, num_updates=13310, lr=4.60884e-05, gnorm=0.323, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64269
2023-02-17 09:37:59 - progress_bar.py[line:274] - INFO: epoch 002:   1774 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.91, wpb=111.8, bsz=40, num_updates=13320, lr=4.60839e-05, gnorm=0.303, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64280
2023-02-17 09:38:10 - progress_bar.py[line:274] - INFO: epoch 002:   1784 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=112.3, bsz=40, num_updates=13330, lr=4.60794e-05, gnorm=0.269, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64291
2023-02-17 09:38:21 - progress_bar.py[line:274] - INFO: epoch 002:   1794 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=111.7, bsz=40, num_updates=13340, lr=4.60749e-05, gnorm=0.36, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64302
2023-02-17 09:38:32 - progress_bar.py[line:274] - INFO: epoch 002:   1804 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.92, wpb=110.6, bsz=40, num_updates=13350, lr=4.60704e-05, gnorm=0.31, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=64313
2023-02-17 09:38:43 - progress_bar.py[line:274] - INFO: epoch 002:   1814 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.92, wpb=111.3, bsz=40, num_updates=13360, lr=4.60658e-05, gnorm=0.334, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64324
2023-02-17 09:38:54 - progress_bar.py[line:274] - INFO: epoch 002:   1824 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.9, wpb=111.3, bsz=40, num_updates=13370, lr=4.60613e-05, gnorm=0.4, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=64335
2023-02-17 09:39:05 - progress_bar.py[line:274] - INFO: epoch 002:   1834 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.9, wpb=111.6, bsz=40, num_updates=13380, lr=4.60568e-05, gnorm=0.297, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64346
2023-02-17 09:39:17 - progress_bar.py[line:274] - INFO: epoch 002:   1844 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.88, wpb=113, bsz=40, num_updates=13390, lr=4.60523e-05, gnorm=0.356, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64358
2023-02-17 09:39:28 - progress_bar.py[line:274] - INFO: epoch 002:   1854 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.89, wpb=111.3, bsz=40, num_updates=13400, lr=4.60478e-05, gnorm=0.41, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=64369
2023-02-17 09:39:39 - progress_bar.py[line:274] - INFO: epoch 002:   1864 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.6, ups=0.93, wpb=112.2, bsz=40, num_updates=13410, lr=4.60433e-05, gnorm=0.35, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64380
2023-02-17 09:39:50 - progress_bar.py[line:274] - INFO: epoch 002:   1874 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.89, wpb=112.1, bsz=40, num_updates=13420, lr=4.60388e-05, gnorm=0.362, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64391
2023-02-17 09:40:01 - progress_bar.py[line:274] - INFO: epoch 002:   1884 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.9, wpb=112.6, bsz=40, num_updates=13430, lr=4.60343e-05, gnorm=0.285, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=64402
2023-02-17 09:40:12 - progress_bar.py[line:274] - INFO: epoch 002:   1894 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.91, wpb=111, bsz=40, num_updates=13440, lr=4.60298e-05, gnorm=0.233, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=64413
2023-02-17 09:40:23 - progress_bar.py[line:274] - INFO: epoch 002:   1904 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.89, wpb=113, bsz=40, num_updates=13450, lr=4.60253e-05, gnorm=0.336, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64424
2023-02-17 09:40:35 - progress_bar.py[line:274] - INFO: epoch 002:   1914 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.88, wpb=113.2, bsz=40, num_updates=13460, lr=4.60208e-05, gnorm=0.277, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64435
2023-02-17 09:40:45 - progress_bar.py[line:274] - INFO: epoch 002:   1924 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.3, ups=0.92, wpb=112.9, bsz=40, num_updates=13470, lr=4.60163e-05, gnorm=0.405, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=64446
2023-02-17 09:40:57 - progress_bar.py[line:274] - INFO: epoch 002:   1934 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.88, wpb=112, bsz=40, num_updates=13480, lr=4.60118e-05, gnorm=0.354, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=64458
2023-02-17 09:41:08 - progress_bar.py[line:274] - INFO: epoch 002:   1944 / 11564 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.054, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.91, wpb=112.5, bsz=40, num_updates=13490, lr=4.60073e-05, gnorm=0.28, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64469
2023-02-17 09:41:19 - progress_bar.py[line:274] - INFO: epoch 002:   1954 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.3, ups=0.93, wpb=112.4, bsz=40, num_updates=13500, lr=4.60028e-05, gnorm=0.456, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=64479
2023-02-17 09:41:29 - progress_bar.py[line:274] - INFO: epoch 002:   1964 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.9, ups=0.91, wpb=112.8, bsz=40, num_updates=13510, lr=4.59983e-05, gnorm=0.389, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64490
2023-02-17 09:41:41 - progress_bar.py[line:274] - INFO: epoch 002:   1974 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.5, ups=0.87, wpb=111.8, bsz=40, num_updates=13520, lr=4.59938e-05, gnorm=0.355, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64502
2023-02-17 09:41:52 - progress_bar.py[line:274] - INFO: epoch 002:   1984 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.89, wpb=113.3, bsz=40, num_updates=13530, lr=4.59893e-05, gnorm=0.337, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64513
2023-02-17 09:42:04 - progress_bar.py[line:274] - INFO: epoch 002:   1994 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.88, wpb=112.4, bsz=40, num_updates=13540, lr=4.59848e-05, gnorm=0.325, clip=0, loss_scale=2048, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64524
2023-02-17 09:42:15 - progress_bar.py[line:274] - INFO: epoch 002:   2004 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.91, wpb=110.8, bsz=40, num_updates=13550, lr=4.59803e-05, gnorm=0.347, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64536
2023-02-17 09:42:25 - progress_bar.py[line:274] - INFO: epoch 002:   2014 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.92, wpb=112.3, bsz=40, num_updates=13560, lr=4.59758e-05, gnorm=0.505, clip=10, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64546
2023-02-17 09:42:36 - progress_bar.py[line:274] - INFO: epoch 002:   2024 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.7, ups=0.92, wpb=111.7, bsz=40, num_updates=13570, lr=4.59713e-05, gnorm=0.374, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64557
2023-02-17 09:42:48 - progress_bar.py[line:274] - INFO: epoch 002:   2034 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=13580, lr=4.59668e-05, gnorm=0.319, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64569
2023-02-17 09:42:59 - progress_bar.py[line:274] - INFO: epoch 002:   2044 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.88, wpb=111.9, bsz=40, num_updates=13590, lr=4.59623e-05, gnorm=0.421, clip=0, loss_scale=2048, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=64580
2023-02-17 09:43:10 - progress_bar.py[line:274] - INFO: epoch 002:   2054 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.91, wpb=111.1, bsz=40, num_updates=13600, lr=4.59578e-05, gnorm=0.273, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64591
2023-02-17 09:43:21 - progress_bar.py[line:274] - INFO: epoch 002:   2064 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.89, wpb=111.9, bsz=40, num_updates=13610, lr=4.59532e-05, gnorm=0.355, clip=0, loss_scale=2048, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=64602
2023-02-17 09:43:32 - progress_bar.py[line:274] - INFO: epoch 002:   2074 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.9, wpb=112.6, bsz=40, num_updates=13620, lr=4.59487e-05, gnorm=0.281, clip=0, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64613
2023-02-17 09:43:43 - progress_bar.py[line:274] - INFO: epoch 002:   2084 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.91, wpb=111.3, bsz=40, num_updates=13630, lr=4.59442e-05, gnorm=0.266, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=64624
2023-02-17 09:43:55 - progress_bar.py[line:274] - INFO: epoch 002:   2094 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=112, bsz=40, num_updates=13640, lr=4.59397e-05, gnorm=0.324, clip=0, loss_scale=2048, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=64635
2023-02-17 09:44:02 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-17 09:44:07 - progress_bar.py[line:274] - INFO: epoch 002:   2105 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=90.5, ups=0.81, wpb=111.1, bsz=40, num_updates=13650, lr=4.59352e-05, gnorm=0.411, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=64648
2023-02-17 09:44:18 - progress_bar.py[line:274] - INFO: epoch 002:   2115 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.88, wpb=112.2, bsz=40, num_updates=13660, lr=4.59307e-05, gnorm=0.396, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64659
2023-02-17 09:44:29 - progress_bar.py[line:274] - INFO: epoch 002:   2125 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=112.2, bsz=40, num_updates=13670, lr=4.59262e-05, gnorm=0.378, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64670
2023-02-17 09:44:40 - progress_bar.py[line:274] - INFO: epoch 002:   2135 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.9, wpb=113.3, bsz=40, num_updates=13680, lr=4.59217e-05, gnorm=0.361, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64681
2023-02-17 09:44:51 - progress_bar.py[line:274] - INFO: epoch 002:   2145 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.91, wpb=111.6, bsz=40, num_updates=13690, lr=4.59172e-05, gnorm=0.264, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64692
2023-02-17 09:45:03 - progress_bar.py[line:274] - INFO: epoch 002:   2155 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=112.6, bsz=40, num_updates=13700, lr=4.59127e-05, gnorm=0.26, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64704
2023-02-17 09:45:14 - progress_bar.py[line:274] - INFO: epoch 002:   2165 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.9, ups=0.92, wpb=111.9, bsz=40, num_updates=13710, lr=4.59082e-05, gnorm=0.478, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=64714
2023-02-17 09:45:24 - progress_bar.py[line:274] - INFO: epoch 002:   2175 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.92, wpb=111.8, bsz=40, num_updates=13720, lr=4.59037e-05, gnorm=0.343, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=64725
2023-02-17 09:45:35 - progress_bar.py[line:274] - INFO: epoch 002:   2185 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=111.9, bsz=40, num_updates=13730, lr=4.58992e-05, gnorm=0.287, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64736
2023-02-17 09:45:47 - progress_bar.py[line:274] - INFO: epoch 002:   2195 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.88, wpb=112.6, bsz=40, num_updates=13740, lr=4.58947e-05, gnorm=0.356, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64748
2023-02-17 09:45:58 - progress_bar.py[line:274] - INFO: epoch 002:   2205 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=111.8, bsz=40, num_updates=13750, lr=4.58902e-05, gnorm=0.358, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=64759
2023-02-17 09:46:09 - progress_bar.py[line:274] - INFO: epoch 002:   2215 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.88, wpb=112.9, bsz=40, num_updates=13760, lr=4.58857e-05, gnorm=0.247, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64770
2023-02-17 09:46:21 - progress_bar.py[line:274] - INFO: epoch 002:   2225 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.6, ups=0.9, wpb=113.5, bsz=40, num_updates=13770, lr=4.58812e-05, gnorm=0.416, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64781
2023-02-17 09:46:31 - progress_bar.py[line:274] - INFO: epoch 002:   2235 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=106.7, ups=0.95, wpb=112.7, bsz=40, num_updates=13780, lr=4.58767e-05, gnorm=0.32, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=64792
2023-02-17 09:46:42 - progress_bar.py[line:274] - INFO: epoch 002:   2245 / 11564 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.9, wpb=112.3, bsz=40, num_updates=13790, lr=4.58722e-05, gnorm=0.285, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64803
2023-02-17 09:46:53 - progress_bar.py[line:274] - INFO: epoch 002:   2255 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.1, ups=0.92, wpb=112, bsz=40, num_updates=13800, lr=4.58677e-05, gnorm=0.291, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64814
2023-02-17 09:47:04 - progress_bar.py[line:274] - INFO: epoch 002:   2265 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.88, wpb=112.7, bsz=40, num_updates=13810, lr=4.58632e-05, gnorm=0.286, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64825
2023-02-17 09:47:16 - progress_bar.py[line:274] - INFO: epoch 002:   2275 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.3, bsz=40, num_updates=13820, lr=4.58587e-05, gnorm=0.344, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64837
2023-02-17 09:47:27 - progress_bar.py[line:274] - INFO: epoch 002:   2285 / 11564 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.5, ups=0.9, wpb=111.9, bsz=40, num_updates=13830, lr=4.58542e-05, gnorm=0.285, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64848
2023-02-17 09:47:38 - progress_bar.py[line:274] - INFO: epoch 002:   2295 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=112.9, bsz=40, num_updates=13840, lr=4.58497e-05, gnorm=0.296, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64859
2023-02-17 09:47:49 - progress_bar.py[line:274] - INFO: epoch 002:   2305 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=111.9, bsz=40, num_updates=13850, lr=4.58452e-05, gnorm=0.394, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64870
2023-02-17 09:48:00 - progress_bar.py[line:274] - INFO: epoch 002:   2315 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.92, wpb=110.7, bsz=40, num_updates=13860, lr=4.58407e-05, gnorm=0.359, clip=0, loss_scale=1024, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=64881
2023-02-17 09:48:11 - progress_bar.py[line:274] - INFO: epoch 002:   2325 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.9, ups=0.92, wpb=112.5, bsz=40, num_updates=13870, lr=4.58361e-05, gnorm=0.467, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64892
2023-02-17 09:48:22 - progress_bar.py[line:274] - INFO: epoch 002:   2335 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.88, wpb=112.4, bsz=40, num_updates=13880, lr=4.58316e-05, gnorm=0.311, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=64903
2023-02-17 09:48:33 - progress_bar.py[line:274] - INFO: epoch 002:   2345 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.9, wpb=111.7, bsz=40, num_updates=13890, lr=4.58271e-05, gnorm=0.314, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64914
2023-02-17 09:48:44 - progress_bar.py[line:274] - INFO: epoch 002:   2355 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.9, wpb=112.9, bsz=40, num_updates=13900, lr=4.58226e-05, gnorm=0.236, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64925
2023-02-17 09:48:55 - progress_bar.py[line:274] - INFO: epoch 002:   2365 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.93, wpb=110.6, bsz=40, num_updates=13910, lr=4.58181e-05, gnorm=0.458, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64936
2023-02-17 09:49:06 - progress_bar.py[line:274] - INFO: epoch 002:   2375 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=110.8, bsz=40, num_updates=13920, lr=4.58136e-05, gnorm=0.395, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64947
2023-02-17 09:49:17 - progress_bar.py[line:274] - INFO: epoch 002:   2385 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=111.6, bsz=40, num_updates=13930, lr=4.58091e-05, gnorm=0.323, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=64958
2023-02-17 09:49:28 - progress_bar.py[line:274] - INFO: epoch 002:   2395 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.92, wpb=112.5, bsz=40, num_updates=13940, lr=4.58046e-05, gnorm=0.349, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64969
2023-02-17 09:49:39 - progress_bar.py[line:274] - INFO: epoch 002:   2405 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.9, ups=0.89, wpb=111.9, bsz=40, num_updates=13950, lr=4.58001e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=64980
2023-02-17 09:49:51 - progress_bar.py[line:274] - INFO: epoch 002:   2415 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.88, wpb=112.3, bsz=40, num_updates=13960, lr=4.57956e-05, gnorm=0.439, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=64992
2023-02-17 09:50:02 - progress_bar.py[line:274] - INFO: epoch 002:   2425 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.9, wpb=111.3, bsz=40, num_updates=13970, lr=4.57911e-05, gnorm=0.294, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=65003
2023-02-17 09:50:13 - progress_bar.py[line:274] - INFO: epoch 002:   2435 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=112.4, bsz=40, num_updates=13980, lr=4.57866e-05, gnorm=0.394, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=65014
2023-02-17 09:50:24 - progress_bar.py[line:274] - INFO: epoch 002:   2445 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=13990, lr=4.57821e-05, gnorm=0.39, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=65025
2023-02-17 09:50:35 - progress_bar.py[line:274] - INFO: epoch 002:   2455 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102, ups=0.92, wpb=111, bsz=40, num_updates=14000, lr=4.57776e-05, gnorm=0.277, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=65036
2023-02-17 09:50:35 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 09:50:36 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 09:50:36 - train.py[line:551] - INFO: load:1.19 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 09:52:38 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 09:52:38 - train.py[line:551] - INFO: load:1.22 valid_run:122.02 task_valid:119.14 collect_output:1.76
2023-02-17 09:54:38 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 09:54:38 - train.py[line:551] - INFO: load:1.24 valid_run:241.86 task_valid:234.87 collect_output:4.84
2023-02-17 09:56:40 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 09:56:40 - train.py[line:551] - INFO: load:1.27 valid_run:363.82 task_valid:351.31 collect_output:9.35
2023-02-17 09:58:42 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 09:58:42 - train.py[line:551] - INFO: load:1.29 valid_run:485.73 task_valid:464.87 collect_output:16.69
2023-02-17 10:00:43 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 10:00:43 - train.py[line:551] - INFO: load:1.32 valid_run:605.99 task_valid:581.99 collect_output:18.82
2023-02-17 10:02:45 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 10:02:45 - train.py[line:551] - INFO: load:1.35 valid_run:728.71 task_valid:700.52 collect_output:21.97
2023-02-17 10:04:48 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 10:04:48 - train.py[line:551] - INFO: load:1.37 valid_run:851.56 task_valid:818.49 collect_output:25.85
2023-02-17 10:06:50 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 10:06:50 - train.py[line:551] - INFO: load:1.40 valid_run:973.19 task_valid:934.81 collect_output:30.16
2023-02-17 10:08:54 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 10:08:54 - train.py[line:551] - INFO: load:1.42 valid_run:1096.78 task_valid:1051.82 collect_output:35.73
2023-02-17 10:10:55 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 10:10:55 - train.py[line:551] - INFO: load:1.45 valid_run:1218.36 task_valid:1164.31 collect_output:43.80
2023-02-17 10:12:55 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 10:12:55 - train.py[line:551] - INFO: load:1.47 valid_run:1338.40 task_valid:1279.73 collect_output:47.41
2023-02-17 10:14:57 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 10:14:57 - train.py[line:551] - INFO: load:1.50 valid_run:1459.88 task_valid:1396.52 collect_output:51.07
2023-02-17 10:16:56 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 10:16:56 - train.py[line:551] - INFO: load:1.53 valid_run:1578.74 task_valid:1510.17 collect_output:55.26
2023-02-17 10:18:57 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 10:18:57 - train.py[line:551] - INFO: load:1.55 valid_run:1699.54 task_valid:1627.75 collect_output:57.48
2023-02-17 10:20:58 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 10:20:58 - train.py[line:551] - INFO: load:1.58 valid_run:1820.42 task_valid:1743.69 collect_output:61.41
2023-02-17 10:22:59 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 10:22:59 - train.py[line:551] - INFO: load:1.60 valid_run:1941.46 task_valid:1857.71 collect_output:67.41
2023-02-17 10:25:00 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 10:25:00 - train.py[line:551] - INFO: load:1.63 valid_run:2062.81 task_valid:1973.76 collect_output:71.69
2023-02-17 10:27:01 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 10:27:01 - train.py[line:551] - INFO: load:1.66 valid_run:2183.35 task_valid:2091.58 collect_output:73.40
2023-02-17 10:29:02 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 10:29:02 - train.py[line:551] - INFO: load:1.68 valid_run:2304.44 task_valid:2208.47 collect_output:76.60
2023-02-17 10:31:02 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 10:31:02 - train.py[line:551] - INFO: load:1.71 valid_run:2424.76 task_valid:2324.97 collect_output:79.40
2023-02-17 10:33:04 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 10:33:04 - train.py[line:551] - INFO: load:1.74 valid_run:2546.41 task_valid:2441.46 collect_output:83.54
2023-02-17 10:35:06 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 10:35:06 - train.py[line:551] - INFO: load:1.76 valid_run:2668.31 task_valid:2560.28 collect_output:85.61
2023-02-17 10:37:06 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 10:37:06 - train.py[line:551] - INFO: load:1.79 valid_run:2788.52 task_valid:2674.33 collect_output:90.74
2023-02-17 10:39:06 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 10:39:06 - train.py[line:551] - INFO: load:1.82 valid_run:2908.21 task_valid:2790.32 collect_output:93.43
2023-02-17 10:41:08 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 10:41:08 - train.py[line:551] - INFO: load:1.84 valid_run:3029.87 task_valid:2906.60 collect_output:97.81
2023-02-17 10:43:10 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 10:43:10 - train.py[line:551] - INFO: load:1.87 valid_run:3152.54 task_valid:3022.39 collect_output:103.65
2023-02-17 10:45:10 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 10:45:10 - train.py[line:551] - INFO: load:1.89 valid_run:3272.01 task_valid:3136.25 collect_output:108.25
2023-02-17 10:47:12 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 10:47:12 - train.py[line:551] - INFO: load:1.92 valid_run:3393.84 task_valid:3255.56 collect_output:109.77
2023-02-17 10:49:14 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 10:49:14 - train.py[line:551] - INFO: load:1.95 valid_run:3515.39 task_valid:3370.95 collect_output:114.92
2023-02-17 10:51:15 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 10:51:15 - train.py[line:551] - INFO: load:1.97 valid_run:3637.09 task_valid:3489.25 collect_output:117.30
2023-02-17 10:53:16 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 10:53:16 - train.py[line:551] - INFO: load:2.00 valid_run:3758.09 task_valid:3607.60 collect_output:118.93

====================================================================================================
SGG eval:     R @ 50: 0.6159;     R @ 100: 0.6564;     R @ 500: 0.6791;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4197;    mR @ 100: 0.4552;    mR @ 500: 0.5123;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.6875) (covering:0.2286) (eating:0.7647) (flying in:0.5909) (growing on:0.3750) (hanging from:0.4774) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9706) (says:0.0000) (sitting on:0.7069) (standing on:0.3743) (using:0.5500) (walking in:0.0000) (walking on:0.6081) (watching:0.4444) 
--------------------------------------------------------
====================================================================================================

2023-02-17 10:53:47 - train.py[line:487] - INFO: 0.6563594346829641
2023-02-17 10:53:47 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.6159;     R @ 100: 0.6564;     R @ 500: 0.6791;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4197;    mR @ 100: 0.4552;    mR @ 500: 0.5123;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.6875) (covering:0.2286) (eating:0.7647) (flying in:0.5909) (growing on:0.3750) (hanging from:0.4774) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9706) (says:0.0000) (sitting on:0.7069) (standing on:0.3743) (using:0.5500) (walking in:0.0000) (walking on:0.6081) (watching:0.4444) 
--------------------------------------------------------
====================================================================================================

2023-02-17 10:53:47 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.266 | loss_v1 0 | loss_v2 0 | nll_loss 0.1 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.656359 | ppl 1.07 | vqa_score 0.5315 | wps 118.3 | wpb 72 | bsz 24 | num_updates 14000 | best_R@100 0.69202
2023-02-17 10:53:47 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 14000 updates
2023-02-17 10:53:47 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_14000.pt
2023-02-17 10:53:53 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_14000.pt
2023-02-17 10:53:55 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_14000.pt (epoch 2 @ 14000 updates, score 0.6563594346829641) (writing took 8.090696794912219 seconds)
2023-02-17 10:54:06 - progress_bar.py[line:274] - INFO: epoch 002:   2465 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=112.3, bsz=40, num_updates=14010, lr=4.57731e-05, gnorm=0.322, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=68847
2023-02-17 10:54:17 - progress_bar.py[line:274] - INFO: epoch 002:   2475 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.91, wpb=113.4, bsz=40, num_updates=14020, lr=4.57686e-05, gnorm=0.317, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=68858
2023-02-17 10:54:28 - progress_bar.py[line:274] - INFO: epoch 002:   2485 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=110.6, bsz=40, num_updates=14030, lr=4.57641e-05, gnorm=0.374, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=68869
2023-02-17 10:54:40 - progress_bar.py[line:274] - INFO: epoch 002:   2495 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.91, wpb=110.5, bsz=40, num_updates=14040, lr=4.57596e-05, gnorm=0.334, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=68880
2023-02-17 10:54:50 - progress_bar.py[line:274] - INFO: epoch 002:   2505 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=106.4, ups=0.95, wpb=112.4, bsz=40, num_updates=14050, lr=4.57551e-05, gnorm=0.306, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=68891
2023-02-17 10:55:01 - progress_bar.py[line:274] - INFO: epoch 002:   2515 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.88, wpb=112.7, bsz=40, num_updates=14060, lr=4.57506e-05, gnorm=0.268, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=68902
2023-02-17 10:55:13 - progress_bar.py[line:274] - INFO: epoch 002:   2525 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.9, wpb=110.3, bsz=40, num_updates=14070, lr=4.57461e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=68914
2023-02-17 10:55:24 - progress_bar.py[line:274] - INFO: epoch 002:   2535 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.91, wpb=112.1, bsz=40, num_updates=14080, lr=4.57416e-05, gnorm=0.342, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=68925
2023-02-17 10:55:35 - progress_bar.py[line:274] - INFO: epoch 002:   2545 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.88, wpb=112.3, bsz=40, num_updates=14090, lr=4.57371e-05, gnorm=0.516, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=68936
2023-02-17 10:55:46 - progress_bar.py[line:274] - INFO: epoch 002:   2555 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104, ups=0.92, wpb=113.4, bsz=40, num_updates=14100, lr=4.57326e-05, gnorm=0.332, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=68947
2023-02-17 10:55:57 - progress_bar.py[line:274] - INFO: epoch 002:   2565 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.92, wpb=111.7, bsz=40, num_updates=14110, lr=4.57281e-05, gnorm=0.294, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=68958
2023-02-17 10:56:08 - progress_bar.py[line:274] - INFO: epoch 002:   2575 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.2, ups=0.92, wpb=111.6, bsz=40, num_updates=14120, lr=4.57236e-05, gnorm=0.342, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=68968
2023-02-17 10:56:19 - progress_bar.py[line:274] - INFO: epoch 002:   2585 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=111.4, bsz=40, num_updates=14130, lr=4.5719e-05, gnorm=0.318, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=68980
2023-02-17 10:56:30 - progress_bar.py[line:274] - INFO: epoch 002:   2595 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.91, wpb=113.4, bsz=40, num_updates=14140, lr=4.57145e-05, gnorm=0.358, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=68991
2023-02-17 10:56:41 - progress_bar.py[line:274] - INFO: epoch 002:   2605 / 11564 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.1, ups=0.92, wpb=111.1, bsz=40, num_updates=14150, lr=4.571e-05, gnorm=0.301, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69002
2023-02-17 10:56:52 - progress_bar.py[line:274] - INFO: epoch 002:   2615 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.89, wpb=113.1, bsz=40, num_updates=14160, lr=4.57055e-05, gnorm=0.328, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69013
2023-02-17 10:56:55 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-17 10:57:04 - progress_bar.py[line:274] - INFO: epoch 002:   2626 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=92.2, ups=0.81, wpb=113.7, bsz=40, num_updates=14170, lr=4.5701e-05, gnorm=0.314, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=69025
2023-02-17 10:57:15 - progress_bar.py[line:274] - INFO: epoch 002:   2636 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.9, wpb=111.8, bsz=40, num_updates=14180, lr=4.56965e-05, gnorm=0.289, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69036
2023-02-17 10:57:27 - progress_bar.py[line:274] - INFO: epoch 002:   2646 / 11564 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.89, wpb=111.8, bsz=40, num_updates=14190, lr=4.5692e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=69048
2023-02-17 10:57:37 - progress_bar.py[line:274] - INFO: epoch 002:   2656 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.5, ups=0.93, wpb=111.9, bsz=40, num_updates=14200, lr=4.56875e-05, gnorm=0.354, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=69058
2023-02-17 10:57:49 - progress_bar.py[line:274] - INFO: epoch 002:   2666 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.7, ups=0.88, wpb=111.6, bsz=40, num_updates=14210, lr=4.5683e-05, gnorm=0.296, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=69070
2023-02-17 10:58:00 - progress_bar.py[line:274] - INFO: epoch 002:   2676 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.9, wpb=110.8, bsz=40, num_updates=14220, lr=4.56785e-05, gnorm=0.431, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=69081
2023-02-17 10:58:11 - progress_bar.py[line:274] - INFO: epoch 002:   2686 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.7, ups=0.89, wpb=112.3, bsz=40, num_updates=14230, lr=4.5674e-05, gnorm=0.388, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69092
2023-02-17 10:58:22 - progress_bar.py[line:274] - INFO: epoch 002:   2696 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.91, wpb=112.2, bsz=40, num_updates=14240, lr=4.56695e-05, gnorm=0.293, clip=0, loss_scale=1024, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=69103
2023-02-17 10:58:33 - progress_bar.py[line:274] - INFO: epoch 002:   2706 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.2, ups=0.92, wpb=111.1, bsz=40, num_updates=14250, lr=4.5665e-05, gnorm=0.367, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=69114
2023-02-17 10:58:44 - progress_bar.py[line:274] - INFO: epoch 002:   2716 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.92, wpb=110.7, bsz=40, num_updates=14260, lr=4.56605e-05, gnorm=0.45, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69125
2023-02-17 10:58:54 - progress_bar.py[line:274] - INFO: epoch 002:   2726 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.4, ups=0.94, wpb=110.7, bsz=40, num_updates=14270, lr=4.5656e-05, gnorm=0.285, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=69135
2023-02-17 10:59:06 - progress_bar.py[line:274] - INFO: epoch 002:   2736 / 11564 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=97.4, ups=0.87, wpb=111.7, bsz=40, num_updates=14280, lr=4.56515e-05, gnorm=0.29, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69147
2023-02-17 10:59:17 - progress_bar.py[line:274] - INFO: epoch 002:   2746 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.89, wpb=112.6, bsz=40, num_updates=14290, lr=4.5647e-05, gnorm=0.342, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69158
2023-02-17 10:59:28 - progress_bar.py[line:274] - INFO: epoch 002:   2756 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=111.9, bsz=40, num_updates=14300, lr=4.56425e-05, gnorm=0.396, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=69169
2023-02-17 10:59:39 - progress_bar.py[line:274] - INFO: epoch 002:   2766 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.91, wpb=111.3, bsz=40, num_updates=14310, lr=4.5638e-05, gnorm=0.305, clip=0, loss_scale=1024, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=69180
2023-02-17 10:59:50 - progress_bar.py[line:274] - INFO: epoch 002:   2776 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=112.3, bsz=40, num_updates=14320, lr=4.56335e-05, gnorm=0.332, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69191
2023-02-17 11:00:01 - progress_bar.py[line:274] - INFO: epoch 002:   2786 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.9, ups=0.9, wpb=112.7, bsz=40, num_updates=14330, lr=4.5629e-05, gnorm=0.273, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=69202
2023-02-17 11:00:12 - progress_bar.py[line:274] - INFO: epoch 002:   2796 / 11564 loss=0.185, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.91, wpb=111.6, bsz=40, num_updates=14340, lr=4.56245e-05, gnorm=0.302, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69213
2023-02-17 11:00:24 - progress_bar.py[line:274] - INFO: epoch 002:   2806 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.89, wpb=112.4, bsz=40, num_updates=14350, lr=4.562e-05, gnorm=0.278, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69225
2023-02-17 11:00:35 - progress_bar.py[line:274] - INFO: epoch 002:   2816 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100, ups=0.9, wpb=111.6, bsz=40, num_updates=14360, lr=4.56155e-05, gnorm=0.359, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69236
2023-02-17 11:00:46 - progress_bar.py[line:274] - INFO: epoch 002:   2826 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.9, wpb=110.8, bsz=40, num_updates=14370, lr=4.5611e-05, gnorm=0.29, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69247
2023-02-17 11:00:57 - progress_bar.py[line:274] - INFO: epoch 002:   2836 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.4, bsz=40, num_updates=14380, lr=4.56064e-05, gnorm=0.382, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69258
2023-02-17 11:01:09 - progress_bar.py[line:274] - INFO: epoch 002:   2846 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.88, wpb=113.6, bsz=40, num_updates=14390, lr=4.56019e-05, gnorm=0.342, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69270
2023-02-17 11:01:20 - progress_bar.py[line:274] - INFO: epoch 002:   2856 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.91, wpb=110.7, bsz=40, num_updates=14400, lr=4.55974e-05, gnorm=0.366, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=69281
2023-02-17 11:01:31 - progress_bar.py[line:274] - INFO: epoch 002:   2866 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112.4, bsz=40, num_updates=14410, lr=4.55929e-05, gnorm=0.402, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69292
2023-02-17 11:01:42 - progress_bar.py[line:274] - INFO: epoch 002:   2876 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.93, wpb=110.7, bsz=40, num_updates=14420, lr=4.55884e-05, gnorm=0.361, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69303
2023-02-17 11:01:53 - progress_bar.py[line:274] - INFO: epoch 002:   2886 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.88, wpb=112.9, bsz=40, num_updates=14430, lr=4.55839e-05, gnorm=0.434, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69314
2023-02-17 11:02:04 - progress_bar.py[line:274] - INFO: epoch 002:   2896 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.6, ups=0.89, wpb=110.1, bsz=40, num_updates=14440, lr=4.55794e-05, gnorm=0.34, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69325
2023-02-17 11:02:15 - progress_bar.py[line:274] - INFO: epoch 002:   2906 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.91, wpb=111.8, bsz=40, num_updates=14450, lr=4.55749e-05, gnorm=0.298, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69336
2023-02-17 11:02:26 - progress_bar.py[line:274] - INFO: epoch 002:   2916 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=112.6, bsz=40, num_updates=14460, lr=4.55704e-05, gnorm=0.301, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69347
2023-02-17 11:02:37 - progress_bar.py[line:274] - INFO: epoch 002:   2926 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=112.1, bsz=40, num_updates=14470, lr=4.55659e-05, gnorm=0.31, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69358
2023-02-17 11:02:49 - progress_bar.py[line:274] - INFO: epoch 002:   2936 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.5, bsz=40, num_updates=14480, lr=4.55614e-05, gnorm=0.349, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69370
2023-02-17 11:03:00 - progress_bar.py[line:274] - INFO: epoch 002:   2946 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.9, wpb=113.4, bsz=40, num_updates=14490, lr=4.55569e-05, gnorm=0.32, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69381
2023-02-17 11:03:11 - progress_bar.py[line:274] - INFO: epoch 002:   2956 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=112.6, bsz=40, num_updates=14500, lr=4.55524e-05, gnorm=0.274, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69392
2023-02-17 11:03:22 - progress_bar.py[line:274] - INFO: epoch 002:   2966 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.93, wpb=110.9, bsz=40, num_updates=14510, lr=4.55479e-05, gnorm=0.321, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69403
2023-02-17 11:03:33 - progress_bar.py[line:274] - INFO: epoch 002:   2976 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.88, wpb=113.5, bsz=40, num_updates=14520, lr=4.55434e-05, gnorm=0.248, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69414
2023-02-17 11:03:44 - progress_bar.py[line:274] - INFO: epoch 002:   2986 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.88, wpb=111.4, bsz=40, num_updates=14530, lr=4.55389e-05, gnorm=0.361, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=69425
2023-02-17 11:03:56 - progress_bar.py[line:274] - INFO: epoch 002:   2996 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.9, wpb=112.6, bsz=40, num_updates=14540, lr=4.55344e-05, gnorm=0.355, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69437
2023-02-17 11:04:07 - progress_bar.py[line:274] - INFO: epoch 002:   3006 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=112.7, bsz=40, num_updates=14550, lr=4.55299e-05, gnorm=0.39, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69448
2023-02-17 11:04:18 - progress_bar.py[line:274] - INFO: epoch 002:   3016 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101, ups=0.9, wpb=112.5, bsz=40, num_updates=14560, lr=4.55254e-05, gnorm=0.354, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69459
2023-02-17 11:04:29 - progress_bar.py[line:274] - INFO: epoch 002:   3026 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.87, wpb=112.8, bsz=40, num_updates=14570, lr=4.55209e-05, gnorm=0.484, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=69470
2023-02-17 11:04:41 - progress_bar.py[line:274] - INFO: epoch 002:   3036 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.88, wpb=110.9, bsz=40, num_updates=14580, lr=4.55164e-05, gnorm=0.301, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69482
2023-02-17 11:04:52 - progress_bar.py[line:274] - INFO: epoch 002:   3046 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.91, wpb=109.4, bsz=40, num_updates=14590, lr=4.55119e-05, gnorm=0.331, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69493
2023-02-17 11:05:03 - progress_bar.py[line:274] - INFO: epoch 002:   3056 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.2, bsz=40, num_updates=14600, lr=4.55074e-05, gnorm=0.322, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69504
2023-02-17 11:05:14 - progress_bar.py[line:274] - INFO: epoch 002:   3066 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.3, ups=0.88, wpb=111.3, bsz=40, num_updates=14610, lr=4.55029e-05, gnorm=0.336, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69515
2023-02-17 11:05:25 - progress_bar.py[line:274] - INFO: epoch 002:   3076 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=108.3, ups=0.96, wpb=113.1, bsz=40, num_updates=14620, lr=4.54984e-05, gnorm=0.349, clip=0, loss_scale=1024, train_wall=10, gb_free=10.6, ema_decay=0.9999, wall=69526
2023-02-17 11:05:35 - progress_bar.py[line:274] - INFO: epoch 002:   3086 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.9, ups=0.93, wpb=111.3, bsz=40, num_updates=14630, lr=4.54939e-05, gnorm=0.391, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69536
2023-02-17 11:05:47 - progress_bar.py[line:274] - INFO: epoch 002:   3096 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112, bsz=40, num_updates=14640, lr=4.54893e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69548
2023-02-17 11:05:58 - progress_bar.py[line:274] - INFO: epoch 002:   3106 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.8, ups=0.88, wpb=112.2, bsz=40, num_updates=14650, lr=4.54848e-05, gnorm=0.467, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69559
2023-02-17 11:06:09 - progress_bar.py[line:274] - INFO: epoch 002:   3116 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.1, ups=0.88, wpb=110.3, bsz=40, num_updates=14660, lr=4.54803e-05, gnorm=0.392, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69570
2023-02-17 11:06:21 - progress_bar.py[line:274] - INFO: epoch 002:   3126 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.6, ups=0.9, wpb=112, bsz=40, num_updates=14670, lr=4.54758e-05, gnorm=0.253, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69581
2023-02-17 11:06:32 - progress_bar.py[line:274] - INFO: epoch 002:   3136 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.9, ups=0.88, wpb=113, bsz=40, num_updates=14680, lr=4.54713e-05, gnorm=0.382, clip=10, loss_scale=2048, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=69593
2023-02-17 11:06:43 - progress_bar.py[line:274] - INFO: epoch 002:   3146 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.6, ups=0.91, wpb=113, bsz=40, num_updates=14690, lr=4.54668e-05, gnorm=0.354, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69604
2023-02-17 11:06:47 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-17 11:06:55 - progress_bar.py[line:274] - INFO: epoch 002:   3157 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=91.4, ups=0.82, wpb=111.3, bsz=40, num_updates=14700, lr=4.54623e-05, gnorm=0.384, clip=0, loss_scale=1024, train_wall=12, gb_free=11, ema_decay=0.9999, wall=69616
2023-02-17 11:07:06 - progress_bar.py[line:274] - INFO: epoch 002:   3167 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.2, ups=0.93, wpb=112.2, bsz=40, num_updates=14710, lr=4.54578e-05, gnorm=0.373, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=69627
2023-02-17 11:07:17 - progress_bar.py[line:274] - INFO: epoch 002:   3177 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.88, wpb=111.7, bsz=40, num_updates=14720, lr=4.54533e-05, gnorm=0.326, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69638
2023-02-17 11:07:28 - progress_bar.py[line:274] - INFO: epoch 002:   3187 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.4, ups=0.91, wpb=111.6, bsz=40, num_updates=14730, lr=4.54488e-05, gnorm=0.411, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69649
2023-02-17 11:07:40 - progress_bar.py[line:274] - INFO: epoch 002:   3197 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.88, wpb=111.4, bsz=40, num_updates=14740, lr=4.54443e-05, gnorm=0.359, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69661
2023-02-17 11:07:51 - progress_bar.py[line:274] - INFO: epoch 002:   3207 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.91, wpb=111.9, bsz=40, num_updates=14750, lr=4.54398e-05, gnorm=0.391, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69672
2023-02-17 11:08:02 - progress_bar.py[line:274] - INFO: epoch 002:   3217 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.8, ups=0.91, wpb=111.2, bsz=40, num_updates=14760, lr=4.54353e-05, gnorm=0.326, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69683
2023-02-17 11:08:13 - progress_bar.py[line:274] - INFO: epoch 002:   3227 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.91, wpb=112.2, bsz=40, num_updates=14770, lr=4.54308e-05, gnorm=0.324, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69694
2023-02-17 11:08:24 - progress_bar.py[line:274] - INFO: epoch 002:   3237 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.2, ups=0.88, wpb=111.2, bsz=40, num_updates=14780, lr=4.54263e-05, gnorm=0.505, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69705
2023-02-17 11:08:35 - progress_bar.py[line:274] - INFO: epoch 002:   3247 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.89, wpb=112.5, bsz=40, num_updates=14790, lr=4.54218e-05, gnorm=0.395, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69716
2023-02-17 11:08:47 - progress_bar.py[line:274] - INFO: epoch 002:   3257 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.2, bsz=40, num_updates=14800, lr=4.54173e-05, gnorm=0.282, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69727
2023-02-17 11:08:57 - progress_bar.py[line:274] - INFO: epoch 002:   3267 / 11564 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.051, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.92, wpb=112.3, bsz=40, num_updates=14810, lr=4.54128e-05, gnorm=0.205, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69738
2023-02-17 11:09:09 - progress_bar.py[line:274] - INFO: epoch 002:   3277 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.9, wpb=111.8, bsz=40, num_updates=14820, lr=4.54083e-05, gnorm=0.365, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69749
2023-02-17 11:09:20 - progress_bar.py[line:274] - INFO: epoch 002:   3287 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=112.4, bsz=40, num_updates=14830, lr=4.54038e-05, gnorm=0.315, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69761
2023-02-17 11:09:31 - progress_bar.py[line:274] - INFO: epoch 002:   3297 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.3, ups=0.89, wpb=111.5, bsz=40, num_updates=14840, lr=4.53993e-05, gnorm=0.217, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69772
2023-02-17 11:09:36 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-17 11:09:43 - progress_bar.py[line:274] - INFO: epoch 002:   3308 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=93.1, ups=0.83, wpb=111.8, bsz=40, num_updates=14850, lr=4.53948e-05, gnorm=0.332, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=69784
2023-02-17 11:09:54 - progress_bar.py[line:274] - INFO: epoch 002:   3318 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=107, ups=0.95, wpb=113.2, bsz=40, num_updates=14860, lr=4.53903e-05, gnorm=0.289, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=69795
2023-02-17 11:10:05 - progress_bar.py[line:274] - INFO: epoch 002:   3328 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=95.9, ups=0.87, wpb=110.3, bsz=40, num_updates=14870, lr=4.53858e-05, gnorm=0.4, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69806
2023-02-17 11:10:16 - progress_bar.py[line:274] - INFO: epoch 002:   3338 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.2, ups=0.89, wpb=111.6, bsz=40, num_updates=14880, lr=4.53813e-05, gnorm=0.37, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69817
2023-02-17 11:10:27 - progress_bar.py[line:274] - INFO: epoch 002:   3348 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.91, wpb=111, bsz=40, num_updates=14890, lr=4.53768e-05, gnorm=0.268, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69828
2023-02-17 11:10:39 - progress_bar.py[line:274] - INFO: epoch 002:   3358 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.89, wpb=112.6, bsz=40, num_updates=14900, lr=4.53722e-05, gnorm=0.349, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69840
2023-02-17 11:10:50 - progress_bar.py[line:274] - INFO: epoch 002:   3368 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=114.1, nsentences=40, sample_size=114.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.88, wpb=114.1, bsz=40, num_updates=14910, lr=4.53677e-05, gnorm=0.362, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=69851
2023-02-17 11:11:01 - progress_bar.py[line:274] - INFO: epoch 002:   3378 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.88, wpb=112, bsz=40, num_updates=14920, lr=4.53632e-05, gnorm=0.391, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69862
2023-02-17 11:11:12 - progress_bar.py[line:274] - INFO: epoch 002:   3388 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111.9, bsz=40, num_updates=14930, lr=4.53587e-05, gnorm=0.293, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=69873
2023-02-17 11:11:23 - progress_bar.py[line:274] - INFO: epoch 002:   3398 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.92, wpb=112.5, bsz=40, num_updates=14940, lr=4.53542e-05, gnorm=0.318, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69884
2023-02-17 11:11:35 - progress_bar.py[line:274] - INFO: epoch 002:   3408 / 11564 loss=0.187, loss_v1=0, loss_v2=0, nll_loss=0.055, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=112.2, bsz=40, num_updates=14950, lr=4.53497e-05, gnorm=0.19, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=69896
2023-02-17 11:11:46 - progress_bar.py[line:274] - INFO: epoch 002:   3418 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.91, wpb=111.2, bsz=40, num_updates=14960, lr=4.53452e-05, gnorm=0.265, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69907
2023-02-17 11:11:57 - progress_bar.py[line:274] - INFO: epoch 002:   3428 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.2, ups=0.92, wpb=112.8, bsz=40, num_updates=14970, lr=4.53407e-05, gnorm=0.323, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=69917
2023-02-17 11:12:08 - progress_bar.py[line:274] - INFO: epoch 002:   3438 / 11564 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.4, ups=0.89, wpb=112.3, bsz=40, num_updates=14980, lr=4.53362e-05, gnorm=0.24, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=69929
2023-02-17 11:12:19 - progress_bar.py[line:274] - INFO: epoch 002:   3448 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.4, ups=0.86, wpb=112.5, bsz=40, num_updates=14990, lr=4.53317e-05, gnorm=0.36, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=69940
2023-02-17 11:12:31 - progress_bar.py[line:274] - INFO: epoch 002:   3458 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.89, wpb=111.7, bsz=40, num_updates=15000, lr=4.53272e-05, gnorm=0.38, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=69952
2023-02-17 11:12:31 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 11:12:32 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 11:12:32 - train.py[line:551] - INFO: load:0.85 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 11:14:34 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 11:14:34 - train.py[line:551] - INFO: load:0.87 valid_run:122.33 task_valid:118.84 collect_output:2.41
2023-02-17 11:16:34 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 11:16:34 - train.py[line:551] - INFO: load:0.90 valid_run:242.12 task_valid:234.49 collect_output:5.53
2023-02-17 11:18:36 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 11:18:36 - train.py[line:551] - INFO: load:0.92 valid_run:363.96 task_valid:350.82 collect_output:10.00
2023-02-17 11:20:38 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 11:20:38 - train.py[line:551] - INFO: load:0.95 valid_run:485.75 task_valid:464.32 collect_output:17.27
2023-02-17 11:22:38 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 11:22:38 - train.py[line:551] - INFO: load:0.98 valid_run:606.08 task_valid:581.53 collect_output:19.36
2023-02-17 11:24:41 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 11:24:41 - train.py[line:551] - INFO: load:1.00 valid_run:728.91 task_valid:700.16 collect_output:22.51
2023-02-17 11:26:44 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 11:26:44 - train.py[line:551] - INFO: load:1.03 valid_run:851.82 task_valid:818.12 collect_output:26.45
2023-02-17 11:28:46 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 11:28:46 - train.py[line:551] - INFO: load:1.06 valid_run:973.52 task_valid:934.47 collect_output:30.75
2023-02-17 11:30:49 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 11:30:49 - train.py[line:551] - INFO: load:1.08 valid_run:1096.95 task_valid:1051.51 collect_output:36.10
2023-02-17 11:32:51 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 11:32:51 - train.py[line:551] - INFO: load:1.11 valid_run:1218.41 task_valid:1164.03 collect_output:43.99
2023-02-17 11:34:51 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 11:34:51 - train.py[line:551] - INFO: load:1.14 valid_run:1338.39 task_valid:1279.47 collect_output:47.50
2023-02-17 11:36:52 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 11:36:52 - train.py[line:551] - INFO: load:1.16 valid_run:1459.88 task_valid:1396.42 collect_output:51.03
2023-02-17 11:38:51 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 11:38:51 - train.py[line:551] - INFO: load:1.19 valid_run:1578.75 task_valid:1510.14 collect_output:55.15
2023-02-17 11:40:52 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 11:40:52 - train.py[line:551] - INFO: load:1.21 valid_run:1699.54 task_valid:1627.64 collect_output:57.44
2023-02-17 11:42:53 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 11:42:53 - train.py[line:551] - INFO: load:1.24 valid_run:1820.29 task_valid:1743.54 collect_output:61.29
2023-02-17 11:44:54 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 11:44:54 - train.py[line:551] - INFO: load:1.26 valid_run:1941.26 task_valid:1857.32 collect_output:67.47
2023-02-17 11:46:55 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 11:46:55 - train.py[line:551] - INFO: load:1.29 valid_run:2062.51 task_valid:1973.35 collect_output:71.70
2023-02-17 11:48:56 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 11:48:56 - train.py[line:551] - INFO: load:1.32 valid_run:2183.13 task_valid:2091.20 collect_output:73.46
2023-02-17 11:50:57 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 11:50:57 - train.py[line:551] - INFO: load:1.34 valid_run:2304.17 task_valid:2208.04 collect_output:76.67
2023-02-17 11:52:57 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 11:52:57 - train.py[line:551] - INFO: load:1.37 valid_run:2424.37 task_valid:2324.50 collect_output:79.41
2023-02-17 11:54:59 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 11:54:59 - train.py[line:551] - INFO: load:1.39 valid_run:2545.96 task_valid:2440.98 collect_output:83.50
2023-02-17 11:57:01 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 11:57:01 - train.py[line:551] - INFO: load:1.42 valid_run:2667.69 task_valid:2559.66 collect_output:85.53
2023-02-17 11:59:01 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 11:59:01 - train.py[line:551] - INFO: load:1.44 valid_run:2787.88 task_valid:2673.84 collect_output:90.51
2023-02-17 12:01:01 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 12:01:01 - train.py[line:551] - INFO: load:1.47 valid_run:2907.54 task_valid:2789.84 collect_output:93.17
2023-02-17 12:03:02 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 12:03:02 - train.py[line:551] - INFO: load:1.49 valid_run:3029.05 task_valid:2905.87 collect_output:97.58
2023-02-17 12:05:05 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 12:05:05 - train.py[line:551] - INFO: load:1.52 valid_run:3151.75 task_valid:3021.67 collect_output:103.45
2023-02-17 12:07:05 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 12:07:05 - train.py[line:551] - INFO: load:1.54 valid_run:3271.24 task_valid:3135.68 collect_output:107.93
2023-02-17 12:09:06 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 12:09:06 - train.py[line:551] - INFO: load:1.57 valid_run:3392.94 task_valid:3254.89 collect_output:109.45
2023-02-17 12:11:08 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 12:11:08 - train.py[line:551] - INFO: load:1.60 valid_run:3514.44 task_valid:3370.30 collect_output:114.53
2023-02-17 12:13:10 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 12:13:10 - train.py[line:551] - INFO: load:1.62 valid_run:3636.21 task_valid:3488.74 collect_output:116.86
2023-02-17 12:15:11 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 12:15:11 - train.py[line:551] - INFO: load:1.65 valid_run:3757.20 task_valid:3607.14 collect_output:118.47

====================================================================================================
SGG eval:     R @ 50: 0.6100;     R @ 100: 0.6561;     R @ 500: 0.6781;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3935;    mR @ 100: 0.4636;    mR @ 500: 0.5155;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.6875) (covering:0.2286) (eating:0.7647) (flying in:0.6818) (growing on:0.3750) (hanging from:0.4839) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9706) (says:0.0000) (sitting on:0.6944) (standing on:0.3743) (using:0.5500) (walking in:0.0000) (walking on:0.6081) (watching:0.4444) 
--------------------------------------------------------
====================================================================================================

2023-02-17 12:15:41 - train.py[line:487] - INFO: 0.6560564043799337

====================================================================================================
SGG eval:     R @ 50: 0.6100;     R @ 100: 0.6561;     R @ 500: 0.6781;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3935;    mR @ 100: 0.4636;    mR @ 500: 0.5155;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8171) (covered in:0.6875) (covering:0.2286) (eating:0.7647) (flying in:0.6818) (growing on:0.3750) (hanging from:0.4839) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9706) (says:0.0000) (sitting on:0.6944) (standing on:0.3743) (using:0.5500) (walking in:0.0000) (walking on:0.6081) (watching:0.4444) 
--------------------------------------------------------
====================================================================================================

2023-02-17 12:15:41 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 12:15:41 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.262 | loss_v1 0 | loss_v2 0 | nll_loss 0.097 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.656056 | ppl 1.07 | vqa_score 0.5248 | wps 118.4 | wpb 72 | bsz 24 | num_updates 15000 | best_R@100 0.69202
2023-02-17 12:15:41 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 15000 updates
2023-02-17 12:15:41 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_15000.pt
2023-02-17 12:15:47 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_15000.pt
2023-02-17 12:15:49 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_15000.pt (epoch 2 @ 15000 updates, score 0.6560564043799337) (writing took 7.857882617041469 seconds)
2023-02-17 12:16:00 - progress_bar.py[line:274] - INFO: epoch 002:   3468 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=111.3, bsz=40, num_updates=15010, lr=4.53227e-05, gnorm=0.295, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=73761
2023-02-17 12:16:11 - progress_bar.py[line:274] - INFO: epoch 002:   3478 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.9, wpb=112.2, bsz=40, num_updates=15020, lr=4.53182e-05, gnorm=0.361, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=73772
2023-02-17 12:16:22 - progress_bar.py[line:274] - INFO: epoch 002:   3488 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.5, ups=0.92, wpb=111.7, bsz=40, num_updates=15030, lr=4.53137e-05, gnorm=0.472, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=73783
2023-02-17 12:16:33 - progress_bar.py[line:274] - INFO: epoch 002:   3498 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112.5, bsz=40, num_updates=15040, lr=4.53092e-05, gnorm=0.306, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=73794
2023-02-17 12:16:44 - progress_bar.py[line:274] - INFO: epoch 002:   3508 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.2, ups=0.91, wpb=112.6, bsz=40, num_updates=15050, lr=4.53047e-05, gnorm=0.208, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=73805
2023-02-17 12:16:56 - progress_bar.py[line:274] - INFO: epoch 002:   3518 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.4, ups=0.89, wpb=113.7, bsz=40, num_updates=15060, lr=4.53002e-05, gnorm=0.339, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=73817
2023-02-17 12:17:07 - progress_bar.py[line:274] - INFO: epoch 002:   3528 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.5, ups=0.88, wpb=112.3, bsz=40, num_updates=15070, lr=4.52957e-05, gnorm=0.362, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=73828
2023-02-17 12:17:18 - progress_bar.py[line:274] - INFO: epoch 002:   3538 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.1, ups=0.91, wpb=111.6, bsz=40, num_updates=15080, lr=4.52912e-05, gnorm=0.402, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=73839
2023-02-17 12:17:29 - progress_bar.py[line:274] - INFO: epoch 002:   3548 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.91, wpb=113.6, bsz=40, num_updates=15090, lr=4.52867e-05, gnorm=0.28, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=73850
2023-02-17 12:17:40 - progress_bar.py[line:274] - INFO: epoch 002:   3558 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.9, bsz=40, num_updates=15100, lr=4.52822e-05, gnorm=0.369, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=73861
2023-02-17 12:17:51 - progress_bar.py[line:274] - INFO: epoch 002:   3568 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.6, ups=0.91, wpb=111.8, bsz=40, num_updates=15110, lr=4.52777e-05, gnorm=0.27, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=73872
2023-02-17 12:18:03 - progress_bar.py[line:274] - INFO: epoch 002:   3578 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=110.7, bsz=40, num_updates=15120, lr=4.52732e-05, gnorm=0.288, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=73884
2023-02-17 12:18:14 - progress_bar.py[line:274] - INFO: epoch 002:   3588 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.4, ups=0.92, wpb=112.5, bsz=40, num_updates=15130, lr=4.52687e-05, gnorm=0.322, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=73895
2023-02-17 12:18:24 - progress_bar.py[line:274] - INFO: epoch 002:   3598 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.92, wpb=111.5, bsz=40, num_updates=15140, lr=4.52642e-05, gnorm=0.219, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=73905
2023-02-17 12:18:35 - progress_bar.py[line:274] - INFO: epoch 002:   3608 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.5, ups=0.91, wpb=111.9, bsz=40, num_updates=15150, lr=4.52596e-05, gnorm=0.366, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=73916
2023-02-17 12:18:46 - progress_bar.py[line:274] - INFO: epoch 002:   3618 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.8, ups=0.92, wpb=110.7, bsz=40, num_updates=15160, lr=4.52551e-05, gnorm=0.37, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=73927
2023-02-17 12:18:57 - progress_bar.py[line:274] - INFO: epoch 002:   3628 / 11564 loss=0.191, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.2, bsz=40, num_updates=15170, lr=4.52506e-05, gnorm=0.25, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=73938
2023-02-17 12:19:09 - progress_bar.py[line:274] - INFO: epoch 002:   3638 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.5, ups=0.89, wpb=112, bsz=40, num_updates=15180, lr=4.52461e-05, gnorm=0.273, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=73950
2023-02-17 12:19:20 - progress_bar.py[line:274] - INFO: epoch 002:   3648 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.89, wpb=112, bsz=40, num_updates=15190, lr=4.52416e-05, gnorm=0.376, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=73961
2023-02-17 12:19:31 - progress_bar.py[line:274] - INFO: epoch 002:   3658 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.1, ups=0.89, wpb=112, bsz=40, num_updates=15200, lr=4.52371e-05, gnorm=0.351, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=73972
2023-02-17 12:19:42 - progress_bar.py[line:274] - INFO: epoch 002:   3668 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.93, wpb=111.1, bsz=40, num_updates=15210, lr=4.52326e-05, gnorm=0.422, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=73983
2023-02-17 12:19:53 - progress_bar.py[line:274] - INFO: epoch 002:   3678 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.91, wpb=111.3, bsz=40, num_updates=15220, lr=4.52281e-05, gnorm=0.329, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=73994
2023-02-17 12:20:04 - progress_bar.py[line:274] - INFO: epoch 002:   3688 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=111.3, bsz=40, num_updates=15230, lr=4.52236e-05, gnorm=0.302, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=74005
2023-02-17 12:20:15 - progress_bar.py[line:274] - INFO: epoch 002:   3698 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.91, wpb=112, bsz=40, num_updates=15240, lr=4.52191e-05, gnorm=0.34, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74016
2023-02-17 12:20:26 - progress_bar.py[line:274] - INFO: epoch 002:   3708 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102, ups=0.91, wpb=112.1, bsz=40, num_updates=15250, lr=4.52146e-05, gnorm=0.389, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74027
2023-02-17 12:20:38 - progress_bar.py[line:274] - INFO: epoch 002:   3718 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.87, wpb=112.4, bsz=40, num_updates=15260, lr=4.52101e-05, gnorm=0.28, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74039
2023-02-17 12:20:49 - progress_bar.py[line:274] - INFO: epoch 002:   3728 / 11564 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.4, ups=0.92, wpb=113.5, bsz=40, num_updates=15270, lr=4.52056e-05, gnorm=0.312, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74050
2023-02-17 12:21:00 - progress_bar.py[line:274] - INFO: epoch 002:   3738 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.89, wpb=112.2, bsz=40, num_updates=15280, lr=4.52011e-05, gnorm=0.373, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74061
2023-02-17 12:21:11 - progress_bar.py[line:274] - INFO: epoch 002:   3748 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.3, ups=0.92, wpb=111.6, bsz=40, num_updates=15290, lr=4.51966e-05, gnorm=0.437, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74072
2023-02-17 12:21:22 - progress_bar.py[line:274] - INFO: epoch 002:   3758 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.7, ups=0.92, wpb=112.6, bsz=40, num_updates=15300, lr=4.51921e-05, gnorm=0.362, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74082
2023-02-17 12:21:33 - progress_bar.py[line:274] - INFO: epoch 002:   3768 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.87, wpb=112.2, bsz=40, num_updates=15310, lr=4.51876e-05, gnorm=0.334, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74094
2023-02-17 12:21:54 - progress_bar.py[line:274] - INFO: epoch 002:   3778 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.5, ups=0.9, wpb=112.2, bsz=40, num_updates=15320, lr=4.51831e-05, gnorm=0.306, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74105
2023-02-17 12:22:05 - progress_bar.py[line:274] - INFO: epoch 002:   3788 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.7, ups=0.89, wpb=112.7, bsz=40, num_updates=15330, lr=4.51786e-05, gnorm=0.272, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74126
2023-02-17 12:22:16 - progress_bar.py[line:274] - INFO: epoch 002:   3798 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.6, ups=0.92, wpb=112.6, bsz=40, num_updates=15340, lr=4.51741e-05, gnorm=0.256, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74137
2023-02-17 12:22:27 - progress_bar.py[line:274] - INFO: epoch 002:   3808 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.88, wpb=112.4, bsz=40, num_updates=15350, lr=4.51696e-05, gnorm=0.337, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74148
2023-02-17 12:22:38 - progress_bar.py[line:274] - INFO: epoch 002:   3818 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.7, ups=0.91, wpb=112.3, bsz=40, num_updates=15360, lr=4.51651e-05, gnorm=0.338, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74159
2023-02-17 12:22:49 - progress_bar.py[line:274] - INFO: epoch 002:   3828 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.91, wpb=111.4, bsz=40, num_updates=15370, lr=4.51606e-05, gnorm=0.322, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=74170
2023-02-17 12:23:00 - progress_bar.py[line:274] - INFO: epoch 002:   3838 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.9, wpb=111, bsz=40, num_updates=15380, lr=4.51561e-05, gnorm=0.352, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74181
2023-02-17 12:23:11 - progress_bar.py[line:274] - INFO: epoch 002:   3848 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.9, wpb=111.8, bsz=40, num_updates=15390, lr=4.51516e-05, gnorm=0.336, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=74192
2023-02-17 12:23:23 - progress_bar.py[line:274] - INFO: epoch 002:   3858 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99, ups=0.89, wpb=111.6, bsz=40, num_updates=15400, lr=4.51471e-05, gnorm=0.242, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74204
2023-02-17 12:23:33 - progress_bar.py[line:274] - INFO: epoch 002:   3868 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.3, ups=0.93, wpb=113.1, bsz=40, num_updates=15410, lr=4.51425e-05, gnorm=0.322, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=74214
2023-02-17 12:23:45 - progress_bar.py[line:274] - INFO: epoch 002:   3878 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.9, wpb=111.7, bsz=40, num_updates=15420, lr=4.5138e-05, gnorm=0.413, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74225
2023-02-17 12:23:56 - progress_bar.py[line:274] - INFO: epoch 002:   3888 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.89, wpb=111.9, bsz=40, num_updates=15430, lr=4.51335e-05, gnorm=0.28, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74237
2023-02-17 12:24:07 - progress_bar.py[line:274] - INFO: epoch 002:   3898 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.91, wpb=111.5, bsz=40, num_updates=15440, lr=4.5129e-05, gnorm=0.274, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74248
2023-02-17 12:24:17 - progress_bar.py[line:274] - INFO: epoch 002:   3908 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=105.8, ups=0.93, wpb=113.6, bsz=40, num_updates=15450, lr=4.51245e-05, gnorm=0.279, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74258
2023-02-17 12:24:28 - progress_bar.py[line:274] - INFO: epoch 002:   3918 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.92, wpb=110.4, bsz=40, num_updates=15460, lr=4.512e-05, gnorm=0.348, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74269
2023-02-17 12:24:40 - progress_bar.py[line:274] - INFO: epoch 002:   3928 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=112.7, bsz=40, num_updates=15470, lr=4.51155e-05, gnorm=0.244, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74280
2023-02-17 12:24:51 - progress_bar.py[line:274] - INFO: epoch 002:   3938 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.91, wpb=110.5, bsz=40, num_updates=15480, lr=4.5111e-05, gnorm=0.312, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74291
2023-02-17 12:25:02 - progress_bar.py[line:274] - INFO: epoch 002:   3948 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.9, wpb=111.9, bsz=40, num_updates=15490, lr=4.51065e-05, gnorm=0.222, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74303
2023-02-17 12:25:12 - progress_bar.py[line:274] - INFO: epoch 002:   3958 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.4, ups=0.93, wpb=113, bsz=40, num_updates=15500, lr=4.5102e-05, gnorm=0.291, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=74313
2023-02-17 12:25:24 - progress_bar.py[line:274] - INFO: epoch 002:   3968 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.87, wpb=112.5, bsz=40, num_updates=15510, lr=4.50975e-05, gnorm=0.295, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74325
2023-02-17 12:25:35 - progress_bar.py[line:274] - INFO: epoch 002:   3978 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103, ups=0.92, wpb=112.2, bsz=40, num_updates=15520, lr=4.5093e-05, gnorm=0.289, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74336
2023-02-17 12:25:46 - progress_bar.py[line:274] - INFO: epoch 002:   3988 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.88, wpb=111.9, bsz=40, num_updates=15530, lr=4.50885e-05, gnorm=0.264, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74347
2023-02-17 12:25:57 - progress_bar.py[line:274] - INFO: epoch 002:   3998 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.89, wpb=111.8, bsz=40, num_updates=15540, lr=4.5084e-05, gnorm=0.365, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74358
2023-02-17 12:26:08 - progress_bar.py[line:274] - INFO: epoch 002:   4008 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.91, wpb=113.5, bsz=40, num_updates=15550, lr=4.50795e-05, gnorm=0.259, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=74369
2023-02-17 12:26:19 - progress_bar.py[line:274] - INFO: epoch 002:   4018 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103, ups=0.91, wpb=113.6, bsz=40, num_updates=15560, lr=4.5075e-05, gnorm=0.277, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74380
2023-02-17 12:26:30 - progress_bar.py[line:274] - INFO: epoch 002:   4028 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.9, ups=0.91, wpb=112.4, bsz=40, num_updates=15570, lr=4.50705e-05, gnorm=0.234, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74391
2023-02-17 12:26:42 - progress_bar.py[line:274] - INFO: epoch 002:   4038 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.4, ups=0.9, wpb=111, bsz=40, num_updates=15580, lr=4.5066e-05, gnorm=0.339, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74403
2023-02-17 12:26:53 - progress_bar.py[line:274] - INFO: epoch 002:   4048 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=15590, lr=4.50615e-05, gnorm=0.299, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74414
2023-02-17 12:27:04 - progress_bar.py[line:274] - INFO: epoch 002:   4058 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.6, ups=0.88, wpb=112.6, bsz=40, num_updates=15600, lr=4.5057e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74425
2023-02-17 12:27:15 - progress_bar.py[line:274] - INFO: epoch 002:   4068 / 11564 loss=0.197, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=100.7, ups=0.91, wpb=111.2, bsz=40, num_updates=15610, lr=4.50525e-05, gnorm=0.324, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74436
2023-02-17 12:27:26 - progress_bar.py[line:274] - INFO: epoch 002:   4078 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.6, ups=0.88, wpb=111.4, bsz=40, num_updates=15620, lr=4.5048e-05, gnorm=0.331, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74447
2023-02-17 12:27:37 - progress_bar.py[line:274] - INFO: epoch 002:   4088 / 11564 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.8, ups=0.91, wpb=112.4, bsz=40, num_updates=15630, lr=4.50435e-05, gnorm=0.217, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74458
2023-02-17 12:27:48 - progress_bar.py[line:274] - INFO: epoch 002:   4098 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.1, ups=0.91, wpb=113.7, bsz=40, num_updates=15640, lr=4.5039e-05, gnorm=0.367, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74469
2023-02-17 12:27:59 - progress_bar.py[line:274] - INFO: epoch 002:   4108 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=103.7, ups=0.92, wpb=112.9, bsz=40, num_updates=15650, lr=4.50345e-05, gnorm=0.362, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74480
2023-02-17 12:28:10 - progress_bar.py[line:274] - INFO: epoch 002:   4118 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.5, ups=0.92, wpb=112.6, bsz=40, num_updates=15660, lr=4.503e-05, gnorm=0.327, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74491
2023-02-17 12:28:21 - progress_bar.py[line:274] - INFO: epoch 002:   4128 / 11564 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=104.7, ups=0.93, wpb=112.3, bsz=40, num_updates=15670, lr=4.50254e-05, gnorm=0.204, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74502
2023-02-17 12:28:32 - progress_bar.py[line:274] - INFO: epoch 002:   4138 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.5, ups=0.88, wpb=111.4, bsz=40, num_updates=15680, lr=4.50209e-05, gnorm=0.332, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74513
2023-02-17 12:28:43 - progress_bar.py[line:274] - INFO: epoch 002:   4148 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.3, ups=0.91, wpb=111.2, bsz=40, num_updates=15690, lr=4.50164e-05, gnorm=0.519, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74524
2023-02-17 12:28:54 - progress_bar.py[line:274] - INFO: epoch 002:   4158 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.3, ups=0.91, wpb=111.6, bsz=40, num_updates=15700, lr=4.50119e-05, gnorm=0.273, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74535
2023-02-17 12:29:06 - progress_bar.py[line:274] - INFO: epoch 002:   4168 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.7, ups=0.87, wpb=111.9, bsz=40, num_updates=15710, lr=4.50074e-05, gnorm=0.401, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74547
2023-02-17 12:29:17 - progress_bar.py[line:274] - INFO: epoch 002:   4178 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.92, wpb=111.2, bsz=40, num_updates=15720, lr=4.50029e-05, gnorm=0.414, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74558
2023-02-17 12:29:28 - progress_bar.py[line:274] - INFO: epoch 002:   4188 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.7, ups=0.91, wpb=112.2, bsz=40, num_updates=15730, lr=4.49984e-05, gnorm=0.362, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74569
2023-02-17 12:29:39 - progress_bar.py[line:274] - INFO: epoch 002:   4198 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.7, ups=0.88, wpb=111.7, bsz=40, num_updates=15740, lr=4.49939e-05, gnorm=0.389, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74580
2023-02-17 12:29:50 - progress_bar.py[line:274] - INFO: epoch 002:   4208 / 11564 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.063, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=15750, lr=4.49894e-05, gnorm=0.264, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74591
2023-02-17 12:30:01 - progress_bar.py[line:274] - INFO: epoch 002:   4218 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=104.6, ups=0.93, wpb=112.2, bsz=40, num_updates=15760, lr=4.49849e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74602
2023-02-17 12:30:12 - progress_bar.py[line:274] - INFO: epoch 002:   4228 / 11564 loss=0.203, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.3, ups=0.91, wpb=113.9, bsz=40, num_updates=15770, lr=4.49804e-05, gnorm=0.322, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74613
2023-02-17 12:30:23 - progress_bar.py[line:274] - INFO: epoch 002:   4238 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.9, wpb=110.6, bsz=40, num_updates=15780, lr=4.49759e-05, gnorm=0.339, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74624
2023-02-17 12:30:35 - progress_bar.py[line:274] - INFO: epoch 002:   4248 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.2, ups=0.87, wpb=111.8, bsz=40, num_updates=15790, lr=4.49714e-05, gnorm=0.358, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74636
2023-02-17 12:30:46 - progress_bar.py[line:274] - INFO: epoch 002:   4258 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.3, ups=0.87, wpb=113.6, bsz=40, num_updates=15800, lr=4.49669e-05, gnorm=0.406, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74647
2023-02-17 12:30:57 - progress_bar.py[line:274] - INFO: epoch 002:   4268 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.06, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.9, ups=0.91, wpb=113.5, bsz=40, num_updates=15810, lr=4.49624e-05, gnorm=0.245, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74658
2023-02-17 12:31:08 - progress_bar.py[line:274] - INFO: epoch 002:   4278 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.1, ups=0.91, wpb=112.6, bsz=40, num_updates=15820, lr=4.49579e-05, gnorm=0.348, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74669
2023-02-17 12:31:20 - progress_bar.py[line:274] - INFO: epoch 002:   4288 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.8, ups=0.88, wpb=110.4, bsz=40, num_updates=15830, lr=4.49534e-05, gnorm=0.602, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74681
2023-02-17 12:31:31 - progress_bar.py[line:274] - INFO: epoch 002:   4298 / 11564 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.91, wpb=110.4, bsz=40, num_updates=15840, lr=4.49489e-05, gnorm=0.293, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74692
2023-02-17 12:31:42 - progress_bar.py[line:274] - INFO: epoch 002:   4308 / 11564 loss=0.199, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.92, wpb=111.5, bsz=40, num_updates=15850, lr=4.49444e-05, gnorm=0.284, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=74703
2023-02-17 12:31:53 - progress_bar.py[line:274] - INFO: epoch 002:   4318 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.064, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.89, wpb=112.8, bsz=40, num_updates=15860, lr=4.49399e-05, gnorm=0.321, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=74714
2023-02-17 12:32:04 - progress_bar.py[line:274] - INFO: epoch 002:   4328 / 11564 loss=0.189, loss_v1=0, loss_v2=0, nll_loss=0.056, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.9, wpb=110.9, bsz=40, num_updates=15870, lr=4.49354e-05, gnorm=0.24, clip=0, loss_scale=2048, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74725
2023-02-17 12:32:15 - progress_bar.py[line:274] - INFO: epoch 002:   4338 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.2, ups=0.88, wpb=112.3, bsz=40, num_updates=15880, lr=4.49309e-05, gnorm=0.427, clip=10, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74736
2023-02-17 12:32:21 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-17 12:32:28 - progress_bar.py[line:274] - INFO: epoch 002:   4349 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=88.9, ups=0.8, wpb=111, bsz=40, num_updates=15890, lr=4.49264e-05, gnorm=0.483, clip=10, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=74749
2023-02-17 12:32:39 - progress_bar.py[line:274] - INFO: epoch 002:   4359 / 11564 loss=0.193, loss_v1=0, loss_v2=0, nll_loss=0.061, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.7, ups=0.89, wpb=111.6, bsz=40, num_updates=15900, lr=4.49219e-05, gnorm=0.348, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=74760
2023-02-17 12:32:50 - progress_bar.py[line:274] - INFO: epoch 002:   4369 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.89, wpb=112.8, bsz=40, num_updates=15910, lr=4.49174e-05, gnorm=0.426, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74771
2023-02-17 12:33:01 - progress_bar.py[line:274] - INFO: epoch 002:   4379 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.1, ups=0.92, wpb=112.2, bsz=40, num_updates=15920, lr=4.49128e-05, gnorm=0.629, clip=20, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=74782
2023-02-17 12:33:12 - progress_bar.py[line:274] - INFO: epoch 002:   4389 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.2, ups=0.88, wpb=111.3, bsz=40, num_updates=15930, lr=4.49083e-05, gnorm=0.457, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74793
2023-02-17 12:33:24 - progress_bar.py[line:274] - INFO: epoch 002:   4399 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.89, wpb=110.5, bsz=40, num_updates=15940, lr=4.49038e-05, gnorm=0.331, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74805
2023-02-17 12:33:35 - progress_bar.py[line:274] - INFO: epoch 002:   4409 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.057, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=99.4, ups=0.88, wpb=112.5, bsz=40, num_updates=15950, lr=4.48993e-05, gnorm=0.269, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=74816
2023-02-17 12:33:46 - progress_bar.py[line:274] - INFO: epoch 002:   4419 / 11564 loss=0.196, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.2, ups=0.88, wpb=111.4, bsz=40, num_updates=15960, lr=4.48948e-05, gnorm=0.221, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=74827
2023-02-17 12:33:57 - progress_bar.py[line:274] - INFO: epoch 002:   4429 / 11564 loss=0.19, loss_v1=0, loss_v2=0, nll_loss=0.058, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.3, ups=0.91, wpb=113, bsz=40, num_updates=15970, lr=4.48903e-05, gnorm=0.273, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=74838
2023-02-17 12:34:09 - progress_bar.py[line:274] - INFO: epoch 002:   4439 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.89, wpb=112.2, bsz=40, num_updates=15980, lr=4.48858e-05, gnorm=0.335, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74850
2023-02-17 12:34:20 - progress_bar.py[line:274] - INFO: epoch 002:   4449 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.6, ups=0.89, wpb=110.6, bsz=40, num_updates=15990, lr=4.48813e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=74861
2023-02-17 12:34:31 - progress_bar.py[line:274] - INFO: epoch 002:   4459 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.9, wpb=110.2, bsz=40, num_updates=16000, lr=4.48768e-05, gnorm=0.414, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=74872
2023-02-17 12:34:31 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-17 12:34:32 - train.py[line:549] - INFO: 0 / 6234
2023-02-17 12:34:32 - train.py[line:551] - INFO: load:1.04 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-17 12:36:35 - train.py[line:549] - INFO: 200 / 6234
2023-02-17 12:36:35 - train.py[line:551] - INFO: load:1.06 valid_run:122.36 task_valid:119.46 collect_output:1.79
2023-02-17 12:38:35 - train.py[line:549] - INFO: 400 / 6234
2023-02-17 12:38:35 - train.py[line:551] - INFO: load:1.09 valid_run:242.55 task_valid:235.65 collect_output:4.73
2023-02-17 12:40:37 - train.py[line:549] - INFO: 600 / 6234
2023-02-17 12:40:37 - train.py[line:551] - INFO: load:1.11 valid_run:364.69 task_valid:352.50 collect_output:8.98
2023-02-17 12:42:39 - train.py[line:549] - INFO: 800 / 6234
2023-02-17 12:42:39 - train.py[line:551] - INFO: load:1.14 valid_run:486.70 task_valid:466.41 collect_output:16.03
2023-02-17 12:44:40 - train.py[line:549] - INFO: 1000 / 6234
2023-02-17 12:44:40 - train.py[line:551] - INFO: load:1.17 valid_run:607.27 task_valid:583.88 collect_output:18.09
2023-02-17 12:46:43 - train.py[line:549] - INFO: 1200 / 6234
2023-02-17 12:46:43 - train.py[line:551] - INFO: load:1.19 valid_run:730.28 task_valid:702.76 collect_output:21.21
2023-02-17 12:48:46 - train.py[line:549] - INFO: 1400 / 6234
2023-02-17 12:48:46 - train.py[line:551] - INFO: load:1.22 valid_run:853.30 task_valid:820.90 collect_output:25.06
2023-02-17 12:50:48 - train.py[line:549] - INFO: 1600 / 6234
2023-02-17 12:50:48 - train.py[line:551] - INFO: load:1.25 valid_run:975.22 task_valid:937.53 collect_output:29.29
2023-02-17 12:52:52 - train.py[line:549] - INFO: 1800 / 6234
2023-02-17 12:52:52 - train.py[line:551] - INFO: load:1.27 valid_run:1098.95 task_valid:1054.86 collect_output:34.65
2023-02-17 12:54:53 - train.py[line:549] - INFO: 2000 / 6234
2023-02-17 12:54:53 - train.py[line:551] - INFO: load:1.30 valid_run:1220.53 task_valid:1167.51 collect_output:42.53
2023-02-17 12:56:54 - train.py[line:549] - INFO: 2200 / 6234
2023-02-17 12:56:54 - train.py[line:551] - INFO: load:1.32 valid_run:1340.80 task_valid:1283.33 collect_output:45.95
2023-02-17 12:58:55 - train.py[line:549] - INFO: 2400 / 6234
2023-02-17 12:58:55 - train.py[line:551] - INFO: load:1.35 valid_run:1462.34 task_valid:1400.30 collect_output:49.49
2023-02-17 13:00:54 - train.py[line:549] - INFO: 2600 / 6234
2023-02-17 13:00:54 - train.py[line:551] - INFO: load:1.37 valid_run:1581.27 task_valid:1514.16 collect_output:53.54
2023-02-17 13:02:55 - train.py[line:549] - INFO: 2800 / 6234
2023-02-17 13:02:55 - train.py[line:551] - INFO: load:1.40 valid_run:1702.33 task_valid:1631.97 collect_output:55.77
2023-02-17 13:04:56 - train.py[line:549] - INFO: 3000 / 6234
2023-02-17 13:04:56 - train.py[line:551] - INFO: load:1.42 valid_run:1823.18 task_valid:1748.07 collect_output:59.51
2023-02-17 13:06:57 - train.py[line:549] - INFO: 3200 / 6234
2023-02-17 13:06:57 - train.py[line:551] - INFO: load:1.45 valid_run:1944.20 task_valid:1862.06 collect_output:65.51
2023-02-17 13:08:59 - train.py[line:549] - INFO: 3400 / 6234
2023-02-17 13:08:59 - train.py[line:551] - INFO: load:1.48 valid_run:2065.61 task_valid:1978.15 collect_output:69.80
2023-02-17 13:11:00 - train.py[line:549] - INFO: 3600 / 6234
2023-02-17 13:11:00 - train.py[line:551] - INFO: load:1.50 valid_run:2186.27 task_valid:2096.07 collect_output:71.54
2023-02-17 13:13:01 - train.py[line:549] - INFO: 3800 / 6234
2023-02-17 13:13:01 - train.py[line:551] - INFO: load:1.53 valid_run:2307.63 task_valid:2213.15 collect_output:74.78
2023-02-17 13:15:02 - train.py[line:549] - INFO: 4000 / 6234
2023-02-17 13:15:02 - train.py[line:551] - INFO: load:1.56 valid_run:2428.05 task_valid:2329.86 collect_output:77.45
2023-02-17 13:17:03 - train.py[line:549] - INFO: 4200 / 6234
2023-02-17 13:17:03 - train.py[line:551] - INFO: load:1.58 valid_run:2549.67 task_valid:2446.42 collect_output:81.48
2023-02-17 13:19:05 - train.py[line:549] - INFO: 4400 / 6234
2023-02-17 13:19:05 - train.py[line:551] - INFO: load:1.61 valid_run:2671.55 task_valid:2565.27 collect_output:83.51
2023-02-17 13:21:06 - train.py[line:549] - INFO: 4600 / 6234
2023-02-17 13:21:06 - train.py[line:551] - INFO: load:1.64 valid_run:2791.86 task_valid:2679.46 collect_output:88.61
2023-02-17 13:23:05 - train.py[line:549] - INFO: 4800 / 6234
2023-02-17 13:23:05 - train.py[line:551] - INFO: load:1.66 valid_run:2911.60 task_valid:2795.52 collect_output:91.28
2023-02-17 13:25:07 - train.py[line:549] - INFO: 5000 / 6234
2023-02-17 13:25:07 - train.py[line:551] - INFO: load:1.69 valid_run:3033.26 task_valid:2911.81 collect_output:95.63
2023-02-17 13:27:10 - train.py[line:549] - INFO: 5200 / 6234
2023-02-17 13:27:10 - train.py[line:551] - INFO: load:1.71 valid_run:3155.94 task_valid:3027.82 collect_output:101.30
2023-02-17 13:29:09 - train.py[line:549] - INFO: 5400 / 6234
2023-02-17 13:29:09 - train.py[line:551] - INFO: load:1.74 valid_run:3275.60 task_valid:3141.95 collect_output:105.83
2023-02-17 13:31:11 - train.py[line:549] - INFO: 5600 / 6234
2023-02-17 13:31:11 - train.py[line:551] - INFO: load:1.77 valid_run:3397.39 task_valid:3261.24 collect_output:107.31
2023-02-17 13:33:13 - train.py[line:549] - INFO: 5800 / 6234
2023-02-17 13:33:13 - train.py[line:551] - INFO: load:1.79 valid_run:3519.05 task_valid:3376.85 collect_output:112.32
2023-02-17 13:35:15 - train.py[line:549] - INFO: 6000 / 6234
2023-02-17 13:35:15 - train.py[line:551] - INFO: load:1.82 valid_run:3640.96 task_valid:3495.38 collect_output:114.70
2023-02-17 13:37:16 - train.py[line:549] - INFO: 6200 / 6234
2023-02-17 13:37:16 - train.py[line:551] - INFO: load:1.85 valid_run:3761.99 task_valid:3613.78 collect_output:116.31

====================================================================================================
SGG eval:     R @ 50: 0.6043;     R @ 100: 0.6466;     R @ 500: 0.6719;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3870;    mR @ 100: 0.4649;    mR @ 500: 0.5097;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6875) (covering:0.2286) (eating:0.7647) (flying in:0.8636) (growing on:0.3750) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9412) (says:0.0000) (sitting on:0.6944) (standing on:0.3743) (using:0.5500) (walking in:0.0000) (walking on:0.6216) (watching:0.4444) 
--------------------------------------------------------
====================================================================================================

2023-02-17 13:37:47 - train.py[line:487] - INFO: 0.6466170104405399

====================================================================================================
SGG eval:     R @ 50: 0.6043;     R @ 100: 0.6466;     R @ 500: 0.6719;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3870;    mR @ 100: 0.4649;    mR @ 500: 0.5097;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6875) (covering:0.2286) (eating:0.7647) (flying in:0.8636) (growing on:0.3750) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9412) (says:0.0000) (sitting on:0.6944) (standing on:0.3743) (using:0.5500) (walking in:0.0000) (walking on:0.6216) (watching:0.4444) 
--------------------------------------------------------
====================================================================================================

2023-02-17 13:37:47 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-17 13:37:47 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.254 | loss_v1 0 | loss_v2 0 | nll_loss 0.09 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.646617 | ppl 1.06 | vqa_score 0.5236 | wps 118.2 | wpb 72 | bsz 24 | num_updates 16000 | best_R@100 0.69202
2023-02-17 13:37:47 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 16000 updates
2023-02-17 13:37:47 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_16000.pt
2023-02-17 13:37:53 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_16000.pt
2023-02-17 13:37:55 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_same_step_optNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.04_5e-5_480/checkpoint_2_16000.pt (epoch 2 @ 16000 updates, score 0.6466170104405399) (writing took 8.276989107951522 seconds)
2023-02-17 13:38:07 - progress_bar.py[line:274] - INFO: epoch 002:   4469 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=0.3, ups=0, wpb=111.6, bsz=40, num_updates=16010, lr=4.48723e-05, gnorm=0.418, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=78688
2023-02-17 13:38:19 - progress_bar.py[line:274] - INFO: epoch 002:   4479 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=111.7, bsz=40, num_updates=16020, lr=4.48678e-05, gnorm=0.443, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=78700
2023-02-17 13:38:30 - progress_bar.py[line:274] - INFO: epoch 002:   4489 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.6, ups=0.91, wpb=111.1, bsz=40, num_updates=16030, lr=4.48633e-05, gnorm=0.38, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=78711
2023-02-17 13:38:40 - progress_bar.py[line:274] - INFO: epoch 002:   4499 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=105.4, ups=0.94, wpb=111.7, bsz=40, num_updates=16040, lr=4.48588e-05, gnorm=0.29, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=78721
2023-02-17 13:38:51 - progress_bar.py[line:274] - INFO: epoch 002:   4509 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.059, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101, ups=0.9, wpb=112.4, bsz=40, num_updates=16050, lr=4.48543e-05, gnorm=0.255, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=78732
2023-02-17 13:39:03 - progress_bar.py[line:274] - INFO: epoch 002:   4519 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.065, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.9, ups=0.89, wpb=112.8, bsz=40, num_updates=16060, lr=4.48498e-05, gnorm=0.263, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=78744
2023-02-17 13:39:14 - progress_bar.py[line:274] - INFO: epoch 002:   4529 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=96.9, ups=0.87, wpb=111.2, bsz=40, num_updates=16070, lr=4.48453e-05, gnorm=0.363, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=78755
2023-02-17 13:39:25 - progress_bar.py[line:274] - INFO: epoch 002:   4539 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.91, wpb=110.8, bsz=40, num_updates=16080, lr=4.48408e-05, gnorm=0.364, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=78766
2023-02-17 13:39:36 - progress_bar.py[line:274] - INFO: epoch 002:   4549 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98, ups=0.88, wpb=110.8, bsz=40, num_updates=16090, lr=4.48363e-05, gnorm=0.436, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=78777
2023-02-17 13:39:48 - progress_bar.py[line:274] - INFO: epoch 002:   4559 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.8, ups=0.91, wpb=112.4, bsz=40, num_updates=16100, lr=4.48318e-05, gnorm=0.367, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=78788
2023-02-17 13:39:59 - progress_bar.py[line:274] - INFO: epoch 002:   4569 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.88, wpb=112.3, bsz=40, num_updates=16110, lr=4.48273e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=78800
2023-02-17 13:40:10 - progress_bar.py[line:274] - INFO: epoch 002:   4579 / 11564 loss=0.198, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=98.6, ups=0.88, wpb=111.9, bsz=40, num_updates=16120, lr=4.48228e-05, gnorm=0.269, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=78811
2023-02-17 13:40:21 - progress_bar.py[line:274] - INFO: epoch 002:   4589 / 11564 loss=0.201, loss_v1=0, loss_v2=0, nll_loss=0.068, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=111.6, bsz=40, num_updates=16130, lr=4.48183e-05, gnorm=0.339, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=78822
2023-02-17 13:40:32 - progress_bar.py[line:274] - INFO: epoch 002:   4599 / 11564 loss=0.194, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=102.8, ups=0.91, wpb=113.3, bsz=40, num_updates=16140, lr=4.48138e-05, gnorm=0.241, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=78833
2023-02-17 13:40:44 - progress_bar.py[line:274] - INFO: epoch 002:   4609 / 11564 loss=0.192, loss_v1=0, loss_v2=0, nll_loss=0.066, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.89, wpb=112.5, bsz=40, num_updates=16150, lr=4.48093e-05, gnorm=0.275, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=78845
2023-02-17 13:40:55 - progress_bar.py[line:274] - INFO: epoch 002:   4619 / 11564 loss=0.195, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=101.9, ups=0.91, wpb=112.2, bsz=40, num_updates=16160, lr=4.48048e-05, gnorm=0.326, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=78856
2023-02-17 13:41:05 - progress_bar.py[line:274] - INFO: epoch 002:   4629 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.07, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.7, ups=0.92, wpb=112.8, bsz=40, num_updates=16170, lr=4.48003e-05, gnorm=0.358, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=78866
2023-02-17 13:41:17 - progress_bar.py[line:274] - INFO: epoch 002:   4639 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.88, wpb=112, bsz=40, num_updates=16180, lr=4.47957e-05, gnorm=0.331, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=78878
2023-02-17 13:41:28 - progress_bar.py[line:274] - INFO: epoch 002:   4649 / 11564 loss=0.188, loss_v1=0, loss_v2=0, nll_loss=0.062, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.04, wps=102.7, ups=0.91, wpb=113.1, bsz=40, num_updates=16190, lr=4.47912e-05, gnorm=0.263, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=78889
2023-02-17 13:41:39 - progress_bar.py[line:274] - INFO: epoch 002:   4659 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.5, ups=0.9, wpb=111.1, bsz=40, num_updates=16200, lr=4.47867e-05, gnorm=0.306, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=78900
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1556487
Killing subprocess 1556488
Main process received SIGINT, exiting
