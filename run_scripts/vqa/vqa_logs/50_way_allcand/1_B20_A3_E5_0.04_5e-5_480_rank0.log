2022-09-28 15:35:20 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2022-09-28 15:35:20 - utils.py[line:261] - INFO: Start init
2022-09-28 15:35:20 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2022-09-28 15:35:20 - utils.py[line:261] - INFO: Start init
2022-09-28 15:35:20 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2022-09-28 15:35:20 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-09-28 15:35:20 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2022-09-28 15:35:20 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2022-09-28 15:35:23 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way_allcand', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 5, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 6000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 15, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [3], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way_allcand/1_B20_A3_E5_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 6000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='15', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=5, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way_allcand/1_B20_A3_E5_0.04_5e-5_480', save_interval=10, save_interval_updates=6000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way_allcand', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[3], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=6000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer'}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2022-09-28 15:35:24 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2022-09-28 15:35:24 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2022-09-28 15:35:28 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2022-09-28 15:35:28 - train.py[line:118] - INFO: task: VqaGenTask
2022-09-28 15:35:28 - train.py[line:119] - INFO: model: OFAModel
2022-09-28 15:35:28 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2022-09-28 15:35:28 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2022-09-28 15:35:28 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv slice_id 0 row count 45697 total row count 91394
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-09-28 15:35:28 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv slice_id 1 row count 45697 total row count 91394
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-09-28 15:35:28 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-09-28 15:35:29 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-09-28 15:35:29 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-09-28 15:35:29 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-09-28 15:35:29 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-09-28 15:35:29 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2022-09-28 15:35:29 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2022-09-28 15:35:29 - trainer.py[line:458] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2022-09-28 15:35:37 - trainer.py[line:594] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2022-09-28 15:35:37 - trainer.py[line:594] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2022-09-28 15:35:38 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2022-09-28 15:35:38 - trainer.py[line:273] - INFO: Exponential Moving Average Shadow Model is initialized.
2022-09-28 15:35:38 - trainer.py[line:623] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2022-09-28 15:35:38 - trainer.py[line:643] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 315642 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 315642 total row count 631284
2022-09-28 15:35:39 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 26305, warmup steps 1052, warmup_factor 0.0009505703422053232
Total steps 26305, warmup steps 1052, warmup_factor 0.0009505703422053232
2022-09-28 15:35:39 - trainer.py[line:707] - INFO: begin training epoch 1
2022-09-28 15:35:39 - train.py[line:312] - INFO: Start iterating over samples
2022-09-28 15:35:47 - trainer.py[line:930] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-09-28 15:36:19 - progress_bar.py[line:274] - INFO: epoch 001:     11 / 5261 loss=1.361, loss_v1=0, loss_v2=0, nll_loss=1.146, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=2.21, wps=98.7, ups=0.33, wpb=303.3, bsz=120, num_updates=10, lr=4.75285e-07, gnorm=12.141, clip=100, loss_scale=64, train_wall=38, gb_free=9.9, ema_decay=0.9999, wall=50
2022-09-28 15:36:50 - progress_bar.py[line:274] - INFO: epoch 001:     21 / 5261 loss=1.282, loss_v1=0, loss_v2=0, nll_loss=1.068, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=2.1, wps=98.2, ups=0.32, wpb=306, bsz=120, num_updates=20, lr=9.5057e-07, gnorm=9.834, clip=100, loss_scale=64, train_wall=31, gb_free=10, ema_decay=0.9999, wall=81
2022-09-28 15:37:20 - progress_bar.py[line:274] - INFO: epoch 001:     31 / 5261 loss=1.212, loss_v1=0, loss_v2=0, nll_loss=1.01, ntokens=306.5, nsentences=120, sample_size=306.5, sample_size_v1=0, sample_size_v2=0, ppl=2.01, wps=99.9, ups=0.33, wpb=306.5, bsz=120, num_updates=30, lr=1.42586e-06, gnorm=7.679, clip=100, loss_scale=64, train_wall=31, gb_free=9.9, ema_decay=0.9999, wall=112
2022-09-28 15:37:51 - progress_bar.py[line:274] - INFO: epoch 001:     41 / 5261 loss=1.193, loss_v1=0, loss_v2=0, nll_loss=1.012, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=2.02, wps=99.5, ups=0.33, wpb=301.7, bsz=120, num_updates=40, lr=1.90114e-06, gnorm=5.14, clip=100, loss_scale=64, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=142
2022-09-28 15:38:21 - progress_bar.py[line:274] - INFO: epoch 001:     51 / 5261 loss=1.049, loss_v1=0, loss_v2=0, nll_loss=0.866, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=101.6, ups=0.33, wpb=305, bsz=120, num_updates=50, lr=2.37643e-06, gnorm=3.374, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=172
2022-09-28 15:38:51 - progress_bar.py[line:274] - INFO: epoch 001:     61 / 5261 loss=0.969, loss_v1=0, loss_v2=0, nll_loss=0.784, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=100.7, ups=0.33, wpb=305, bsz=120, num_updates=60, lr=2.85171e-06, gnorm=3.027, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=202
2022-09-28 15:39:21 - progress_bar.py[line:274] - INFO: epoch 001:     71 / 5261 loss=0.983, loss_v1=0, loss_v2=0, nll_loss=0.809, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.75, wps=102.3, ups=0.34, wpb=305, bsz=120, num_updates=70, lr=3.327e-06, gnorm=2.8, clip=100, loss_scale=64, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=232
2022-09-28 15:39:51 - progress_bar.py[line:274] - INFO: epoch 001:     81 / 5261 loss=0.925, loss_v1=0, loss_v2=0, nll_loss=0.754, ntokens=306.7, nsentences=120, sample_size=306.7, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=103.1, ups=0.34, wpb=306.7, bsz=120, num_updates=80, lr=3.80228e-06, gnorm=2.343, clip=100, loss_scale=64, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=262
2022-09-28 15:40:20 - progress_bar.py[line:274] - INFO: epoch 001:     91 / 5261 loss=0.964, loss_v1=0, loss_v2=0, nll_loss=0.8, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=103, ups=0.34, wpb=303.3, bsz=120, num_updates=90, lr=4.27757e-06, gnorm=2.124, clip=100, loss_scale=64, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=291
2022-09-28 15:40:50 - progress_bar.py[line:274] - INFO: epoch 001:    101 / 5261 loss=0.99, loss_v1=0, loss_v2=0, nll_loss=0.834, ntokens=300.1, nsentences=120, sample_size=300.1, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=101.1, ups=0.34, wpb=300.1, bsz=120, num_updates=100, lr=4.75285e-06, gnorm=2.141, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=321
2022-09-28 15:41:19 - progress_bar.py[line:274] - INFO: epoch 001:    111 / 5261 loss=0.927, loss_v1=0, loss_v2=0, nll_loss=0.767, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=103.5, ups=0.34, wpb=304.2, bsz=120, num_updates=110, lr=5.22814e-06, gnorm=1.806, clip=100, loss_scale=64, train_wall=29, gb_free=10, ema_decay=0.9999, wall=350
2022-09-28 15:41:49 - progress_bar.py[line:274] - INFO: epoch 001:    121 / 5261 loss=0.884, loss_v1=0, loss_v2=0, nll_loss=0.721, ntokens=306.1, nsentences=120, sample_size=306.1, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=104, ups=0.34, wpb=306.1, bsz=120, num_updates=120, lr=5.70342e-06, gnorm=1.631, clip=100, loss_scale=64, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=380
2022-09-28 15:42:19 - progress_bar.py[line:274] - INFO: epoch 001:    131 / 5261 loss=0.908, loss_v1=0, loss_v2=0, nll_loss=0.747, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=100.7, ups=0.33, wpb=305.5, bsz=120, num_updates=130, lr=6.17871e-06, gnorm=1.724, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=410
2022-09-28 15:42:48 - progress_bar.py[line:274] - INFO: epoch 001:    141 / 5261 loss=0.898, loss_v1=0, loss_v2=0, nll_loss=0.734, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=103.7, ups=0.34, wpb=301.7, bsz=120, num_updates=140, lr=6.65399e-06, gnorm=1.646, clip=100, loss_scale=64, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=439
2022-09-28 15:43:18 - progress_bar.py[line:274] - INFO: epoch 001:    151 / 5261 loss=0.922, loss_v1=0, loss_v2=0, nll_loss=0.76, ntokens=299.7, nsentences=120, sample_size=299.7, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=100, ups=0.33, wpb=299.7, bsz=120, num_updates=150, lr=7.12928e-06, gnorm=1.59, clip=100, loss_scale=64, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=469
2022-09-28 15:43:48 - progress_bar.py[line:274] - INFO: epoch 001:    161 / 5261 loss=0.865, loss_v1=0, loss_v2=0, nll_loss=0.699, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=100.4, ups=0.33, wpb=304.6, bsz=120, num_updates=160, lr=7.60456e-06, gnorm=1.722, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=500
2022-09-28 15:44:19 - progress_bar.py[line:274] - INFO: epoch 001:    171 / 5261 loss=0.859, loss_v1=0, loss_v2=0, nll_loss=0.69, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=100.7, ups=0.33, wpb=304.3, bsz=120, num_updates=170, lr=8.07985e-06, gnorm=1.692, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=530
2022-09-28 15:44:48 - progress_bar.py[line:274] - INFO: epoch 001:    181 / 5261 loss=0.878, loss_v1=0, loss_v2=0, nll_loss=0.71, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=103.5, ups=0.34, wpb=302.5, bsz=120, num_updates=180, lr=8.55513e-06, gnorm=1.637, clip=100, loss_scale=64, train_wall=29, gb_free=10, ema_decay=0.9999, wall=559
2022-09-28 15:45:17 - progress_bar.py[line:274] - INFO: epoch 001:    191 / 5261 loss=0.85, loss_v1=0, loss_v2=0, nll_loss=0.68, ntokens=305.6, nsentences=120, sample_size=305.6, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=103.2, ups=0.34, wpb=305.6, bsz=120, num_updates=190, lr=9.03042e-06, gnorm=1.768, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=589
2022-09-28 15:45:47 - progress_bar.py[line:274] - INFO: epoch 001:    201 / 5261 loss=0.794, loss_v1=0, loss_v2=0, nll_loss=0.618, ntokens=309.2, nsentences=120, sample_size=309.2, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=103.5, ups=0.33, wpb=309.2, bsz=120, num_updates=200, lr=9.5057e-06, gnorm=1.785, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=618
2022-09-28 15:46:18 - progress_bar.py[line:274] - INFO: epoch 001:    211 / 5261 loss=0.822, loss_v1=0, loss_v2=0, nll_loss=0.653, ntokens=308.2, nsentences=120, sample_size=308.2, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=100.8, ups=0.33, wpb=308.2, bsz=120, num_updates=210, lr=9.98099e-06, gnorm=1.816, clip=100, loss_scale=64, train_wall=31, gb_free=10, ema_decay=0.9999, wall=649
2022-09-28 15:46:48 - progress_bar.py[line:274] - INFO: epoch 001:    221 / 5261 loss=0.84, loss_v1=0, loss_v2=0, nll_loss=0.664, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=101.5, ups=0.33, wpb=304, bsz=120, num_updates=220, lr=1.04563e-05, gnorm=1.783, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=679
2022-09-28 15:47:18 - progress_bar.py[line:274] - INFO: epoch 001:    231 / 5261 loss=0.802, loss_v1=0, loss_v2=0, nll_loss=0.615, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=101.6, ups=0.34, wpb=302.9, bsz=120, num_updates=230, lr=1.09316e-05, gnorm=1.952, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=709
2022-09-28 15:47:47 - progress_bar.py[line:274] - INFO: epoch 001:    241 / 5261 loss=0.794, loss_v1=0, loss_v2=0, nll_loss=0.609, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=102.1, ups=0.34, wpb=304.5, bsz=120, num_updates=240, lr=1.14068e-05, gnorm=1.979, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=739
2022-09-28 15:48:17 - progress_bar.py[line:274] - INFO: epoch 001:    251 / 5261 loss=0.779, loss_v1=0, loss_v2=0, nll_loss=0.589, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=101.5, ups=0.33, wpb=304.5, bsz=120, num_updates=250, lr=1.18821e-05, gnorm=1.752, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=769
2022-09-28 15:48:48 - progress_bar.py[line:274] - INFO: epoch 001:    261 / 5261 loss=0.753, loss_v1=0, loss_v2=0, nll_loss=0.553, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=100.3, ups=0.33, wpb=304.5, bsz=120, num_updates=260, lr=1.23574e-05, gnorm=1.723, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=799
2022-09-28 15:49:17 - progress_bar.py[line:274] - INFO: epoch 001:    271 / 5261 loss=0.781, loss_v1=0, loss_v2=0, nll_loss=0.589, ntokens=300.3, nsentences=120, sample_size=300.3, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=102.1, ups=0.34, wpb=300.3, bsz=120, num_updates=270, lr=1.28327e-05, gnorm=1.688, clip=100, loss_scale=64, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=828
2022-09-28 15:49:47 - progress_bar.py[line:274] - INFO: epoch 001:    281 / 5261 loss=0.742, loss_v1=0, loss_v2=0, nll_loss=0.548, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=101.7, ups=0.33, wpb=304.2, bsz=120, num_updates=280, lr=1.3308e-05, gnorm=1.509, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=858
2022-09-28 15:50:17 - progress_bar.py[line:274] - INFO: epoch 001:    291 / 5261 loss=0.751, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=101.4, ups=0.33, wpb=304.8, bsz=120, num_updates=290, lr=1.37833e-05, gnorm=1.642, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=888
2022-09-28 15:50:46 - progress_bar.py[line:274] - INFO: epoch 001:    301 / 5261 loss=0.723, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=307.1, nsentences=120, sample_size=307.1, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=105, ups=0.34, wpb=307.1, bsz=120, num_updates=300, lr=1.42586e-05, gnorm=1.499, clip=100, loss_scale=64, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=918
2022-09-28 15:51:16 - progress_bar.py[line:274] - INFO: epoch 001:    311 / 5261 loss=0.725, loss_v1=0, loss_v2=0, nll_loss=0.524, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=102.1, ups=0.34, wpb=302.9, bsz=120, num_updates=310, lr=1.47338e-05, gnorm=1.584, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=947
2022-09-28 15:51:46 - progress_bar.py[line:274] - INFO: epoch 001:    321 / 5261 loss=0.752, loss_v1=0, loss_v2=0, nll_loss=0.548, ntokens=299, nsentences=120, sample_size=299, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=98.8, ups=0.33, wpb=299, bsz=120, num_updates=320, lr=1.52091e-05, gnorm=1.529, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=978
2022-09-28 15:52:16 - progress_bar.py[line:274] - INFO: epoch 001:    331 / 5261 loss=0.7, loss_v1=0, loss_v2=0, nll_loss=0.496, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=101.9, ups=0.33, wpb=304.2, bsz=120, num_updates=330, lr=1.56844e-05, gnorm=1.46, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1007
2022-09-28 15:52:46 - progress_bar.py[line:274] - INFO: epoch 001:    341 / 5261 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=309.1, nsentences=120, sample_size=309.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=102.8, ups=0.33, wpb=309.1, bsz=120, num_updates=340, lr=1.61597e-05, gnorm=1.359, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=1037
2022-09-28 15:53:16 - progress_bar.py[line:274] - INFO: epoch 001:    351 / 5261 loss=0.675, loss_v1=0, loss_v2=0, nll_loss=0.472, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=101.2, ups=0.33, wpb=305, bsz=120, num_updates=350, lr=1.6635e-05, gnorm=1.224, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1068
2022-09-28 15:53:47 - progress_bar.py[line:274] - INFO: epoch 001:    361 / 5261 loss=0.687, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=100.5, ups=0.33, wpb=302.5, bsz=120, num_updates=360, lr=1.71103e-05, gnorm=1.472, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1098
2022-09-28 15:54:16 - progress_bar.py[line:274] - INFO: epoch 001:    371 / 5261 loss=0.686, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=102.5, ups=0.34, wpb=304.6, bsz=120, num_updates=370, lr=1.75856e-05, gnorm=1.309, clip=100, loss_scale=64, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=1127
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 846020
Killing subprocess 846021
Main process received SIGINT, exiting
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/__init__.py", line 196, in <module>
    from torch._C import *
RuntimeError: KeyboardInterrupt: 
Fatal Python error: initsite: Failed to import the site module
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site.py", line 579, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site.py", line 566, in main
    known_paths = addsitepackages(known_paths)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site.py", line 349, in addsitepackages
    addsitedir(sitedir, known_paths)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site.py", line 207, in addsitedir
    addpackage(sitedir, name, known_paths)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site.py", line 168, in addpackage
    exec(line)
  File "<string>", line 1, in <module>
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/types.py", line 32, in <module>
KeyboardInterrupt
2022-09-28 19:44:43 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2022-09-28 19:44:43 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2022-09-28 19:44:43 - utils.py[line:261] - INFO: Start init
2022-09-28 19:44:43 - utils.py[line:261] - INFO: Start init
2022-09-28 19:44:43 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2022-09-28 19:44:43 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-09-28 19:44:43 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2022-09-28 19:44:43 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2022-09-28 19:44:47 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way_allcand', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 5, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 6000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 15, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [3], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way_allcand/1_B20_A3_E5_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 6000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='15', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=5, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way_allcand/1_B20_A3_E5_0.04_5e-5_480', save_interval=10, save_interval_updates=6000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way_allcand', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[3], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=6000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer'}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2022-09-28 19:44:47 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2022-09-28 19:44:47 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2022-09-28 19:44:52 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2022-09-28 19:44:52 - train.py[line:118] - INFO: task: VqaGenTask
2022-09-28 19:44:52 - train.py[line:119] - INFO: model: OFAModel
2022-09-28 19:44:52 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2022-09-28 19:44:52 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2022-09-28 19:44:52 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 0 row count 141030 total row count 282060
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 1 row count 141030 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-09-28 19:44:52 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-09-28 19:44:52 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-09-28 19:44:52 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-09-28 19:44:52 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-09-28 19:44:52 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-09-28 19:44:52 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-09-28 19:44:52 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2022-09-28 19:44:52 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2022-09-28 19:44:52 - trainer.py[line:458] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2022-09-28 19:45:00 - trainer.py[line:594] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2022-09-28 19:45:00 - trainer.py[line:594] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2022-09-28 19:45:01 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2022-09-28 19:45:01 - trainer.py[line:273] - INFO: Exponential Moving Average Shadow Model is initialized.
2022-09-28 19:45:01 - trainer.py[line:623] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2022-09-28 19:45:01 - trainer.py[line:643] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 315642 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 315642 total row count 631284
2022-09-28 19:45:02 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 26305, warmup steps 1052, warmup_factor 0.0009505703422053232
Total steps 26305, warmup steps 1052, warmup_factor 0.0009505703422053232
2022-09-28 19:45:02 - trainer.py[line:707] - INFO: begin training epoch 1
2022-09-28 19:45:02 - train.py[line:312] - INFO: Start iterating over samples
2022-09-28 19:45:11 - trainer.py[line:930] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-09-28 19:45:42 - progress_bar.py[line:274] - INFO: epoch 001:     11 / 5261 loss=1.361, loss_v1=0, loss_v2=0, nll_loss=1.146, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=2.21, wps=99.6, ups=0.33, wpb=303.3, bsz=120, num_updates=10, lr=4.75285e-07, gnorm=12.148, clip=100, loss_scale=64, train_wall=37, gb_free=9.9, ema_decay=0.9999, wall=50
2022-09-28 19:46:14 - progress_bar.py[line:274] - INFO: epoch 001:     21 / 5261 loss=1.282, loss_v1=0, loss_v2=0, nll_loss=1.068, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=2.1, wps=97, ups=0.32, wpb=306, bsz=120, num_updates=20, lr=9.5057e-07, gnorm=9.844, clip=100, loss_scale=64, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=82
2022-09-28 19:46:46 - progress_bar.py[line:274] - INFO: epoch 001:     31 / 5261 loss=1.212, loss_v1=0, loss_v2=0, nll_loss=1.01, ntokens=306.5, nsentences=120, sample_size=306.5, sample_size_v1=0, sample_size_v2=0, ppl=2.01, wps=95.5, ups=0.31, wpb=306.5, bsz=120, num_updates=30, lr=1.42586e-06, gnorm=7.69, clip=100, loss_scale=64, train_wall=32, gb_free=10, ema_decay=0.9999, wall=114
2022-09-28 19:47:18 - progress_bar.py[line:274] - INFO: epoch 001:     41 / 5261 loss=1.193, loss_v1=0, loss_v2=0, nll_loss=1.012, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=2.02, wps=94.3, ups=0.31, wpb=301.7, bsz=120, num_updates=40, lr=1.90114e-06, gnorm=5.136, clip=100, loss_scale=64, train_wall=32, gb_free=9.7, ema_decay=0.9999, wall=146
2022-09-28 19:47:48 - progress_bar.py[line:274] - INFO: epoch 001:     51 / 5261 loss=1.049, loss_v1=0, loss_v2=0, nll_loss=0.866, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=101.3, ups=0.33, wpb=305, bsz=120, num_updates=50, lr=2.37643e-06, gnorm=3.381, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=176
2022-09-28 19:48:18 - progress_bar.py[line:274] - INFO: epoch 001:     61 / 5261 loss=0.969, loss_v1=0, loss_v2=0, nll_loss=0.784, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=101.1, ups=0.33, wpb=305, bsz=120, num_updates=60, lr=2.85171e-06, gnorm=3.027, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=206
2022-09-28 19:48:48 - progress_bar.py[line:274] - INFO: epoch 001:     71 / 5261 loss=0.983, loss_v1=0, loss_v2=0, nll_loss=0.809, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.75, wps=101.4, ups=0.33, wpb=305, bsz=120, num_updates=70, lr=3.327e-06, gnorm=2.796, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=236
2022-09-28 19:49:19 - progress_bar.py[line:274] - INFO: epoch 001:     81 / 5261 loss=0.925, loss_v1=0, loss_v2=0, nll_loss=0.754, ntokens=306.7, nsentences=120, sample_size=306.7, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=98.5, ups=0.32, wpb=306.7, bsz=120, num_updates=80, lr=3.80228e-06, gnorm=2.347, clip=100, loss_scale=64, train_wall=31, gb_free=9.8, ema_decay=0.9999, wall=267
2022-09-28 19:49:53 - progress_bar.py[line:274] - INFO: epoch 001:     91 / 5261 loss=0.964, loss_v1=0, loss_v2=0, nll_loss=0.8, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=102.4, ups=0.34, wpb=303.3, bsz=120, num_updates=90, lr=4.27757e-06, gnorm=2.119, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=297
2022-09-28 19:50:26 - progress_bar.py[line:274] - INFO: epoch 001:    101 / 5261 loss=0.99, loss_v1=0, loss_v2=0, nll_loss=0.834, ntokens=300.1, nsentences=120, sample_size=300.1, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=92.7, ups=0.31, wpb=300.1, bsz=120, num_updates=100, lr=4.75285e-06, gnorm=2.125, clip=100, loss_scale=64, train_wall=32, gb_free=10, ema_decay=0.9999, wall=333
2022-09-28 19:50:55 - progress_bar.py[line:274] - INFO: epoch 001:    111 / 5261 loss=0.927, loss_v1=0, loss_v2=0, nll_loss=0.767, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=103.1, ups=0.34, wpb=304.2, bsz=120, num_updates=110, lr=5.22814e-06, gnorm=1.804, clip=100, loss_scale=64, train_wall=29, gb_free=10, ema_decay=0.9999, wall=363
2022-09-28 19:51:25 - progress_bar.py[line:274] - INFO: epoch 001:    121 / 5261 loss=0.884, loss_v1=0, loss_v2=0, nll_loss=0.721, ntokens=306.1, nsentences=120, sample_size=306.1, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=103.4, ups=0.34, wpb=306.1, bsz=120, num_updates=120, lr=5.70342e-06, gnorm=1.631, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=392
2022-09-28 19:51:55 - progress_bar.py[line:274] - INFO: epoch 001:    131 / 5261 loss=0.908, loss_v1=0, loss_v2=0, nll_loss=0.747, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=100.4, ups=0.33, wpb=305.5, bsz=120, num_updates=130, lr=6.17871e-06, gnorm=1.723, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=423
2022-09-28 19:52:24 - progress_bar.py[line:274] - INFO: epoch 001:    141 / 5261 loss=0.898, loss_v1=0, loss_v2=0, nll_loss=0.734, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=103.5, ups=0.34, wpb=301.7, bsz=120, num_updates=140, lr=6.65399e-06, gnorm=1.653, clip=100, loss_scale=64, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=452
2022-09-28 19:52:54 - progress_bar.py[line:274] - INFO: epoch 001:    151 / 5261 loss=0.922, loss_v1=0, loss_v2=0, nll_loss=0.76, ntokens=299.7, nsentences=120, sample_size=299.7, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=99.8, ups=0.33, wpb=299.7, bsz=120, num_updates=150, lr=7.12928e-06, gnorm=1.592, clip=100, loss_scale=64, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=482
2022-09-28 19:53:26 - progress_bar.py[line:274] - INFO: epoch 001:    161 / 5261 loss=0.865, loss_v1=0, loss_v2=0, nll_loss=0.699, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=97, ups=0.32, wpb=304.6, bsz=120, num_updates=160, lr=7.60456e-06, gnorm=1.709, clip=100, loss_scale=64, train_wall=31, gb_free=10, ema_decay=0.9999, wall=514
2022-09-28 19:53:56 - progress_bar.py[line:274] - INFO: epoch 001:    171 / 5261 loss=0.859, loss_v1=0, loss_v2=0, nll_loss=0.69, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=99.9, ups=0.33, wpb=304.3, bsz=120, num_updates=170, lr=8.07985e-06, gnorm=1.7, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=544
2022-09-28 19:54:26 - progress_bar.py[line:274] - INFO: epoch 001:    181 / 5261 loss=0.878, loss_v1=0, loss_v2=0, nll_loss=0.71, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=102.1, ups=0.34, wpb=302.5, bsz=120, num_updates=180, lr=8.55513e-06, gnorm=1.641, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=574
2022-09-28 19:54:57 - progress_bar.py[line:274] - INFO: epoch 001:    191 / 5261 loss=0.85, loss_v1=0, loss_v2=0, nll_loss=0.68, ntokens=305.6, nsentences=120, sample_size=305.6, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=99.4, ups=0.33, wpb=305.6, bsz=120, num_updates=190, lr=9.03042e-06, gnorm=1.801, clip=100, loss_scale=64, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=605
2022-09-28 19:55:27 - progress_bar.py[line:274] - INFO: epoch 001:    201 / 5261 loss=0.794, loss_v1=0, loss_v2=0, nll_loss=0.618, ntokens=309.2, nsentences=120, sample_size=309.2, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=102.7, ups=0.33, wpb=309.2, bsz=120, num_updates=200, lr=9.5057e-06, gnorm=1.798, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=635
2022-09-28 19:55:57 - progress_bar.py[line:274] - INFO: epoch 001:    211 / 5261 loss=0.822, loss_v1=0, loss_v2=0, nll_loss=0.653, ntokens=308.2, nsentences=120, sample_size=308.2, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=100.9, ups=0.33, wpb=308.2, bsz=120, num_updates=210, lr=9.98099e-06, gnorm=1.84, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=665
2022-09-28 19:56:29 - progress_bar.py[line:274] - INFO: epoch 001:    221 / 5261 loss=0.84, loss_v1=0, loss_v2=0, nll_loss=0.665, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=96.2, ups=0.32, wpb=304, bsz=120, num_updates=220, lr=1.04563e-05, gnorm=1.801, clip=100, loss_scale=64, train_wall=32, gb_free=10, ema_decay=0.9999, wall=697
2022-09-28 19:56:59 - progress_bar.py[line:274] - INFO: epoch 001:    231 / 5261 loss=0.803, loss_v1=0, loss_v2=0, nll_loss=0.617, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=101.3, ups=0.33, wpb=302.9, bsz=120, num_updates=230, lr=1.09316e-05, gnorm=1.965, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=727
2022-09-28 19:57:30 - progress_bar.py[line:274] - INFO: epoch 001:    241 / 5261 loss=0.794, loss_v1=0, loss_v2=0, nll_loss=0.61, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=99.3, ups=0.33, wpb=304.5, bsz=120, num_updates=240, lr=1.14068e-05, gnorm=2.028, clip=100, loss_scale=64, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=757
2022-09-28 19:58:00 - progress_bar.py[line:274] - INFO: epoch 001:    251 / 5261 loss=0.779, loss_v1=0, loss_v2=0, nll_loss=0.59, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=101.5, ups=0.33, wpb=304.5, bsz=120, num_updates=250, lr=1.18821e-05, gnorm=1.779, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=788
2022-09-28 19:58:33 - progress_bar.py[line:274] - INFO: epoch 001:    261 / 5261 loss=0.755, loss_v1=0, loss_v2=0, nll_loss=0.555, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=93.1, ups=0.31, wpb=304.5, bsz=120, num_updates=260, lr=1.23574e-05, gnorm=1.724, clip=100, loss_scale=64, train_wall=33, gb_free=10.2, ema_decay=0.9999, wall=820
2022-09-28 19:59:03 - progress_bar.py[line:274] - INFO: epoch 001:    271 / 5261 loss=0.782, loss_v1=0, loss_v2=0, nll_loss=0.59, ntokens=300.3, nsentences=120, sample_size=300.3, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=99.9, ups=0.33, wpb=300.3, bsz=120, num_updates=270, lr=1.28327e-05, gnorm=1.742, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=850
2022-09-28 19:59:33 - progress_bar.py[line:274] - INFO: epoch 001:    281 / 5261 loss=0.742, loss_v1=0, loss_v2=0, nll_loss=0.548, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=101.6, ups=0.33, wpb=304.2, bsz=120, num_updates=280, lr=1.3308e-05, gnorm=1.487, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=880
2022-09-28 20:00:03 - progress_bar.py[line:274] - INFO: epoch 001:    291 / 5261 loss=0.751, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=101.7, ups=0.33, wpb=304.8, bsz=120, num_updates=290, lr=1.37833e-05, gnorm=1.639, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=910
2022-09-28 20:00:33 - progress_bar.py[line:274] - INFO: epoch 001:    301 / 5261 loss=0.723, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=307.1, nsentences=120, sample_size=307.1, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=102.4, ups=0.33, wpb=307.1, bsz=120, num_updates=300, lr=1.42586e-05, gnorm=1.521, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=940
2022-09-28 20:01:02 - progress_bar.py[line:274] - INFO: epoch 001:    311 / 5261 loss=0.725, loss_v1=0, loss_v2=0, nll_loss=0.525, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=102.1, ups=0.34, wpb=302.9, bsz=120, num_updates=310, lr=1.47338e-05, gnorm=1.623, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=970
2022-09-28 20:01:33 - progress_bar.py[line:274] - INFO: epoch 001:    321 / 5261 loss=0.752, loss_v1=0, loss_v2=0, nll_loss=0.548, ntokens=299, nsentences=120, sample_size=299, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=98.7, ups=0.33, wpb=299, bsz=120, num_updates=320, lr=1.52091e-05, gnorm=1.627, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1000
2022-09-28 20:02:03 - progress_bar.py[line:274] - INFO: epoch 001:    331 / 5261 loss=0.7, loss_v1=0, loss_v2=0, nll_loss=0.496, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=101.8, ups=0.33, wpb=304.2, bsz=120, num_updates=330, lr=1.56844e-05, gnorm=1.56, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1031
2022-09-28 20:02:35 - progress_bar.py[line:274] - INFO: epoch 001:    341 / 5261 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=309.1, nsentences=120, sample_size=309.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.5, ups=0.33, wpb=309.1, bsz=120, num_updates=340, lr=1.61597e-05, gnorm=1.428, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1061
2022-09-28 20:03:05 - progress_bar.py[line:274] - INFO: epoch 001:    351 / 5261 loss=0.676, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=101.2, ups=0.33, wpb=305, bsz=120, num_updates=350, lr=1.6635e-05, gnorm=1.249, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1092
2022-09-28 20:03:35 - progress_bar.py[line:274] - INFO: epoch 001:    361 / 5261 loss=0.689, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=100.5, ups=0.33, wpb=302.5, bsz=120, num_updates=360, lr=1.71103e-05, gnorm=1.519, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1123
2022-09-28 20:04:04 - progress_bar.py[line:274] - INFO: epoch 001:    371 / 5261 loss=0.687, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=102.6, ups=0.34, wpb=304.6, bsz=120, num_updates=370, lr=1.75856e-05, gnorm=1.374, clip=100, loss_scale=64, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=1152
2022-09-28 20:04:35 - progress_bar.py[line:274] - INFO: epoch 001:    381 / 5261 loss=0.691, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=100.3, ups=0.33, wpb=302.2, bsz=120, num_updates=380, lr=1.80608e-05, gnorm=1.51, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=1182
2022-09-28 20:05:05 - progress_bar.py[line:274] - INFO: epoch 001:    391 / 5261 loss=0.69, loss_v1=0, loss_v2=0, nll_loss=0.485, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=99.9, ups=0.33, wpb=303.8, bsz=120, num_updates=390, lr=1.85361e-05, gnorm=1.519, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1213
2022-09-28 20:05:37 - progress_bar.py[line:274] - INFO: epoch 001:    401 / 5261 loss=0.664, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=96.4, ups=0.32, wpb=304.8, bsz=120, num_updates=400, lr=1.90114e-05, gnorm=1.244, clip=100, loss_scale=64, train_wall=32, gb_free=10, ema_decay=0.9999, wall=1245
2022-09-28 20:06:07 - progress_bar.py[line:274] - INFO: epoch 001:    411 / 5261 loss=0.686, loss_v1=0, loss_v2=0, nll_loss=0.474, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=100.6, ups=0.33, wpb=303.1, bsz=120, num_updates=410, lr=1.94867e-05, gnorm=1.532, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1275
2022-09-28 20:06:38 - progress_bar.py[line:274] - INFO: epoch 001:    421 / 5261 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.1, ups=0.33, wpb=305.4, bsz=120, num_updates=420, lr=1.9962e-05, gnorm=1.509, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=1305
2022-09-28 20:07:07 - progress_bar.py[line:274] - INFO: epoch 001:    431 / 5261 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=104.3, ups=0.34, wpb=304.5, bsz=120, num_updates=430, lr=2.04373e-05, gnorm=1.476, clip=100, loss_scale=64, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=1335
2022-09-28 20:07:38 - progress_bar.py[line:274] - INFO: epoch 001:    441 / 5261 loss=0.665, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.2, ups=0.33, wpb=303.4, bsz=120, num_updates=440, lr=2.09125e-05, gnorm=1.328, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1365
2022-09-28 20:08:08 - progress_bar.py[line:274] - INFO: epoch 001:    451 / 5261 loss=0.671, loss_v1=0, loss_v2=0, nll_loss=0.46, ntokens=301.8, nsentences=120, sample_size=301.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=99, ups=0.33, wpb=301.8, bsz=120, num_updates=450, lr=2.13878e-05, gnorm=1.615, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=1396
2022-09-28 20:08:38 - progress_bar.py[line:274] - INFO: epoch 001:    461 / 5261 loss=0.669, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=102.7, ups=0.34, wpb=304.2, bsz=120, num_updates=460, lr=2.18631e-05, gnorm=1.464, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1425
2022-09-28 20:09:08 - progress_bar.py[line:274] - INFO: epoch 001:    471 / 5261 loss=0.655, loss_v1=0, loss_v2=0, nll_loss=0.457, ntokens=306.7, nsentences=120, sample_size=306.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.9, ups=0.33, wpb=306.7, bsz=120, num_updates=470, lr=2.23384e-05, gnorm=1.38, clip=100, loss_scale=64, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=1455
2022-09-28 20:09:38 - progress_bar.py[line:274] - INFO: epoch 001:    481 / 5261 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.2, ups=0.33, wpb=305.7, bsz=120, num_updates=480, lr=2.28137e-05, gnorm=1.249, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1486
2022-09-28 20:10:09 - progress_bar.py[line:274] - INFO: epoch 001:    491 / 5261 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.3, ups=0.33, wpb=304.4, bsz=120, num_updates=490, lr=2.3289e-05, gnorm=1.236, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=1516
2022-09-28 20:10:39 - progress_bar.py[line:274] - INFO: epoch 001:    501 / 5261 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=102.2, ups=0.33, wpb=305.7, bsz=120, num_updates=500, lr=2.37643e-05, gnorm=1.357, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=1546
2022-09-28 20:11:09 - progress_bar.py[line:274] - INFO: epoch 001:    511 / 5261 loss=0.672, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=302.1, nsentences=120, sample_size=302.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=99.3, ups=0.33, wpb=302.1, bsz=120, num_updates=510, lr=2.42395e-05, gnorm=1.239, clip=80, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1577
2022-09-28 20:11:39 - progress_bar.py[line:274] - INFO: epoch 001:    521 / 5261 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=102.7, ups=0.34, wpb=303.5, bsz=120, num_updates=520, lr=2.47148e-05, gnorm=1.245, clip=100, loss_scale=128, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=1606
2022-09-28 20:12:08 - progress_bar.py[line:274] - INFO: epoch 001:    531 / 5261 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=102.1, ups=0.34, wpb=303.5, bsz=120, num_updates=530, lr=2.51901e-05, gnorm=1.271, clip=90, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1636
2022-09-28 20:12:38 - progress_bar.py[line:274] - INFO: epoch 001:    541 / 5261 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101.3, ups=0.33, wpb=303.5, bsz=120, num_updates=540, lr=2.56654e-05, gnorm=1.193, clip=80, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1666
2022-09-28 20:13:09 - progress_bar.py[line:274] - INFO: epoch 001:    551 / 5261 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=307.2, nsentences=120, sample_size=307.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.6, ups=0.33, wpb=307.2, bsz=120, num_updates=550, lr=2.61407e-05, gnorm=1.418, clip=100, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1697
2022-09-28 20:13:39 - progress_bar.py[line:274] - INFO: epoch 001:    561 / 5261 loss=0.656, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.3, ups=0.33, wpb=303.5, bsz=120, num_updates=560, lr=2.6616e-05, gnorm=1.215, clip=80, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=1727
2022-09-28 20:14:09 - progress_bar.py[line:274] - INFO: epoch 001:    571 / 5261 loss=0.655, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.7, ups=0.33, wpb=303.8, bsz=120, num_updates=570, lr=2.70913e-05, gnorm=1.196, clip=100, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1757
2022-09-28 20:14:39 - progress_bar.py[line:274] - INFO: epoch 001:    581 / 5261 loss=0.665, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=101.3, ups=0.33, wpb=304.5, bsz=120, num_updates=580, lr=2.75665e-05, gnorm=1.204, clip=90, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1787
2022-09-28 20:15:10 - progress_bar.py[line:274] - INFO: epoch 001:    591 / 5261 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.4, ups=0.33, wpb=303.2, bsz=120, num_updates=590, lr=2.80418e-05, gnorm=1.428, clip=80, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1817
2022-09-28 20:15:40 - progress_bar.py[line:274] - INFO: epoch 001:    601 / 5261 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=308.3, nsentences=120, sample_size=308.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=102.3, ups=0.33, wpb=308.3, bsz=120, num_updates=600, lr=2.85171e-05, gnorm=1.241, clip=100, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1848
2022-09-28 20:16:10 - progress_bar.py[line:274] - INFO: epoch 001:    611 / 5261 loss=0.675, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=300.9, nsentences=120, sample_size=300.9, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=99.9, ups=0.33, wpb=300.9, bsz=120, num_updates=610, lr=2.89924e-05, gnorm=1.139, clip=80, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1878
2022-09-28 20:16:40 - progress_bar.py[line:274] - INFO: epoch 001:    621 / 5261 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=306.1, nsentences=120, sample_size=306.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.8, ups=0.33, wpb=306.1, bsz=120, num_updates=620, lr=2.94677e-05, gnorm=1.209, clip=100, loss_scale=128, train_wall=30, gb_free=9.5, ema_decay=0.9999, wall=1908
2022-09-28 20:17:11 - progress_bar.py[line:274] - INFO: epoch 001:    631 / 5261 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.5, ups=0.33, wpb=303.7, bsz=120, num_updates=630, lr=2.9943e-05, gnorm=1.177, clip=90, loss_scale=128, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=1938
2022-09-28 20:17:41 - progress_bar.py[line:274] - INFO: epoch 001:    641 / 5261 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=103.6, ups=0.34, wpb=304.5, bsz=120, num_updates=640, lr=3.04183e-05, gnorm=1.141, clip=80, loss_scale=128, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=1969
2022-09-28 20:18:13 - progress_bar.py[line:274] - INFO: epoch 001:    651 / 5261 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=94.6, ups=0.31, wpb=305.7, bsz=120, num_updates=650, lr=3.08935e-05, gnorm=1.123, clip=70, loss_scale=128, train_wall=32, gb_free=9.7, ema_decay=0.9999, wall=2001
2022-09-28 20:18:43 - progress_bar.py[line:274] - INFO: epoch 001:    661 / 5261 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=102.8, ups=0.34, wpb=304.3, bsz=120, num_updates=660, lr=3.13688e-05, gnorm=1.121, clip=100, loss_scale=128, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=2031
2022-09-28 20:19:13 - progress_bar.py[line:274] - INFO: epoch 001:    671 / 5261 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=101.9, ups=0.34, wpb=303.1, bsz=120, num_updates=670, lr=3.18441e-05, gnorm=1.108, clip=80, loss_scale=128, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=2060
2022-09-28 20:19:42 - progress_bar.py[line:274] - INFO: epoch 001:    681 / 5261 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=307.8, nsentences=120, sample_size=307.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=103.8, ups=0.34, wpb=307.8, bsz=120, num_updates=680, lr=3.23194e-05, gnorm=1.075, clip=60, loss_scale=128, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=2090
2022-09-28 20:20:12 - progress_bar.py[line:274] - INFO: epoch 001:    691 / 5261 loss=0.656, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=103.3, ups=0.34, wpb=303.6, bsz=120, num_updates=690, lr=3.27947e-05, gnorm=1.101, clip=70, loss_scale=128, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=2119
2022-09-28 20:20:41 - progress_bar.py[line:274] - INFO: epoch 001:    701 / 5261 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=102.7, ups=0.34, wpb=304.4, bsz=120, num_updates=700, lr=3.327e-05, gnorm=1.235, clip=100, loss_scale=128, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=2149
2022-09-28 20:21:11 - progress_bar.py[line:274] - INFO: epoch 001:    711 / 5261 loss=0.654, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=103.3, ups=0.34, wpb=305.4, bsz=120, num_updates=710, lr=3.37452e-05, gnorm=1.05, clip=60, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=2179
2022-09-28 20:21:41 - progress_bar.py[line:274] - INFO: epoch 001:    721 / 5261 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.8, ups=0.33, wpb=304.1, bsz=120, num_updates=720, lr=3.42205e-05, gnorm=0.969, clip=30, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2209
2022-09-28 20:22:11 - progress_bar.py[line:274] - INFO: epoch 001:    731 / 5261 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=307.3, nsentences=120, sample_size=307.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=103.2, ups=0.34, wpb=307.3, bsz=120, num_updates=730, lr=3.46958e-05, gnorm=0.967, clip=30, loss_scale=128, train_wall=30, gb_free=9.6, ema_decay=0.9999, wall=2239
2022-09-28 20:22:41 - progress_bar.py[line:274] - INFO: epoch 001:    741 / 5261 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=302.6, nsentences=120, sample_size=302.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.9, ups=0.33, wpb=302.6, bsz=120, num_updates=740, lr=3.51711e-05, gnorm=1.106, clip=80, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2269
2022-09-28 20:23:11 - progress_bar.py[line:274] - INFO: epoch 001:    751 / 5261 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.5, ups=0.33, wpb=304.2, bsz=120, num_updates=750, lr=3.56464e-05, gnorm=1.113, clip=80, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=2299
2022-09-28 20:23:41 - progress_bar.py[line:274] - INFO: epoch 001:    761 / 5261 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=102.7, ups=0.34, wpb=304.3, bsz=120, num_updates=760, lr=3.61217e-05, gnorm=1.124, clip=60, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=2329
2022-09-28 20:24:11 - progress_bar.py[line:274] - INFO: epoch 001:    771 / 5261 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.7, ups=0.33, wpb=302.5, bsz=120, num_updates=770, lr=3.6597e-05, gnorm=1.029, clip=50, loss_scale=128, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=2359
2022-09-28 20:24:41 - progress_bar.py[line:274] - INFO: epoch 001:    781 / 5261 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=305.1, nsentences=120, sample_size=305.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=102.3, ups=0.34, wpb=305.1, bsz=120, num_updates=780, lr=3.70722e-05, gnorm=1.107, clip=60, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=2388
2022-09-28 20:25:11 - progress_bar.py[line:274] - INFO: epoch 001:    791 / 5261 loss=0.654, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=301.6, nsentences=120, sample_size=301.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=99, ups=0.33, wpb=301.6, bsz=120, num_updates=790, lr=3.75475e-05, gnorm=1.05, clip=60, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=2419
2022-09-28 20:25:42 - progress_bar.py[line:274] - INFO: epoch 001:    801 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.3, ups=0.33, wpb=305.7, bsz=120, num_updates=800, lr=3.80228e-05, gnorm=1.109, clip=60, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2449
2022-09-28 20:26:11 - progress_bar.py[line:274] - INFO: epoch 001:    811 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.1, ups=0.34, wpb=302.4, bsz=120, num_updates=810, lr=3.84981e-05, gnorm=0.949, clip=20, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=2479
2022-09-28 20:26:47 - progress_bar.py[line:274] - INFO: epoch 001:    821 / 5261 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.4, ups=0.32, wpb=304.2, bsz=120, num_updates=820, lr=3.89734e-05, gnorm=0.997, clip=50, loss_scale=128, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=2514
2022-09-28 20:27:18 - progress_bar.py[line:274] - INFO: epoch 001:    831 / 5261 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=97.5, ups=0.32, wpb=304.2, bsz=120, num_updates=830, lr=3.94487e-05, gnorm=1.056, clip=60, loss_scale=128, train_wall=31, gb_free=9.8, ema_decay=0.9999, wall=2546
2022-09-28 20:27:51 - progress_bar.py[line:274] - INFO: epoch 001:    841 / 5261 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=92.8, ups=0.31, wpb=303.3, bsz=120, num_updates=840, lr=3.9924e-05, gnorm=1.106, clip=60, loss_scale=128, train_wall=33, gb_free=9.9, ema_decay=0.9999, wall=2578
2022-09-28 20:28:20 - progress_bar.py[line:274] - INFO: epoch 001:    851 / 5261 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=299.8, nsentences=120, sample_size=299.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101.5, ups=0.34, wpb=299.8, bsz=120, num_updates=850, lr=4.03992e-05, gnorm=1.1, clip=60, loss_scale=128, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=2608
2022-09-28 20:28:50 - progress_bar.py[line:274] - INFO: epoch 001:    861 / 5261 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=301, nsentences=120, sample_size=301, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=101.2, ups=0.34, wpb=301, bsz=120, num_updates=860, lr=4.08745e-05, gnorm=1.006, clip=30, loss_scale=128, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=2638
2022-09-28 20:29:20 - progress_bar.py[line:274] - INFO: epoch 001:    871 / 5261 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=301.5, nsentences=120, sample_size=301.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.3, ups=0.33, wpb=301.5, bsz=120, num_updates=870, lr=4.13498e-05, gnorm=1.002, clip=50, loss_scale=128, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=2668
2022-09-28 20:29:50 - progress_bar.py[line:274] - INFO: epoch 001:    881 / 5261 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=306.2, nsentences=120, sample_size=306.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=104.6, ups=0.34, wpb=306.2, bsz=120, num_updates=880, lr=4.18251e-05, gnorm=0.953, clip=30, loss_scale=128, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=2697
2022-09-28 20:30:20 - progress_bar.py[line:274] - INFO: epoch 001:    891 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=306.7, nsentences=120, sample_size=306.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.1, ups=0.33, wpb=306.7, bsz=120, num_updates=890, lr=4.23004e-05, gnorm=0.946, clip=10, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2728
2022-09-28 20:30:51 - progress_bar.py[line:274] - INFO: epoch 001:    901 / 5261 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.1, ups=0.33, wpb=303.2, bsz=120, num_updates=900, lr=4.27757e-05, gnorm=0.955, clip=50, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=2758
2022-09-28 20:31:21 - progress_bar.py[line:274] - INFO: epoch 001:    911 / 5261 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.2, ups=0.33, wpb=302.2, bsz=120, num_updates=910, lr=4.3251e-05, gnorm=0.953, clip=30, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=2788
2022-09-28 20:31:50 - progress_bar.py[line:274] - INFO: epoch 001:    921 / 5261 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=104, ups=0.34, wpb=305.3, bsz=120, num_updates=920, lr=4.37262e-05, gnorm=0.992, clip=30, loss_scale=128, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=2818
2022-09-28 20:32:20 - progress_bar.py[line:274] - INFO: epoch 001:    931 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.1, ups=0.34, wpb=303.6, bsz=120, num_updates=930, lr=4.42015e-05, gnorm=0.891, clip=20, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2848
2022-09-28 20:32:50 - progress_bar.py[line:274] - INFO: epoch 001:    941 / 5261 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.6, ups=0.33, wpb=303.3, bsz=120, num_updates=940, lr=4.46768e-05, gnorm=0.971, clip=40, loss_scale=128, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=2878
2022-09-28 20:33:20 - progress_bar.py[line:274] - INFO: epoch 001:    951 / 5261 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=306.9, nsentences=120, sample_size=306.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=103, ups=0.34, wpb=306.9, bsz=120, num_updates=950, lr=4.51521e-05, gnorm=0.891, clip=30, loss_scale=128, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=2908
2022-09-28 20:33:50 - progress_bar.py[line:274] - INFO: epoch 001:    961 / 5261 loss=0.656, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.2, ups=0.33, wpb=302.2, bsz=120, num_updates=960, lr=4.56274e-05, gnorm=0.959, clip=20, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2938
2022-09-28 20:34:20 - progress_bar.py[line:274] - INFO: epoch 001:    971 / 5261 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.3, ups=0.34, wpb=304.1, bsz=120, num_updates=970, lr=4.61027e-05, gnorm=0.977, clip=50, loss_scale=128, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=2968
2022-09-28 20:34:50 - progress_bar.py[line:274] - INFO: epoch 001:    981 / 5261 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=101.3, ups=0.33, wpb=303.5, bsz=120, num_updates=980, lr=4.65779e-05, gnorm=1.03, clip=40, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=2998
2022-09-28 20:35:20 - progress_bar.py[line:274] - INFO: epoch 001:    991 / 5261 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=102.1, ups=0.34, wpb=303.8, bsz=120, num_updates=990, lr=4.70532e-05, gnorm=0.894, clip=20, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=3027
2022-09-28 20:35:51 - progress_bar.py[line:274] - INFO: epoch 001:   1001 / 5261 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=301.9, nsentences=120, sample_size=301.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=95.9, ups=0.32, wpb=301.9, bsz=120, num_updates=1000, lr=4.75285e-05, gnorm=0.891, clip=20, loss_scale=128, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=3059
2022-09-28 20:36:21 - progress_bar.py[line:274] - INFO: epoch 001:   1011 / 5261 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101, ups=0.33, wpb=302.9, bsz=120, num_updates=1010, lr=4.80038e-05, gnorm=0.893, clip=30, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3089
2022-09-28 20:36:53 - progress_bar.py[line:274] - INFO: epoch 001:   1021 / 5261 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=97.4, ups=0.32, wpb=304.2, bsz=120, num_updates=1020, lr=4.84791e-05, gnorm=0.96, clip=30, loss_scale=128, train_wall=31, gb_free=10, ema_decay=0.9999, wall=3120
2022-09-28 20:37:24 - progress_bar.py[line:274] - INFO: epoch 001:   1031 / 5261 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=95.4, ups=0.31, wpb=303.3, bsz=120, num_updates=1030, lr=4.89544e-05, gnorm=0.993, clip=30, loss_scale=256, train_wall=32, gb_free=10, ema_decay=0.9999, wall=3152
2022-09-28 20:37:54 - progress_bar.py[line:274] - INFO: epoch 001:   1041 / 5261 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=301.9, nsentences=120, sample_size=301.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.2, ups=0.33, wpb=301.9, bsz=120, num_updates=1040, lr=4.94297e-05, gnorm=0.885, clip=20, loss_scale=256, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=3182
2022-09-28 20:38:24 - progress_bar.py[line:274] - INFO: epoch 001:   1051 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=306.3, nsentences=120, sample_size=306.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.5, ups=0.33, wpb=306.3, bsz=120, num_updates=1050, lr=4.99049e-05, gnorm=0.896, clip=20, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3212
2022-09-28 20:38:54 - progress_bar.py[line:274] - INFO: epoch 001:   1061 / 5261 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.3, ups=0.34, wpb=305, bsz=120, num_updates=1060, lr=4.99842e-05, gnorm=0.846, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3242
2022-09-28 20:39:25 - progress_bar.py[line:274] - INFO: epoch 001:   1071 / 5261 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.7, ups=0.33, wpb=302.4, bsz=120, num_updates=1070, lr=4.99644e-05, gnorm=0.952, clip=20, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3272
2022-09-28 20:39:55 - progress_bar.py[line:274] - INFO: epoch 001:   1081 / 5261 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=98.6, ups=0.33, wpb=302.5, bsz=120, num_updates=1080, lr=4.99446e-05, gnorm=0.843, clip=10, loss_scale=256, train_wall=31, gb_free=9.6, ema_decay=0.9999, wall=3303
2022-09-28 20:40:25 - progress_bar.py[line:274] - INFO: epoch 001:   1091 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=302.1, nsentences=120, sample_size=302.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.8, ups=0.33, wpb=302.1, bsz=120, num_updates=1090, lr=4.99248e-05, gnorm=0.819, clip=20, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=3333
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1916021
Killing subprocess 1916022
Main process received SIGINT, exiting
2022-10-01 05:23:48 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2022-10-01 05:23:48 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2022-10-01 05:23:48 - utils.py[line:261] - INFO: Start init
2022-10-01 05:23:48 - utils.py[line:261] - INFO: Start init
2022-10-01 05:23:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2022-10-01 05:23:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2022-10-01 05:23:49 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2022-10-01 05:23:49 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2022-10-01 05:24:00 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way_allcand', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 5, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 6000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 10, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [3], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way_allcand/1_B20_A3_E5_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 6000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='10', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=5, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way_allcand/1_B20_A3_E5_0.04_5e-5_480', save_interval=10, save_interval_updates=6000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way_allcand', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[3], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=26, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=6000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 26, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer'}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2022-10-01 05:24:00 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2022-10-01 05:24:00 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2022-10-01 05:24:04 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2022-10-01 05:24:04 - train.py[line:118] - INFO: task: VqaGenTask
2022-10-01 05:24:04 - train.py[line:119] - INFO: model: OFAModel
2022-10-01 05:24:04 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2022-10-01 05:24:04 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2022-10-01 05:24:04 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 0 row count 141030 total row count 282060
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 1 row count 141030 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2022-10-01 05:24:05 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2022-10-01 05:24:05 - trainer.py[line:124] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2022-10-01 05:24:05 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-10-01 05:24:05 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-10-01 05:24:05 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2022-10-01 05:24:05 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2022-10-01 05:24:05 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2022-10-01 05:24:05 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2022-10-01 05:24:05 - trainer.py[line:458] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2022-10-01 05:24:13 - trainer.py[line:594] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2022-10-01 05:24:13 - trainer.py[line:594] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2022-10-01 05:24:13 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2022-10-01 05:24:13 - trainer.py[line:273] - INFO: Exponential Moving Average Shadow Model is initialized.
2022-10-01 05:24:14 - trainer.py[line:623] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2022-10-01 05:24:14 - trainer.py[line:643] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 315642 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 315642 total row count 631284
2022-10-01 05:24:15 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 26305, warmup steps 1052, warmup_factor 0.0009505703422053232
2022-10-01 05:24:15 - trainer.py[line:707] - INFO: begin training epoch 1
2022-10-01 05:24:15 - train.py[line:312] - INFO: Start iterating over samples
Total steps 26305, warmup steps 1052, warmup_factor 0.0009505703422053232
2022-10-01 05:24:24 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-10-01 05:24:55 - progress_bar.py[line:274] - INFO: epoch 001:     11 / 5261 loss=1.361, loss_v1=0, loss_v2=0, nll_loss=1.146, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=2.21, wps=99.3, ups=0.33, wpb=303.3, bsz=120, num_updates=10, lr=4.75285e-07, gnorm=12.151, clip=100, loss_scale=64, train_wall=38, gb_free=9.9, ema_decay=0.9999, wall=50
2022-10-01 05:25:27 - progress_bar.py[line:274] - INFO: epoch 001:     21 / 5261 loss=1.282, loss_v1=0, loss_v2=0, nll_loss=1.068, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=2.1, wps=95.4, ups=0.31, wpb=306, bsz=120, num_updates=20, lr=9.5057e-07, gnorm=9.842, clip=100, loss_scale=64, train_wall=32, gb_free=10, ema_decay=0.9999, wall=82
2022-10-01 05:26:00 - progress_bar.py[line:274] - INFO: epoch 001:     31 / 5261 loss=1.212, loss_v1=0, loss_v2=0, nll_loss=1.01, ntokens=306.5, nsentences=120, sample_size=306.5, sample_size_v1=0, sample_size_v2=0, ppl=2.01, wps=93.2, ups=0.3, wpb=306.5, bsz=120, num_updates=30, lr=1.42586e-06, gnorm=7.699, clip=100, loss_scale=64, train_wall=33, gb_free=10, ema_decay=0.9999, wall=115
2022-10-01 05:26:30 - progress_bar.py[line:274] - INFO: epoch 001:     41 / 5261 loss=1.193, loss_v1=0, loss_v2=0, nll_loss=1.012, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=2.02, wps=99.2, ups=0.33, wpb=301.7, bsz=120, num_updates=40, lr=1.90114e-06, gnorm=5.138, clip=100, loss_scale=64, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=145
2022-10-01 05:27:02 - progress_bar.py[line:274] - INFO: epoch 001:     51 / 5261 loss=1.049, loss_v1=0, loss_v2=0, nll_loss=0.867, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=97.6, ups=0.32, wpb=305, bsz=120, num_updates=50, lr=2.37643e-06, gnorm=3.385, clip=100, loss_scale=64, train_wall=31, gb_free=10, ema_decay=0.9999, wall=177
2022-10-01 05:27:32 - progress_bar.py[line:274] - INFO: epoch 001:     61 / 5261 loss=0.969, loss_v1=0, loss_v2=0, nll_loss=0.784, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=100.8, ups=0.33, wpb=305, bsz=120, num_updates=60, lr=2.85171e-06, gnorm=3.021, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=207
2022-10-01 05:28:02 - progress_bar.py[line:274] - INFO: epoch 001:     71 / 5261 loss=0.983, loss_v1=0, loss_v2=0, nll_loss=0.809, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.75, wps=101.5, ups=0.33, wpb=305, bsz=120, num_updates=70, lr=3.327e-06, gnorm=2.799, clip=100, loss_scale=64, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=237
2022-10-01 05:28:32 - progress_bar.py[line:274] - INFO: epoch 001:     81 / 5261 loss=0.925, loss_v1=0, loss_v2=0, nll_loss=0.754, ntokens=306.7, nsentences=120, sample_size=306.7, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=102.8, ups=0.34, wpb=306.7, bsz=120, num_updates=80, lr=3.80228e-06, gnorm=2.348, clip=100, loss_scale=64, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=267
2022-10-01 05:29:01 - progress_bar.py[line:274] - INFO: epoch 001:     91 / 5261 loss=0.964, loss_v1=0, loss_v2=0, nll_loss=0.8, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=102.7, ups=0.34, wpb=303.3, bsz=120, num_updates=90, lr=4.27757e-06, gnorm=2.118, clip=100, loss_scale=64, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=296
2022-10-01 05:29:31 - progress_bar.py[line:274] - INFO: epoch 001:    101 / 5261 loss=0.99, loss_v1=0, loss_v2=0, nll_loss=0.833, ntokens=300.1, nsentences=120, sample_size=300.1, sample_size_v1=0, sample_size_v2=0, ppl=1.78, wps=101, ups=0.34, wpb=300.1, bsz=120, num_updates=100, lr=4.75285e-06, gnorm=2.146, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=326
2022-10-01 05:30:01 - progress_bar.py[line:274] - INFO: epoch 001:    111 / 5261 loss=0.927, loss_v1=0, loss_v2=0, nll_loss=0.767, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=103.1, ups=0.34, wpb=304.2, bsz=120, num_updates=110, lr=5.22814e-06, gnorm=1.813, clip=100, loss_scale=64, train_wall=29, gb_free=10, ema_decay=0.9999, wall=355
2022-10-01 05:30:30 - progress_bar.py[line:274] - INFO: epoch 001:    121 / 5261 loss=0.884, loss_v1=0, loss_v2=0, nll_loss=0.721, ntokens=306.1, nsentences=120, sample_size=306.1, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=103.6, ups=0.34, wpb=306.1, bsz=120, num_updates=120, lr=5.70342e-06, gnorm=1.624, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=385
2022-10-01 05:31:01 - progress_bar.py[line:274] - INFO: epoch 001:    131 / 5261 loss=0.908, loss_v1=0, loss_v2=0, nll_loss=0.747, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=100.5, ups=0.33, wpb=305.5, bsz=120, num_updates=130, lr=6.17871e-06, gnorm=1.735, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=415
2022-10-01 05:31:30 - progress_bar.py[line:274] - INFO: epoch 001:    141 / 5261 loss=0.898, loss_v1=0, loss_v2=0, nll_loss=0.734, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=103.4, ups=0.34, wpb=301.7, bsz=120, num_updates=140, lr=6.65399e-06, gnorm=1.632, clip=100, loss_scale=64, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=445
2022-10-01 05:32:00 - progress_bar.py[line:274] - INFO: epoch 001:    151 / 5261 loss=0.922, loss_v1=0, loss_v2=0, nll_loss=0.76, ntokens=299.7, nsentences=120, sample_size=299.7, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=99.8, ups=0.33, wpb=299.7, bsz=120, num_updates=150, lr=7.12928e-06, gnorm=1.575, clip=100, loss_scale=64, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=475
2022-10-01 05:32:30 - progress_bar.py[line:274] - INFO: epoch 001:    161 / 5261 loss=0.865, loss_v1=0, loss_v2=0, nll_loss=0.699, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=100, ups=0.33, wpb=304.6, bsz=120, num_updates=160, lr=7.60456e-06, gnorm=1.726, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=505
2022-10-01 05:33:00 - progress_bar.py[line:274] - INFO: epoch 001:    171 / 5261 loss=0.859, loss_v1=0, loss_v2=0, nll_loss=0.69, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=100.6, ups=0.33, wpb=304.3, bsz=120, num_updates=170, lr=8.07985e-06, gnorm=1.704, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=535
2022-10-01 05:33:30 - progress_bar.py[line:274] - INFO: epoch 001:    181 / 5261 loss=0.878, loss_v1=0, loss_v2=0, nll_loss=0.71, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=102.9, ups=0.34, wpb=302.5, bsz=120, num_updates=180, lr=8.55513e-06, gnorm=1.639, clip=100, loss_scale=64, train_wall=29, gb_free=10, ema_decay=0.9999, wall=565
2022-10-01 05:33:59 - progress_bar.py[line:274] - INFO: epoch 001:    191 / 5261 loss=0.85, loss_v1=0, loss_v2=0, nll_loss=0.68, ntokens=305.6, nsentences=120, sample_size=305.6, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=102.9, ups=0.34, wpb=305.6, bsz=120, num_updates=190, lr=9.03042e-06, gnorm=1.763, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=594
2022-10-01 05:34:29 - progress_bar.py[line:274] - INFO: epoch 001:    201 / 5261 loss=0.793, loss_v1=0, loss_v2=0, nll_loss=0.618, ntokens=309.2, nsentences=120, sample_size=309.2, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=103.2, ups=0.33, wpb=309.2, bsz=120, num_updates=200, lr=9.5057e-06, gnorm=1.784, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=624
2022-10-01 05:35:00 - progress_bar.py[line:274] - INFO: epoch 001:    211 / 5261 loss=0.822, loss_v1=0, loss_v2=0, nll_loss=0.653, ntokens=308.2, nsentences=120, sample_size=308.2, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=100.6, ups=0.33, wpb=308.2, bsz=120, num_updates=210, lr=9.98099e-06, gnorm=1.825, clip=100, loss_scale=64, train_wall=31, gb_free=10, ema_decay=0.9999, wall=655
2022-10-01 05:35:30 - progress_bar.py[line:274] - INFO: epoch 001:    221 / 5261 loss=0.84, loss_v1=0, loss_v2=0, nll_loss=0.664, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=101.2, ups=0.33, wpb=304, bsz=120, num_updates=220, lr=1.04563e-05, gnorm=1.78, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=685
2022-10-01 05:36:00 - progress_bar.py[line:274] - INFO: epoch 001:    231 / 5261 loss=0.802, loss_v1=0, loss_v2=0, nll_loss=0.616, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=101.5, ups=0.33, wpb=302.9, bsz=120, num_updates=230, lr=1.09316e-05, gnorm=1.956, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=715
2022-10-01 05:36:30 - progress_bar.py[line:274] - INFO: epoch 001:    241 / 5261 loss=0.794, loss_v1=0, loss_v2=0, nll_loss=0.609, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=101.7, ups=0.33, wpb=304.5, bsz=120, num_updates=240, lr=1.14068e-05, gnorm=1.967, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=745
2022-10-01 05:37:00 - progress_bar.py[line:274] - INFO: epoch 001:    251 / 5261 loss=0.779, loss_v1=0, loss_v2=0, nll_loss=0.589, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=101.2, ups=0.33, wpb=304.5, bsz=120, num_updates=250, lr=1.18821e-05, gnorm=1.769, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=775
2022-10-01 05:37:30 - progress_bar.py[line:274] - INFO: epoch 001:    261 / 5261 loss=0.754, loss_v1=0, loss_v2=0, nll_loss=0.554, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=100.3, ups=0.33, wpb=304.5, bsz=120, num_updates=260, lr=1.23574e-05, gnorm=1.684, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=805
2022-10-01 05:38:00 - progress_bar.py[line:274] - INFO: epoch 001:    271 / 5261 loss=0.781, loss_v1=0, loss_v2=0, nll_loss=0.589, ntokens=300.3, nsentences=120, sample_size=300.3, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=101.9, ups=0.34, wpb=300.3, bsz=120, num_updates=270, lr=1.28327e-05, gnorm=1.678, clip=100, loss_scale=64, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=835
2022-10-01 05:38:30 - progress_bar.py[line:274] - INFO: epoch 001:    281 / 5261 loss=0.742, loss_v1=0, loss_v2=0, nll_loss=0.548, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=101.3, ups=0.33, wpb=304.2, bsz=120, num_updates=280, lr=1.3308e-05, gnorm=1.492, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=865
2022-10-01 05:39:00 - progress_bar.py[line:274] - INFO: epoch 001:    291 / 5261 loss=0.751, loss_v1=0, loss_v2=0, nll_loss=0.557, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=101.2, ups=0.33, wpb=304.8, bsz=120, num_updates=290, lr=1.37833e-05, gnorm=1.619, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=895
2022-10-01 05:39:29 - progress_bar.py[line:274] - INFO: epoch 001:    301 / 5261 loss=0.723, loss_v1=0, loss_v2=0, nll_loss=0.531, ntokens=307.1, nsentences=120, sample_size=307.1, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=104.7, ups=0.34, wpb=307.1, bsz=120, num_updates=300, lr=1.42586e-05, gnorm=1.497, clip=100, loss_scale=64, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=924
2022-10-01 05:39:59 - progress_bar.py[line:274] - INFO: epoch 001:    311 / 5261 loss=0.725, loss_v1=0, loss_v2=0, nll_loss=0.525, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=101.9, ups=0.34, wpb=302.9, bsz=120, num_updates=310, lr=1.47338e-05, gnorm=1.583, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=954
2022-10-01 05:40:29 - progress_bar.py[line:274] - INFO: epoch 001:    321 / 5261 loss=0.752, loss_v1=0, loss_v2=0, nll_loss=0.548, ntokens=299, nsentences=120, sample_size=299, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=98.5, ups=0.33, wpb=299, bsz=120, num_updates=320, lr=1.52091e-05, gnorm=1.557, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=984
2022-10-01 05:40:59 - progress_bar.py[line:274] - INFO: epoch 001:    331 / 5261 loss=0.699, loss_v1=0, loss_v2=0, nll_loss=0.496, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=101.7, ups=0.33, wpb=304.2, bsz=120, num_updates=330, lr=1.56844e-05, gnorm=1.478, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1014
2022-10-01 05:41:30 - progress_bar.py[line:274] - INFO: epoch 001:    341 / 5261 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=309.1, nsentences=120, sample_size=309.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=102.4, ups=0.33, wpb=309.1, bsz=120, num_updates=340, lr=1.61597e-05, gnorm=1.372, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=1044
2022-10-01 05:42:00 - progress_bar.py[line:274] - INFO: epoch 001:    351 / 5261 loss=0.676, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=101.2, ups=0.33, wpb=305, bsz=120, num_updates=350, lr=1.6635e-05, gnorm=1.215, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1075
2022-10-01 05:42:30 - progress_bar.py[line:274] - INFO: epoch 001:    361 / 5261 loss=0.687, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=100.3, ups=0.33, wpb=302.5, bsz=120, num_updates=360, lr=1.71103e-05, gnorm=1.476, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1105
2022-10-01 05:43:00 - progress_bar.py[line:274] - INFO: epoch 001:    371 / 5261 loss=0.686, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=102.5, ups=0.34, wpb=304.6, bsz=120, num_updates=370, lr=1.75856e-05, gnorm=1.293, clip=100, loss_scale=64, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=1134
2022-10-01 05:43:30 - progress_bar.py[line:274] - INFO: epoch 001:    381 / 5261 loss=0.69, loss_v1=0, loss_v2=0, nll_loss=0.485, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=100.4, ups=0.33, wpb=302.2, bsz=120, num_updates=380, lr=1.80608e-05, gnorm=1.44, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=1165
2022-10-01 05:44:00 - progress_bar.py[line:274] - INFO: epoch 001:    391 / 5261 loss=0.691, loss_v1=0, loss_v2=0, nll_loss=0.486, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=99.7, ups=0.33, wpb=303.8, bsz=120, num_updates=390, lr=1.85361e-05, gnorm=1.466, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1195
2022-10-01 05:44:31 - progress_bar.py[line:274] - INFO: epoch 001:    401 / 5261 loss=0.664, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=98.3, ups=0.32, wpb=304.8, bsz=120, num_updates=400, lr=1.90114e-05, gnorm=1.17, clip=100, loss_scale=64, train_wall=31, gb_free=10, ema_decay=0.9999, wall=1226
2022-10-01 05:45:01 - progress_bar.py[line:274] - INFO: epoch 001:    411 / 5261 loss=0.686, loss_v1=0, loss_v2=0, nll_loss=0.474, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=101.1, ups=0.33, wpb=303.1, bsz=120, num_updates=410, lr=1.94867e-05, gnorm=1.433, clip=100, loss_scale=64, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1256
2022-10-01 05:45:33 - progress_bar.py[line:274] - INFO: epoch 001:    421 / 5261 loss=0.654, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=96.7, ups=0.32, wpb=305.4, bsz=120, num_updates=420, lr=1.9962e-05, gnorm=1.491, clip=100, loss_scale=64, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=1288
2022-10-01 05:46:04 - progress_bar.py[line:274] - INFO: epoch 001:    431 / 5261 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=97.9, ups=0.32, wpb=304.5, bsz=120, num_updates=430, lr=2.04373e-05, gnorm=1.405, clip=100, loss_scale=64, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=1319
2022-10-01 05:46:34 - progress_bar.py[line:274] - INFO: epoch 001:    441 / 5261 loss=0.663, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=102.3, ups=0.34, wpb=303.4, bsz=120, num_updates=440, lr=2.09125e-05, gnorm=1.264, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=1348
2022-10-01 05:47:04 - progress_bar.py[line:274] - INFO: epoch 001:    451 / 5261 loss=0.67, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=301.8, nsentences=120, sample_size=301.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.2, ups=0.33, wpb=301.8, bsz=120, num_updates=450, lr=2.13878e-05, gnorm=1.557, clip=100, loss_scale=64, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=1378
2022-10-01 05:47:33 - progress_bar.py[line:274] - INFO: epoch 001:    461 / 5261 loss=0.669, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=102.5, ups=0.34, wpb=304.2, bsz=120, num_updates=460, lr=2.18631e-05, gnorm=1.459, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1408
2022-10-01 05:48:03 - progress_bar.py[line:274] - INFO: epoch 001:    471 / 5261 loss=0.654, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=306.7, nsentences=120, sample_size=306.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.7, ups=0.33, wpb=306.7, bsz=120, num_updates=470, lr=2.23384e-05, gnorm=1.318, clip=100, loss_scale=64, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=1438
2022-10-01 05:48:34 - progress_bar.py[line:274] - INFO: epoch 001:    481 / 5261 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.2, ups=0.33, wpb=305.7, bsz=120, num_updates=480, lr=2.28137e-05, gnorm=1.162, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1469
2022-10-01 05:49:05 - progress_bar.py[line:274] - INFO: epoch 001:    491 / 5261 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.7, ups=0.33, wpb=304.4, bsz=120, num_updates=490, lr=2.3289e-05, gnorm=1.195, clip=100, loss_scale=64, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=1499
2022-10-01 05:49:36 - progress_bar.py[line:274] - INFO: epoch 001:    501 / 5261 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=97.6, ups=0.32, wpb=305.7, bsz=120, num_updates=500, lr=2.37643e-05, gnorm=1.285, clip=100, loss_scale=64, train_wall=31, gb_free=9.9, ema_decay=0.9999, wall=1531
2022-10-01 05:50:06 - progress_bar.py[line:274] - INFO: epoch 001:    511 / 5261 loss=0.672, loss_v1=0, loss_v2=0, nll_loss=0.467, ntokens=302.1, nsentences=120, sample_size=302.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=99.3, ups=0.33, wpb=302.1, bsz=120, num_updates=510, lr=2.42395e-05, gnorm=1.212, clip=100, loss_scale=64, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1561
2022-10-01 05:50:36 - progress_bar.py[line:274] - INFO: epoch 001:    521 / 5261 loss=0.659, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=102.5, ups=0.34, wpb=303.5, bsz=120, num_updates=520, lr=2.47148e-05, gnorm=1.171, clip=90, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1591
2022-10-01 05:51:06 - progress_bar.py[line:274] - INFO: epoch 001:    531 / 5261 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=102.1, ups=0.34, wpb=303.5, bsz=120, num_updates=530, lr=2.51901e-05, gnorm=1.199, clip=80, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1620
2022-10-01 05:51:36 - progress_bar.py[line:274] - INFO: epoch 001:    541 / 5261 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.8, ups=0.33, wpb=303.5, bsz=120, num_updates=540, lr=2.56654e-05, gnorm=1.093, clip=80, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1651
2022-10-01 05:52:06 - progress_bar.py[line:274] - INFO: epoch 001:    551 / 5261 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=307.2, nsentences=120, sample_size=307.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.3, ups=0.33, wpb=307.2, bsz=120, num_updates=550, lr=2.61407e-05, gnorm=1.316, clip=100, loss_scale=128, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=1681
2022-10-01 05:52:36 - progress_bar.py[line:274] - INFO: epoch 001:    561 / 5261 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.1, ups=0.33, wpb=303.5, bsz=120, num_updates=560, lr=2.6616e-05, gnorm=1.156, clip=80, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=1711
2022-10-01 05:53:07 - progress_bar.py[line:274] - INFO: epoch 001:    571 / 5261 loss=0.656, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.5, ups=0.33, wpb=303.8, bsz=120, num_updates=570, lr=2.70913e-05, gnorm=1.134, clip=80, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1741
2022-10-01 05:53:37 - progress_bar.py[line:274] - INFO: epoch 001:    581 / 5261 loss=0.663, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=101, ups=0.33, wpb=304.5, bsz=120, num_updates=580, lr=2.75665e-05, gnorm=1.203, clip=90, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1772
2022-10-01 05:54:07 - progress_bar.py[line:274] - INFO: epoch 001:    591 / 5261 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.5, ups=0.33, wpb=303.2, bsz=120, num_updates=590, lr=2.80418e-05, gnorm=1.369, clip=80, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1802
2022-10-01 05:54:37 - progress_bar.py[line:274] - INFO: epoch 001:    601 / 5261 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=308.3, nsentences=120, sample_size=308.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=101.8, ups=0.33, wpb=308.3, bsz=120, num_updates=600, lr=2.85171e-05, gnorm=1.214, clip=100, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=1832
2022-10-01 05:55:08 - progress_bar.py[line:274] - INFO: epoch 001:    611 / 5261 loss=0.675, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=300.9, nsentences=120, sample_size=300.9, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=99.9, ups=0.33, wpb=300.9, bsz=120, num_updates=610, lr=2.89924e-05, gnorm=1.13, clip=80, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=1862
2022-10-01 05:55:38 - progress_bar.py[line:274] - INFO: epoch 001:    621 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=306.1, nsentences=120, sample_size=306.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.5, ups=0.33, wpb=306.1, bsz=120, num_updates=620, lr=2.94677e-05, gnorm=1.144, clip=90, loss_scale=128, train_wall=30, gb_free=9.5, ema_decay=0.9999, wall=1893
2022-10-01 05:56:08 - progress_bar.py[line:274] - INFO: epoch 001:    631 / 5261 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.3, ups=0.33, wpb=303.7, bsz=120, num_updates=630, lr=2.9943e-05, gnorm=1.155, clip=90, loss_scale=128, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=1923
2022-10-01 05:56:38 - progress_bar.py[line:274] - INFO: epoch 001:    641 / 5261 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=103.3, ups=0.34, wpb=304.5, bsz=120, num_updates=640, lr=3.04183e-05, gnorm=1.161, clip=80, loss_scale=128, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=1953
2022-10-01 05:57:08 - progress_bar.py[line:274] - INFO: epoch 001:    651 / 5261 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=102.4, ups=0.33, wpb=305.7, bsz=120, num_updates=650, lr=3.08935e-05, gnorm=1.113, clip=60, loss_scale=128, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=1983
2022-10-01 05:57:37 - progress_bar.py[line:274] - INFO: epoch 001:    661 / 5261 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=102.7, ups=0.34, wpb=304.3, bsz=120, num_updates=660, lr=3.13688e-05, gnorm=1.061, clip=60, loss_scale=128, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=2012
2022-10-01 05:58:07 - progress_bar.py[line:274] - INFO: epoch 001:    671 / 5261 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=101.7, ups=0.34, wpb=303.1, bsz=120, num_updates=670, lr=3.18441e-05, gnorm=1.097, clip=80, loss_scale=128, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=2042
2022-10-01 05:58:37 - progress_bar.py[line:274] - INFO: epoch 001:    681 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=307.8, nsentences=120, sample_size=307.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=103.7, ups=0.34, wpb=307.8, bsz=120, num_updates=680, lr=3.23194e-05, gnorm=0.994, clip=50, loss_scale=128, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=2072
2022-10-01 05:59:06 - progress_bar.py[line:274] - INFO: epoch 001:    691 / 5261 loss=0.655, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=103.1, ups=0.34, wpb=303.6, bsz=120, num_updates=690, lr=3.27947e-05, gnorm=1.069, clip=70, loss_scale=128, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=2101
2022-10-01 05:59:37 - progress_bar.py[line:274] - INFO: epoch 001:    701 / 5261 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.4, ups=0.33, wpb=304.4, bsz=120, num_updates=700, lr=3.327e-05, gnorm=1.271, clip=100, loss_scale=128, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=2132
2022-10-01 06:00:07 - progress_bar.py[line:274] - INFO: epoch 001:    711 / 5261 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.8, ups=0.33, wpb=305.4, bsz=120, num_updates=710, lr=3.37452e-05, gnorm=1.016, clip=40, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=2162
2022-10-01 06:00:37 - progress_bar.py[line:274] - INFO: epoch 001:    721 / 5261 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.8, ups=0.33, wpb=304.1, bsz=120, num_updates=720, lr=3.42205e-05, gnorm=0.946, clip=20, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2192
2022-10-01 06:01:07 - progress_bar.py[line:274] - INFO: epoch 001:    731 / 5261 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=307.3, nsentences=120, sample_size=307.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.7, ups=0.33, wpb=307.3, bsz=120, num_updates=730, lr=3.46958e-05, gnorm=0.952, clip=30, loss_scale=128, train_wall=30, gb_free=9.6, ema_decay=0.9999, wall=2222
2022-10-01 06:01:37 - progress_bar.py[line:274] - INFO: epoch 001:    741 / 5261 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=302.6, nsentences=120, sample_size=302.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100, ups=0.33, wpb=302.6, bsz=120, num_updates=740, lr=3.51711e-05, gnorm=1.079, clip=80, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2252
2022-10-01 06:02:08 - progress_bar.py[line:274] - INFO: epoch 001:    751 / 5261 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=99.2, ups=0.33, wpb=304.2, bsz=120, num_updates=750, lr=3.56464e-05, gnorm=1.086, clip=70, loss_scale=128, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=2283
2022-10-01 06:02:39 - progress_bar.py[line:274] - INFO: epoch 001:    761 / 5261 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98, ups=0.32, wpb=304.3, bsz=120, num_updates=760, lr=3.61217e-05, gnorm=1.11, clip=60, loss_scale=128, train_wall=31, gb_free=10, ema_decay=0.9999, wall=2314
2022-10-01 06:03:09 - progress_bar.py[line:274] - INFO: epoch 001:    771 / 5261 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99.5, ups=0.33, wpb=302.5, bsz=120, num_updates=770, lr=3.6597e-05, gnorm=0.999, clip=30, loss_scale=128, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=2344
2022-10-01 06:03:39 - progress_bar.py[line:274] - INFO: epoch 001:    781 / 5261 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=305.1, nsentences=120, sample_size=305.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=102.2, ups=0.33, wpb=305.1, bsz=120, num_updates=780, lr=3.70722e-05, gnorm=1.065, clip=60, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=2374
2022-10-01 06:04:10 - progress_bar.py[line:274] - INFO: epoch 001:    791 / 5261 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=301.6, nsentences=120, sample_size=301.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=98.8, ups=0.33, wpb=301.6, bsz=120, num_updates=790, lr=3.75475e-05, gnorm=1.057, clip=60, loss_scale=128, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=2405
2022-10-01 06:04:40 - progress_bar.py[line:274] - INFO: epoch 001:    801 / 5261 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.6, ups=0.33, wpb=305.7, bsz=120, num_updates=800, lr=3.80228e-05, gnorm=1.121, clip=70, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2435
2022-10-01 06:05:11 - progress_bar.py[line:274] - INFO: epoch 001:    811 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.3, ups=0.32, wpb=302.4, bsz=120, num_updates=810, lr=3.84981e-05, gnorm=0.966, clip=40, loss_scale=128, train_wall=31, gb_free=10, ema_decay=0.9999, wall=2465
2022-10-01 06:05:41 - progress_bar.py[line:274] - INFO: epoch 001:    821 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.9, ups=0.33, wpb=304.2, bsz=120, num_updates=820, lr=3.89734e-05, gnorm=1.003, clip=60, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2496
2022-10-01 06:06:11 - progress_bar.py[line:274] - INFO: epoch 001:    831 / 5261 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.2, ups=0.33, wpb=304.2, bsz=120, num_updates=830, lr=3.94487e-05, gnorm=1.102, clip=80, loss_scale=128, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=2526
2022-10-01 06:06:41 - progress_bar.py[line:274] - INFO: epoch 001:    841 / 5261 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.3, ups=0.33, wpb=303.3, bsz=120, num_updates=840, lr=3.9924e-05, gnorm=1.145, clip=60, loss_scale=128, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=2556
2022-10-01 06:07:11 - progress_bar.py[line:274] - INFO: epoch 001:    851 / 5261 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=299.8, nsentences=120, sample_size=299.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101.3, ups=0.34, wpb=299.8, bsz=120, num_updates=850, lr=4.03992e-05, gnorm=1.044, clip=40, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2586
2022-10-01 06:07:41 - progress_bar.py[line:274] - INFO: epoch 001:    861 / 5261 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=301, nsentences=120, sample_size=301, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.7, ups=0.33, wpb=301, bsz=120, num_updates=860, lr=4.08745e-05, gnorm=1.052, clip=70, loss_scale=128, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=2616
2022-10-01 06:08:12 - progress_bar.py[line:274] - INFO: epoch 001:    871 / 5261 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=301.5, nsentences=120, sample_size=301.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=97.8, ups=0.32, wpb=301.5, bsz=120, num_updates=870, lr=4.13498e-05, gnorm=1.02, clip=40, loss_scale=128, train_wall=31, gb_free=9.9, ema_decay=0.9999, wall=2647
2022-10-01 06:08:41 - progress_bar.py[line:274] - INFO: epoch 001:    881 / 5261 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=306.2, nsentences=120, sample_size=306.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=104.2, ups=0.34, wpb=306.2, bsz=120, num_updates=880, lr=4.18251e-05, gnorm=1.03, clip=70, loss_scale=128, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=2676
2022-10-01 06:09:12 - progress_bar.py[line:274] - INFO: epoch 001:    891 / 5261 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=306.7, nsentences=120, sample_size=306.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.7, ups=0.33, wpb=306.7, bsz=120, num_updates=890, lr=4.23004e-05, gnorm=0.963, clip=40, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2706
2022-10-01 06:09:42 - progress_bar.py[line:274] - INFO: epoch 001:    901 / 5261 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=98.8, ups=0.33, wpb=303.2, bsz=120, num_updates=900, lr=4.27757e-05, gnorm=1.002, clip=60, loss_scale=128, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=2737
2022-10-01 06:10:12 - progress_bar.py[line:274] - INFO: epoch 001:    911 / 5261 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100, ups=0.33, wpb=302.2, bsz=120, num_updates=910, lr=4.3251e-05, gnorm=1.004, clip=40, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=2767
2022-10-01 06:10:42 - progress_bar.py[line:274] - INFO: epoch 001:    921 / 5261 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=103.1, ups=0.34, wpb=305.3, bsz=120, num_updates=920, lr=4.37262e-05, gnorm=0.958, clip=40, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=2797
2022-10-01 06:11:12 - progress_bar.py[line:274] - INFO: epoch 001:    931 / 5261 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.8, ups=0.34, wpb=303.6, bsz=120, num_updates=930, lr=4.42015e-05, gnorm=0.944, clip=30, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2827
2022-10-01 06:11:42 - progress_bar.py[line:274] - INFO: epoch 001:    941 / 5261 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=101.3, ups=0.33, wpb=303.3, bsz=120, num_updates=940, lr=4.46768e-05, gnorm=0.949, clip=30, loss_scale=128, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=2857
2022-10-01 06:12:12 - progress_bar.py[line:274] - INFO: epoch 001:    951 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=306.9, nsentences=120, sample_size=306.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.2, ups=0.33, wpb=306.9, bsz=120, num_updates=950, lr=4.51521e-05, gnorm=0.905, clip=20, loss_scale=128, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=2887
2022-10-01 06:12:42 - progress_bar.py[line:274] - INFO: epoch 001:    961 / 5261 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100, ups=0.33, wpb=302.2, bsz=120, num_updates=960, lr=4.56274e-05, gnorm=1.025, clip=50, loss_scale=128, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=2917
2022-10-01 06:13:12 - progress_bar.py[line:274] - INFO: epoch 001:    971 / 5261 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.8, ups=0.33, wpb=304.1, bsz=120, num_updates=970, lr=4.61027e-05, gnorm=0.967, clip=50, loss_scale=128, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=2947
2022-10-01 06:13:42 - progress_bar.py[line:274] - INFO: epoch 001:    981 / 5261 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.9, ups=0.33, wpb=303.5, bsz=120, num_updates=980, lr=4.65779e-05, gnorm=1.065, clip=60, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=2977
2022-10-01 06:14:12 - progress_bar.py[line:274] - INFO: epoch 001:    991 / 5261 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101.5, ups=0.33, wpb=303.8, bsz=120, num_updates=990, lr=4.70532e-05, gnorm=0.926, clip=20, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=3007
2022-10-01 06:14:42 - progress_bar.py[line:274] - INFO: epoch 001:   1001 / 5261 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=301.9, nsentences=120, sample_size=301.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.4, ups=0.33, wpb=301.9, bsz=120, num_updates=1000, lr=4.75285e-05, gnorm=0.98, clip=30, loss_scale=128, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=3037
2022-10-01 06:15:12 - progress_bar.py[line:274] - INFO: epoch 001:   1011 / 5261 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.6, ups=0.33, wpb=302.9, bsz=120, num_updates=1010, lr=4.80038e-05, gnorm=0.957, clip=30, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3067
2022-10-01 06:15:42 - progress_bar.py[line:274] - INFO: epoch 001:   1021 / 5261 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.7, ups=0.33, wpb=304.2, bsz=120, num_updates=1020, lr=4.84791e-05, gnorm=0.983, clip=40, loss_scale=128, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3097
2022-10-01 06:16:12 - progress_bar.py[line:274] - INFO: epoch 001:   1031 / 5261 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=100.3, ups=0.33, wpb=303.3, bsz=120, num_updates=1030, lr=4.89544e-05, gnorm=1.016, clip=60, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3127
2022-10-01 06:16:42 - progress_bar.py[line:274] - INFO: epoch 001:   1041 / 5261 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=301.9, nsentences=120, sample_size=301.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=102.8, ups=0.34, wpb=301.9, bsz=120, num_updates=1040, lr=4.94297e-05, gnorm=0.883, clip=30, loss_scale=256, train_wall=29, gb_free=9.7, ema_decay=0.9999, wall=3157
2022-10-01 06:17:12 - progress_bar.py[line:274] - INFO: epoch 001:   1051 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=306.3, nsentences=120, sample_size=306.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=102, ups=0.33, wpb=306.3, bsz=120, num_updates=1050, lr=4.99049e-05, gnorm=0.921, clip=20, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3187
2022-10-01 06:17:42 - progress_bar.py[line:274] - INFO: epoch 001:   1061 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.8, ups=0.33, wpb=305, bsz=120, num_updates=1060, lr=4.99842e-05, gnorm=0.886, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3217
2022-10-01 06:18:12 - progress_bar.py[line:274] - INFO: epoch 001:   1071 / 5261 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99, ups=0.33, wpb=302.4, bsz=120, num_updates=1070, lr=4.99644e-05, gnorm=0.967, clip=30, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3247
2022-10-01 06:18:43 - progress_bar.py[line:274] - INFO: epoch 001:   1081 / 5261 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=98.2, ups=0.32, wpb=302.5, bsz=120, num_updates=1080, lr=4.99446e-05, gnorm=0.85, clip=10, loss_scale=256, train_wall=31, gb_free=9.6, ema_decay=0.9999, wall=3278
2022-10-01 06:19:13 - progress_bar.py[line:274] - INFO: epoch 001:   1091 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=302.1, nsentences=120, sample_size=302.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.7, ups=0.33, wpb=302.1, bsz=120, num_updates=1090, lr=4.99248e-05, gnorm=0.814, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=3308
2022-10-01 06:19:43 - progress_bar.py[line:274] - INFO: epoch 001:   1101 / 5261 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=302.7, nsentences=120, sample_size=302.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=101.9, ups=0.34, wpb=302.7, bsz=120, num_updates=1100, lr=4.9905e-05, gnorm=0.87, clip=20, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3338
2022-10-01 06:20:13 - progress_bar.py[line:274] - INFO: epoch 001:   1111 / 5261 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=301.6, nsentences=120, sample_size=301.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.9, ups=0.33, wpb=301.6, bsz=120, num_updates=1110, lr=4.98852e-05, gnorm=0.919, clip=30, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3367
2022-10-01 06:20:43 - progress_bar.py[line:274] - INFO: epoch 001:   1121 / 5261 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=307, nsentences=120, sample_size=307, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.4, ups=0.33, wpb=307, bsz=120, num_updates=1120, lr=4.98654e-05, gnorm=0.921, clip=30, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=3398
2022-10-01 06:21:13 - progress_bar.py[line:274] - INFO: epoch 001:   1131 / 5261 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=302.7, nsentences=120, sample_size=302.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=99.9, ups=0.33, wpb=302.7, bsz=120, num_updates=1130, lr=4.98456e-05, gnorm=0.84, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3428
2022-10-01 06:21:43 - progress_bar.py[line:274] - INFO: epoch 001:   1141 / 5261 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=102.6, ups=0.34, wpb=302.4, bsz=120, num_updates=1140, lr=4.98258e-05, gnorm=0.846, clip=10, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=3458
2022-10-01 06:22:15 - progress_bar.py[line:274] - INFO: epoch 001:   1151 / 5261 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=95.6, ups=0.31, wpb=305.9, bsz=120, num_updates=1150, lr=4.9806e-05, gnorm=0.81, clip=10, loss_scale=256, train_wall=32, gb_free=10.2, ema_decay=0.9999, wall=3490
2022-10-01 06:22:44 - progress_bar.py[line:274] - INFO: epoch 001:   1161 / 5261 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=102.5, ups=0.34, wpb=302.4, bsz=120, num_updates=1160, lr=4.97862e-05, gnorm=0.84, clip=10, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=3519
2022-10-01 06:23:14 - progress_bar.py[line:274] - INFO: epoch 001:   1171 / 5261 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=301.4, nsentences=120, sample_size=301.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=101.3, ups=0.34, wpb=301.4, bsz=120, num_updates=1170, lr=4.97664e-05, gnorm=0.823, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=3549
2022-10-01 06:23:43 - progress_bar.py[line:274] - INFO: epoch 001:   1181 / 5261 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=102.5, ups=0.34, wpb=302.4, bsz=120, num_updates=1180, lr=4.97466e-05, gnorm=1.043, clip=50, loss_scale=256, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=3578
2022-10-01 06:24:14 - progress_bar.py[line:274] - INFO: epoch 001:   1191 / 5261 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.3, ups=0.33, wpb=303.4, bsz=120, num_updates=1190, lr=4.97268e-05, gnorm=0.863, clip=20, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=3609
2022-10-01 06:24:43 - progress_bar.py[line:274] - INFO: epoch 001:   1201 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=307, nsentences=120, sample_size=307, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=103.4, ups=0.34, wpb=307, bsz=120, num_updates=1200, lr=4.9707e-05, gnorm=0.859, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3638
2022-10-01 06:25:14 - progress_bar.py[line:274] - INFO: epoch 001:   1211 / 5261 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.6, ups=0.33, wpb=304.6, bsz=120, num_updates=1210, lr=4.96872e-05, gnorm=0.922, clip=30, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=3669
2022-10-01 06:25:44 - progress_bar.py[line:274] - INFO: epoch 001:   1221 / 5261 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=306.1, nsentences=120, sample_size=306.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.8, ups=0.33, wpb=306.1, bsz=120, num_updates=1220, lr=4.96674e-05, gnorm=0.9, clip=30, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=3699
2022-10-01 06:26:14 - progress_bar.py[line:274] - INFO: epoch 001:   1231 / 5261 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.3, ups=0.33, wpb=303.4, bsz=120, num_updates=1230, lr=4.96476e-05, gnorm=0.953, clip=40, loss_scale=256, train_wall=30, gb_free=9.6, ema_decay=0.9999, wall=3729
2022-10-01 06:26:44 - progress_bar.py[line:274] - INFO: epoch 001:   1241 / 5261 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=308, nsentences=120, sample_size=308, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.9, ups=0.33, wpb=308, bsz=120, num_updates=1240, lr=4.96278e-05, gnorm=0.829, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=3758
2022-10-01 06:27:14 - progress_bar.py[line:274] - INFO: epoch 001:   1251 / 5261 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.6, ups=0.33, wpb=303.6, bsz=120, num_updates=1250, lr=4.9608e-05, gnorm=0.791, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=3789
2022-10-01 06:27:44 - progress_bar.py[line:274] - INFO: epoch 001:   1261 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.4, ups=0.34, wpb=304.6, bsz=120, num_updates=1260, lr=4.95882e-05, gnorm=0.829, clip=10, loss_scale=256, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=3819
2022-10-01 06:28:14 - progress_bar.py[line:274] - INFO: epoch 001:   1271 / 5261 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.3, ups=0.33, wpb=305, bsz=120, num_updates=1270, lr=4.95684e-05, gnorm=0.992, clip=40, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=3849
2022-10-01 06:28:44 - progress_bar.py[line:274] - INFO: epoch 001:   1281 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.3, ups=0.34, wpb=303.6, bsz=120, num_updates=1280, lr=4.95486e-05, gnorm=0.857, clip=20, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=3879
2022-10-01 06:29:14 - progress_bar.py[line:274] - INFO: epoch 001:   1291 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=306.4, nsentences=120, sample_size=306.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=103, ups=0.34, wpb=306.4, bsz=120, num_updates=1290, lr=4.95288e-05, gnorm=0.77, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=3909
2022-10-01 06:29:44 - progress_bar.py[line:274] - INFO: epoch 001:   1301 / 5261 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.4, ups=0.33, wpb=303.2, bsz=120, num_updates=1300, lr=4.9509e-05, gnorm=0.889, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=3938
2022-10-01 06:30:14 - progress_bar.py[line:274] - INFO: epoch 001:   1311 / 5261 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=308, nsentences=120, sample_size=308, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.2, ups=0.33, wpb=308, bsz=120, num_updates=1310, lr=4.94892e-05, gnorm=0.9, clip=30, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=3969
2022-10-01 06:30:44 - progress_bar.py[line:274] - INFO: epoch 001:   1321 / 5261 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=102.7, ups=0.34, wpb=303.1, bsz=120, num_updates=1320, lr=4.94694e-05, gnorm=0.766, clip=0, loss_scale=256, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=3998
2022-10-01 06:31:14 - progress_bar.py[line:274] - INFO: epoch 001:   1331 / 5261 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100, ups=0.33, wpb=302.5, bsz=120, num_updates=1330, lr=4.94496e-05, gnorm=0.863, clip=30, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=4029
2022-10-01 06:31:44 - progress_bar.py[line:274] - INFO: epoch 001:   1341 / 5261 loss=0.673, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=300.4, nsentences=120, sample_size=300.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=99.6, ups=0.33, wpb=300.4, bsz=120, num_updates=1340, lr=4.94298e-05, gnorm=0.991, clip=40, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=4059
2022-10-01 06:32:14 - progress_bar.py[line:274] - INFO: epoch 001:   1351 / 5261 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=302.6, nsentences=120, sample_size=302.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=100.4, ups=0.33, wpb=302.6, bsz=120, num_updates=1350, lr=4.941e-05, gnorm=0.872, clip=30, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=4089
2022-10-01 06:32:44 - progress_bar.py[line:274] - INFO: epoch 001:   1361 / 5261 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=103.4, ups=0.34, wpb=304.5, bsz=120, num_updates=1360, lr=4.93902e-05, gnorm=0.859, clip=10, loss_scale=256, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=4118
2022-10-01 06:33:13 - progress_bar.py[line:274] - INFO: epoch 001:   1371 / 5261 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=300.8, nsentences=120, sample_size=300.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.3, ups=0.34, wpb=300.8, bsz=120, num_updates=1370, lr=4.93704e-05, gnorm=0.883, clip=30, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=4148
2022-10-01 06:33:44 - progress_bar.py[line:274] - INFO: epoch 001:   1381 / 5261 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.4, ups=0.33, wpb=303.7, bsz=120, num_updates=1380, lr=4.93506e-05, gnorm=0.812, clip=10, loss_scale=256, train_wall=30, gb_free=10.4, ema_decay=0.9999, wall=4179
2022-10-01 06:34:15 - progress_bar.py[line:274] - INFO: epoch 001:   1391 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.7, ups=0.32, wpb=303.5, bsz=120, num_updates=1390, lr=4.93308e-05, gnorm=0.89, clip=20, loss_scale=256, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=4210
2022-10-01 06:34:45 - progress_bar.py[line:274] - INFO: epoch 001:   1401 / 5261 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.3, ups=0.33, wpb=304, bsz=120, num_updates=1400, lr=4.9311e-05, gnorm=0.908, clip=30, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=4240
2022-10-01 06:35:16 - progress_bar.py[line:274] - INFO: epoch 001:   1411 / 5261 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.1, ups=0.33, wpb=302.9, bsz=120, num_updates=1410, lr=4.92912e-05, gnorm=0.873, clip=10, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=4270
2022-10-01 06:35:47 - progress_bar.py[line:274] - INFO: epoch 001:   1421 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.2, ups=0.32, wpb=304.6, bsz=120, num_updates=1420, lr=4.92714e-05, gnorm=0.781, clip=0, loss_scale=256, train_wall=31, gb_free=9.9, ema_decay=0.9999, wall=4302
2022-10-01 06:36:16 - progress_bar.py[line:274] - INFO: epoch 001:   1431 / 5261 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=103, ups=0.34, wpb=303.7, bsz=120, num_updates=1430, lr=4.92516e-05, gnorm=0.761, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=4331
2022-10-01 06:36:47 - progress_bar.py[line:274] - INFO: epoch 001:   1441 / 5261 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=301.5, nsentences=120, sample_size=301.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99.5, ups=0.33, wpb=301.5, bsz=120, num_updates=1440, lr=4.92318e-05, gnorm=0.844, clip=10, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=4361
2022-10-01 06:37:17 - progress_bar.py[line:274] - INFO: epoch 001:   1451 / 5261 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.8, ups=0.33, wpb=304.6, bsz=120, num_updates=1450, lr=4.9212e-05, gnorm=0.811, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=4392
2022-10-01 06:37:48 - progress_bar.py[line:274] - INFO: epoch 001:   1461 / 5261 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.8, ups=0.33, wpb=304, bsz=120, num_updates=1460, lr=4.91922e-05, gnorm=0.791, clip=0, loss_scale=256, train_wall=31, gb_free=10.3, ema_decay=0.9999, wall=4422
2022-10-01 06:38:18 - progress_bar.py[line:274] - INFO: epoch 001:   1471 / 5261 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=100.9, ups=0.33, wpb=303.8, bsz=120, num_updates=1470, lr=4.91724e-05, gnorm=0.786, clip=10, loss_scale=256, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=4453
2022-10-01 06:38:50 - progress_bar.py[line:274] - INFO: epoch 001:   1481 / 5261 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=94.4, ups=0.31, wpb=303.2, bsz=120, num_updates=1480, lr=4.91526e-05, gnorm=0.768, clip=10, loss_scale=256, train_wall=32, gb_free=9.9, ema_decay=0.9999, wall=4485
2022-10-01 06:39:20 - progress_bar.py[line:274] - INFO: epoch 001:   1491 / 5261 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=302.3, nsentences=120, sample_size=302.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.2, ups=0.33, wpb=302.3, bsz=120, num_updates=1490, lr=4.91328e-05, gnorm=0.833, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=4515
2022-10-01 06:39:52 - progress_bar.py[line:274] - INFO: epoch 001:   1501 / 5261 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=94.3, ups=0.31, wpb=302.9, bsz=120, num_updates=1500, lr=4.9113e-05, gnorm=0.778, clip=0, loss_scale=256, train_wall=32, gb_free=10.1, ema_decay=0.9999, wall=4547
2022-10-01 06:40:21 - progress_bar.py[line:274] - INFO: epoch 001:   1511 / 5261 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=304.7, nsentences=120, sample_size=304.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=104.4, ups=0.34, wpb=304.7, bsz=120, num_updates=1510, lr=4.90932e-05, gnorm=0.767, clip=0, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=4576
2022-10-01 06:40:50 - progress_bar.py[line:274] - INFO: epoch 001:   1521 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=103.4, ups=0.34, wpb=303.5, bsz=120, num_updates=1520, lr=4.90734e-05, gnorm=0.761, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=4605
2022-10-01 06:41:20 - progress_bar.py[line:274] - INFO: epoch 001:   1531 / 5261 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=305.2, nsentences=120, sample_size=305.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=104, ups=0.34, wpb=305.2, bsz=120, num_updates=1530, lr=4.90536e-05, gnorm=0.829, clip=10, loss_scale=256, train_wall=29, gb_free=9.9, ema_decay=0.9999, wall=4635
2022-10-01 06:41:50 - progress_bar.py[line:274] - INFO: epoch 001:   1541 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=307.8, nsentences=120, sample_size=307.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.8, ups=0.33, wpb=307.8, bsz=120, num_updates=1540, lr=4.90338e-05, gnorm=0.752, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=4665
2022-10-01 06:42:08 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2022-10-01 06:42:23 - progress_bar.py[line:274] - INFO: epoch 001:   1552 / 5261 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=305.2, nsentences=120, sample_size=305.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=94.9, ups=0.31, wpb=305.2, bsz=120, num_updates=1550, lr=4.9014e-05, gnorm=0.775, clip=10, loss_scale=256, train_wall=32, gb_free=9.7, ema_decay=0.9999, wall=4697
2022-10-01 06:42:52 - progress_bar.py[line:274] - INFO: epoch 001:   1562 / 5261 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.7, ups=0.33, wpb=304.8, bsz=120, num_updates=1560, lr=4.89942e-05, gnorm=0.788, clip=0, loss_scale=256, train_wall=30, gb_free=9.2, ema_decay=0.9999, wall=4727
2022-10-01 06:43:22 - progress_bar.py[line:274] - INFO: epoch 001:   1572 / 5261 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.5, ups=0.34, wpb=302.5, bsz=120, num_updates=1570, lr=4.89744e-05, gnorm=0.873, clip=30, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=4757
2022-10-01 06:43:52 - progress_bar.py[line:274] - INFO: epoch 001:   1582 / 5261 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.9, ups=0.33, wpb=303.2, bsz=120, num_updates=1580, lr=4.89546e-05, gnorm=0.791, clip=10, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=4787
2022-10-01 06:44:22 - progress_bar.py[line:274] - INFO: epoch 001:   1592 / 5261 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.8, ups=0.33, wpb=304.4, bsz=120, num_updates=1590, lr=4.89348e-05, gnorm=0.777, clip=10, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=4817
2022-10-01 06:44:52 - progress_bar.py[line:274] - INFO: epoch 001:   1602 / 5261 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.8, ups=0.33, wpb=302.9, bsz=120, num_updates=1600, lr=4.8915e-05, gnorm=0.882, clip=10, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=4847
2022-10-01 06:45:23 - progress_bar.py[line:274] - INFO: epoch 001:   1612 / 5261 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=97.5, ups=0.32, wpb=302.5, bsz=120, num_updates=1610, lr=4.88952e-05, gnorm=0.992, clip=30, loss_scale=256, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=4878
2022-10-01 06:45:53 - progress_bar.py[line:274] - INFO: epoch 001:   1622 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=301.9, nsentences=120, sample_size=301.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.4, ups=0.33, wpb=301.9, bsz=120, num_updates=1620, lr=4.88754e-05, gnorm=0.886, clip=20, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=4908
2022-10-01 06:46:23 - progress_bar.py[line:274] - INFO: epoch 001:   1632 / 5261 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=301.9, nsentences=120, sample_size=301.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=101.6, ups=0.34, wpb=301.9, bsz=120, num_updates=1630, lr=4.88556e-05, gnorm=0.903, clip=30, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=4938
2022-10-01 06:46:53 - progress_bar.py[line:274] - INFO: epoch 001:   1642 / 5261 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=301.9, nsentences=120, sample_size=301.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.5, ups=0.34, wpb=301.9, bsz=120, num_updates=1640, lr=4.88358e-05, gnorm=0.805, clip=10, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=4968
2022-10-01 06:47:23 - progress_bar.py[line:274] - INFO: epoch 001:   1652 / 5261 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=99.8, ups=0.33, wpb=303.1, bsz=120, num_updates=1650, lr=4.8816e-05, gnorm=0.827, clip=20, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=4998
2022-10-01 06:47:53 - progress_bar.py[line:274] - INFO: epoch 001:   1662 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=306.2, nsentences=120, sample_size=306.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.7, ups=0.33, wpb=306.2, bsz=120, num_updates=1660, lr=4.87962e-05, gnorm=0.807, clip=20, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=5028
2022-10-01 06:48:24 - progress_bar.py[line:274] - INFO: epoch 001:   1672 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=305.1, nsentences=120, sample_size=305.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.3, ups=0.33, wpb=305.1, bsz=120, num_updates=1670, lr=4.87764e-05, gnorm=0.734, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=5059
2022-10-01 06:48:54 - progress_bar.py[line:274] - INFO: epoch 001:   1682 / 5261 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.5, ups=0.33, wpb=304.1, bsz=120, num_updates=1680, lr=4.87566e-05, gnorm=0.721, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=5089
2022-10-01 06:49:23 - progress_bar.py[line:274] - INFO: epoch 001:   1692 / 5261 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=102.3, ups=0.34, wpb=303.6, bsz=120, num_updates=1690, lr=4.87368e-05, gnorm=1.003, clip=20, loss_scale=256, train_wall=30, gb_free=9.5, ema_decay=0.9999, wall=5118
2022-10-01 06:49:53 - progress_bar.py[line:274] - INFO: epoch 001:   1702 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=305.2, nsentences=120, sample_size=305.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.9, ups=0.34, wpb=305.2, bsz=120, num_updates=1700, lr=4.8717e-05, gnorm=0.731, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=5148
2022-10-01 06:50:23 - progress_bar.py[line:274] - INFO: epoch 001:   1712 / 5261 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=301.3, nsentences=120, sample_size=301.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101, ups=0.34, wpb=301.3, bsz=120, num_updates=1710, lr=4.86972e-05, gnorm=0.776, clip=10, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=5178
2022-10-01 06:50:53 - progress_bar.py[line:274] - INFO: epoch 001:   1722 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.7, ups=0.33, wpb=304.2, bsz=120, num_updates=1720, lr=4.86774e-05, gnorm=0.722, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=5208
2022-10-01 06:51:25 - progress_bar.py[line:274] - INFO: epoch 001:   1732 / 5261 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=307.2, nsentences=120, sample_size=307.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96, ups=0.31, wpb=307.2, bsz=120, num_updates=1730, lr=4.86576e-05, gnorm=0.766, clip=10, loss_scale=256, train_wall=32, gb_free=10.1, ema_decay=0.9999, wall=5240
2022-10-01 06:51:55 - progress_bar.py[line:274] - INFO: epoch 001:   1742 / 5261 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=306.3, nsentences=120, sample_size=306.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.3, ups=0.33, wpb=306.3, bsz=120, num_updates=1740, lr=4.86378e-05, gnorm=0.801, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=5270
2022-10-01 06:52:25 - progress_bar.py[line:274] - INFO: epoch 001:   1752 / 5261 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.8, ups=0.33, wpb=304.3, bsz=120, num_updates=1750, lr=4.8618e-05, gnorm=0.71, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=5300
2022-10-01 06:52:55 - progress_bar.py[line:274] - INFO: epoch 001:   1762 / 5261 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=302.3, nsentences=120, sample_size=302.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.6, ups=0.34, wpb=302.3, bsz=120, num_updates=1760, lr=4.85982e-05, gnorm=0.741, clip=0, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=5330
2022-10-01 06:53:25 - progress_bar.py[line:274] - INFO: epoch 001:   1772 / 5261 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.5, ups=0.33, wpb=305.7, bsz=120, num_updates=1770, lr=4.85784e-05, gnorm=0.792, clip=20, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=5360
2022-10-01 06:53:55 - progress_bar.py[line:274] - INFO: epoch 001:   1782 / 5261 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=300.2, nsentences=120, sample_size=300.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.9, ups=0.33, wpb=300.2, bsz=120, num_updates=1780, lr=4.85586e-05, gnorm=0.735, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=5390
2022-10-01 06:54:25 - progress_bar.py[line:274] - INFO: epoch 001:   1792 / 5261 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=302.7, nsentences=120, sample_size=302.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.1, ups=0.33, wpb=302.7, bsz=120, num_updates=1790, lr=4.85388e-05, gnorm=0.728, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=5420
2022-10-01 06:54:55 - progress_bar.py[line:274] - INFO: epoch 001:   1802 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=301.5, nsentences=120, sample_size=301.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.1, ups=0.33, wpb=301.5, bsz=120, num_updates=1800, lr=4.8519e-05, gnorm=0.836, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=5450
2022-10-01 06:55:25 - progress_bar.py[line:274] - INFO: epoch 001:   1812 / 5261 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=301, nsentences=120, sample_size=301, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.1, ups=0.33, wpb=301, bsz=120, num_updates=1810, lr=4.84992e-05, gnorm=0.765, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=5480
2022-10-01 06:55:55 - progress_bar.py[line:274] - INFO: epoch 001:   1822 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.2, ups=0.34, wpb=303.7, bsz=120, num_updates=1820, lr=4.84794e-05, gnorm=0.747, clip=0, loss_scale=256, train_wall=30, gb_free=9.5, ema_decay=0.9999, wall=5510
2022-10-01 06:56:26 - progress_bar.py[line:274] - INFO: epoch 001:   1832 / 5261 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=301.1, nsentences=120, sample_size=301.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.6, ups=0.33, wpb=301.1, bsz=120, num_updates=1830, lr=4.84596e-05, gnorm=0.847, clip=20, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=5540
2022-10-01 06:56:56 - progress_bar.py[line:274] - INFO: epoch 001:   1842 / 5261 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.7, ups=0.33, wpb=305.4, bsz=120, num_updates=1840, lr=4.84398e-05, gnorm=0.774, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=5570
2022-10-01 06:57:26 - progress_bar.py[line:274] - INFO: epoch 001:   1852 / 5261 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.6, ups=0.33, wpb=305, bsz=120, num_updates=1850, lr=4.842e-05, gnorm=0.799, clip=10, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=5600
2022-10-01 06:57:56 - progress_bar.py[line:274] - INFO: epoch 001:   1862 / 5261 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.8, ups=0.33, wpb=303.8, bsz=120, num_updates=1860, lr=4.84002e-05, gnorm=0.764, clip=20, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=5631
2022-10-01 06:58:26 - progress_bar.py[line:274] - INFO: epoch 001:   1872 / 5261 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=301.4, nsentences=120, sample_size=301.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100, ups=0.33, wpb=301.4, bsz=120, num_updates=1870, lr=4.83804e-05, gnorm=0.74, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=5661
2022-10-01 06:58:56 - progress_bar.py[line:274] - INFO: epoch 001:   1882 / 5261 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=301.5, nsentences=120, sample_size=301.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.4, ups=0.33, wpb=301.5, bsz=120, num_updates=1880, lr=4.83606e-05, gnorm=0.732, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=5691
2022-10-01 06:59:27 - progress_bar.py[line:274] - INFO: epoch 001:   1892 / 5261 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.9, ups=0.33, wpb=303.8, bsz=120, num_updates=1890, lr=4.83408e-05, gnorm=0.747, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=5721
2022-10-01 06:59:56 - progress_bar.py[line:274] - INFO: epoch 001:   1902 / 5261 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=306.3, nsentences=120, sample_size=306.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=105, ups=0.34, wpb=306.3, bsz=120, num_updates=1900, lr=4.8321e-05, gnorm=0.704, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=5751
2022-10-01 07:00:26 - progress_bar.py[line:274] - INFO: epoch 001:   1912 / 5261 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.6, ups=0.33, wpb=304.4, bsz=120, num_updates=1910, lr=4.83012e-05, gnorm=0.777, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=5781
2022-10-01 07:00:55 - progress_bar.py[line:274] - INFO: epoch 001:   1922 / 5261 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.4, ups=0.34, wpb=303.2, bsz=120, num_updates=1920, lr=4.82814e-05, gnorm=0.785, clip=10, loss_scale=256, train_wall=30, gb_free=9.5, ema_decay=0.9999, wall=5810
2022-10-01 07:01:25 - progress_bar.py[line:274] - INFO: epoch 001:   1932 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.4, ups=0.34, wpb=304.5, bsz=120, num_updates=1930, lr=4.82616e-05, gnorm=0.763, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=5840
2022-10-01 07:01:54 - progress_bar.py[line:274] - INFO: epoch 001:   1942 / 5261 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=104.2, ups=0.34, wpb=302.4, bsz=120, num_updates=1940, lr=4.82418e-05, gnorm=0.789, clip=10, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=5869
2022-10-01 07:02:26 - progress_bar.py[line:274] - INFO: epoch 001:   1952 / 5261 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=302.3, nsentences=120, sample_size=302.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=95.9, ups=0.32, wpb=302.3, bsz=120, num_updates=1950, lr=4.8222e-05, gnorm=0.806, clip=0, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=5901
2022-10-01 07:02:56 - progress_bar.py[line:274] - INFO: epoch 001:   1962 / 5261 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=301.8, nsentences=120, sample_size=301.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.8, ups=0.33, wpb=301.8, bsz=120, num_updates=1960, lr=4.82022e-05, gnorm=0.856, clip=20, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=5931
2022-10-01 07:03:29 - progress_bar.py[line:274] - INFO: epoch 001:   1972 / 5261 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=92.5, ups=0.31, wpb=303.3, bsz=120, num_updates=1970, lr=4.81824e-05, gnorm=0.805, clip=0, loss_scale=256, train_wall=33, gb_free=9.9, ema_decay=0.9999, wall=5964
2022-10-01 07:03:59 - progress_bar.py[line:274] - INFO: epoch 001:   1982 / 5261 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=305.1, nsentences=120, sample_size=305.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.9, ups=0.33, wpb=305.1, bsz=120, num_updates=1980, lr=4.81626e-05, gnorm=0.822, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=5994
2022-10-01 07:04:29 - progress_bar.py[line:274] - INFO: epoch 001:   1992 / 5261 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.9, ups=0.33, wpb=305.7, bsz=120, num_updates=1990, lr=4.81428e-05, gnorm=0.758, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=6024
2022-10-01 07:04:59 - progress_bar.py[line:274] - INFO: epoch 001:   2002 / 5261 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=306.7, nsentences=120, sample_size=306.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.8, ups=0.34, wpb=306.7, bsz=120, num_updates=2000, lr=4.8123e-05, gnorm=0.802, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=6054
2022-10-01 07:05:29 - progress_bar.py[line:274] - INFO: epoch 001:   2012 / 5261 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.4, ups=0.33, wpb=306, bsz=120, num_updates=2010, lr=4.81032e-05, gnorm=0.833, clip=30, loss_scale=256, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=6084
2022-10-01 07:05:58 - progress_bar.py[line:274] - INFO: epoch 001:   2022 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=103, ups=0.34, wpb=303.4, bsz=120, num_updates=2020, lr=4.80834e-05, gnorm=0.768, clip=10, loss_scale=256, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=6113
2022-10-01 07:06:29 - progress_bar.py[line:274] - INFO: epoch 001:   2032 / 5261 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.6, ups=0.33, wpb=304, bsz=120, num_updates=2030, lr=4.80636e-05, gnorm=0.738, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=6144
2022-10-01 07:07:00 - progress_bar.py[line:274] - INFO: epoch 001:   2042 / 5261 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=300.4, nsentences=120, sample_size=300.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.9, ups=0.33, wpb=300.4, bsz=120, num_updates=2040, lr=4.80438e-05, gnorm=0.944, clip=30, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=6175
2022-10-01 07:07:30 - progress_bar.py[line:274] - INFO: epoch 001:   2052 / 5261 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101, ups=0.33, wpb=303.9, bsz=120, num_updates=2050, lr=4.8024e-05, gnorm=0.798, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=6205
2022-10-01 07:08:00 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2022-10-01 07:08:03 - progress_bar.py[line:274] - INFO: epoch 001:   2063 / 5261 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=92.6, ups=0.3, wpb=305.9, bsz=120, num_updates=2060, lr=4.80042e-05, gnorm=0.719, clip=0, loss_scale=256, train_wall=33, gb_free=10, ema_decay=0.9999, wall=6238
2022-10-01 07:08:33 - progress_bar.py[line:274] - INFO: epoch 001:   2073 / 5261 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.7, ups=0.34, wpb=303.1, bsz=120, num_updates=2070, lr=4.79844e-05, gnorm=0.824, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=6267
2022-10-01 07:09:03 - progress_bar.py[line:274] - INFO: epoch 001:   2083 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.6, ups=0.33, wpb=305.3, bsz=120, num_updates=2080, lr=4.79646e-05, gnorm=0.873, clip=10, loss_scale=256, train_wall=30, gb_free=9.6, ema_decay=0.9999, wall=6298
2022-10-01 07:09:33 - progress_bar.py[line:274] - INFO: epoch 001:   2093 / 5261 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.2, ups=0.33, wpb=304.8, bsz=120, num_updates=2090, lr=4.79448e-05, gnorm=0.747, clip=0, loss_scale=256, train_wall=30, gb_free=9.6, ema_decay=0.9999, wall=6328
2022-10-01 07:10:02 - progress_bar.py[line:274] - INFO: epoch 001:   2103 / 5261 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=301.4, nsentences=120, sample_size=301.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.7, ups=0.34, wpb=301.4, bsz=120, num_updates=2100, lr=4.7925e-05, gnorm=0.785, clip=10, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=6357
2022-10-01 07:10:32 - progress_bar.py[line:274] - INFO: epoch 001:   2113 / 5261 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.7, ups=0.33, wpb=304.8, bsz=120, num_updates=2110, lr=4.79052e-05, gnorm=0.804, clip=10, loss_scale=256, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=6387
2022-10-01 07:11:02 - progress_bar.py[line:274] - INFO: epoch 001:   2123 / 5261 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.5, ups=0.33, wpb=303.4, bsz=120, num_updates=2120, lr=4.78854e-05, gnorm=0.686, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=6417
2022-10-01 07:11:32 - progress_bar.py[line:274] - INFO: epoch 001:   2133 / 5261 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.9, ups=0.34, wpb=302.2, bsz=120, num_updates=2130, lr=4.78656e-05, gnorm=0.745, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=6447
2022-10-01 07:12:02 - progress_bar.py[line:274] - INFO: epoch 001:   2143 / 5261 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.3, ups=0.34, wpb=303.3, bsz=120, num_updates=2140, lr=4.78458e-05, gnorm=0.754, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=6477
2022-10-01 07:12:31 - progress_bar.py[line:274] - INFO: epoch 001:   2153 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=300.7, nsentences=120, sample_size=300.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.2, ups=0.34, wpb=300.7, bsz=120, num_updates=2150, lr=4.7826e-05, gnorm=0.828, clip=20, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=6506
2022-10-01 07:13:01 - progress_bar.py[line:274] - INFO: epoch 001:   2163 / 5261 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.6, ups=0.33, wpb=301.7, bsz=120, num_updates=2160, lr=4.78062e-05, gnorm=0.795, clip=10, loss_scale=256, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=6536
2022-10-01 07:13:31 - progress_bar.py[line:274] - INFO: epoch 001:   2173 / 5261 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=103.4, ups=0.34, wpb=304, bsz=120, num_updates=2170, lr=4.77864e-05, gnorm=0.712, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=6566
2022-10-01 07:14:01 - progress_bar.py[line:274] - INFO: epoch 001:   2183 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.6, ups=0.33, wpb=303.5, bsz=120, num_updates=2180, lr=4.77666e-05, gnorm=0.694, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=6596
2022-10-01 07:14:31 - progress_bar.py[line:274] - INFO: epoch 001:   2193 / 5261 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.3, ups=0.33, wpb=304, bsz=120, num_updates=2190, lr=4.77468e-05, gnorm=0.665, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=6626
2022-10-01 07:15:01 - progress_bar.py[line:274] - INFO: epoch 001:   2203 / 5261 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=302.8, nsentences=120, sample_size=302.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.7, ups=0.33, wpb=302.8, bsz=120, num_updates=2200, lr=4.7727e-05, gnorm=0.74, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=6656
2022-10-01 07:15:32 - progress_bar.py[line:274] - INFO: epoch 001:   2213 / 5261 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=298.6, nsentences=120, sample_size=298.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=98.4, ups=0.33, wpb=298.6, bsz=120, num_updates=2210, lr=4.77072e-05, gnorm=0.807, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=6687
2022-10-01 07:16:02 - progress_bar.py[line:274] - INFO: epoch 001:   2223 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=304.9, nsentences=120, sample_size=304.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.1, ups=0.33, wpb=304.9, bsz=120, num_updates=2220, lr=4.76874e-05, gnorm=0.758, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=6716
2022-10-01 07:16:32 - progress_bar.py[line:274] - INFO: epoch 001:   2233 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.4, ups=0.33, wpb=301.7, bsz=120, num_updates=2230, lr=4.76676e-05, gnorm=0.731, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=6747
2022-10-01 07:17:03 - progress_bar.py[line:274] - INFO: epoch 001:   2243 / 5261 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=304.9, nsentences=120, sample_size=304.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.4, ups=0.33, wpb=304.9, bsz=120, num_updates=2240, lr=4.76478e-05, gnorm=0.679, clip=0, loss_scale=256, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=6777
2022-10-01 07:17:33 - progress_bar.py[line:274] - INFO: epoch 001:   2253 / 5261 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=101, ups=0.33, wpb=305.3, bsz=120, num_updates=2250, lr=4.7628e-05, gnorm=0.801, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=6808
2022-10-01 07:18:02 - progress_bar.py[line:274] - INFO: epoch 001:   2263 / 5261 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.8, ups=0.34, wpb=303.4, bsz=120, num_updates=2260, lr=4.76082e-05, gnorm=0.801, clip=10, loss_scale=256, train_wall=29, gb_free=9.9, ema_decay=0.9999, wall=6837
2022-10-01 07:18:32 - progress_bar.py[line:274] - INFO: epoch 001:   2273 / 5261 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=103.3, ups=0.34, wpb=304.6, bsz=120, num_updates=2270, lr=4.75884e-05, gnorm=0.732, clip=10, loss_scale=256, train_wall=29, gb_free=9.8, ema_decay=0.9999, wall=6867
2022-10-01 07:19:02 - progress_bar.py[line:274] - INFO: epoch 001:   2283 / 5261 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.7, ups=0.33, wpb=304.3, bsz=120, num_updates=2280, lr=4.75686e-05, gnorm=0.764, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=6897
2022-10-01 07:19:33 - progress_bar.py[line:274] - INFO: epoch 001:   2293 / 5261 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.8, ups=0.33, wpb=304.1, bsz=120, num_updates=2290, lr=4.75488e-05, gnorm=0.791, clip=10, loss_scale=256, train_wall=31, gb_free=10.3, ema_decay=0.9999, wall=6927
2022-10-01 07:20:03 - progress_bar.py[line:274] - INFO: epoch 001:   2303 / 5261 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=306.5, nsentences=120, sample_size=306.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.2, ups=0.33, wpb=306.5, bsz=120, num_updates=2300, lr=4.7529e-05, gnorm=0.916, clip=20, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=6958
2022-10-01 07:20:33 - progress_bar.py[line:274] - INFO: epoch 001:   2313 / 5261 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=300.2, nsentences=120, sample_size=300.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.7, ups=0.34, wpb=300.2, bsz=120, num_updates=2310, lr=4.75092e-05, gnorm=0.738, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=6988
2022-10-01 07:21:03 - progress_bar.py[line:274] - INFO: epoch 001:   2323 / 5261 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.7, ups=0.33, wpb=304.1, bsz=120, num_updates=2320, lr=4.74894e-05, gnorm=0.796, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=7018
2022-10-01 07:21:34 - progress_bar.py[line:274] - INFO: epoch 001:   2333 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=301.4, nsentences=120, sample_size=301.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.2, ups=0.33, wpb=301.4, bsz=120, num_updates=2330, lr=4.74696e-05, gnorm=0.714, clip=0, loss_scale=256, train_wall=31, gb_free=9.8, ema_decay=0.9999, wall=7049
2022-10-01 07:22:05 - progress_bar.py[line:274] - INFO: epoch 001:   2343 / 5261 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.5, ups=0.33, wpb=305.3, bsz=120, num_updates=2340, lr=4.74498e-05, gnorm=0.67, clip=0, loss_scale=256, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=7079
2022-10-01 07:22:35 - progress_bar.py[line:274] - INFO: epoch 001:   2353 / 5261 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=306.2, nsentences=120, sample_size=306.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102, ups=0.33, wpb=306.2, bsz=120, num_updates=2350, lr=4.743e-05, gnorm=0.743, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=7109
2022-10-01 07:23:05 - progress_bar.py[line:274] - INFO: epoch 001:   2363 / 5261 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=306.6, nsentences=120, sample_size=306.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.3, ups=0.33, wpb=306.6, bsz=120, num_updates=2360, lr=4.74102e-05, gnorm=0.735, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=7139
2022-10-01 07:23:35 - progress_bar.py[line:274] - INFO: epoch 001:   2373 / 5261 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.4, ups=0.33, wpb=304.4, bsz=120, num_updates=2370, lr=4.73904e-05, gnorm=0.805, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=7169
2022-10-01 07:24:05 - progress_bar.py[line:274] - INFO: epoch 001:   2383 / 5261 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=300.5, nsentences=120, sample_size=300.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.1, ups=0.33, wpb=300.5, bsz=120, num_updates=2380, lr=4.73706e-05, gnorm=0.715, clip=0, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=7200
2022-10-01 07:24:38 - progress_bar.py[line:274] - INFO: epoch 001:   2393 / 5261 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=93.5, ups=0.31, wpb=305.7, bsz=120, num_updates=2390, lr=4.73508e-05, gnorm=0.709, clip=10, loss_scale=256, train_wall=33, gb_free=10.2, ema_decay=0.9999, wall=7233
2022-10-01 07:25:09 - progress_bar.py[line:274] - INFO: epoch 001:   2403 / 5261 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=302.6, nsentences=120, sample_size=302.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97, ups=0.32, wpb=302.6, bsz=120, num_updates=2400, lr=4.7331e-05, gnorm=0.858, clip=10, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=7264
2022-10-01 07:25:39 - progress_bar.py[line:274] - INFO: epoch 001:   2413 / 5261 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.3, ups=0.33, wpb=306, bsz=120, num_updates=2410, lr=4.73112e-05, gnorm=0.684, clip=0, loss_scale=256, train_wall=30, gb_free=9.3, ema_decay=0.9999, wall=7294
2022-10-01 07:26:09 - progress_bar.py[line:274] - INFO: epoch 001:   2423 / 5261 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101, ups=0.33, wpb=302.2, bsz=120, num_updates=2420, lr=4.72914e-05, gnorm=0.656, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=7324
2022-10-01 07:26:40 - progress_bar.py[line:274] - INFO: epoch 001:   2433 / 5261 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.6, ups=0.32, wpb=302.2, bsz=120, num_updates=2430, lr=4.72716e-05, gnorm=0.709, clip=0, loss_scale=256, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=7355
2022-10-01 07:27:10 - progress_bar.py[line:274] - INFO: epoch 001:   2443 / 5261 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=299.9, nsentences=120, sample_size=299.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.4, ups=0.33, wpb=299.9, bsz=120, num_updates=2440, lr=4.72518e-05, gnorm=0.677, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=7385
2022-10-01 07:27:41 - progress_bar.py[line:274] - INFO: epoch 001:   2453 / 5261 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.4, ups=0.32, wpb=303.2, bsz=120, num_updates=2450, lr=4.7232e-05, gnorm=0.639, clip=0, loss_scale=256, train_wall=31, gb_free=9.8, ema_decay=0.9999, wall=7416
2022-10-01 07:28:14 - progress_bar.py[line:274] - INFO: epoch 001:   2463 / 5261 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=95.3, ups=0.31, wpb=305.7, bsz=120, num_updates=2460, lr=4.72122e-05, gnorm=0.705, clip=0, loss_scale=256, train_wall=32, gb_free=10.3, ema_decay=0.9999, wall=7448
2022-10-01 07:28:44 - progress_bar.py[line:274] - INFO: epoch 001:   2473 / 5261 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.2, ups=0.33, wpb=305.5, bsz=120, num_updates=2470, lr=4.71924e-05, gnorm=0.784, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=7479
2022-10-01 07:29:14 - progress_bar.py[line:274] - INFO: epoch 001:   2483 / 5261 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=103, ups=0.34, wpb=304.4, bsz=120, num_updates=2480, lr=4.71726e-05, gnorm=0.728, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=7508
2022-10-01 07:29:44 - progress_bar.py[line:274] - INFO: epoch 001:   2493 / 5261 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.9, ups=0.33, wpb=303.7, bsz=120, num_updates=2490, lr=4.71528e-05, gnorm=0.673, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=7539
2022-10-01 07:30:14 - progress_bar.py[line:274] - INFO: epoch 001:   2503 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.2, ups=0.33, wpb=304.2, bsz=120, num_updates=2500, lr=4.7133e-05, gnorm=0.67, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=7569
2022-10-01 07:30:45 - progress_bar.py[line:274] - INFO: epoch 001:   2513 / 5261 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=301.8, nsentences=120, sample_size=301.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.8, ups=0.32, wpb=301.8, bsz=120, num_updates=2510, lr=4.71132e-05, gnorm=0.676, clip=0, loss_scale=256, train_wall=31, gb_free=9.9, ema_decay=0.9999, wall=7600
2022-10-01 07:31:15 - progress_bar.py[line:274] - INFO: epoch 001:   2523 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=304.7, nsentences=120, sample_size=304.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.7, ups=0.34, wpb=304.7, bsz=120, num_updates=2520, lr=4.70934e-05, gnorm=0.709, clip=10, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=7629
2022-10-01 07:31:45 - progress_bar.py[line:274] - INFO: epoch 001:   2533 / 5261 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.2, ups=0.33, wpb=304.3, bsz=120, num_updates=2530, lr=4.70736e-05, gnorm=0.765, clip=20, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=7660
2022-10-01 07:32:16 - progress_bar.py[line:274] - INFO: epoch 001:   2543 / 5261 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=300.5, nsentences=120, sample_size=300.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=95.3, ups=0.32, wpb=300.5, bsz=120, num_updates=2540, lr=4.70538e-05, gnorm=0.713, clip=0, loss_scale=256, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=7691
2022-10-01 07:32:48 - progress_bar.py[line:274] - INFO: epoch 001:   2553 / 5261 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=308.2, nsentences=120, sample_size=308.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.2, ups=0.32, wpb=308.2, bsz=120, num_updates=2550, lr=4.7034e-05, gnorm=0.734, clip=0, loss_scale=256, train_wall=32, gb_free=9.7, ema_decay=0.9999, wall=7723
2022-10-01 07:33:19 - progress_bar.py[line:274] - INFO: epoch 001:   2563 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.4, ups=0.32, wpb=304.6, bsz=120, num_updates=2560, lr=4.70142e-05, gnorm=0.722, clip=0, loss_scale=256, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=7754
2022-10-01 07:33:49 - progress_bar.py[line:274] - INFO: epoch 001:   2573 / 5261 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.6, ups=0.33, wpb=302.4, bsz=120, num_updates=2570, lr=4.69944e-05, gnorm=0.704, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=7784
2022-10-01 07:34:20 - progress_bar.py[line:274] - INFO: epoch 001:   2583 / 5261 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=304.9, nsentences=120, sample_size=304.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.1, ups=0.33, wpb=304.9, bsz=120, num_updates=2580, lr=4.69746e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=7815
2022-10-01 07:34:50 - progress_bar.py[line:274] - INFO: epoch 001:   2593 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=301.8, nsentences=120, sample_size=301.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.6, ups=0.33, wpb=301.8, bsz=120, num_updates=2590, lr=4.69548e-05, gnorm=0.697, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=7845
2022-10-01 07:35:21 - progress_bar.py[line:274] - INFO: epoch 001:   2603 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=97.4, ups=0.32, wpb=303.9, bsz=120, num_updates=2600, lr=4.6935e-05, gnorm=0.773, clip=0, loss_scale=512, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=7876
2022-10-01 07:35:24 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2022-10-01 07:35:54 - progress_bar.py[line:274] - INFO: epoch 001:   2614 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=306.2, nsentences=120, sample_size=306.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=91.8, ups=0.3, wpb=306.2, bsz=120, num_updates=2610, lr=4.69152e-05, gnorm=0.689, clip=0, loss_scale=256, train_wall=33, gb_free=10.1, ema_decay=0.9999, wall=7909
2022-10-01 07:36:24 - progress_bar.py[line:274] - INFO: epoch 001:   2624 / 5261 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.2, ups=0.34, wpb=303.3, bsz=120, num_updates=2620, lr=4.68954e-05, gnorm=0.722, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=7939
2022-10-01 07:36:54 - progress_bar.py[line:274] - INFO: epoch 001:   2634 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=305.1, nsentences=120, sample_size=305.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.3, ups=0.33, wpb=305.1, bsz=120, num_updates=2630, lr=4.68756e-05, gnorm=0.675, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=7969
2022-10-01 07:37:24 - progress_bar.py[line:274] - INFO: epoch 001:   2644 / 5261 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102, ups=0.33, wpb=305.4, bsz=120, num_updates=2640, lr=4.68558e-05, gnorm=0.672, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=7999
2022-10-01 07:37:53 - progress_bar.py[line:274] - INFO: epoch 001:   2654 / 5261 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=104.1, ups=0.34, wpb=305.3, bsz=120, num_updates=2650, lr=4.6836e-05, gnorm=0.723, clip=0, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=8028
2022-10-01 07:38:23 - progress_bar.py[line:274] - INFO: epoch 001:   2664 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.8, ups=0.34, wpb=303.5, bsz=120, num_updates=2660, lr=4.68162e-05, gnorm=0.792, clip=10, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=8058
2022-10-01 07:38:53 - progress_bar.py[line:274] - INFO: epoch 001:   2674 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=308.2, nsentences=120, sample_size=308.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=104.1, ups=0.34, wpb=308.2, bsz=120, num_updates=2670, lr=4.67964e-05, gnorm=0.694, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=8087
2022-10-01 07:39:22 - progress_bar.py[line:274] - INFO: epoch 001:   2684 / 5261 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=103.8, ups=0.34, wpb=304.8, bsz=120, num_updates=2680, lr=4.67766e-05, gnorm=0.671, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=8117
2022-10-01 07:39:53 - progress_bar.py[line:274] - INFO: epoch 001:   2694 / 5261 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=96.7, ups=0.32, wpb=303.1, bsz=120, num_updates=2690, lr=4.67568e-05, gnorm=0.771, clip=0, loss_scale=256, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=8148
2022-10-01 07:40:23 - progress_bar.py[line:274] - INFO: epoch 001:   2704 / 5261 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.9, ups=0.33, wpb=303.3, bsz=120, num_updates=2700, lr=4.6737e-05, gnorm=0.815, clip=20, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=8178
2022-10-01 07:40:53 - progress_bar.py[line:274] - INFO: epoch 001:   2714 / 5261 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=301.8, nsentences=120, sample_size=301.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.9, ups=0.33, wpb=301.8, bsz=120, num_updates=2710, lr=4.67172e-05, gnorm=0.702, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=8208
2022-10-01 07:41:23 - progress_bar.py[line:274] - INFO: epoch 001:   2724 / 5261 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=306.1, nsentences=120, sample_size=306.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=104.8, ups=0.34, wpb=306.1, bsz=120, num_updates=2720, lr=4.66974e-05, gnorm=0.758, clip=10, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=8237
2022-10-01 07:41:53 - progress_bar.py[line:274] - INFO: epoch 001:   2734 / 5261 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.6, ups=0.33, wpb=302.9, bsz=120, num_updates=2730, lr=4.66776e-05, gnorm=0.747, clip=10, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=8267
2022-10-01 07:42:21 - progress_bar.py[line:274] - INFO: epoch 001:   2744 / 5261 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=105.3, ups=0.35, wpb=303.4, bsz=120, num_updates=2740, lr=4.66578e-05, gnorm=0.769, clip=10, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=8296
2022-10-01 07:42:51 - progress_bar.py[line:274] - INFO: epoch 001:   2754 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=103.6, ups=0.34, wpb=305.3, bsz=120, num_updates=2750, lr=4.6638e-05, gnorm=0.667, clip=0, loss_scale=256, train_wall=29, gb_free=9.6, ema_decay=0.9999, wall=8326
2022-10-01 07:43:21 - progress_bar.py[line:274] - INFO: epoch 001:   2764 / 5261 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=100.9, ups=0.33, wpb=302.9, bsz=120, num_updates=2760, lr=4.66182e-05, gnorm=0.731, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=8356
2022-10-01 07:43:51 - progress_bar.py[line:274] - INFO: epoch 001:   2774 / 5261 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=302.7, nsentences=120, sample_size=302.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.7, ups=0.34, wpb=302.7, bsz=120, num_updates=2770, lr=4.65984e-05, gnorm=0.793, clip=20, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=8386
2022-10-01 07:44:21 - progress_bar.py[line:274] - INFO: epoch 001:   2784 / 5261 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=303, nsentences=120, sample_size=303, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.2, ups=0.33, wpb=303, bsz=120, num_updates=2780, lr=4.65786e-05, gnorm=0.756, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=8415
2022-10-01 07:44:51 - progress_bar.py[line:274] - INFO: epoch 001:   2794 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.6, ups=0.33, wpb=304, bsz=120, num_updates=2790, lr=4.65588e-05, gnorm=0.664, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=8446
2022-10-01 07:45:21 - progress_bar.py[line:274] - INFO: epoch 001:   2804 / 5261 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=301.8, nsentences=120, sample_size=301.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102, ups=0.34, wpb=301.8, bsz=120, num_updates=2800, lr=4.6539e-05, gnorm=0.661, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=8476
2022-10-01 07:45:51 - progress_bar.py[line:274] - INFO: epoch 001:   2814 / 5261 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.3, ups=0.33, wpb=304.2, bsz=120, num_updates=2810, lr=4.65192e-05, gnorm=0.752, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=8506
2022-10-01 07:46:21 - progress_bar.py[line:274] - INFO: epoch 001:   2824 / 5261 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.7, ups=0.33, wpb=303.8, bsz=120, num_updates=2820, lr=4.64994e-05, gnorm=0.644, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=8536
2022-10-01 07:46:51 - progress_bar.py[line:274] - INFO: epoch 001:   2834 / 5261 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=300.8, nsentences=120, sample_size=300.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.7, ups=0.33, wpb=300.8, bsz=120, num_updates=2830, lr=4.64796e-05, gnorm=0.714, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=8566
2022-10-01 07:47:21 - progress_bar.py[line:274] - INFO: epoch 001:   2844 / 5261 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=305.6, nsentences=120, sample_size=305.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.2, ups=0.33, wpb=305.6, bsz=120, num_updates=2840, lr=4.64598e-05, gnorm=0.766, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=8596
2022-10-01 07:47:51 - progress_bar.py[line:274] - INFO: epoch 001:   2854 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=103.2, ups=0.34, wpb=304.8, bsz=120, num_updates=2850, lr=4.644e-05, gnorm=0.717, clip=0, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=8625
2022-10-01 07:48:20 - progress_bar.py[line:274] - INFO: epoch 001:   2864 / 5261 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=301, nsentences=120, sample_size=301, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.9, ups=0.34, wpb=301, bsz=120, num_updates=2860, lr=4.64202e-05, gnorm=0.788, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=8655
2022-10-01 07:48:50 - progress_bar.py[line:274] - INFO: epoch 001:   2874 / 5261 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.4, ups=0.33, wpb=305.9, bsz=120, num_updates=2870, lr=4.64004e-05, gnorm=0.741, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=8685
2022-10-01 07:49:20 - progress_bar.py[line:274] - INFO: epoch 001:   2884 / 5261 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.7, ups=0.34, wpb=305.5, bsz=120, num_updates=2880, lr=4.63806e-05, gnorm=0.692, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=8715
2022-10-01 07:49:50 - progress_bar.py[line:274] - INFO: epoch 001:   2894 / 5261 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=302.6, nsentences=120, sample_size=302.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.1, ups=0.34, wpb=302.6, bsz=120, num_updates=2890, lr=4.63608e-05, gnorm=0.706, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=8744
2022-10-01 07:50:20 - progress_bar.py[line:274] - INFO: epoch 001:   2904 / 5261 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=307.9, nsentences=120, sample_size=307.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.3, ups=0.33, wpb=307.9, bsz=120, num_updates=2900, lr=4.6341e-05, gnorm=0.696, clip=0, loss_scale=256, train_wall=31, gb_free=9.8, ema_decay=0.9999, wall=8775
2022-10-01 07:50:50 - progress_bar.py[line:274] - INFO: epoch 001:   2914 / 5261 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=307.1, nsentences=120, sample_size=307.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=104.2, ups=0.34, wpb=307.1, bsz=120, num_updates=2910, lr=4.63212e-05, gnorm=0.7, clip=0, loss_scale=256, train_wall=29, gb_free=9.1, ema_decay=0.9999, wall=8805
2022-10-01 07:51:20 - progress_bar.py[line:274] - INFO: epoch 001:   2924 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=301, nsentences=120, sample_size=301, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.1, ups=0.33, wpb=301, bsz=120, num_updates=2920, lr=4.63014e-05, gnorm=0.691, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=8835
2022-10-01 07:51:50 - progress_bar.py[line:274] - INFO: epoch 001:   2934 / 5261 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.5, ups=0.34, wpb=302.2, bsz=120, num_updates=2930, lr=4.62816e-05, gnorm=0.74, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=8865
2022-10-01 07:52:20 - progress_bar.py[line:274] - INFO: epoch 001:   2944 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.8, ups=0.34, wpb=304.4, bsz=120, num_updates=2940, lr=4.62618e-05, gnorm=0.687, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=8894
2022-10-01 07:52:49 - progress_bar.py[line:274] - INFO: epoch 001:   2954 / 5261 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=304.9, nsentences=120, sample_size=304.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102, ups=0.33, wpb=304.9, bsz=120, num_updates=2950, lr=4.6242e-05, gnorm=0.627, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=8924
2022-10-01 07:53:20 - progress_bar.py[line:274] - INFO: epoch 001:   2964 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.6, ups=0.32, wpb=303.9, bsz=120, num_updates=2960, lr=4.62222e-05, gnorm=0.714, clip=0, loss_scale=256, train_wall=31, gb_free=9.3, ema_decay=0.9999, wall=8955
2022-10-01 07:53:51 - progress_bar.py[line:274] - INFO: epoch 001:   2974 / 5261 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.9, ups=0.32, wpb=304.6, bsz=120, num_updates=2970, lr=4.62024e-05, gnorm=0.64, clip=0, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=8986
2022-10-01 07:54:22 - progress_bar.py[line:274] - INFO: epoch 001:   2984 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.8, ups=0.33, wpb=306, bsz=120, num_updates=2980, lr=4.61826e-05, gnorm=0.639, clip=0, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=9017
2022-10-01 07:54:51 - progress_bar.py[line:274] - INFO: epoch 001:   2994 / 5261 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=302.6, nsentences=120, sample_size=302.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=103, ups=0.34, wpb=302.6, bsz=120, num_updates=2990, lr=4.61628e-05, gnorm=0.693, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=9046
2022-10-01 07:55:21 - progress_bar.py[line:274] - INFO: epoch 001:   3004 / 5261 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=306.6, nsentences=120, sample_size=306.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=103.6, ups=0.34, wpb=306.6, bsz=120, num_updates=3000, lr=4.6143e-05, gnorm=0.726, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=9076
2022-10-01 07:55:50 - progress_bar.py[line:274] - INFO: epoch 001:   3014 / 5261 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=105.5, ups=0.35, wpb=303.9, bsz=120, num_updates=3010, lr=4.61232e-05, gnorm=0.724, clip=10, loss_scale=256, train_wall=29, gb_free=9.8, ema_decay=0.9999, wall=9104
2022-10-01 07:56:19 - progress_bar.py[line:274] - INFO: epoch 001:   3024 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.2, ups=0.33, wpb=302.4, bsz=120, num_updates=3020, lr=4.61034e-05, gnorm=0.712, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=9134
2022-10-01 07:56:50 - progress_bar.py[line:274] - INFO: epoch 001:   3034 / 5261 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=301.2, nsentences=120, sample_size=301.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=98.2, ups=0.33, wpb=301.2, bsz=120, num_updates=3030, lr=4.60836e-05, gnorm=0.767, clip=10, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=9165
2022-10-01 07:57:22 - progress_bar.py[line:274] - INFO: epoch 001:   3044 / 5261 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=95.3, ups=0.31, wpb=304.2, bsz=120, num_updates=3040, lr=4.60638e-05, gnorm=0.683, clip=0, loss_scale=256, train_wall=32, gb_free=10.3, ema_decay=0.9999, wall=9197
2022-10-01 07:57:52 - progress_bar.py[line:274] - INFO: epoch 001:   3054 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.5, ups=0.33, wpb=303.9, bsz=120, num_updates=3050, lr=4.6044e-05, gnorm=0.733, clip=10, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=9227
2022-10-01 07:58:22 - progress_bar.py[line:274] - INFO: epoch 001:   3064 / 5261 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=307.7, nsentences=120, sample_size=307.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=103.7, ups=0.34, wpb=307.7, bsz=120, num_updates=3060, lr=4.60242e-05, gnorm=0.712, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=9257
2022-10-01 07:58:52 - progress_bar.py[line:274] - INFO: epoch 001:   3074 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.2, ups=0.33, wpb=305.9, bsz=120, num_updates=3070, lr=4.60044e-05, gnorm=0.767, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=9287
2022-10-01 07:59:22 - progress_bar.py[line:274] - INFO: epoch 001:   3084 / 5261 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=301.5, nsentences=120, sample_size=301.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=101.4, ups=0.34, wpb=301.5, bsz=120, num_updates=3080, lr=4.59846e-05, gnorm=0.784, clip=10, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=9317
2022-10-01 07:59:52 - progress_bar.py[line:274] - INFO: epoch 001:   3094 / 5261 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=300.7, nsentences=120, sample_size=300.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=99.2, ups=0.33, wpb=300.7, bsz=120, num_updates=3090, lr=4.59648e-05, gnorm=0.711, clip=0, loss_scale=256, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=9347
2022-10-01 08:00:22 - progress_bar.py[line:274] - INFO: epoch 001:   3104 / 5261 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=307.9, nsentences=120, sample_size=307.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.9, ups=0.33, wpb=307.9, bsz=120, num_updates=3100, lr=4.5945e-05, gnorm=0.702, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=9377
2022-10-01 08:00:52 - progress_bar.py[line:274] - INFO: epoch 001:   3114 / 5261 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=103.5, ups=0.34, wpb=305.9, bsz=120, num_updates=3110, lr=4.59252e-05, gnorm=0.7, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=9407
2022-10-01 08:01:22 - progress_bar.py[line:274] - INFO: epoch 001:   3124 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.6, ups=0.33, wpb=302.4, bsz=120, num_updates=3120, lr=4.59054e-05, gnorm=0.663, clip=10, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=9437
2022-10-01 08:01:25 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2022-10-01 08:01:55 - progress_bar.py[line:274] - INFO: epoch 001:   3135 / 5261 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=92.6, ups=0.3, wpb=304.1, bsz=120, num_updates=3130, lr=4.58856e-05, gnorm=0.641, clip=0, loss_scale=256, train_wall=33, gb_free=9.7, ema_decay=0.9999, wall=9470
2022-10-01 08:02:25 - progress_bar.py[line:274] - INFO: epoch 001:   3145 / 5261 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.6, ups=0.33, wpb=304.8, bsz=120, num_updates=3140, lr=4.58658e-05, gnorm=0.715, clip=0, loss_scale=256, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=9500
2022-10-01 08:02:55 - progress_bar.py[line:274] - INFO: epoch 001:   3155 / 5261 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=301, nsentences=120, sample_size=301, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.7, ups=0.33, wpb=301, bsz=120, num_updates=3150, lr=4.5846e-05, gnorm=0.759, clip=10, loss_scale=256, train_wall=30, gb_free=9.5, ema_decay=0.9999, wall=9530
2022-10-01 08:03:25 - progress_bar.py[line:274] - INFO: epoch 001:   3165 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.4, ups=0.33, wpb=303.6, bsz=120, num_updates=3160, lr=4.58262e-05, gnorm=0.683, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=9560
2022-10-01 08:03:57 - progress_bar.py[line:274] - INFO: epoch 001:   3175 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=96.4, ups=0.32, wpb=305.4, bsz=120, num_updates=3170, lr=4.58064e-05, gnorm=0.735, clip=0, loss_scale=256, train_wall=32, gb_free=9.9, ema_decay=0.9999, wall=9592
2022-10-01 08:04:27 - progress_bar.py[line:274] - INFO: epoch 001:   3185 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=304.9, nsentences=120, sample_size=304.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.8, ups=0.33, wpb=304.9, bsz=120, num_updates=3180, lr=4.57866e-05, gnorm=0.67, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=9622
2022-10-01 08:04:57 - progress_bar.py[line:274] - INFO: epoch 001:   3195 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.2, ups=0.33, wpb=305.9, bsz=120, num_updates=3190, lr=4.57668e-05, gnorm=0.663, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=9652
2022-10-01 08:05:28 - progress_bar.py[line:274] - INFO: epoch 001:   3205 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.9, ups=0.33, wpb=304.3, bsz=120, num_updates=3200, lr=4.5747e-05, gnorm=0.704, clip=0, loss_scale=256, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=9682
2022-10-01 08:05:58 - progress_bar.py[line:274] - INFO: epoch 001:   3215 / 5261 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101, ups=0.33, wpb=303.3, bsz=120, num_updates=3210, lr=4.57272e-05, gnorm=0.626, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=9713
2022-10-01 08:06:28 - progress_bar.py[line:274] - INFO: epoch 001:   3225 / 5261 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=301.5, nsentences=120, sample_size=301.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.7, ups=0.33, wpb=301.5, bsz=120, num_updates=3220, lr=4.57074e-05, gnorm=0.712, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=9742
2022-10-01 08:06:57 - progress_bar.py[line:274] - INFO: epoch 001:   3235 / 5261 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=102.6, ups=0.34, wpb=302.9, bsz=120, num_updates=3230, lr=4.56876e-05, gnorm=0.795, clip=10, loss_scale=256, train_wall=29, gb_free=9.9, ema_decay=0.9999, wall=9772
2022-10-01 08:07:28 - progress_bar.py[line:274] - INFO: epoch 001:   3245 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.6, ups=0.32, wpb=304.6, bsz=120, num_updates=3240, lr=4.56678e-05, gnorm=0.692, clip=10, loss_scale=256, train_wall=31, gb_free=9.9, ema_decay=0.9999, wall=9803
2022-10-01 08:07:57 - progress_bar.py[line:274] - INFO: epoch 001:   3255 / 5261 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=301.3, nsentences=120, sample_size=301.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.5, ups=0.34, wpb=301.3, bsz=120, num_updates=3250, lr=4.5648e-05, gnorm=0.662, clip=0, loss_scale=256, train_wall=29, gb_free=10.3, ema_decay=0.9999, wall=9832
2022-10-01 08:08:28 - progress_bar.py[line:274] - INFO: epoch 001:   3265 / 5261 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.4, ups=0.33, wpb=304.2, bsz=120, num_updates=3260, lr=4.56282e-05, gnorm=0.744, clip=10, loss_scale=256, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=9863
2022-10-01 08:08:57 - progress_bar.py[line:274] - INFO: epoch 001:   3275 / 5261 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=300.2, nsentences=120, sample_size=300.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=101.1, ups=0.34, wpb=300.2, bsz=120, num_updates=3270, lr=4.56084e-05, gnorm=0.723, clip=0, loss_scale=256, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=9892
2022-10-01 08:09:27 - progress_bar.py[line:274] - INFO: epoch 001:   3285 / 5261 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101, ups=0.33, wpb=303.3, bsz=120, num_updates=3280, lr=4.55886e-05, gnorm=0.634, clip=0, loss_scale=256, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=9922
2022-10-01 08:09:57 - progress_bar.py[line:274] - INFO: epoch 001:   3295 / 5261 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=306.3, nsentences=120, sample_size=306.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.9, ups=0.34, wpb=306.3, bsz=120, num_updates=3290, lr=4.55688e-05, gnorm=0.691, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=9952
2022-10-01 08:10:27 - progress_bar.py[line:274] - INFO: epoch 001:   3305 / 5261 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=301.4, nsentences=120, sample_size=301.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=100.5, ups=0.33, wpb=301.4, bsz=120, num_updates=3300, lr=4.5549e-05, gnorm=0.712, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=9982
2022-10-01 08:10:56 - progress_bar.py[line:274] - INFO: epoch 001:   3315 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=104.7, ups=0.34, wpb=306, bsz=120, num_updates=3310, lr=4.55292e-05, gnorm=0.628, clip=0, loss_scale=256, train_wall=29, gb_free=9.7, ema_decay=0.9999, wall=10011
2022-10-01 08:11:27 - progress_bar.py[line:274] - INFO: epoch 001:   3325 / 5261 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100, ups=0.33, wpb=302.4, bsz=120, num_updates=3320, lr=4.55094e-05, gnorm=0.699, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=10042
2022-10-01 08:11:57 - progress_bar.py[line:274] - INFO: epoch 001:   3335 / 5261 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.9, ups=0.33, wpb=304.4, bsz=120, num_updates=3330, lr=4.54896e-05, gnorm=0.692, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=10071
2022-10-01 08:12:27 - progress_bar.py[line:274] - INFO: epoch 001:   3345 / 5261 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=306.3, nsentences=120, sample_size=306.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.1, ups=0.33, wpb=306.3, bsz=120, num_updates=3340, lr=4.54698e-05, gnorm=0.64, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=10101
2022-10-01 08:12:56 - progress_bar.py[line:274] - INFO: epoch 001:   3355 / 5261 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.5, ups=0.34, wpb=304.1, bsz=120, num_updates=3350, lr=4.545e-05, gnorm=0.7, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=10131
2022-10-01 08:13:26 - progress_bar.py[line:274] - INFO: epoch 001:   3365 / 5261 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.5, ups=0.33, wpb=304, bsz=120, num_updates=3360, lr=4.54302e-05, gnorm=0.693, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=10161
2022-10-01 08:13:56 - progress_bar.py[line:274] - INFO: epoch 001:   3375 / 5261 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.4, ups=0.34, wpb=304.6, bsz=120, num_updates=3370, lr=4.54104e-05, gnorm=0.719, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=10191
2022-10-01 08:14:26 - progress_bar.py[line:274] - INFO: epoch 001:   3385 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.1, ups=0.34, wpb=304.2, bsz=120, num_updates=3380, lr=4.53906e-05, gnorm=0.763, clip=10, loss_scale=256, train_wall=30, gb_free=9.5, ema_decay=0.9999, wall=10221
2022-10-01 08:14:55 - progress_bar.py[line:274] - INFO: epoch 001:   3395 / 5261 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=302.7, nsentences=120, sample_size=302.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102, ups=0.34, wpb=302.7, bsz=120, num_updates=3390, lr=4.53708e-05, gnorm=0.711, clip=0, loss_scale=256, train_wall=30, gb_free=9.6, ema_decay=0.9999, wall=10250
2022-10-01 08:15:25 - progress_bar.py[line:274] - INFO: epoch 001:   3405 / 5261 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.2, ups=0.33, wpb=305.4, bsz=120, num_updates=3400, lr=4.5351e-05, gnorm=0.661, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=10280
2022-10-01 08:15:56 - progress_bar.py[line:274] - INFO: epoch 001:   3415 / 5261 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.5, ups=0.32, wpb=303.6, bsz=120, num_updates=3410, lr=4.53312e-05, gnorm=0.786, clip=0, loss_scale=256, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=10311
2022-10-01 08:16:28 - progress_bar.py[line:274] - INFO: epoch 001:   3425 / 5261 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=97.3, ups=0.32, wpb=303.7, bsz=120, num_updates=3420, lr=4.53114e-05, gnorm=0.637, clip=0, loss_scale=256, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=10343
2022-10-01 08:16:58 - progress_bar.py[line:274] - INFO: epoch 001:   3435 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.6, ups=0.33, wpb=302.9, bsz=120, num_updates=3430, lr=4.52916e-05, gnorm=0.793, clip=10, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=10373
2022-10-01 08:17:28 - progress_bar.py[line:274] - INFO: epoch 001:   3445 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=305.8, nsentences=120, sample_size=305.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=104.7, ups=0.34, wpb=305.8, bsz=120, num_updates=3440, lr=4.52718e-05, gnorm=0.662, clip=0, loss_scale=256, train_wall=29, gb_free=9.8, ema_decay=0.9999, wall=10403
2022-10-01 08:17:58 - progress_bar.py[line:274] - INFO: epoch 001:   3455 / 5261 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.3, ups=0.33, wpb=305.4, bsz=120, num_updates=3450, lr=4.5252e-05, gnorm=0.696, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=10433
2022-10-01 08:18:29 - progress_bar.py[line:274] - INFO: epoch 001:   3465 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=301.8, nsentences=120, sample_size=301.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=98.6, ups=0.33, wpb=301.8, bsz=120, num_updates=3460, lr=4.52322e-05, gnorm=0.847, clip=20, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=10464
2022-10-01 08:18:59 - progress_bar.py[line:274] - INFO: epoch 001:   3475 / 5261 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.9, ups=0.34, wpb=304, bsz=120, num_updates=3470, lr=4.52125e-05, gnorm=0.683, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=10493
2022-10-01 08:19:29 - progress_bar.py[line:274] - INFO: epoch 001:   3485 / 5261 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=306.1, nsentences=120, sample_size=306.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.5, ups=0.33, wpb=306.1, bsz=120, num_updates=3480, lr=4.51927e-05, gnorm=0.789, clip=0, loss_scale=256, train_wall=30, gb_free=9.6, ema_decay=0.9999, wall=10524
2022-10-01 08:20:01 - progress_bar.py[line:274] - INFO: epoch 001:   3495 / 5261 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=94.7, ups=0.31, wpb=301.7, bsz=120, num_updates=3490, lr=4.51729e-05, gnorm=0.729, clip=0, loss_scale=256, train_wall=32, gb_free=10.2, ema_decay=0.9999, wall=10555
2022-10-01 08:20:30 - progress_bar.py[line:274] - INFO: epoch 001:   3505 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=301, nsentences=120, sample_size=301, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.4, ups=0.34, wpb=301, bsz=120, num_updates=3500, lr=4.51531e-05, gnorm=0.79, clip=20, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=10585
2022-10-01 08:21:00 - progress_bar.py[line:274] - INFO: epoch 001:   3515 / 5261 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.8, ups=0.33, wpb=304.8, bsz=120, num_updates=3510, lr=4.51333e-05, gnorm=0.781, clip=10, loss_scale=256, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=10615
2022-10-01 08:21:30 - progress_bar.py[line:274] - INFO: epoch 001:   3525 / 5261 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=306.3, nsentences=120, sample_size=306.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.8, ups=0.34, wpb=306.3, bsz=120, num_updates=3520, lr=4.51135e-05, gnorm=0.677, clip=0, loss_scale=256, train_wall=30, gb_free=8.9, ema_decay=0.9999, wall=10645
2022-10-01 08:22:00 - progress_bar.py[line:274] - INFO: epoch 001:   3535 / 5261 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.7, ups=0.33, wpb=303.1, bsz=120, num_updates=3530, lr=4.50937e-05, gnorm=0.816, clip=20, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=10675
2022-10-01 08:22:30 - progress_bar.py[line:274] - INFO: epoch 001:   3545 / 5261 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=101.8, ups=0.34, wpb=303.4, bsz=120, num_updates=3540, lr=4.50739e-05, gnorm=0.695, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=10705
2022-10-01 08:22:59 - progress_bar.py[line:274] - INFO: epoch 001:   3555 / 5261 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=103.3, ups=0.34, wpb=305.3, bsz=120, num_updates=3550, lr=4.50541e-05, gnorm=0.68, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=10734
2022-10-01 08:23:29 - progress_bar.py[line:274] - INFO: epoch 001:   3565 / 5261 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.7, ups=0.34, wpb=304.6, bsz=120, num_updates=3560, lr=4.50343e-05, gnorm=0.715, clip=10, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=10764
2022-10-01 08:23:59 - progress_bar.py[line:274] - INFO: epoch 001:   3575 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=306.3, nsentences=120, sample_size=306.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.7, ups=0.33, wpb=306.3, bsz=120, num_updates=3570, lr=4.50145e-05, gnorm=0.725, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=10794
2022-10-01 08:24:29 - progress_bar.py[line:274] - INFO: epoch 001:   3585 / 5261 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=302.7, nsentences=120, sample_size=302.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.9, ups=0.33, wpb=302.7, bsz=120, num_updates=3580, lr=4.49947e-05, gnorm=0.68, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=10824
2022-10-01 08:24:59 - progress_bar.py[line:274] - INFO: epoch 001:   3595 / 5261 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=103, ups=0.34, wpb=302.4, bsz=120, num_updates=3590, lr=4.49749e-05, gnorm=0.635, clip=0, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=10854
2022-10-01 08:25:28 - progress_bar.py[line:274] - INFO: epoch 001:   3605 / 5261 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.8, ups=0.34, wpb=303.8, bsz=120, num_updates=3600, lr=4.49551e-05, gnorm=0.647, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=10883
2022-10-01 08:25:58 - progress_bar.py[line:274] - INFO: epoch 001:   3615 / 5261 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=302.6, nsentences=120, sample_size=302.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.6, ups=0.34, wpb=302.6, bsz=120, num_updates=3610, lr=4.49353e-05, gnorm=0.614, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=10913
2022-10-01 08:26:28 - progress_bar.py[line:274] - INFO: epoch 001:   3625 / 5261 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.1, ups=0.34, wpb=303.1, bsz=120, num_updates=3620, lr=4.49155e-05, gnorm=0.674, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=10943
2022-10-01 08:26:58 - progress_bar.py[line:274] - INFO: epoch 001:   3635 / 5261 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.1, ups=0.33, wpb=305.3, bsz=120, num_updates=3630, lr=4.48957e-05, gnorm=0.694, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=10973
2022-10-01 08:27:28 - progress_bar.py[line:274] - INFO: epoch 001:   3645 / 5261 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.6, ups=0.33, wpb=305.5, bsz=120, num_updates=3640, lr=4.48759e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=11003
2022-10-01 08:27:58 - progress_bar.py[line:274] - INFO: epoch 001:   3655 / 5261 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=306.3, nsentences=120, sample_size=306.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=103, ups=0.34, wpb=306.3, bsz=120, num_updates=3650, lr=4.48561e-05, gnorm=0.684, clip=10, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11032
2022-10-01 08:28:27 - progress_bar.py[line:274] - INFO: epoch 001:   3665 / 5261 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=306.2, nsentences=120, sample_size=306.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.3, ups=0.33, wpb=306.2, bsz=120, num_updates=3660, lr=4.48363e-05, gnorm=0.683, clip=0, loss_scale=512, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=11062
2022-10-01 08:28:58 - progress_bar.py[line:274] - INFO: epoch 001:   3675 / 5261 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.3, ups=0.33, wpb=305.5, bsz=120, num_updates=3670, lr=4.48165e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11092
2022-10-01 08:29:27 - progress_bar.py[line:274] - INFO: epoch 001:   3685 / 5261 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=103, ups=0.34, wpb=305, bsz=120, num_updates=3680, lr=4.47967e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11122
2022-10-01 08:29:57 - progress_bar.py[line:274] - INFO: epoch 001:   3695 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=307.2, nsentences=120, sample_size=307.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103, ups=0.34, wpb=307.2, bsz=120, num_updates=3690, lr=4.47769e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=11152
2022-10-01 08:30:27 - progress_bar.py[line:274] - INFO: epoch 001:   3705 / 5261 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.2, ups=0.33, wpb=304.3, bsz=120, num_updates=3700, lr=4.47571e-05, gnorm=0.712, clip=10, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11182
2022-10-01 08:30:57 - progress_bar.py[line:274] - INFO: epoch 001:   3715 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.8, ups=0.34, wpb=305.7, bsz=120, num_updates=3710, lr=4.47373e-05, gnorm=0.758, clip=10, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=11212
2022-10-01 08:31:26 - progress_bar.py[line:274] - INFO: epoch 001:   3725 / 5261 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=300, nsentences=120, sample_size=300, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=103, ups=0.34, wpb=300, bsz=120, num_updates=3720, lr=4.47175e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=11241
2022-10-01 08:31:56 - progress_bar.py[line:274] - INFO: epoch 001:   3735 / 5261 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=99.9, ups=0.33, wpb=302.5, bsz=120, num_updates=3730, lr=4.46977e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11271
2022-10-01 08:32:27 - progress_bar.py[line:274] - INFO: epoch 001:   3745 / 5261 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=306.1, nsentences=120, sample_size=306.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.5, ups=0.33, wpb=306.1, bsz=120, num_updates=3740, lr=4.46779e-05, gnorm=0.658, clip=10, loss_scale=512, train_wall=30, gb_free=9.5, ema_decay=0.9999, wall=11301
2022-10-01 08:32:57 - progress_bar.py[line:274] - INFO: epoch 001:   3755 / 5261 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=306.8, nsentences=120, sample_size=306.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.9, ups=0.33, wpb=306.8, bsz=120, num_updates=3750, lr=4.46581e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11332
2022-10-01 08:33:27 - progress_bar.py[line:274] - INFO: epoch 001:   3765 / 5261 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.8, ups=0.33, wpb=303.6, bsz=120, num_updates=3760, lr=4.46383e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=11362
2022-10-01 08:33:57 - progress_bar.py[line:274] - INFO: epoch 001:   3775 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=305.1, nsentences=120, sample_size=305.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.8, ups=0.33, wpb=305.1, bsz=120, num_updates=3770, lr=4.46185e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=11392
2022-10-01 08:34:27 - progress_bar.py[line:274] - INFO: epoch 001:   3785 / 5261 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.8, ups=0.33, wpb=305.9, bsz=120, num_updates=3780, lr=4.45987e-05, gnorm=0.727, clip=10, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=11422
2022-10-01 08:34:57 - progress_bar.py[line:274] - INFO: epoch 001:   3795 / 5261 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=308.6, nsentences=120, sample_size=308.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=102.9, ups=0.33, wpb=308.6, bsz=120, num_updates=3790, lr=4.45789e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11452
2022-10-01 08:35:26 - progress_bar.py[line:274] - INFO: epoch 001:   3805 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103.6, ups=0.34, wpb=304.2, bsz=120, num_updates=3800, lr=4.45591e-05, gnorm=0.728, clip=0, loss_scale=512, train_wall=29, gb_free=10, ema_decay=0.9999, wall=11481
2022-10-01 08:35:57 - progress_bar.py[line:274] - INFO: epoch 001:   3815 / 5261 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.8, ups=0.33, wpb=303.8, bsz=120, num_updates=3810, lr=4.45393e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=11512
2022-10-01 08:36:27 - progress_bar.py[line:274] - INFO: epoch 001:   3825 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.2, ups=0.33, wpb=302.9, bsz=120, num_updates=3820, lr=4.45195e-05, gnorm=0.66, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=11542
2022-10-01 08:36:56 - progress_bar.py[line:274] - INFO: epoch 001:   3835 / 5261 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.3, ups=0.34, wpb=303.1, bsz=120, num_updates=3830, lr=4.44997e-05, gnorm=0.714, clip=0, loss_scale=512, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=11571
2022-10-01 08:37:27 - progress_bar.py[line:274] - INFO: epoch 001:   3845 / 5261 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=301.3, nsentences=120, sample_size=301.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.4, ups=0.33, wpb=301.3, bsz=120, num_updates=3840, lr=4.44799e-05, gnorm=0.714, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11601
2022-10-01 08:37:56 - progress_bar.py[line:274] - INFO: epoch 001:   3855 / 5261 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=306.6, nsentences=120, sample_size=306.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=103, ups=0.34, wpb=306.6, bsz=120, num_updates=3850, lr=4.44601e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=11631
2022-10-01 08:38:27 - progress_bar.py[line:274] - INFO: epoch 001:   3865 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.3, ups=0.33, wpb=305.7, bsz=120, num_updates=3860, lr=4.44403e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11662
2022-10-01 08:38:58 - progress_bar.py[line:274] - INFO: epoch 001:   3875 / 5261 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=307, nsentences=120, sample_size=307, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99, ups=0.32, wpb=307, bsz=120, num_updates=3870, lr=4.44205e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=31, gb_free=9.9, ema_decay=0.9999, wall=11693
2022-10-01 08:39:30 - progress_bar.py[line:274] - INFO: epoch 001:   3885 / 5261 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=93.5, ups=0.31, wpb=303.7, bsz=120, num_updates=3880, lr=4.44007e-05, gnorm=0.73, clip=10, loss_scale=512, train_wall=32, gb_free=9.8, ema_decay=0.9999, wall=11725
2022-10-01 08:40:00 - progress_bar.py[line:274] - INFO: epoch 001:   3895 / 5261 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=302, nsentences=120, sample_size=302, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.4, ups=0.34, wpb=302, bsz=120, num_updates=3890, lr=4.43809e-05, gnorm=0.602, clip=0, loss_scale=512, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=11755
2022-10-01 08:40:30 - progress_bar.py[line:274] - INFO: epoch 001:   3905 / 5261 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99, ups=0.33, wpb=302.5, bsz=120, num_updates=3900, lr=4.43611e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=11785
2022-10-01 08:41:00 - progress_bar.py[line:274] - INFO: epoch 001:   3915 / 5261 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.6, ups=0.34, wpb=303.1, bsz=120, num_updates=3910, lr=4.43413e-05, gnorm=0.689, clip=10, loss_scale=512, train_wall=29, gb_free=9.8, ema_decay=0.9999, wall=11815
2022-10-01 08:41:30 - progress_bar.py[line:274] - INFO: epoch 001:   3925 / 5261 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=308.9, nsentences=120, sample_size=308.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.8, ups=0.33, wpb=308.9, bsz=120, num_updates=3920, lr=4.43215e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11845
2022-10-01 08:42:00 - progress_bar.py[line:274] - INFO: epoch 001:   3935 / 5261 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=305.2, nsentences=120, sample_size=305.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.5, ups=0.33, wpb=305.2, bsz=120, num_updates=3930, lr=4.43017e-05, gnorm=0.659, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=11875
2022-10-01 08:42:32 - progress_bar.py[line:274] - INFO: epoch 001:   3945 / 5261 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97.7, ups=0.32, wpb=305.5, bsz=120, num_updates=3940, lr=4.42819e-05, gnorm=0.635, clip=0, loss_scale=512, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=11906
2022-10-01 08:43:02 - progress_bar.py[line:274] - INFO: epoch 001:   3955 / 5261 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.1, ups=0.33, wpb=305.7, bsz=120, num_updates=3950, lr=4.42621e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=11937
2022-10-01 08:43:32 - progress_bar.py[line:274] - INFO: epoch 001:   3965 / 5261 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=301.6, nsentences=120, sample_size=301.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.2, ups=0.33, wpb=301.6, bsz=120, num_updates=3960, lr=4.42423e-05, gnorm=0.695, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11967
2022-10-01 08:44:02 - progress_bar.py[line:274] - INFO: epoch 001:   3975 / 5261 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.1, ups=0.34, wpb=304.6, bsz=120, num_updates=3970, lr=4.42225e-05, gnorm=0.767, clip=10, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=11997
2022-10-01 08:44:32 - progress_bar.py[line:274] - INFO: epoch 001:   3985 / 5261 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.1, ups=0.33, wpb=305.5, bsz=120, num_updates=3980, lr=4.42027e-05, gnorm=0.683, clip=0, loss_scale=512, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=12027
2022-10-01 08:45:01 - progress_bar.py[line:274] - INFO: epoch 001:   3995 / 5261 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=308.1, nsentences=120, sample_size=308.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=104.8, ups=0.34, wpb=308.1, bsz=120, num_updates=3990, lr=4.41829e-05, gnorm=0.709, clip=10, loss_scale=512, train_wall=29, gb_free=9.5, ema_decay=0.9999, wall=12056
2022-10-01 08:45:31 - progress_bar.py[line:274] - INFO: epoch 001:   4005 / 5261 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=306.2, nsentences=120, sample_size=306.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.9, ups=0.34, wpb=306.2, bsz=120, num_updates=4000, lr=4.41631e-05, gnorm=0.663, clip=10, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=12086
2022-10-01 08:46:01 - progress_bar.py[line:274] - INFO: epoch 001:   4015 / 5261 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=306.4, nsentences=120, sample_size=306.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.5, ups=0.33, wpb=306.4, bsz=120, num_updates=4010, lr=4.41433e-05, gnorm=0.743, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=12116
2022-10-01 08:46:31 - progress_bar.py[line:274] - INFO: epoch 001:   4025 / 5261 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=306.5, nsentences=120, sample_size=306.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=103.1, ups=0.34, wpb=306.5, bsz=120, num_updates=4020, lr=4.41235e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=12146
2022-10-01 08:47:01 - progress_bar.py[line:274] - INFO: epoch 001:   4035 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=306.7, nsentences=120, sample_size=306.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.3, ups=0.33, wpb=306.7, bsz=120, num_updates=4030, lr=4.41037e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=12176
2022-10-01 08:47:32 - progress_bar.py[line:274] - INFO: epoch 001:   4045 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=96.3, ups=0.32, wpb=305.3, bsz=120, num_updates=4040, lr=4.40839e-05, gnorm=0.68, clip=0, loss_scale=512, train_wall=32, gb_free=10, ema_decay=0.9999, wall=12207
2022-10-01 08:48:03 - progress_bar.py[line:274] - INFO: epoch 001:   4055 / 5261 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=306.2, nsentences=120, sample_size=306.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.5, ups=0.33, wpb=306.2, bsz=120, num_updates=4050, lr=4.40641e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=12238
2022-10-01 08:48:33 - progress_bar.py[line:274] - INFO: epoch 001:   4065 / 5261 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=307.9, nsentences=120, sample_size=307.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.3, ups=0.33, wpb=307.9, bsz=120, num_updates=4060, lr=4.40443e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=12268
2022-10-01 08:49:03 - progress_bar.py[line:274] - INFO: epoch 001:   4075 / 5261 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=301.6, nsentences=120, sample_size=301.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.3, ups=0.34, wpb=301.6, bsz=120, num_updates=4070, lr=4.40245e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=12297
2022-10-01 08:49:32 - progress_bar.py[line:274] - INFO: epoch 001:   4085 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.2, ups=0.34, wpb=302.4, bsz=120, num_updates=4080, lr=4.40047e-05, gnorm=0.805, clip=30, loss_scale=512, train_wall=30, gb_free=10.4, ema_decay=0.9999, wall=12327
2022-10-01 08:50:02 - progress_bar.py[line:274] - INFO: epoch 001:   4095 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.4, ups=0.33, wpb=303.9, bsz=120, num_updates=4090, lr=4.39849e-05, gnorm=0.695, clip=10, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=12357
2022-10-01 08:50:32 - progress_bar.py[line:274] - INFO: epoch 001:   4105 / 5261 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.1, ups=0.34, wpb=304.6, bsz=120, num_updates=4100, lr=4.39651e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=12387
2022-10-01 08:51:02 - progress_bar.py[line:274] - INFO: epoch 001:   4115 / 5261 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.9, ups=0.33, wpb=304.3, bsz=120, num_updates=4110, lr=4.39453e-05, gnorm=0.66, clip=0, loss_scale=512, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=12417
2022-10-01 08:51:32 - progress_bar.py[line:274] - INFO: epoch 001:   4125 / 5261 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.2, ups=0.33, wpb=302.9, bsz=120, num_updates=4120, lr=4.39255e-05, gnorm=0.637, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=12447
2022-10-01 08:52:02 - progress_bar.py[line:274] - INFO: epoch 001:   4135 / 5261 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.3, ups=0.34, wpb=304.5, bsz=120, num_updates=4130, lr=4.39057e-05, gnorm=0.728, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=12477
2022-10-01 08:52:32 - progress_bar.py[line:274] - INFO: epoch 001:   4145 / 5261 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=301.1, nsentences=120, sample_size=301.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99, ups=0.33, wpb=301.1, bsz=120, num_updates=4140, lr=4.38859e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=12507
2022-10-01 08:52:47 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2022-10-01 08:53:05 - progress_bar.py[line:274] - INFO: epoch 001:   4156 / 5261 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=302.7, nsentences=120, sample_size=302.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=93.4, ups=0.31, wpb=302.7, bsz=120, num_updates=4150, lr=4.38661e-05, gnorm=0.707, clip=10, loss_scale=512, train_wall=32, gb_free=10.2, ema_decay=0.9999, wall=12539
2022-10-01 08:53:34 - progress_bar.py[line:274] - INFO: epoch 001:   4166 / 5261 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=103.4, ups=0.34, wpb=304.8, bsz=120, num_updates=4160, lr=4.38463e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=12569
2022-10-01 08:54:04 - progress_bar.py[line:274] - INFO: epoch 001:   4176 / 5261 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.3, ups=0.33, wpb=303.5, bsz=120, num_updates=4170, lr=4.38265e-05, gnorm=0.74, clip=10, loss_scale=512, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=12599
2022-10-01 08:54:37 - progress_bar.py[line:274] - INFO: epoch 001:   4186 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=93.8, ups=0.31, wpb=303.5, bsz=120, num_updates=4180, lr=4.38067e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=32, gb_free=9.5, ema_decay=0.9999, wall=12632
2022-10-01 08:55:06 - progress_bar.py[line:274] - INFO: epoch 001:   4196 / 5261 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.8, ups=0.34, wpb=305, bsz=120, num_updates=4190, lr=4.37869e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=30, gb_free=9.5, ema_decay=0.9999, wall=12661
2022-10-01 08:55:36 - progress_bar.py[line:274] - INFO: epoch 001:   4206 / 5261 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.5, ups=0.33, wpb=304.3, bsz=120, num_updates=4200, lr=4.37671e-05, gnorm=0.674, clip=10, loss_scale=512, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=12691
2022-10-01 08:56:07 - progress_bar.py[line:274] - INFO: epoch 001:   4216 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.4, ups=0.33, wpb=304.5, bsz=120, num_updates=4210, lr=4.37473e-05, gnorm=0.702, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=12722
2022-10-01 08:56:38 - progress_bar.py[line:274] - INFO: epoch 001:   4226 / 5261 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=304.9, nsentences=120, sample_size=304.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.1, ups=0.32, wpb=304.9, bsz=120, num_updates=4220, lr=4.37275e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=12752
2022-10-01 08:57:08 - progress_bar.py[line:274] - INFO: epoch 001:   4236 / 5261 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=307.1, nsentences=120, sample_size=307.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.1, ups=0.33, wpb=307.1, bsz=120, num_updates=4230, lr=4.37077e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=12782
2022-10-01 08:57:37 - progress_bar.py[line:274] - INFO: epoch 001:   4246 / 5261 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=103.2, ups=0.34, wpb=304, bsz=120, num_updates=4240, lr=4.36879e-05, gnorm=0.607, clip=0, loss_scale=512, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=12812
2022-10-01 08:58:07 - progress_bar.py[line:274] - INFO: epoch 001:   4256 / 5261 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.4, ups=0.33, wpb=304.1, bsz=120, num_updates=4250, lr=4.36681e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=12842
2022-10-01 08:58:37 - progress_bar.py[line:274] - INFO: epoch 001:   4266 / 5261 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.4, ups=0.33, wpb=303.5, bsz=120, num_updates=4260, lr=4.36483e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=12872
2022-10-01 08:59:07 - progress_bar.py[line:274] - INFO: epoch 001:   4276 / 5261 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.5, ups=0.33, wpb=303.7, bsz=120, num_updates=4270, lr=4.36285e-05, gnorm=0.736, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=12902
2022-10-01 08:59:38 - progress_bar.py[line:274] - INFO: epoch 001:   4286 / 5261 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.1, ups=0.33, wpb=302.9, bsz=120, num_updates=4280, lr=4.36087e-05, gnorm=0.671, clip=0, loss_scale=512, train_wall=31, gb_free=9.7, ema_decay=0.9999, wall=12933
2022-10-01 09:00:04 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2022-10-01 09:00:10 - progress_bar.py[line:274] - INFO: epoch 001:   4297 / 5261 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=92.8, ups=0.31, wpb=303.6, bsz=120, num_updates=4290, lr=4.35889e-05, gnorm=0.639, clip=0, loss_scale=256, train_wall=33, gb_free=10.1, ema_decay=0.9999, wall=12965
2022-10-01 09:00:41 - progress_bar.py[line:274] - INFO: epoch 001:   4307 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.8, ups=0.33, wpb=304.3, bsz=120, num_updates=4300, lr=4.35691e-05, gnorm=0.732, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=12996
2022-10-01 09:01:11 - progress_bar.py[line:274] - INFO: epoch 001:   4317 / 5261 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.7, ups=0.33, wpb=303.6, bsz=120, num_updates=4310, lr=4.35493e-05, gnorm=0.665, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=13026
2022-10-01 09:01:41 - progress_bar.py[line:274] - INFO: epoch 001:   4327 / 5261 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.8, ups=0.34, wpb=305.3, bsz=120, num_updates=4320, lr=4.35295e-05, gnorm=0.606, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=13056
2022-10-01 09:02:11 - progress_bar.py[line:274] - INFO: epoch 001:   4337 / 5261 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=307.1, nsentences=120, sample_size=307.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.6, ups=0.33, wpb=307.1, bsz=120, num_updates=4330, lr=4.35097e-05, gnorm=0.677, clip=10, loss_scale=256, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=13086
2022-10-01 09:02:41 - progress_bar.py[line:274] - INFO: epoch 001:   4347 / 5261 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.6, ups=0.33, wpb=305.4, bsz=120, num_updates=4340, lr=4.34899e-05, gnorm=0.612, clip=0, loss_scale=256, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=13116
2022-10-01 09:03:12 - progress_bar.py[line:274] - INFO: epoch 001:   4357 / 5261 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.4, ups=0.33, wpb=304.5, bsz=120, num_updates=4350, lr=4.34701e-05, gnorm=0.687, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=13147
2022-10-01 09:03:42 - progress_bar.py[line:274] - INFO: epoch 001:   4367 / 5261 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.3, ups=0.33, wpb=305.9, bsz=120, num_updates=4360, lr=4.34503e-05, gnorm=0.696, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=13177
2022-10-01 09:04:12 - progress_bar.py[line:274] - INFO: epoch 001:   4377 / 5261 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.5, ups=0.33, wpb=304.1, bsz=120, num_updates=4370, lr=4.34305e-05, gnorm=0.712, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=13207
2022-10-01 09:04:42 - progress_bar.py[line:274] - INFO: epoch 001:   4387 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=306.8, nsentences=120, sample_size=306.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103.5, ups=0.34, wpb=306.8, bsz=120, num_updates=4380, lr=4.34107e-05, gnorm=0.672, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=13237
2022-10-01 09:05:11 - progress_bar.py[line:274] - INFO: epoch 001:   4397 / 5261 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=305.1, nsentences=120, sample_size=305.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=103.2, ups=0.34, wpb=305.1, bsz=120, num_updates=4390, lr=4.33909e-05, gnorm=0.701, clip=0, loss_scale=256, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=13266
2022-10-01 09:05:42 - progress_bar.py[line:274] - INFO: epoch 001:   4407 / 5261 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=302.1, nsentences=120, sample_size=302.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.9, ups=0.33, wpb=302.1, bsz=120, num_updates=4400, lr=4.33711e-05, gnorm=0.635, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=13296
2022-10-01 09:06:12 - progress_bar.py[line:274] - INFO: epoch 001:   4417 / 5261 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.3, ups=0.33, wpb=305, bsz=120, num_updates=4410, lr=4.33513e-05, gnorm=0.673, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=13327
2022-10-01 09:06:42 - progress_bar.py[line:274] - INFO: epoch 001:   4427 / 5261 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=305.1, nsentences=120, sample_size=305.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101, ups=0.33, wpb=305.1, bsz=120, num_updates=4420, lr=4.33315e-05, gnorm=0.625, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=13357
2022-10-01 09:07:12 - progress_bar.py[line:274] - INFO: epoch 001:   4437 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=101.9, ups=0.34, wpb=302.5, bsz=120, num_updates=4430, lr=4.33117e-05, gnorm=0.72, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=13387
2022-10-01 09:07:42 - progress_bar.py[line:274] - INFO: epoch 001:   4447 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=301.1, nsentences=120, sample_size=301.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.8, ups=0.33, wpb=301.1, bsz=120, num_updates=4440, lr=4.32919e-05, gnorm=0.65, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=13417
2022-10-01 09:08:13 - progress_bar.py[line:274] - INFO: epoch 001:   4457 / 5261 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=98, ups=0.32, wpb=303.3, bsz=120, num_updates=4450, lr=4.32721e-05, gnorm=0.69, clip=0, loss_scale=256, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=13448
2022-10-01 09:08:43 - progress_bar.py[line:274] - INFO: epoch 001:   4467 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=304.7, nsentences=120, sample_size=304.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.4, ups=0.33, wpb=304.7, bsz=120, num_updates=4460, lr=4.32523e-05, gnorm=0.691, clip=20, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=13478
2022-10-01 09:09:13 - progress_bar.py[line:274] - INFO: epoch 001:   4477 / 5261 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.5, ups=0.34, wpb=302.5, bsz=120, num_updates=4470, lr=4.32325e-05, gnorm=0.631, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=13508
2022-10-01 09:09:42 - progress_bar.py[line:274] - INFO: epoch 001:   4487 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=104.5, ups=0.34, wpb=303.9, bsz=120, num_updates=4480, lr=4.32127e-05, gnorm=0.655, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=13537
2022-10-01 09:10:12 - progress_bar.py[line:274] - INFO: epoch 001:   4497 / 5261 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=305.1, nsentences=120, sample_size=305.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.9, ups=0.34, wpb=305.1, bsz=120, num_updates=4490, lr=4.31929e-05, gnorm=0.645, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=13567
2022-10-01 09:10:41 - progress_bar.py[line:274] - INFO: epoch 001:   4507 / 5261 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=103.2, ups=0.34, wpb=303.4, bsz=120, num_updates=4500, lr=4.31731e-05, gnorm=0.731, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=13596
2022-10-01 09:11:12 - progress_bar.py[line:274] - INFO: epoch 001:   4517 / 5261 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=301.5, nsentences=120, sample_size=301.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.2, ups=0.33, wpb=301.5, bsz=120, num_updates=4510, lr=4.31533e-05, gnorm=0.648, clip=0, loss_scale=256, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=13627
2022-10-01 09:11:41 - progress_bar.py[line:274] - INFO: epoch 001:   4527 / 5261 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=300.7, nsentences=120, sample_size=300.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.1, ups=0.34, wpb=300.7, bsz=120, num_updates=4520, lr=4.31335e-05, gnorm=0.62, clip=0, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=13656
2022-10-01 09:12:11 - progress_bar.py[line:274] - INFO: epoch 001:   4537 / 5261 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.7, ups=0.34, wpb=305.3, bsz=120, num_updates=4530, lr=4.31137e-05, gnorm=0.649, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=13686
2022-10-01 09:12:41 - progress_bar.py[line:274] - INFO: epoch 001:   4547 / 5261 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.3, ups=0.33, wpb=304.4, bsz=120, num_updates=4540, lr=4.30939e-05, gnorm=0.674, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=13716
2022-10-01 09:13:11 - progress_bar.py[line:274] - INFO: epoch 001:   4557 / 5261 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.5, ups=0.34, wpb=301.7, bsz=120, num_updates=4550, lr=4.30741e-05, gnorm=0.629, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=13746
2022-10-01 09:13:41 - progress_bar.py[line:274] - INFO: epoch 001:   4567 / 5261 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.9, ups=0.34, wpb=303.8, bsz=120, num_updates=4560, lr=4.30543e-05, gnorm=0.646, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=13775
2022-10-01 09:14:10 - progress_bar.py[line:274] - INFO: epoch 001:   4577 / 5261 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.8, ups=0.34, wpb=302.5, bsz=120, num_updates=4570, lr=4.30345e-05, gnorm=0.669, clip=10, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=13805
2022-10-01 09:14:40 - progress_bar.py[line:274] - INFO: epoch 001:   4587 / 5261 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=306.3, nsentences=120, sample_size=306.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.9, ups=0.33, wpb=306.3, bsz=120, num_updates=4580, lr=4.30147e-05, gnorm=0.61, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=13835
2022-10-01 09:15:10 - progress_bar.py[line:274] - INFO: epoch 001:   4597 / 5261 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.3, ups=0.34, wpb=305.4, bsz=120, num_updates=4590, lr=4.29949e-05, gnorm=0.623, clip=0, loss_scale=256, train_wall=30, gb_free=9.6, ema_decay=0.9999, wall=13865
2022-10-01 09:15:40 - progress_bar.py[line:274] - INFO: epoch 001:   4607 / 5261 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.1, ups=0.33, wpb=305, bsz=120, num_updates=4600, lr=4.29751e-05, gnorm=0.58, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=13895
2022-10-01 09:16:10 - progress_bar.py[line:274] - INFO: epoch 001:   4617 / 5261 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=301.9, nsentences=120, sample_size=301.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.4, ups=0.34, wpb=301.9, bsz=120, num_updates=4610, lr=4.29553e-05, gnorm=0.618, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=13925
2022-10-01 09:16:40 - progress_bar.py[line:274] - INFO: epoch 001:   4627 / 5261 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=304.7, nsentences=120, sample_size=304.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.1, ups=0.33, wpb=304.7, bsz=120, num_updates=4620, lr=4.29355e-05, gnorm=0.661, clip=0, loss_scale=256, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=13955
2022-10-01 09:17:10 - progress_bar.py[line:274] - INFO: epoch 001:   4637 / 5261 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.4, ups=0.34, wpb=305.3, bsz=120, num_updates=4630, lr=4.29157e-05, gnorm=0.63, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=13985
2022-10-01 09:17:41 - progress_bar.py[line:274] - INFO: epoch 001:   4647 / 5261 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=307.1, nsentences=120, sample_size=307.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.5, ups=0.33, wpb=307.1, bsz=120, num_updates=4640, lr=4.28959e-05, gnorm=0.669, clip=0, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=14016
2022-10-01 09:18:11 - progress_bar.py[line:274] - INFO: epoch 001:   4657 / 5261 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.5, ups=0.33, wpb=302.5, bsz=120, num_updates=4650, lr=4.28761e-05, gnorm=0.633, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=14046
2022-10-01 09:18:41 - progress_bar.py[line:274] - INFO: epoch 001:   4667 / 5261 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.2, ups=0.33, wpb=302.4, bsz=120, num_updates=4660, lr=4.28563e-05, gnorm=0.674, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=14076
2022-10-01 09:19:11 - progress_bar.py[line:274] - INFO: epoch 001:   4677 / 5261 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=305.2, nsentences=120, sample_size=305.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.9, ups=0.33, wpb=305.2, bsz=120, num_updates=4670, lr=4.28365e-05, gnorm=0.65, clip=0, loss_scale=256, train_wall=30, gb_free=9.5, ema_decay=0.9999, wall=14105
2022-10-01 09:19:40 - progress_bar.py[line:274] - INFO: epoch 001:   4687 / 5261 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.2, ups=0.34, wpb=302.5, bsz=120, num_updates=4680, lr=4.28167e-05, gnorm=0.704, clip=10, loss_scale=256, train_wall=30, gb_free=9.6, ema_decay=0.9999, wall=14135
2022-10-01 09:20:10 - progress_bar.py[line:274] - INFO: epoch 001:   4697 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=298.5, nsentences=120, sample_size=298.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.9, ups=0.33, wpb=298.5, bsz=120, num_updates=4690, lr=4.27969e-05, gnorm=0.64, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=14165
2022-10-01 09:20:40 - progress_bar.py[line:274] - INFO: epoch 001:   4707 / 5261 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.5, ups=0.33, wpb=305.3, bsz=120, num_updates=4700, lr=4.27771e-05, gnorm=0.621, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=14195
2022-10-01 09:21:10 - progress_bar.py[line:274] - INFO: epoch 001:   4717 / 5261 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.9, ups=0.33, wpb=304.6, bsz=120, num_updates=4710, lr=4.27573e-05, gnorm=0.649, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=14225
2022-10-01 09:21:41 - progress_bar.py[line:274] - INFO: epoch 001:   4727 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.9, ups=0.33, wpb=303.9, bsz=120, num_updates=4720, lr=4.27375e-05, gnorm=0.668, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=14255
2022-10-01 09:22:12 - progress_bar.py[line:274] - INFO: epoch 001:   4737 / 5261 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=97.2, ups=0.32, wpb=303.8, bsz=120, num_updates=4730, lr=4.27177e-05, gnorm=0.682, clip=0, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=14287
2022-10-01 09:22:42 - progress_bar.py[line:274] - INFO: epoch 001:   4747 / 5261 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.2, ups=0.33, wpb=303.6, bsz=120, num_updates=4740, lr=4.26979e-05, gnorm=0.597, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=14317
2022-10-01 09:23:12 - progress_bar.py[line:274] - INFO: epoch 001:   4757 / 5261 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.2, ups=0.33, wpb=306, bsz=120, num_updates=4750, lr=4.26781e-05, gnorm=0.697, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=14347
2022-10-01 09:23:42 - progress_bar.py[line:274] - INFO: epoch 001:   4767 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.6, ups=0.34, wpb=305.5, bsz=120, num_updates=4760, lr=4.26583e-05, gnorm=0.624, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=14377
2022-10-01 09:24:12 - progress_bar.py[line:274] - INFO: epoch 001:   4777 / 5261 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=305.2, nsentences=120, sample_size=305.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=103.3, ups=0.34, wpb=305.2, bsz=120, num_updates=4770, lr=4.26385e-05, gnorm=0.66, clip=0, loss_scale=256, train_wall=29, gb_free=9.9, ema_decay=0.9999, wall=14406
2022-10-01 09:24:41 - progress_bar.py[line:274] - INFO: epoch 001:   4787 / 5261 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.7, ups=0.34, wpb=302.2, bsz=120, num_updates=4780, lr=4.26187e-05, gnorm=0.681, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=14436
2022-10-01 09:25:11 - progress_bar.py[line:274] - INFO: epoch 001:   4797 / 5261 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=104, ups=0.34, wpb=306, bsz=120, num_updates=4790, lr=4.25989e-05, gnorm=0.747, clip=10, loss_scale=256, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=14466
2022-10-01 09:25:42 - progress_bar.py[line:274] - INFO: epoch 001:   4807 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.7, ups=0.32, wpb=305.3, bsz=120, num_updates=4800, lr=4.25791e-05, gnorm=0.588, clip=0, loss_scale=512, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=14497
2022-10-01 09:26:12 - progress_bar.py[line:274] - INFO: epoch 001:   4817 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=304.5, nsentences=120, sample_size=304.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102, ups=0.33, wpb=304.5, bsz=120, num_updates=4810, lr=4.25593e-05, gnorm=0.613, clip=0, loss_scale=512, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=14526
2022-10-01 09:26:42 - progress_bar.py[line:274] - INFO: epoch 001:   4827 / 5261 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.3, ups=0.33, wpb=303.9, bsz=120, num_updates=4820, lr=4.25395e-05, gnorm=0.613, clip=0, loss_scale=512, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=14557
2022-10-01 09:27:12 - progress_bar.py[line:274] - INFO: epoch 001:   4837 / 5261 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=307.7, nsentences=120, sample_size=307.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.3, ups=0.33, wpb=307.7, bsz=120, num_updates=4830, lr=4.25197e-05, gnorm=0.589, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=14587
2022-10-01 09:27:43 - progress_bar.py[line:274] - INFO: epoch 001:   4847 / 5261 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=99.9, ups=0.33, wpb=302.4, bsz=120, num_updates=4840, lr=4.24999e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=30, gb_free=10.3, ema_decay=0.9999, wall=14617
2022-10-01 09:28:13 - progress_bar.py[line:274] - INFO: epoch 001:   4857 / 5261 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.9, ups=0.33, wpb=303.1, bsz=120, num_updates=4850, lr=4.24801e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=30, gb_free=10.4, ema_decay=0.9999, wall=14647
2022-10-01 09:28:43 - progress_bar.py[line:274] - INFO: epoch 001:   4867 / 5261 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=101.4, ups=0.33, wpb=303.8, bsz=120, num_updates=4860, lr=4.24603e-05, gnorm=0.692, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=14677
2022-10-01 09:29:12 - progress_bar.py[line:274] - INFO: epoch 001:   4877 / 5261 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=103.5, ups=0.34, wpb=304.1, bsz=120, num_updates=4870, lr=4.24405e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=14707
2022-10-01 09:29:42 - progress_bar.py[line:274] - INFO: epoch 001:   4887 / 5261 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.7, ups=0.34, wpb=302.2, bsz=120, num_updates=4880, lr=4.24207e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=14736
2022-10-01 09:30:11 - progress_bar.py[line:274] - INFO: epoch 001:   4897 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.7, ups=0.33, wpb=303.7, bsz=120, num_updates=4890, lr=4.24009e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=14766
2022-10-01 09:30:42 - progress_bar.py[line:274] - INFO: epoch 001:   4907 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=306.5, nsentences=120, sample_size=306.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.3, ups=0.33, wpb=306.5, bsz=120, num_updates=4900, lr=4.23811e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=14797
2022-10-01 09:30:51 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2022-10-01 09:31:15 - progress_bar.py[line:274] - INFO: epoch 001:   4918 / 5261 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=92.4, ups=0.3, wpb=304.1, bsz=120, num_updates=4910, lr=4.23613e-05, gnorm=0.637, clip=0, loss_scale=256, train_wall=33, gb_free=10, ema_decay=0.9999, wall=14829
2022-10-01 09:31:44 - progress_bar.py[line:274] - INFO: epoch 001:   4928 / 5261 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=306.6, nsentences=120, sample_size=306.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103.2, ups=0.34, wpb=306.6, bsz=120, num_updates=4920, lr=4.23415e-05, gnorm=0.657, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=14859
2022-10-01 09:32:14 - progress_bar.py[line:274] - INFO: epoch 001:   4938 / 5261 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.6, ups=0.33, wpb=305.5, bsz=120, num_updates=4930, lr=4.23217e-05, gnorm=0.696, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=14889
2022-10-01 09:32:44 - progress_bar.py[line:274] - INFO: epoch 001:   4948 / 5261 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.3, ups=0.34, wpb=303.9, bsz=120, num_updates=4940, lr=4.23019e-05, gnorm=0.686, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=14919
2022-10-01 09:33:14 - progress_bar.py[line:274] - INFO: epoch 001:   4958 / 5261 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=301.8, nsentences=120, sample_size=301.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100, ups=0.33, wpb=301.8, bsz=120, num_updates=4950, lr=4.22821e-05, gnorm=0.667, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=14949
2022-10-01 09:33:44 - progress_bar.py[line:274] - INFO: epoch 001:   4968 / 5261 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=100.9, ups=0.33, wpb=302.4, bsz=120, num_updates=4960, lr=4.22623e-05, gnorm=0.666, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=14979
2022-10-01 09:34:16 - progress_bar.py[line:274] - INFO: epoch 001:   4978 / 5261 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=307.4, nsentences=120, sample_size=307.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=97, ups=0.32, wpb=307.4, bsz=120, num_updates=4970, lr=4.22425e-05, gnorm=0.645, clip=0, loss_scale=256, train_wall=32, gb_free=10.3, ema_decay=0.9999, wall=15011
2022-10-01 09:34:46 - progress_bar.py[line:274] - INFO: epoch 001:   4988 / 5261 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=99.4, ups=0.33, wpb=301.7, bsz=120, num_updates=4980, lr=4.22227e-05, gnorm=0.754, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=15041
2022-10-01 09:35:16 - progress_bar.py[line:274] - INFO: epoch 001:   4998 / 5261 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=303, nsentences=120, sample_size=303, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=102.7, ups=0.34, wpb=303, bsz=120, num_updates=4990, lr=4.22029e-05, gnorm=0.734, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=15071
2022-10-01 09:35:46 - progress_bar.py[line:274] - INFO: epoch 001:   5008 / 5261 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=304.8, nsentences=120, sample_size=304.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.6, ups=0.33, wpb=304.8, bsz=120, num_updates=5000, lr=4.21831e-05, gnorm=0.624, clip=0, loss_scale=256, train_wall=31, gb_free=9.9, ema_decay=0.9999, wall=15101
2022-10-01 09:36:17 - progress_bar.py[line:274] - INFO: epoch 001:   5018 / 5261 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.3, ups=0.33, wpb=302.9, bsz=120, num_updates=5010, lr=4.21633e-05, gnorm=0.61, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=15132
2022-10-01 09:36:46 - progress_bar.py[line:274] - INFO: epoch 001:   5028 / 5261 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=305.1, nsentences=120, sample_size=305.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=104.5, ups=0.34, wpb=305.1, bsz=120, num_updates=5020, lr=4.21435e-05, gnorm=0.562, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=15161
2022-10-01 09:37:16 - progress_bar.py[line:274] - INFO: epoch 001:   5038 / 5261 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.7, ups=0.33, wpb=304.1, bsz=120, num_updates=5030, lr=4.21237e-05, gnorm=0.59, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=15191
2022-10-01 09:37:46 - progress_bar.py[line:274] - INFO: epoch 001:   5048 / 5261 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=308.7, nsentences=120, sample_size=308.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101, ups=0.33, wpb=308.7, bsz=120, num_updates=5040, lr=4.21039e-05, gnorm=0.626, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=15221
2022-10-01 09:38:16 - progress_bar.py[line:274] - INFO: epoch 001:   5058 / 5261 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.5, ups=0.34, wpb=302.4, bsz=120, num_updates=5050, lr=4.20841e-05, gnorm=0.592, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=15251
2022-10-01 09:38:46 - progress_bar.py[line:274] - INFO: epoch 001:   5068 / 5261 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.7, ups=0.33, wpb=304.1, bsz=120, num_updates=5060, lr=4.20643e-05, gnorm=0.676, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=15281
2022-10-01 09:39:16 - progress_bar.py[line:274] - INFO: epoch 001:   5078 / 5261 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.1, ups=0.34, wpb=303.4, bsz=120, num_updates=5070, lr=4.20445e-05, gnorm=0.679, clip=10, loss_scale=256, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=15311
2022-10-01 09:39:46 - progress_bar.py[line:274] - INFO: epoch 001:   5088 / 5261 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.1, ups=0.34, wpb=304.6, bsz=120, num_updates=5080, lr=4.20247e-05, gnorm=0.658, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=15340
2022-10-01 09:40:15 - progress_bar.py[line:274] - INFO: epoch 001:   5098 / 5261 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=300.9, nsentences=120, sample_size=300.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.6, ups=0.34, wpb=300.9, bsz=120, num_updates=5090, lr=4.20049e-05, gnorm=0.738, clip=10, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=15370
2022-10-01 09:40:45 - progress_bar.py[line:274] - INFO: epoch 001:   5108 / 5261 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=302, nsentences=120, sample_size=302, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100, ups=0.33, wpb=302, bsz=120, num_updates=5100, lr=4.19851e-05, gnorm=0.7, clip=0, loss_scale=256, train_wall=30, gb_free=9.5, ema_decay=0.9999, wall=15400
2022-10-01 09:41:15 - progress_bar.py[line:274] - INFO: epoch 001:   5118 / 5261 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=102.5, ups=0.34, wpb=304.2, bsz=120, num_updates=5110, lr=4.19653e-05, gnorm=0.69, clip=10, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=15430
2022-10-01 09:41:45 - progress_bar.py[line:274] - INFO: epoch 001:   5128 / 5261 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.5, ups=0.34, wpb=302.5, bsz=120, num_updates=5120, lr=4.19455e-05, gnorm=0.66, clip=0, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=15459
2022-10-01 09:42:15 - progress_bar.py[line:274] - INFO: epoch 001:   5138 / 5261 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=100.2, ups=0.33, wpb=304.1, bsz=120, num_updates=5130, lr=4.19257e-05, gnorm=0.697, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=15490
2022-10-01 09:42:45 - progress_bar.py[line:274] - INFO: epoch 001:   5148 / 5261 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=302, nsentences=120, sample_size=302, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.8, ups=0.33, wpb=302, bsz=120, num_updates=5140, lr=4.19059e-05, gnorm=0.682, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=15520
2022-10-01 09:43:18 - progress_bar.py[line:274] - INFO: epoch 001:   5158 / 5261 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=94.4, ups=0.31, wpb=306, bsz=120, num_updates=5150, lr=4.18861e-05, gnorm=0.666, clip=0, loss_scale=256, train_wall=32, gb_free=10.1, ema_decay=0.9999, wall=15552
2022-10-01 09:43:48 - progress_bar.py[line:274] - INFO: epoch 001:   5168 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=306.4, nsentences=120, sample_size=306.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.5, ups=0.33, wpb=306.4, bsz=120, num_updates=5160, lr=4.18663e-05, gnorm=0.622, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=15583
2022-10-01 09:44:18 - progress_bar.py[line:274] - INFO: epoch 001:   5178 / 5261 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=306.8, nsentences=120, sample_size=306.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.9, ups=0.33, wpb=306.8, bsz=120, num_updates=5170, lr=4.18465e-05, gnorm=0.641, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=15613
2022-10-01 09:44:48 - progress_bar.py[line:274] - INFO: epoch 001:   5188 / 5261 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=304.7, nsentences=120, sample_size=304.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=104.5, ups=0.34, wpb=304.7, bsz=120, num_updates=5180, lr=4.18267e-05, gnorm=0.624, clip=0, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=15642
2022-10-01 09:45:18 - progress_bar.py[line:274] - INFO: epoch 001:   5198 / 5261 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=302.1, nsentences=120, sample_size=302.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.9, ups=0.33, wpb=302.1, bsz=120, num_updates=5190, lr=4.18069e-05, gnorm=0.775, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=15672
2022-10-01 09:45:48 - progress_bar.py[line:274] - INFO: epoch 001:   5208 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.5, ups=0.33, wpb=303.7, bsz=120, num_updates=5200, lr=4.17871e-05, gnorm=0.694, clip=0, loss_scale=256, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=15703
2022-10-01 09:46:18 - progress_bar.py[line:274] - INFO: epoch 001:   5218 / 5261 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.2, ups=0.33, wpb=303.6, bsz=120, num_updates=5210, lr=4.17673e-05, gnorm=0.676, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=15733
2022-10-01 09:46:48 - progress_bar.py[line:274] - INFO: epoch 001:   5228 / 5261 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.2, ups=0.33, wpb=303.9, bsz=120, num_updates=5220, lr=4.17475e-05, gnorm=0.695, clip=0, loss_scale=256, train_wall=30, gb_free=9.4, ema_decay=0.9999, wall=15763
2022-10-01 09:47:19 - progress_bar.py[line:274] - INFO: epoch 001:   5238 / 5261 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.3, ups=0.33, wpb=303.9, bsz=120, num_updates=5230, lr=4.17277e-05, gnorm=0.593, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=15794
2022-10-01 09:47:49 - progress_bar.py[line:274] - INFO: epoch 001:   5248 / 5261 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=304.7, nsentences=120, sample_size=304.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.8, ups=0.33, wpb=304.7, bsz=120, num_updates=5240, lr=4.17079e-05, gnorm=0.629, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=15824
2022-10-01 09:48:19 - progress_bar.py[line:274] - INFO: epoch 001:   5258 / 5261 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=101.2, ups=0.33, wpb=302.5, bsz=120, num_updates=5250, lr=4.16881e-05, gnorm=0.657, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=15854
2022-10-01 09:48:28 - train.py[line:339] - INFO: end of epoch 1 (average epoch stats below)
2022-10-01 09:48:28 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.625 | loss_v1 0 | loss_v2 0 | nll_loss 0.419 | ntokens 304.011 | nsentences 119.993 | sample_size 304.011 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.34 | wps 100.8 | ups 0.33 | wpb 304 | bsz 120 | num_updates 5253 | lr 4.16822e-05 | gnorm 0.906 | clip 19.8 | loss_scale 256 | train_wall 15816 | gb_free 10.3 | ema_decay 0.9999 | wall 15863
2022-10-01 09:48:28 - trainer.py[line:643] - INFO: loading train data for epoch 2
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv slice_id 1 row count 315642 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv slice_id 0 row count 315642 total row count 631284
2022-10-01 09:48:28 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2022-10-01 09:48:29 - trainer.py[line:707] - INFO: begin training epoch 2
2022-10-01 09:48:29 - train.py[line:312] - INFO: Start iterating over samples
2022-10-01 09:48:51 - progress_bar.py[line:274] - INFO: epoch 002:      7 / 5261 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=294.3, nsentences=116.4, sample_size=294.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=92.6, ups=0.31, wpb=294.3, bsz=116.4, num_updates=5260, lr=4.16683e-05, gnorm=0.618, clip=0, loss_scale=256, train_wall=29, gb_free=10.2, ema_decay=0.9999, wall=15886
2022-10-01 09:49:21 - progress_bar.py[line:274] - INFO: epoch 002:     17 / 5261 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.5, ups=0.33, wpb=305.4, bsz=120, num_updates=5270, lr=4.16485e-05, gnorm=0.625, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=15916
2022-10-01 09:49:52 - progress_bar.py[line:274] - INFO: epoch 002:     27 / 5261 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=306.2, nsentences=120, sample_size=306.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.4, ups=0.33, wpb=306.2, bsz=120, num_updates=5280, lr=4.16287e-05, gnorm=0.664, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=15947
2022-10-01 09:50:22 - progress_bar.py[line:274] - INFO: epoch 002:     37 / 5261 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99.7, ups=0.33, wpb=302.5, bsz=120, num_updates=5290, lr=4.16089e-05, gnorm=0.612, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=15977
2022-10-01 09:50:53 - progress_bar.py[line:274] - INFO: epoch 002:     47 / 5261 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=98.8, ups=0.33, wpb=303.2, bsz=120, num_updates=5300, lr=4.15891e-05, gnorm=0.675, clip=0, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=16008
2022-10-01 09:51:23 - progress_bar.py[line:274] - INFO: epoch 002:     57 / 5261 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.4, ups=0.33, wpb=303.9, bsz=120, num_updates=5310, lr=4.15693e-05, gnorm=0.625, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=16038
2022-10-01 09:51:53 - progress_bar.py[line:274] - INFO: epoch 002:     67 / 5261 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.2, ups=0.33, wpb=306, bsz=120, num_updates=5320, lr=4.15495e-05, gnorm=0.71, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=16068
2022-10-01 09:52:24 - progress_bar.py[line:274] - INFO: epoch 002:     77 / 5261 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=99, ups=0.33, wpb=304.4, bsz=120, num_updates=5330, lr=4.15297e-05, gnorm=0.653, clip=0, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=16099
2022-10-01 09:52:56 - progress_bar.py[line:274] - INFO: epoch 002:     87 / 5261 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=95.7, ups=0.31, wpb=305.9, bsz=120, num_updates=5340, lr=4.15099e-05, gnorm=0.623, clip=0, loss_scale=256, train_wall=32, gb_free=10, ema_decay=0.9999, wall=16131
2022-10-01 09:53:27 - progress_bar.py[line:274] - INFO: epoch 002:     97 / 5261 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96, ups=0.32, wpb=303.5, bsz=120, num_updates=5350, lr=4.14901e-05, gnorm=0.693, clip=0, loss_scale=256, train_wall=31, gb_free=10, ema_decay=0.9999, wall=16162
2022-10-01 09:53:58 - progress_bar.py[line:274] - INFO: epoch 002:    107 / 5261 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.9, ups=0.33, wpb=303.6, bsz=120, num_updates=5360, lr=4.14703e-05, gnorm=0.644, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=16192
2022-10-01 09:54:27 - progress_bar.py[line:274] - INFO: epoch 002:    117 / 5261 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=301.5, nsentences=120, sample_size=301.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.9, ups=0.33, wpb=301.5, bsz=120, num_updates=5370, lr=4.14505e-05, gnorm=0.71, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=16222
2022-10-01 09:54:58 - progress_bar.py[line:274] - INFO: epoch 002:    127 / 5261 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.2, ups=0.33, wpb=303.6, bsz=120, num_updates=5380, lr=4.14307e-05, gnorm=0.664, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=16253
2022-10-01 09:55:27 - progress_bar.py[line:274] - INFO: epoch 002:    137 / 5261 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103.7, ups=0.34, wpb=305, bsz=120, num_updates=5390, lr=4.14109e-05, gnorm=0.647, clip=0, loss_scale=256, train_wall=29, gb_free=9.8, ema_decay=0.9999, wall=16282
2022-10-01 09:55:57 - progress_bar.py[line:274] - INFO: epoch 002:    147 / 5261 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.9, ups=0.33, wpb=302.9, bsz=120, num_updates=5400, lr=4.13911e-05, gnorm=0.696, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=16312
2022-10-01 09:56:27 - progress_bar.py[line:274] - INFO: epoch 002:    157 / 5261 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=301.3, nsentences=120, sample_size=301.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.7, ups=0.33, wpb=301.3, bsz=120, num_updates=5410, lr=4.13713e-05, gnorm=0.716, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=16342
2022-10-01 09:56:57 - progress_bar.py[line:274] - INFO: epoch 002:    167 / 5261 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=305.6, nsentences=120, sample_size=305.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.9, ups=0.34, wpb=305.6, bsz=120, num_updates=5420, lr=4.13515e-05, gnorm=0.702, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=16372
2022-10-01 09:57:27 - progress_bar.py[line:274] - INFO: epoch 002:    177 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.1, ups=0.33, wpb=305, bsz=120, num_updates=5430, lr=4.13317e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=16402
2022-10-01 09:57:57 - progress_bar.py[line:274] - INFO: epoch 002:    187 / 5261 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=304.7, nsentences=120, sample_size=304.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.4, ups=0.33, wpb=304.7, bsz=120, num_updates=5440, lr=4.13119e-05, gnorm=0.731, clip=10, loss_scale=512, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=16432
2022-10-01 09:58:27 - progress_bar.py[line:274] - INFO: epoch 002:    197 / 5261 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102.1, ups=0.34, wpb=303.3, bsz=120, num_updates=5450, lr=4.12921e-05, gnorm=0.708, clip=0, loss_scale=512, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=16462
2022-10-01 09:58:57 - progress_bar.py[line:274] - INFO: epoch 002:    207 / 5261 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=302.2, nsentences=120, sample_size=302.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.7, ups=0.33, wpb=302.2, bsz=120, num_updates=5460, lr=4.12723e-05, gnorm=0.624, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=16492
2022-10-01 09:59:27 - progress_bar.py[line:274] - INFO: epoch 002:    217 / 5261 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=306.4, nsentences=120, sample_size=306.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=101.9, ups=0.33, wpb=306.4, bsz=120, num_updates=5470, lr=4.12525e-05, gnorm=0.605, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=16522
2022-10-01 09:59:56 - progress_bar.py[line:274] - INFO: epoch 002:    227 / 5261 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=103.6, ups=0.34, wpb=303.5, bsz=120, num_updates=5480, lr=4.12327e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=29, gb_free=9.9, ema_decay=0.9999, wall=16551
2022-10-01 10:00:26 - progress_bar.py[line:274] - INFO: epoch 002:    237 / 5261 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.9, ups=0.34, wpb=303.8, bsz=120, num_updates=5490, lr=4.12129e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=16580
2022-10-01 10:00:55 - progress_bar.py[line:274] - INFO: epoch 002:    247 / 5261 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=102.7, ups=0.34, wpb=303.1, bsz=120, num_updates=5500, lr=4.11931e-05, gnorm=0.7, clip=10, loss_scale=512, train_wall=29, gb_free=10, ema_decay=0.9999, wall=16610
2022-10-01 10:01:25 - progress_bar.py[line:274] - INFO: epoch 002:    257 / 5261 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=306.2, nsentences=120, sample_size=306.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.5, ups=0.33, wpb=306.2, bsz=120, num_updates=5510, lr=4.11733e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=16640
2022-10-01 10:01:55 - progress_bar.py[line:274] - INFO: epoch 002:    267 / 5261 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=302.6, nsentences=120, sample_size=302.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.7, ups=0.33, wpb=302.6, bsz=120, num_updates=5520, lr=4.11535e-05, gnorm=0.625, clip=0, loss_scale=512, train_wall=30, gb_free=9.6, ema_decay=0.9999, wall=16670
2022-10-01 10:02:25 - progress_bar.py[line:274] - INFO: epoch 002:    277 / 5261 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=102, ups=0.34, wpb=302.5, bsz=120, num_updates=5530, lr=4.11337e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=16700
2022-10-01 10:02:55 - progress_bar.py[line:274] - INFO: epoch 002:    287 / 5261 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=306, nsentences=120, sample_size=306, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.6, ups=0.33, wpb=306, bsz=120, num_updates=5540, lr=4.11139e-05, gnorm=0.615, clip=0, loss_scale=512, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=16730
2022-10-01 10:03:25 - progress_bar.py[line:274] - INFO: epoch 002:    297 / 5261 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.4, ups=0.33, wpb=303.8, bsz=120, num_updates=5550, lr=4.10941e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=30, gb_free=10, ema_decay=0.9999, wall=16760
2022-10-01 10:03:57 - progress_bar.py[line:274] - INFO: epoch 002:    307 / 5261 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.9, ups=0.32, wpb=304.3, bsz=120, num_updates=5560, lr=4.10743e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=16791
2022-10-01 10:04:11 - trainer.py[line:932] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2022-10-01 10:04:29 - progress_bar.py[line:274] - INFO: epoch 002:    318 / 5261 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=305, nsentences=120, sample_size=305, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=92.8, ups=0.3, wpb=305, bsz=120, num_updates=5570, lr=4.10545e-05, gnorm=0.672, clip=0, loss_scale=256, train_wall=33, gb_free=10.1, ema_decay=0.9999, wall=16824
2022-10-01 10:04:59 - progress_bar.py[line:274] - INFO: epoch 002:    328 / 5261 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.2, ups=0.33, wpb=303.4, bsz=120, num_updates=5580, lr=4.10347e-05, gnorm=0.696, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=16854
2022-10-01 10:05:29 - progress_bar.py[line:274] - INFO: epoch 002:    338 / 5261 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101, ups=0.33, wpb=303.7, bsz=120, num_updates=5590, lr=4.10149e-05, gnorm=0.648, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=16884
2022-10-01 10:06:00 - progress_bar.py[line:274] - INFO: epoch 002:    348 / 5261 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.5, ups=0.33, wpb=304.2, bsz=120, num_updates=5600, lr=4.09951e-05, gnorm=0.716, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=16915
2022-10-01 10:06:30 - progress_bar.py[line:274] - INFO: epoch 002:    358 / 5261 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.8, ups=0.33, wpb=302.5, bsz=120, num_updates=5610, lr=4.09753e-05, gnorm=0.672, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=16945
2022-10-01 10:07:00 - progress_bar.py[line:274] - INFO: epoch 002:    368 / 5261 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=302.6, nsentences=120, sample_size=302.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.5, ups=0.33, wpb=302.6, bsz=120, num_updates=5620, lr=4.09555e-05, gnorm=0.645, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=16975
2022-10-01 10:07:30 - progress_bar.py[line:274] - INFO: epoch 002:    378 / 5261 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=304.7, nsentences=120, sample_size=304.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=104.4, ups=0.34, wpb=304.7, bsz=120, num_updates=5630, lr=4.09357e-05, gnorm=0.653, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=17004
2022-10-01 10:08:00 - progress_bar.py[line:274] - INFO: epoch 002:    388 / 5261 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.5, ups=0.33, wpb=303.1, bsz=120, num_updates=5640, lr=4.09159e-05, gnorm=0.655, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=17035
2022-10-01 10:08:30 - progress_bar.py[line:274] - INFO: epoch 002:    398 / 5261 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=304.2, nsentences=120, sample_size=304.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.7, ups=0.33, wpb=304.2, bsz=120, num_updates=5650, lr=4.08961e-05, gnorm=0.646, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=17065
2022-10-01 10:09:00 - progress_bar.py[line:274] - INFO: epoch 002:    408 / 5261 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101.8, ups=0.33, wpb=304.1, bsz=120, num_updates=5660, lr=4.08763e-05, gnorm=0.667, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=17095
2022-10-01 10:09:30 - progress_bar.py[line:274] - INFO: epoch 002:    418 / 5261 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=310.3, nsentences=120, sample_size=310.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=104.7, ups=0.34, wpb=310.3, bsz=120, num_updates=5670, lr=4.08565e-05, gnorm=0.677, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=17125
2022-10-01 10:10:00 - progress_bar.py[line:274] - INFO: epoch 002:    428 / 5261 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.1, ups=0.33, wpb=302.4, bsz=120, num_updates=5680, lr=4.08367e-05, gnorm=0.628, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=17155
2022-10-01 10:10:30 - progress_bar.py[line:274] - INFO: epoch 002:    438 / 5261 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=302.7, nsentences=120, sample_size=302.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.9, ups=0.33, wpb=302.7, bsz=120, num_updates=5690, lr=4.08169e-05, gnorm=0.706, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=17185
2022-10-01 10:11:01 - progress_bar.py[line:274] - INFO: epoch 002:    448 / 5261 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.9, ups=0.33, wpb=301.7, bsz=120, num_updates=5700, lr=4.07971e-05, gnorm=0.725, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=17215
2022-10-01 10:11:31 - progress_bar.py[line:274] - INFO: epoch 002:    458 / 5261 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=306.8, nsentences=120, sample_size=306.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=104.1, ups=0.34, wpb=306.8, bsz=120, num_updates=5710, lr=4.07773e-05, gnorm=0.673, clip=0, loss_scale=256, train_wall=29, gb_free=10.1, ema_decay=0.9999, wall=17245
2022-10-01 10:12:01 - progress_bar.py[line:274] - INFO: epoch 002:    468 / 5261 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=307, nsentences=120, sample_size=307, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.2, ups=0.33, wpb=307, bsz=120, num_updates=5720, lr=4.07575e-05, gnorm=0.67, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=17275
2022-10-01 10:12:31 - progress_bar.py[line:274] - INFO: epoch 002:    478 / 5261 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=101, ups=0.33, wpb=302.5, bsz=120, num_updates=5730, lr=4.07377e-05, gnorm=0.707, clip=10, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=17305
2022-10-01 10:13:01 - progress_bar.py[line:274] - INFO: epoch 002:    488 / 5261 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.4, ups=0.33, wpb=303.1, bsz=120, num_updates=5740, lr=4.07179e-05, gnorm=0.716, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=17336
2022-10-01 10:13:31 - progress_bar.py[line:274] - INFO: epoch 002:    498 / 5261 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=100.9, ups=0.33, wpb=305.9, bsz=120, num_updates=5750, lr=4.06981e-05, gnorm=0.663, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=17366
2022-10-01 10:14:01 - progress_bar.py[line:274] - INFO: epoch 002:    508 / 5261 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.3, ups=0.34, wpb=305.4, bsz=120, num_updates=5760, lr=4.06783e-05, gnorm=0.677, clip=10, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=17396
2022-10-01 10:14:31 - progress_bar.py[line:274] - INFO: epoch 002:    518 / 5261 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.4, ups=0.33, wpb=303.2, bsz=120, num_updates=5770, lr=4.06585e-05, gnorm=0.705, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=17426
2022-10-01 10:15:01 - progress_bar.py[line:274] - INFO: epoch 002:    528 / 5261 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=306.1, nsentences=120, sample_size=306.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=104, ups=0.34, wpb=306.1, bsz=120, num_updates=5780, lr=4.06387e-05, gnorm=0.641, clip=10, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=17455
2022-10-01 10:15:30 - progress_bar.py[line:274] - INFO: epoch 002:    538 / 5261 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.4, ups=0.33, wpb=303.8, bsz=120, num_updates=5790, lr=4.06189e-05, gnorm=0.656, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=17485
2022-10-01 10:16:00 - progress_bar.py[line:274] - INFO: epoch 002:    548 / 5261 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=305.5, nsentences=120, sample_size=305.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=102.1, ups=0.33, wpb=305.5, bsz=120, num_updates=5800, lr=4.05991e-05, gnorm=0.625, clip=0, loss_scale=256, train_wall=30, gb_free=9.8, ema_decay=0.9999, wall=17515
2022-10-01 10:16:31 - progress_bar.py[line:274] - INFO: epoch 002:    558 / 5261 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=100.9, ups=0.33, wpb=304.1, bsz=120, num_updates=5810, lr=4.05793e-05, gnorm=0.7, clip=20, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=17545
2022-10-01 10:17:01 - progress_bar.py[line:274] - INFO: epoch 002:    568 / 5261 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.4, ups=0.33, wpb=305.9, bsz=120, num_updates=5820, lr=4.05595e-05, gnorm=0.74, clip=10, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=17576
2022-10-01 10:17:31 - progress_bar.py[line:274] - INFO: epoch 002:    578 / 5261 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=307.3, nsentences=120, sample_size=307.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=100.8, ups=0.33, wpb=307.3, bsz=120, num_updates=5830, lr=4.05397e-05, gnorm=0.694, clip=0, loss_scale=256, train_wall=30, gb_free=9.9, ema_decay=0.9999, wall=17606
2022-10-01 10:18:01 - progress_bar.py[line:274] - INFO: epoch 002:    588 / 5261 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=303.2, nsentences=120, sample_size=303.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.1, ups=0.34, wpb=303.2, bsz=120, num_updates=5840, lr=4.05199e-05, gnorm=0.649, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=17636
2022-10-01 10:18:32 - progress_bar.py[line:274] - INFO: epoch 002:    598 / 5261 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=301.5, nsentences=120, sample_size=301.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=96.8, ups=0.32, wpb=301.5, bsz=120, num_updates=5850, lr=4.05001e-05, gnorm=0.701, clip=0, loss_scale=256, train_wall=31, gb_free=10.2, ema_decay=0.9999, wall=17667
2022-10-01 10:19:02 - progress_bar.py[line:274] - INFO: epoch 002:    608 / 5261 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=302.3, nsentences=120, sample_size=302.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=102.2, ups=0.34, wpb=302.3, bsz=120, num_updates=5860, lr=4.04803e-05, gnorm=0.724, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=17696
2022-10-01 10:19:32 - progress_bar.py[line:274] - INFO: epoch 002:    618 / 5261 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=305.1, nsentences=120, sample_size=305.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=100.6, ups=0.33, wpb=305.1, bsz=120, num_updates=5870, lr=4.04605e-05, gnorm=0.707, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=17727
2022-10-01 10:20:02 - progress_bar.py[line:274] - INFO: epoch 002:    628 / 5261 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=302.3, nsentences=120, sample_size=302.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.6, ups=0.34, wpb=302.3, bsz=120, num_updates=5880, lr=4.04407e-05, gnorm=0.666, clip=10, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=17757
2022-10-01 10:20:32 - progress_bar.py[line:274] - INFO: epoch 002:    638 / 5261 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=300.3, nsentences=120, sample_size=300.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=99.4, ups=0.33, wpb=300.3, bsz=120, num_updates=5890, lr=4.04209e-05, gnorm=0.641, clip=0, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=17787
2022-10-01 10:21:01 - progress_bar.py[line:274] - INFO: epoch 002:    648 / 5261 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=306.1, nsentences=120, sample_size=306.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=105.4, ups=0.34, wpb=306.1, bsz=120, num_updates=5900, lr=4.04011e-05, gnorm=0.707, clip=10, loss_scale=256, train_wall=29, gb_free=9.9, ema_decay=0.9999, wall=17816
2022-10-01 10:21:31 - progress_bar.py[line:274] - INFO: epoch 002:    658 / 5261 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=303, nsentences=120, sample_size=303, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.7, ups=0.33, wpb=303, bsz=120, num_updates=5910, lr=4.03813e-05, gnorm=0.709, clip=0, loss_scale=256, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=17846
2022-10-01 10:22:02 - progress_bar.py[line:274] - INFO: epoch 002:    668 / 5261 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=301.8, nsentences=120, sample_size=301.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.6, ups=0.33, wpb=301.8, bsz=120, num_updates=5920, lr=4.03615e-05, gnorm=0.696, clip=0, loss_scale=256, train_wall=30, gb_free=9.7, ema_decay=0.9999, wall=17876
2022-10-01 10:22:31 - progress_bar.py[line:274] - INFO: epoch 002:    678 / 5261 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=101.9, ups=0.34, wpb=303.3, bsz=120, num_updates=5930, lr=4.03417e-05, gnorm=0.66, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=17906
2022-10-01 10:23:01 - progress_bar.py[line:274] - INFO: epoch 002:    688 / 5261 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=303.1, nsentences=120, sample_size=303.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.7, ups=0.34, wpb=303.1, bsz=120, num_updates=5940, lr=4.03219e-05, gnorm=0.673, clip=0, loss_scale=256, train_wall=30, gb_free=10, ema_decay=0.9999, wall=17936
2022-10-01 10:23:32 - progress_bar.py[line:274] - INFO: epoch 002:    698 / 5261 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=304.7, nsentences=120, sample_size=304.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=99.7, ups=0.33, wpb=304.7, bsz=120, num_updates=5950, lr=4.03021e-05, gnorm=0.672, clip=0, loss_scale=256, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=17967
2022-10-01 10:24:02 - progress_bar.py[line:274] - INFO: epoch 002:    708 / 5261 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=306.5, nsentences=120, sample_size=306.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=100, ups=0.33, wpb=306.5, bsz=120, num_updates=5960, lr=4.02823e-05, gnorm=0.629, clip=0, loss_scale=256, train_wall=31, gb_free=9.1, ema_decay=0.9999, wall=17997
2022-10-01 10:24:34 - progress_bar.py[line:274] - INFO: epoch 002:    718 / 5261 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=302.4, nsentences=120, sample_size=302.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=95.8, ups=0.32, wpb=302.4, bsz=120, num_updates=5970, lr=4.02625e-05, gnorm=0.745, clip=10, loss_scale=256, train_wall=31, gb_free=10.1, ema_decay=0.9999, wall=18029
2022-10-01 10:25:04 - progress_bar.py[line:274] - INFO: epoch 002:    728 / 5261 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=101.9, ups=0.34, wpb=303.7, bsz=120, num_updates=5980, lr=4.02427e-05, gnorm=0.698, clip=10, loss_scale=256, train_wall=30, gb_free=10.2, ema_decay=0.9999, wall=18059
2022-10-01 10:25:33 - progress_bar.py[line:274] - INFO: epoch 002:    738 / 5261 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=102.9, ups=0.34, wpb=303.7, bsz=120, num_updates=5990, lr=4.02229e-05, gnorm=0.63, clip=0, loss_scale=256, train_wall=29, gb_free=10, ema_decay=0.9999, wall=18088
2022-10-01 10:26:03 - progress_bar.py[line:274] - INFO: epoch 002:    748 / 5261 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=301.1, nsentences=120, sample_size=301.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=99.9, ups=0.33, wpb=301.1, bsz=120, num_updates=6000, lr=4.02031e-05, gnorm=0.644, clip=0, loss_scale=256, train_wall=30, gb_free=10.1, ema_decay=0.9999, wall=18118
2022-10-01 10:26:03 - train.py[line:505] - INFO: begin validation on "valid" subset
2022-10-01 10:26:04 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2022-10-01 10:26:05 - train.py[line:549] - INFO: 0 / 14103
2022-10-01 10:26:05 - train.py[line:551] - INFO: load:0.90 valid_run:0.00 task_valid:0.00 collect_output:0.00
2022-10-01 10:29:21 - train.py[line:549] - INFO: 200 / 14103
2022-10-01 10:29:21 - train.py[line:551] - INFO: load:0.92 valid_run:196.40 task_valid:188.65 collect_output:6.59
2022-10-01 10:32:30 - train.py[line:549] - INFO: 400 / 14103
2022-10-01 10:32:30 - train.py[line:551] - INFO: load:0.95 valid_run:385.55 task_valid:373.35 collect_output:9.92
2022-10-01 10:35:41 - train.py[line:549] - INFO: 600 / 14103
2022-10-01 10:35:41 - train.py[line:551] - INFO: load:0.97 valid_run:576.53 task_valid:559.29 collect_output:13.79
2022-10-01 10:38:57 - train.py[line:549] - INFO: 800 / 14103
2022-10-01 10:38:57 - train.py[line:551] - INFO: load:1.00 valid_run:771.74 task_valid:751.07 collect_output:15.93
2022-10-01 10:42:08 - train.py[line:549] - INFO: 1000 / 14103
2022-10-01 10:42:08 - train.py[line:551] - INFO: load:1.03 valid_run:962.58 task_valid:937.77 collect_output:18.95
2022-10-01 10:45:21 - train.py[line:549] - INFO: 1200 / 14103
2022-10-01 10:45:21 - train.py[line:551] - INFO: load:1.05 valid_run:1156.19 task_valid:1125.80 collect_output:23.34
2022-10-01 10:48:36 - train.py[line:549] - INFO: 1400 / 14103
2022-10-01 10:48:36 - train.py[line:551] - INFO: load:1.08 valid_run:1350.39 task_valid:1315.27 collect_output:26.93
2022-10-01 10:51:49 - train.py[line:549] - INFO: 1600 / 14103
2022-10-01 10:51:49 - train.py[line:551] - INFO: load:1.10 valid_run:1543.50 task_valid:1504.57 collect_output:29.48
2022-10-01 10:55:10 - train.py[line:549] - INFO: 1800 / 14103
2022-10-01 10:55:10 - train.py[line:551] - INFO: load:1.14 valid_run:1744.75 task_valid:1698.61 collect_output:35.36
2022-10-01 10:58:20 - train.py[line:549] - INFO: 2000 / 14103
2022-10-01 10:58:20 - train.py[line:551] - INFO: load:1.16 valid_run:1934.30 task_valid:1884.96 collect_output:37.36
2022-10-01 11:01:29 - train.py[line:549] - INFO: 2200 / 14103
2022-10-01 11:01:29 - train.py[line:551] - INFO: load:1.18 valid_run:2123.53 task_valid:2070.15 collect_output:40.24
2022-10-01 11:04:41 - train.py[line:549] - INFO: 2400 / 14103
2022-10-01 11:04:41 - train.py[line:551] - INFO: load:1.20 valid_run:2315.90 task_valid:2256.01 collect_output:45.57
2022-10-01 11:07:54 - train.py[line:549] - INFO: 2600 / 14103
2022-10-01 11:07:54 - train.py[line:551] - INFO: load:1.23 valid_run:2508.48 task_valid:2445.12 collect_output:47.93
2022-10-01 11:11:04 - train.py[line:549] - INFO: 2800 / 14103
2022-10-01 11:11:04 - train.py[line:551] - INFO: load:1.25 valid_run:2698.27 task_valid:2629.84 collect_output:51.85
2022-10-01 11:14:18 - train.py[line:549] - INFO: 3000 / 14103
2022-10-01 11:14:18 - train.py[line:551] - INFO: load:1.28 valid_run:2892.31 task_valid:2817.32 collect_output:57.28
2022-10-01 11:17:27 - train.py[line:549] - INFO: 3200 / 14103
2022-10-01 11:17:27 - train.py[line:551] - INFO: load:1.30 valid_run:3080.82 task_valid:3000.00 collect_output:61.99
2022-10-01 11:20:39 - train.py[line:549] - INFO: 3400 / 14103
2022-10-01 11:20:39 - train.py[line:551] - INFO: load:1.32 valid_run:3272.98 task_valid:3184.11 collect_output:68.97
2022-10-01 11:23:51 - train.py[line:549] - INFO: 3600 / 14103
2022-10-01 11:23:51 - train.py[line:551] - INFO: load:1.35 valid_run:3465.53 task_valid:3370.04 collect_output:74.51
2022-10-01 11:27:02 - train.py[line:549] - INFO: 3800 / 14103
2022-10-01 11:27:02 - train.py[line:551] - INFO: load:1.37 valid_run:3656.48 task_valid:3552.85 collect_output:81.56
2022-10-01 11:30:10 - train.py[line:549] - INFO: 4000 / 14103
2022-10-01 11:30:10 - train.py[line:551] - INFO: load:1.39 valid_run:3844.20 task_valid:3735.80 collect_output:85.27
2022-10-01 11:33:23 - train.py[line:549] - INFO: 4200 / 14103
2022-10-01 11:33:23 - train.py[line:551] - INFO: load:1.42 valid_run:4037.26 task_valid:3926.35 collect_output:86.67
2022-10-01 11:36:32 - train.py[line:549] - INFO: 4400 / 14103
2022-10-01 11:36:32 - train.py[line:551] - INFO: load:1.46 valid_run:4226.19 task_valid:4108.80 collect_output:92.02
2022-10-01 11:39:46 - train.py[line:549] - INFO: 4600 / 14103
2022-10-01 11:39:46 - train.py[line:551] - INFO: load:1.48 valid_run:4419.62 task_valid:4295.73 collect_output:97.31
2022-10-01 11:42:54 - train.py[line:549] - INFO: 4800 / 14103
2022-10-01 11:42:54 - train.py[line:551] - INFO: load:1.51 valid_run:4607.79 task_valid:4479.35 collect_output:100.76
2022-10-01 11:46:06 - train.py[line:549] - INFO: 5000 / 14103
2022-10-01 11:46:06 - train.py[line:551] - INFO: load:1.54 valid_run:4800.12 task_valid:4667.43 collect_output:103.84
2022-10-01 11:49:16 - train.py[line:549] - INFO: 5200 / 14103
2022-10-01 11:49:16 - train.py[line:551] - INFO: load:1.56 valid_run:4989.38 task_valid:4851.36 collect_output:108.01
2022-10-01 11:52:29 - train.py[line:549] - INFO: 5400 / 14103
2022-10-01 11:52:29 - train.py[line:551] - INFO: load:1.59 valid_run:5182.28 task_valid:5040.78 collect_output:110.32
2022-10-01 11:55:41 - train.py[line:549] - INFO: 5600 / 14103
2022-10-01 11:55:41 - train.py[line:551] - INFO: load:1.61 valid_run:5374.09 task_valid:5228.43 collect_output:113.29
2022-10-01 11:58:54 - train.py[line:549] - INFO: 5800 / 14103
2022-10-01 11:58:54 - train.py[line:551] - INFO: load:1.63 valid_run:5567.69 task_valid:5415.71 collect_output:118.45
2022-10-01 12:02:07 - train.py[line:549] - INFO: 6000 / 14103
2022-10-01 12:02:07 - train.py[line:551] - INFO: load:1.68 valid_run:5760.20 task_valid:5600.95 collect_output:124.43
2022-10-01 12:05:19 - train.py[line:549] - INFO: 6200 / 14103
2022-10-01 12:05:19 - train.py[line:551] - INFO: load:1.74 valid_run:5952.68 task_valid:5787.54 collect_output:129.19
2022-10-01 12:08:31 - train.py[line:549] - INFO: 6400 / 14103
2022-10-01 12:08:31 - train.py[line:551] - INFO: load:1.77 valid_run:6144.65 task_valid:5975.13 collect_output:132.41
2022-10-01 12:11:40 - train.py[line:549] - INFO: 6600 / 14103
2022-10-01 12:11:40 - train.py[line:551] - INFO: load:1.79 valid_run:6333.61 task_valid:6157.10 collect_output:138.13
2022-10-01 12:14:55 - train.py[line:549] - INFO: 6800 / 14103
2022-10-01 12:14:55 - train.py[line:551] - INFO: load:1.81 valid_run:6528.54 task_valid:6344.01 collect_output:144.96
2022-10-01 12:18:09 - train.py[line:549] - INFO: 7000 / 14103
2022-10-01 12:18:09 - train.py[line:551] - INFO: load:1.84 valid_run:6722.02 task_valid:6533.86 collect_output:147.48
2022-10-01 12:21:24 - train.py[line:549] - INFO: 7200 / 14103
2022-10-01 12:21:24 - train.py[line:551] - INFO: load:1.86 valid_run:6916.72 task_valid:6725.27 collect_output:149.63
2022-10-01 12:24:35 - train.py[line:549] - INFO: 7400 / 14103
2022-10-01 12:24:35 - train.py[line:551] - INFO: load:1.90 valid_run:7108.36 task_valid:6909.04 collect_output:156.34
2022-10-01 12:27:44 - train.py[line:549] - INFO: 7600 / 14103
2022-10-01 12:27:44 - train.py[line:551] - INFO: load:1.92 valid_run:7296.66 task_valid:7090.21 collect_output:162.36
2022-10-01 12:31:00 - train.py[line:549] - INFO: 7800 / 14103
2022-10-01 12:31:00 - train.py[line:551] - INFO: load:1.95 valid_run:7493.28 task_valid:7279.77 collect_output:168.30
2022-10-01 12:34:10 - train.py[line:549] - INFO: 8000 / 14103
2022-10-01 12:34:10 - train.py[line:551] - INFO: load:1.97 valid_run:7682.32 task_valid:7459.51 collect_output:176.53
2022-10-01 12:37:18 - train.py[line:549] - INFO: 8200 / 14103
2022-10-01 12:37:18 - train.py[line:551] - INFO: load:2.00 valid_run:7870.84 task_valid:7643.10 collect_output:180.35
2022-10-01 12:40:27 - train.py[line:549] - INFO: 8400 / 14103
2022-10-01 12:40:27 - train.py[line:551] - INFO: load:2.02 valid_run:8059.16 task_valid:7825.87 collect_output:184.83
2022-10-01 12:43:37 - train.py[line:549] - INFO: 8600 / 14103
2022-10-01 12:43:37 - train.py[line:551] - INFO: load:2.04 valid_run:8249.37 task_valid:8010.50 collect_output:189.33
2022-10-01 12:46:48 - train.py[line:549] - INFO: 8800 / 14103
2022-10-01 12:46:48 - train.py[line:551] - INFO: load:2.07 valid_run:8440.74 task_valid:8198.48 collect_output:191.61
2022-10-01 12:49:58 - train.py[line:549] - INFO: 9000 / 14103
2022-10-01 12:49:58 - train.py[line:551] - INFO: load:2.09 valid_run:8630.82 task_valid:8381.93 collect_output:197.19
2022-10-01 12:53:15 - train.py[line:549] - INFO: 9200 / 14103
2022-10-01 12:53:15 - train.py[line:551] - INFO: load:2.12 valid_run:8827.76 task_valid:8567.88 collect_output:206.93
2022-10-01 12:56:33 - train.py[line:549] - INFO: 9400 / 14103
2022-10-01 12:56:33 - train.py[line:551] - INFO: load:2.15 valid_run:9025.39 task_valid:8759.69 collect_output:211.51
2022-10-01 13:01:33 - train.py[line:549] - INFO: 9600 / 14103
2022-10-01 13:01:33 - train.py[line:551] - INFO: load:2.17 valid_run:9325.21 task_valid:8954.94 collect_output:314.81
2022-10-01 13:04:46 - train.py[line:549] - INFO: 9800 / 14103
2022-10-01 13:04:46 - train.py[line:551] - INFO: load:2.20 valid_run:9518.46 task_valid:9145.61 collect_output:316.15
2022-10-01 13:08:00 - train.py[line:549] - INFO: 10000 / 14103
2022-10-01 13:08:00 - train.py[line:551] - INFO: load:2.26 valid_run:9711.99 task_valid:9335.66 collect_output:318.44
2022-10-01 13:11:11 - train.py[line:549] - INFO: 10200 / 14103
2022-10-01 13:11:11 - train.py[line:551] - INFO: load:2.29 valid_run:9903.16 task_valid:9519.83 collect_output:324.19
2022-10-01 13:14:26 - train.py[line:549] - INFO: 10400 / 14103
2022-10-01 13:14:26 - train.py[line:551] - INFO: load:2.31 valid_run:10098.01 task_valid:9706.21 collect_output:331.41
2022-10-01 13:17:40 - train.py[line:549] - INFO: 10600 / 14103
2022-10-01 13:17:40 - train.py[line:551] - INFO: load:2.33 valid_run:10292.06 task_valid:9889.52 collect_output:340.98
2022-10-01 13:20:54 - train.py[line:549] - INFO: 10800 / 14103
2022-10-01 13:20:54 - train.py[line:551] - INFO: load:2.36 valid_run:10485.77 task_valid:10075.60 collect_output:347.34
2022-10-01 13:24:07 - train.py[line:549] - INFO: 11000 / 14103
2022-10-01 13:24:07 - train.py[line:551] - INFO: load:2.38 valid_run:10679.06 task_valid:10261.96 collect_output:353.10
2022-10-01 13:27:20 - train.py[line:549] - INFO: 11200 / 14103
2022-10-01 13:27:20 - train.py[line:551] - INFO: load:2.41 valid_run:10871.23 task_valid:10447.04 collect_output:359.01
2022-10-01 13:30:34 - train.py[line:549] - INFO: 11400 / 14103
2022-10-01 13:30:34 - train.py[line:551] - INFO: load:2.44 valid_run:11066.00 task_valid:10638.49 collect_output:361.23
2022-10-01 13:33:46 - train.py[line:549] - INFO: 11600 / 14103
2022-10-01 13:33:46 - train.py[line:551] - INFO: load:2.46 valid_run:11257.83 task_valid:10825.13 collect_output:365.30
2022-10-01 13:36:58 - train.py[line:549] - INFO: 11800 / 14103
2022-10-01 13:36:58 - train.py[line:551] - INFO: load:2.48 valid_run:11449.54 task_valid:11011.82 collect_output:369.24
2022-10-01 13:40:09 - train.py[line:549] - INFO: 12000 / 14103
2022-10-01 13:40:09 - train.py[line:551] - INFO: load:2.51 valid_run:11640.31 task_valid:11196.59 collect_output:373.96
2022-10-01 13:43:19 - train.py[line:549] - INFO: 12200 / 14103
2022-10-01 13:43:19 - train.py[line:551] - INFO: load:2.53 valid_run:11830.83 task_valid:11381.91 collect_output:378.07
2022-10-01 13:46:31 - train.py[line:549] - INFO: 12400 / 14103
2022-10-01 13:46:31 - train.py[line:551] - INFO: load:2.56 valid_run:12022.64 task_valid:11567.95 collect_output:382.66
2022-10-01 13:49:41 - train.py[line:549] - INFO: 12600 / 14103
2022-10-01 13:49:41 - train.py[line:551] - INFO: load:2.58 valid_run:12212.71 task_valid:11751.43 collect_output:388.17
2022-10-01 13:52:49 - train.py[line:549] - INFO: 12800 / 14103
2022-10-01 13:52:49 - train.py[line:551] - INFO: load:2.60 valid_run:12399.70 task_valid:11933.71 collect_output:391.73
2022-10-01 13:55:59 - train.py[line:549] - INFO: 13000 / 14103
2022-10-01 13:55:59 - train.py[line:551] - INFO: load:2.63 valid_run:12589.71 task_valid:12117.48 collect_output:396.89
2022-10-01 13:59:11 - train.py[line:549] - INFO: 13200 / 14103
2022-10-01 13:59:11 - train.py[line:551] - INFO: load:2.65 valid_run:12781.78 task_valid:12300.26 collect_output:405.02
2022-10-01 14:02:25 - train.py[line:549] - INFO: 13400 / 14103
2022-10-01 14:02:25 - train.py[line:551] - INFO: load:2.68 valid_run:12976.22 task_valid:12483.63 collect_output:414.97
2022-10-01 14:05:36 - train.py[line:549] - INFO: 13600 / 14103
2022-10-01 14:05:36 - train.py[line:551] - INFO: load:2.70 valid_run:13166.54 task_valid:12664.82 collect_output:423.03
2022-10-01 14:08:51 - train.py[line:549] - INFO: 13800 / 14103
2022-10-01 14:08:51 - train.py[line:551] - INFO: load:2.73 valid_run:13361.79 task_valid:12852.90 collect_output:428.95
2022-10-01 14:12:11 - train.py[line:549] - INFO: 14000 / 14103
2022-10-01 14:12:11 - train.py[line:551] - INFO: load:2.77 valid_run:13561.47 task_valid:13043.31 collect_output:436.74
2022-10-01 14:13:50 - train.py[line:572] - INFO: scores:torch.Size([282060]) preds:torch.Size([282060]) sample_ids:torch.Size([282060])

====================================================================================================
SGG eval:     R @ 50: 0.6018;     R @ 100: 0.6316;     R @ 500: 0.6468;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1389;    mR @ 100: 0.1575;    mR @ 500: 0.1837;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0647) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.2756) (attached to:0.0000) (behind:0.2783) (belonging to:0.0000) (between:0.0000) (carrying:0.6726) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7217) (holding:0.2481) (in:0.3237) (in front of:0.0826) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4274) (of:0.4069) (on:0.9061) (on back of:0.0000) (over:0.0556) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.2500) (says:0.0000) (sitting on:0.1967) (standing on:0.0000) (to:0.0000) (under:0.2500) (using:1.0000) (walking in:0.0000) (walking on:0.0141) (watching:0.3333) (wearing:0.9856) (wears:0.0000) (with:0.0950) 
--------------------------------------------------------
====================================================================================================

2022-10-01 14:14:10 - train.py[line:486] - INFO: 0.6315775978553153

====================================================================================================
SGG eval:     R @ 50: 0.6018;     R @ 100: 0.6316;     R @ 500: 0.6468;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1389;    mR @ 100: 0.1575;    mR @ 500: 0.1837;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0647) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.2756) (attached to:0.0000) (behind:0.2783) (belonging to:0.0000) (between:0.0000) (carrying:0.6726) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7217) (holding:0.2481) (in:0.3237) (in front of:0.0826) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4274) (of:0.4069) (on:0.9061) (on back of:0.0000) (over:0.0556) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.2500) (says:0.0000) (sitting on:0.1967) (standing on:0.0000) (to:0.0000) (under:0.2500) (using:1.0000) (walking in:0.0000) (walking on:0.0141) (watching:0.3333) (wearing:0.9856) (wears:0.0000) (with:0.0950) 
--------------------------------------------------------
====================================================================================================

2022-10-01 14:14:10 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.324 | loss_v1 0 | loss_v2 0 | nll_loss 0.136 | ntokens 59.526 | nsentences 20 | sample_size 59.526 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.631578 | ppl 1.1 | vqa_score 0.9208 | wps 61.3 | wpb 59.5 | bsz 20 | num_updates 6000
2022-10-01 14:14:10 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 6000 updates
2022-10-01 14:14:10 - trainer.py[line:431] - INFO: Saving checkpoint to ./vqa_checkpoints/50_way_allcand/1_B20_A3_E5_0.04_5e-5_480/checkpoint_2_6000.pt
2022-10-01 14:14:16 - trainer.py[line:441] - INFO: Finished saving checkpoint to ./vqa_checkpoints/50_way_allcand/1_B20_A3_E5_0.04_5e-5_480/checkpoint_2_6000.pt
2022-10-01 14:14:24 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/50_way_allcand/1_B20_A3_E5_0.04_5e-5_480/checkpoint_2_6000.pt (epoch 2 @ 6000 updates, score 0.6315775978553153) (writing took 13.792728510918096 seconds)
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3514322
Killing subprocess 3514323
Main process received SIGINT, exiting
