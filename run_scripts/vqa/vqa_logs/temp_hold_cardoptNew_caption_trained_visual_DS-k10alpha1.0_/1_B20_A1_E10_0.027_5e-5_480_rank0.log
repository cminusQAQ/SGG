2023-02-19 17:35:29 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-02-19 17:35:29 - utils.py[line:261] - INFO: Start init
2023-02-19 17:35:29 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-02-19 17:35:29 - utils.py[line:261] - INFO: Start init
2023-02-19 17:35:30 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-02-19 17:35:30 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-02-19 17:35:30 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-02-19 17:35:30 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-02-19 17:35:41 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/temp_hold_cardoptNew_caption_trained_visual_DS-k10alpha1.0_', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 2000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/temp_hold_cardoptNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=10, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/temp_hold_cardoptNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480', save_interval=10, save_interval_updates=2000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/temp_hold_cardoptNew_caption_trained_visual_DS-k10alpha1.0_', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=2000, wandb_project=None, warmup_ratio=0.027, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.027, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-02-19 17:35:41 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-02-19 17:35:41 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-02-19 17:35:46 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-02-19 17:35:46 - train.py[line:118] - INFO: task: VqaGenTask
2023-02-19 17:35:46 - train.py[line:119] - INFO: model: OFAModel
2023-02-19 17:35:46 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-02-19 17:35:46 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-02-19 17:35:46 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-02-19 17:35:46 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-02-19 17:35:46 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-02-19 17:35:47 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-02-19 17:35:47 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-19 17:35:47 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-02-19 17:35:47 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
2023-02-19 17:35:48 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-02-19 17:35:48 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-02-19 17:35:48 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
Done 0.95 cuda cpu, cpu
2023-02-19 17:35:55 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-02-19 17:35:55 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-19 17:35:55 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-02-19 17:35:56 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-02-19 17:35:56 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-02-19 17:35:56 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-02-19 17:35:56 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv slice_id 0 row count 231280 total row count 462560
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_visualDS/query_optNew_caption_trained_visual_DS-k10alpha1.0_train_NA1_E0.tsv slice_id 1 row count 231280 total row count 462560
2023-02-19 17:35:57 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 115640, warmup steps 3122, warmup_factor 0.00032030749519538755
2023-02-19 17:35:58 - trainer.py[line:758] - INFO: begin training epoch 1
2023-02-19 17:35:58 - train.py[line:312] - INFO: Start iterating over samples
Total steps 115640, warmup steps 3122, warmup_factor 0.00032030749519538755
2023-02-19 17:36:19 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 11564 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=88.1, ups=0.78, wpb=112.7, bsz=40, num_updates=10, lr=1.60154e-07, gnorm=8.228, clip=100, loss_scale=128, train_wall=17, gb_free=11.1, ema_decay=0.9999, wall=32
2023-02-19 17:36:31 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 11564 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.507, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=96.2, ups=0.86, wpb=111.2, bsz=40, num_updates=20, lr=3.20307e-07, gnorm=9.314, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44
2023-02-19 17:36:42 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 11564 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=95.9, ups=0.86, wpb=111.7, bsz=40, num_updates=30, lr=4.80461e-07, gnorm=7.786, clip=100, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=56
2023-02-19 17:36:54 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 11564 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=95, ups=0.85, wpb=111.8, bsz=40, num_updates=40, lr=6.40615e-07, gnorm=8.353, clip=100, loss_scale=128, train_wall=12, gb_free=11, ema_decay=0.9999, wall=67
2023-02-19 17:37:06 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 11564 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=96.7, ups=0.87, wpb=111.4, bsz=40, num_updates=50, lr=8.00769e-07, gnorm=8.231, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=79
2023-02-19 17:37:17 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 11564 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=98.9, ups=0.87, wpb=113.4, bsz=40, num_updates=60, lr=9.60922e-07, gnorm=6.129, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=90
2023-02-19 17:37:29 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 11564 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.9, ups=0.87, wpb=112.7, bsz=40, num_updates=70, lr=1.12108e-06, gnorm=6.22, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=102
2023-02-19 17:37:40 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 11564 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=97.6, ups=0.87, wpb=112.1, bsz=40, num_updates=80, lr=1.28123e-06, gnorm=6.149, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=114
2023-02-19 17:37:52 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 11564 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=95.7, ups=0.85, wpb=112.8, bsz=40, num_updates=90, lr=1.44138e-06, gnorm=4.797, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=125
2023-02-19 17:38:03 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 11564 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=98.3, ups=0.88, wpb=111.3, bsz=40, num_updates=100, lr=1.60154e-06, gnorm=4.432, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=137
2023-02-19 17:38:15 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 11564 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=100, ups=0.88, wpb=113.1, bsz=40, num_updates=110, lr=1.76169e-06, gnorm=3.828, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=148
2023-02-19 17:38:26 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 11564 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.5, ups=0.88, wpb=111.4, bsz=40, num_updates=120, lr=1.92184e-06, gnorm=3.669, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=159
2023-02-19 17:38:37 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 11564 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=97.8, ups=0.88, wpb=111.4, bsz=40, num_updates=130, lr=2.082e-06, gnorm=3.436, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=171
2023-02-19 17:38:49 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 11564 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=97.6, ups=0.87, wpb=112.8, bsz=40, num_updates=140, lr=2.24215e-06, gnorm=2.847, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=182
2023-02-19 17:39:00 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 11564 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=100, ups=0.89, wpb=111.7, bsz=40, num_updates=150, lr=2.40231e-06, gnorm=3.085, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=194
2023-02-19 17:39:11 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 11564 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=103.2, ups=0.92, wpb=111.9, bsz=40, num_updates=160, lr=2.56246e-06, gnorm=2.659, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=204
2023-02-19 17:39:22 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 11564 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.8, ups=0.9, wpb=111.4, bsz=40, num_updates=170, lr=2.72261e-06, gnorm=2.557, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=216
2023-02-19 17:39:33 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 11564 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.3, ups=0.89, wpb=112, bsz=40, num_updates=180, lr=2.88277e-06, gnorm=2.692, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=227
2023-02-19 17:39:45 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 11564 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=98.2, ups=0.88, wpb=111.7, bsz=40, num_updates=190, lr=3.04292e-06, gnorm=2.531, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=238
2023-02-19 17:39:56 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 11564 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=101.3, ups=0.9, wpb=112.5, bsz=40, num_updates=200, lr=3.20307e-06, gnorm=2.504, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=249
2023-02-19 17:40:07 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 11564 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=99.6, ups=0.88, wpb=112.7, bsz=40, num_updates=210, lr=3.36323e-06, gnorm=2.374, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=261
2023-02-19 17:40:18 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 11564 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.7, ups=0.9, wpb=112.4, bsz=40, num_updates=220, lr=3.52338e-06, gnorm=2.164, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=272
2023-02-19 17:40:30 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 11564 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=99.7, ups=0.9, wpb=110.9, bsz=40, num_updates=230, lr=3.68354e-06, gnorm=2.529, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=283
2023-02-19 17:40:41 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 11564 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=97.1, ups=0.86, wpb=112.9, bsz=40, num_updates=240, lr=3.84369e-06, gnorm=2.336, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=295
2023-02-19 17:40:52 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 11564 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=103, ups=0.92, wpb=112.1, bsz=40, num_updates=250, lr=4.00384e-06, gnorm=2.164, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=305
2023-02-19 17:41:03 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 11564 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=102, ups=0.91, wpb=112.5, bsz=40, num_updates=260, lr=4.164e-06, gnorm=2.327, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=317
2023-02-19 17:41:14 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 11564 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, wps=100.7, ups=0.91, wpb=111.2, bsz=40, num_updates=270, lr=4.32415e-06, gnorm=2.376, clip=100, loss_scale=128, train_wall=11, gb_free=10, ema_decay=0.9999, wall=328
2023-02-19 17:41:25 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 11564 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.6, ups=0.91, wpb=112.2, bsz=40, num_updates=280, lr=4.4843e-06, gnorm=2.053, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=339
2023-02-19 17:41:37 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 11564 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.15, wps=96.3, ups=0.86, wpb=111.7, bsz=40, num_updates=290, lr=4.64446e-06, gnorm=2.27, clip=100, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=350
2023-02-19 17:41:48 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 11564 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101, ups=0.9, wpb=112.7, bsz=40, num_updates=300, lr=4.80461e-06, gnorm=2.135, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=361
2023-02-19 17:41:59 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 11564 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.8, ups=0.87, wpb=112.1, bsz=40, num_updates=310, lr=4.96477e-06, gnorm=1.982, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=373
2023-02-19 17:42:11 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 11564 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=101.8, ups=0.9, wpb=113.2, bsz=40, num_updates=320, lr=5.12492e-06, gnorm=1.982, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=384
2023-02-19 17:42:22 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 11564 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=99.7, ups=0.89, wpb=112.4, bsz=40, num_updates=330, lr=5.28507e-06, gnorm=2.005, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=395
2023-02-19 17:42:33 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 11564 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=100.2, ups=0.89, wpb=112.1, bsz=40, num_updates=340, lr=5.44523e-06, gnorm=1.877, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=406
2023-02-19 17:42:44 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 11564 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98, ups=0.88, wpb=111.9, bsz=40, num_updates=350, lr=5.60538e-06, gnorm=1.701, clip=100, loss_scale=128, train_wall=11, gb_free=11, ema_decay=0.9999, wall=418
2023-02-19 17:42:56 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 11564 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=98, ups=0.88, wpb=111.1, bsz=40, num_updates=360, lr=5.76553e-06, gnorm=2.035, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=429
2023-02-19 17:43:07 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 11564 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, wps=101.6, ups=0.9, wpb=112.6, bsz=40, num_updates=370, lr=5.92569e-06, gnorm=2.063, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=440
2023-02-19 17:43:18 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 11564 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.13, wps=99.9, ups=0.88, wpb=112.9, bsz=40, num_updates=380, lr=6.08584e-06, gnorm=1.869, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=452
2023-02-19 17:43:29 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 11564 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=101.6, ups=0.91, wpb=111.4, bsz=40, num_updates=390, lr=6.246e-06, gnorm=2.011, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=463
2023-02-19 17:43:41 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 11564 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=97.8, ups=0.87, wpb=112, bsz=40, num_updates=400, lr=6.40615e-06, gnorm=1.799, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=474
2023-02-19 17:43:52 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 11564 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.4, ups=0.89, wpb=111.7, bsz=40, num_updates=410, lr=6.5663e-06, gnorm=1.649, clip=90, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=485
2023-02-19 17:44:03 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 11564 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=98.6, ups=0.88, wpb=111.7, bsz=40, num_updates=420, lr=6.72646e-06, gnorm=1.646, clip=90, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=497
2023-02-19 17:44:14 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 11564 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=100.7, ups=0.9, wpb=112.3, bsz=40, num_updates=430, lr=6.88661e-06, gnorm=1.611, clip=90, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=508
2023-02-19 17:44:26 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 11564 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.12, wps=98.1, ups=0.88, wpb=111.1, bsz=40, num_updates=440, lr=7.04676e-06, gnorm=1.632, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=519
2023-02-19 17:44:37 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 11564 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.5, ups=0.89, wpb=111.7, bsz=40, num_updates=450, lr=7.20692e-06, gnorm=1.536, clip=90, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=530
2023-02-19 17:44:48 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 11564 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.1, ups=0.89, wpb=111.9, bsz=40, num_updates=460, lr=7.36707e-06, gnorm=1.618, clip=90, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=542
2023-02-19 17:44:59 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 11564 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.2, ups=0.9, wpb=113, bsz=40, num_updates=470, lr=7.52723e-06, gnorm=1.585, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=553
2023-02-19 17:45:11 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 11564 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.88, wpb=113.3, bsz=40, num_updates=480, lr=7.68738e-06, gnorm=1.661, clip=90, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=564
2023-02-19 17:45:22 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 11564 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=102.9, ups=0.92, wpb=111.9, bsz=40, num_updates=490, lr=7.84753e-06, gnorm=1.983, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=575
2023-02-19 17:45:33 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 11564 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=102, ups=0.9, wpb=113.3, bsz=40, num_updates=500, lr=8.00769e-06, gnorm=1.977, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=586
2023-02-19 17:45:44 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 11564 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=96.2, ups=0.87, wpb=111, bsz=40, num_updates=510, lr=8.16784e-06, gnorm=1.807, clip=100, loss_scale=128, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=598
2023-02-19 17:45:55 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 11564 loss=0.26, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102, ups=0.91, wpb=112.2, bsz=40, num_updates=520, lr=8.32799e-06, gnorm=1.71, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=609
2023-02-19 17:46:07 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 11564 loss=0.269, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.4, ups=0.89, wpb=112, bsz=40, num_updates=530, lr=8.48815e-06, gnorm=1.824, clip=100, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=620
2023-02-19 17:46:18 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 11564 loss=0.264, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=95.1, ups=0.86, wpb=110.4, bsz=40, num_updates=540, lr=8.6483e-06, gnorm=1.716, clip=90, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=632
2023-02-19 17:46:29 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 11564 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=103, ups=0.92, wpb=112.3, bsz=40, num_updates=550, lr=8.80846e-06, gnorm=1.635, clip=90, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=643
2023-02-19 17:46:40 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 11564 loss=0.253, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.1, ups=0.89, wpb=112.2, bsz=40, num_updates=560, lr=8.96861e-06, gnorm=1.384, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=654
2023-02-19 17:46:52 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 11564 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.7, ups=0.86, wpb=112.4, bsz=40, num_updates=570, lr=9.12876e-06, gnorm=1.605, clip=90, loss_scale=256, train_wall=12, gb_free=11, ema_decay=0.9999, wall=665
2023-02-19 17:47:03 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 11564 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=101.4, ups=0.89, wpb=113.5, bsz=40, num_updates=580, lr=9.28892e-06, gnorm=1.576, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=677
2023-02-19 17:47:15 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 11564 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=99.2, ups=0.88, wpb=112.1, bsz=40, num_updates=590, lr=9.44907e-06, gnorm=1.816, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=688
2023-02-19 17:47:26 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 11564 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.6, ups=0.89, wpb=112, bsz=40, num_updates=600, lr=9.60922e-06, gnorm=1.721, clip=100, loss_scale=256, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=699
2023-02-19 17:47:37 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 11564 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.5, ups=0.87, wpb=112.1, bsz=40, num_updates=610, lr=9.76938e-06, gnorm=1.382, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=711
2023-02-19 17:47:48 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 11564 loss=0.268, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=102.6, ups=0.92, wpb=112, bsz=40, num_updates=620, lr=9.92953e-06, gnorm=1.436, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=722
2023-02-19 17:48:00 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 11564 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.6, ups=0.87, wpb=112.3, bsz=40, num_updates=630, lr=1.00897e-05, gnorm=1.536, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=733
2023-02-19 17:48:11 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 11564 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.11, wps=100.1, ups=0.89, wpb=113, bsz=40, num_updates=640, lr=1.02498e-05, gnorm=1.613, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=744
2023-02-19 17:48:22 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 11564 loss=0.257, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.9, ups=0.88, wpb=111.1, bsz=40, num_updates=650, lr=1.041e-05, gnorm=1.349, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=756
2023-02-19 17:48:34 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 11564 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.2, ups=0.86, wpb=113.5, bsz=40, num_updates=660, lr=1.05701e-05, gnorm=1.478, clip=100, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=767
2023-02-19 17:48:46 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 11564 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97, ups=0.87, wpb=112, bsz=40, num_updates=670, lr=1.07303e-05, gnorm=1.345, clip=80, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=779
2023-02-19 17:48:57 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 11564 loss=0.258, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=97.3, ups=0.88, wpb=110.8, bsz=40, num_updates=680, lr=1.08905e-05, gnorm=1.571, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=790
2023-02-19 17:49:08 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 11564 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.88, wpb=112.3, bsz=40, num_updates=690, lr=1.10506e-05, gnorm=1.26, clip=90, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=802
2023-02-19 17:49:20 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 11564 loss=0.262, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=96.9, ups=0.87, wpb=110.8, bsz=40, num_updates=700, lr=1.12108e-05, gnorm=1.505, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=813
2023-02-19 17:49:31 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 11564 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.9, ups=0.88, wpb=110.7, bsz=40, num_updates=710, lr=1.13709e-05, gnorm=1.412, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=825
2023-02-19 17:49:42 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 11564 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.91, wpb=112.6, bsz=40, num_updates=720, lr=1.15311e-05, gnorm=1.377, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=836
2023-02-19 17:49:53 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 11564 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=100.1, ups=0.9, wpb=111.8, bsz=40, num_updates=730, lr=1.16912e-05, gnorm=1.426, clip=90, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=847
2023-02-19 17:50:04 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.5, ups=0.91, wpb=110.7, bsz=40, num_updates=740, lr=1.18514e-05, gnorm=1.281, clip=70, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=858
2023-02-19 17:50:16 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 11564 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.9, ups=0.89, wpb=112.3, bsz=40, num_updates=750, lr=1.20115e-05, gnorm=1.394, clip=90, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=869
2023-02-19 17:50:27 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 11564 loss=0.265, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.2, ups=0.88, wpb=113, bsz=40, num_updates=760, lr=1.21717e-05, gnorm=1.314, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=880
2023-02-19 17:50:38 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 11564 loss=0.254, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.9, ups=0.89, wpb=112, bsz=40, num_updates=770, lr=1.23318e-05, gnorm=1.407, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=892
2023-02-19 17:50:50 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.87, wpb=112, bsz=40, num_updates=780, lr=1.2492e-05, gnorm=1.294, clip=90, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=903
2023-02-19 17:51:00 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 11564 loss=0.261, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=105, ups=0.93, wpb=112.4, bsz=40, num_updates=790, lr=1.26521e-05, gnorm=1.596, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=914
2023-02-19 17:51:11 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 11564 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.5, ups=0.93, wpb=112.2, bsz=40, num_updates=800, lr=1.28123e-05, gnorm=1.231, clip=70, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=925
2023-02-19 17:51:22 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.9, wpb=113.1, bsz=40, num_updates=810, lr=1.29725e-05, gnorm=1.231, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=936
2023-02-19 17:51:33 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 11564 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.7, ups=0.94, wpb=111.1, bsz=40, num_updates=820, lr=1.31326e-05, gnorm=1.376, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=946
2023-02-19 17:51:44 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 11564 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.88, wpb=113.1, bsz=40, num_updates=830, lr=1.32928e-05, gnorm=1.371, clip=80, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=958
2023-02-19 17:51:56 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=113.7, nsentences=40, sample_size=113.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.89, wpb=113.7, bsz=40, num_updates=840, lr=1.34529e-05, gnorm=1.24, clip=70, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=969
2023-02-19 17:52:07 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 11564 loss=0.252, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.4, ups=0.89, wpb=111.1, bsz=40, num_updates=850, lr=1.36131e-05, gnorm=1.386, clip=70, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=980
2023-02-19 17:52:19 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.3, ups=0.85, wpb=111.6, bsz=40, num_updates=860, lr=1.37732e-05, gnorm=1.153, clip=70, loss_scale=256, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=992
2023-02-19 17:52:30 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.87, wpb=113.3, bsz=40, num_updates=870, lr=1.39334e-05, gnorm=1.347, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1004
2023-02-19 17:52:41 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.7, ups=0.91, wpb=112, bsz=40, num_updates=880, lr=1.40935e-05, gnorm=1.298, clip=90, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1015
2023-02-19 17:52:52 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.3, ups=0.91, wpb=111.8, bsz=40, num_updates=890, lr=1.42537e-05, gnorm=1.212, clip=80, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1026
2023-02-19 17:53:04 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.3, ups=0.88, wpb=111.9, bsz=40, num_updates=900, lr=1.44138e-05, gnorm=1.236, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1037
2023-02-19 17:53:15 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.87, wpb=111.4, bsz=40, num_updates=910, lr=1.4574e-05, gnorm=1.105, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1049
2023-02-19 17:53:26 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.2, ups=0.9, wpb=112.1, bsz=40, num_updates=920, lr=1.47341e-05, gnorm=1.205, clip=70, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1060
2023-02-19 17:53:38 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.88, wpb=111.9, bsz=40, num_updates=930, lr=1.48943e-05, gnorm=1.021, clip=60, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1071
2023-02-19 17:53:49 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.3, nsentences=40, sample_size=110.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.9, wpb=110.3, bsz=40, num_updates=940, lr=1.50545e-05, gnorm=1.08, clip=70, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1082
2023-02-19 17:54:00 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.88, wpb=111.9, bsz=40, num_updates=950, lr=1.52146e-05, gnorm=1.177, clip=70, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1093
2023-02-19 17:54:11 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.88, wpb=113, bsz=40, num_updates=960, lr=1.53748e-05, gnorm=1.148, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1105
2023-02-19 17:54:23 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.89, wpb=112.5, bsz=40, num_updates=970, lr=1.55349e-05, gnorm=1.044, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1116
2023-02-19 17:54:34 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 11564 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.88, wpb=112.6, bsz=40, num_updates=980, lr=1.56951e-05, gnorm=1.206, clip=70, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1128
2023-02-19 17:54:45 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=101, ups=0.91, wpb=111.6, bsz=40, num_updates=990, lr=1.58552e-05, gnorm=1.256, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1139
2023-02-19 17:54:56 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.3, bsz=40, num_updates=1000, lr=1.60154e-05, gnorm=1.002, clip=40, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1150
2023-02-19 17:55:08 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 11564 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.88, wpb=112.8, bsz=40, num_updates=1010, lr=1.61755e-05, gnorm=1.075, clip=60, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1161
2023-02-19 17:55:19 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.8, ups=0.92, wpb=111.9, bsz=40, num_updates=1020, lr=1.63357e-05, gnorm=1.016, clip=40, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1172
2023-02-19 17:55:30 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 11564 loss=0.259, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=99.8, ups=0.89, wpb=111.6, bsz=40, num_updates=1030, lr=1.64958e-05, gnorm=1.31, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1183
2023-02-19 17:55:41 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 11564 loss=0.248, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=114, nsentences=40, sample_size=114, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=105, ups=0.92, wpb=114, bsz=40, num_updates=1040, lr=1.6656e-05, gnorm=1.14, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1194
2023-02-19 17:55:52 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.1, ups=0.87, wpb=112.7, bsz=40, num_updates=1050, lr=1.68161e-05, gnorm=1.176, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1206
2023-02-19 17:56:03 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 11564 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.9, ups=0.92, wpb=110.8, bsz=40, num_updates=1060, lr=1.69763e-05, gnorm=1.087, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1217
2023-02-19 17:56:14 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.2, ups=0.9, wpb=113.4, bsz=40, num_updates=1070, lr=1.71365e-05, gnorm=1.164, clip=60, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=1228
2023-02-19 17:56:25 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=102.1, ups=0.91, wpb=112.2, bsz=40, num_updates=1080, lr=1.72966e-05, gnorm=1.188, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1239
2023-02-19 17:56:37 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 11564 loss=0.263, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, wps=99.5, ups=0.89, wpb=112, bsz=40, num_updates=1090, lr=1.74568e-05, gnorm=1.322, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1250
2023-02-19 17:56:48 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.9, wpb=112.2, bsz=40, num_updates=1100, lr=1.76169e-05, gnorm=1.04, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1261
2023-02-19 17:56:59 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.89, wpb=111.9, bsz=40, num_updates=1110, lr=1.77771e-05, gnorm=0.916, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1272
2023-02-19 17:57:10 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.6, ups=0.89, wpb=111.9, bsz=40, num_updates=1120, lr=1.79372e-05, gnorm=1.015, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1284
2023-02-19 17:57:21 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.89, wpb=111.4, bsz=40, num_updates=1130, lr=1.80974e-05, gnorm=1.123, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1295
2023-02-19 17:57:33 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.6, ups=0.88, wpb=111.5, bsz=40, num_updates=1140, lr=1.82575e-05, gnorm=1.105, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1306
2023-02-19 17:57:44 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.2, ups=0.88, wpb=111, bsz=40, num_updates=1150, lr=1.84177e-05, gnorm=0.925, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1318
2023-02-19 17:57:56 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=111.6, bsz=40, num_updates=1160, lr=1.85778e-05, gnorm=0.943, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1329
2023-02-19 17:58:07 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102, ups=0.91, wpb=112, bsz=40, num_updates=1170, lr=1.8738e-05, gnorm=1.109, clip=50, loss_scale=512, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=1340
2023-02-19 17:58:18 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.9, wpb=112.6, bsz=40, num_updates=1180, lr=1.88981e-05, gnorm=1.003, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1351
2023-02-19 17:58:29 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.6, ups=0.87, wpb=111.6, bsz=40, num_updates=1190, lr=1.90583e-05, gnorm=1.28, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1362
2023-02-19 17:58:41 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 11564 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.5, ups=0.87, wpb=112.1, bsz=40, num_updates=1200, lr=1.92184e-05, gnorm=1.004, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1374
2023-02-19 17:58:51 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103, ups=0.92, wpb=112.4, bsz=40, num_updates=1210, lr=1.93786e-05, gnorm=1.031, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1385
2023-02-19 17:59:03 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.9, ups=0.9, wpb=111.5, bsz=40, num_updates=1220, lr=1.95388e-05, gnorm=1.076, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1396
2023-02-19 17:59:14 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.9, wpb=112.1, bsz=40, num_updates=1230, lr=1.96989e-05, gnorm=1.007, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1407
2023-02-19 17:59:25 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.9, wpb=112.6, bsz=40, num_updates=1240, lr=1.98591e-05, gnorm=0.826, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1418
2023-02-19 17:59:36 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 11564 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.88, wpb=112.6, bsz=40, num_updates=1250, lr=2.00192e-05, gnorm=1.085, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1430
2023-02-19 17:59:48 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.5, ups=0.88, wpb=112.9, bsz=40, num_updates=1260, lr=2.01794e-05, gnorm=0.799, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1441
2023-02-19 17:59:59 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.5, bsz=40, num_updates=1270, lr=2.03395e-05, gnorm=0.925, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1452
2023-02-19 18:00:10 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.89, wpb=110.7, bsz=40, num_updates=1280, lr=2.04997e-05, gnorm=1.062, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1463
2023-02-19 18:00:21 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.8, ups=0.92, wpb=112.7, bsz=40, num_updates=1290, lr=2.06598e-05, gnorm=0.964, clip=40, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=1474
2023-02-19 18:00:32 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.89, wpb=111.5, bsz=40, num_updates=1300, lr=2.082e-05, gnorm=0.919, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1486
2023-02-19 18:00:43 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.9, wpb=111.7, bsz=40, num_updates=1310, lr=2.09801e-05, gnorm=1.159, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1497
2023-02-19 18:00:54 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112, bsz=40, num_updates=1320, lr=2.11403e-05, gnorm=1.015, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1508
2023-02-19 18:01:06 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.7, ups=0.87, wpb=111.7, bsz=40, num_updates=1330, lr=2.13004e-05, gnorm=0.868, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1519
2023-02-19 18:01:17 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 11564 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.6, ups=0.89, wpb=111.1, bsz=40, num_updates=1340, lr=2.14606e-05, gnorm=1.327, clip=90, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1531
2023-02-19 18:01:29 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 11564 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.3, ups=0.87, wpb=112.5, bsz=40, num_updates=1350, lr=2.16208e-05, gnorm=1.008, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1542
2023-02-19 18:01:40 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.9, ups=0.91, wpb=111.6, bsz=40, num_updates=1360, lr=2.17809e-05, gnorm=1.115, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1553
2023-02-19 18:01:51 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.89, wpb=110.5, bsz=40, num_updates=1370, lr=2.19411e-05, gnorm=1.111, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1564
2023-02-19 18:02:02 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.89, wpb=112.4, bsz=40, num_updates=1380, lr=2.21012e-05, gnorm=0.919, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1576
2023-02-19 18:02:13 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.9, wpb=110.6, bsz=40, num_updates=1390, lr=2.22614e-05, gnorm=1.082, clip=60, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1587
2023-02-19 18:02:25 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 11564 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.89, wpb=112, bsz=40, num_updates=1400, lr=2.24215e-05, gnorm=1.109, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1598
2023-02-19 18:02:36 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.89, wpb=112, bsz=40, num_updates=1410, lr=2.25817e-05, gnorm=0.983, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1609
2023-02-19 18:02:47 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 11564 loss=0.256, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.4, ups=0.87, wpb=111.4, bsz=40, num_updates=1420, lr=2.27418e-05, gnorm=1.028, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=1621
2023-02-19 18:02:59 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.88, wpb=112.7, bsz=40, num_updates=1430, lr=2.2902e-05, gnorm=0.753, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1632
2023-02-19 18:03:10 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 11564 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.8, ups=0.88, wpb=111.9, bsz=40, num_updates=1440, lr=2.30621e-05, gnorm=0.839, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1643
2023-02-19 18:03:22 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.87, wpb=113, bsz=40, num_updates=1450, lr=2.32223e-05, gnorm=0.97, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1655
2023-02-19 18:03:33 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.7, ups=0.89, wpb=110.2, bsz=40, num_updates=1460, lr=2.33824e-05, gnorm=0.87, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1666
2023-02-19 18:03:44 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.88, wpb=111.5, bsz=40, num_updates=1470, lr=2.35426e-05, gnorm=0.843, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1678
2023-02-19 18:03:56 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.88, wpb=112.5, bsz=40, num_updates=1480, lr=2.37028e-05, gnorm=1.019, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1689
2023-02-19 18:04:07 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=111.2, bsz=40, num_updates=1490, lr=2.38629e-05, gnorm=0.926, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1700
2023-02-19 18:04:18 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=111.6, bsz=40, num_updates=1500, lr=2.40231e-05, gnorm=0.961, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1711
2023-02-19 18:04:29 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.9, wpb=111.8, bsz=40, num_updates=1510, lr=2.41832e-05, gnorm=1.097, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1722
2023-02-19 18:04:40 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=1520, lr=2.43434e-05, gnorm=1.079, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1734
2023-02-19 18:04:52 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 11564 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.88, wpb=112.7, bsz=40, num_updates=1530, lr=2.45035e-05, gnorm=1.136, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1745
2023-02-19 18:05:03 - progress_bar.py[line:274] - INFO: epoch 001:   1540 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.3, ups=0.86, wpb=112.7, bsz=40, num_updates=1540, lr=2.46637e-05, gnorm=1.029, clip=50, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1757
2023-02-19 18:05:15 - progress_bar.py[line:274] - INFO: epoch 001:   1550 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.9, ups=0.88, wpb=110.8, bsz=40, num_updates=1550, lr=2.48238e-05, gnorm=0.755, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1768
2023-02-19 18:05:26 - progress_bar.py[line:274] - INFO: epoch 001:   1560 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=109.9, nsentences=40, sample_size=109.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.7, ups=0.88, wpb=109.9, bsz=40, num_updates=1560, lr=2.4984e-05, gnorm=1.065, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1779
2023-02-19 18:05:37 - progress_bar.py[line:274] - INFO: epoch 001:   1570 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.9, wpb=111.3, bsz=40, num_updates=1570, lr=2.51441e-05, gnorm=1.124, clip=70, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1790
2023-02-19 18:05:48 - progress_bar.py[line:274] - INFO: epoch 001:   1580 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.89, wpb=113.6, bsz=40, num_updates=1580, lr=2.53043e-05, gnorm=0.758, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1802
2023-02-19 18:05:52 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-19 18:06:01 - progress_bar.py[line:274] - INFO: epoch 001:   1591 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=90.6, ups=0.81, wpb=111.5, bsz=40, num_updates=1590, lr=2.54644e-05, gnorm=0.912, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1814
2023-02-19 18:06:12 - progress_bar.py[line:274] - INFO: epoch 001:   1601 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.88, wpb=112.2, bsz=40, num_updates=1600, lr=2.56246e-05, gnorm=0.974, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1825
2023-02-19 18:06:23 - progress_bar.py[line:274] - INFO: epoch 001:   1611 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.7, ups=0.89, wpb=112.7, bsz=40, num_updates=1610, lr=2.57848e-05, gnorm=0.794, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1837
2023-02-19 18:06:34 - progress_bar.py[line:274] - INFO: epoch 001:   1621 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=111.9, bsz=40, num_updates=1620, lr=2.59449e-05, gnorm=0.809, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1848
2023-02-19 18:06:46 - progress_bar.py[line:274] - INFO: epoch 001:   1631 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.88, wpb=112.8, bsz=40, num_updates=1630, lr=2.61051e-05, gnorm=0.797, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1859
2023-02-19 18:06:57 - progress_bar.py[line:274] - INFO: epoch 001:   1641 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.89, wpb=112.3, bsz=40, num_updates=1640, lr=2.62652e-05, gnorm=1.021, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1870
2023-02-19 18:07:08 - progress_bar.py[line:274] - INFO: epoch 001:   1651 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102, ups=0.9, wpb=113.5, bsz=40, num_updates=1650, lr=2.64254e-05, gnorm=0.803, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1882
2023-02-19 18:07:20 - progress_bar.py[line:274] - INFO: epoch 001:   1661 / 11564 loss=0.246, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.1, ups=0.87, wpb=112.1, bsz=40, num_updates=1660, lr=2.65855e-05, gnorm=1.061, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=1893
2023-02-19 18:07:31 - progress_bar.py[line:274] - INFO: epoch 001:   1671 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.89, wpb=112.7, bsz=40, num_updates=1670, lr=2.67457e-05, gnorm=0.895, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1904
2023-02-19 18:07:42 - progress_bar.py[line:274] - INFO: epoch 001:   1681 / 11564 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.4, ups=0.9, wpb=111.9, bsz=40, num_updates=1680, lr=2.69058e-05, gnorm=0.948, clip=60, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1915
2023-02-19 18:07:53 - progress_bar.py[line:274] - INFO: epoch 001:   1691 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=1690, lr=2.7066e-05, gnorm=0.948, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1927
2023-02-19 18:08:04 - progress_bar.py[line:274] - INFO: epoch 001:   1701 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=104.1, ups=0.92, wpb=112.8, bsz=40, num_updates=1700, lr=2.72261e-05, gnorm=0.859, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1937
2023-02-19 18:08:15 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103, ups=0.92, wpb=112.5, bsz=40, num_updates=1710, lr=2.73863e-05, gnorm=0.616, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1948
2023-02-19 18:08:26 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 11564 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99, ups=0.89, wpb=111.8, bsz=40, num_updates=1720, lr=2.75464e-05, gnorm=0.834, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1960
2023-02-19 18:08:38 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.3, ups=0.86, wpb=112.3, bsz=40, num_updates=1730, lr=2.77066e-05, gnorm=0.815, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1971
2023-02-19 18:08:49 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.89, wpb=112.2, bsz=40, num_updates=1740, lr=2.78668e-05, gnorm=0.921, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1983
2023-02-19 18:09:00 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.4, ups=0.91, wpb=109.6, bsz=40, num_updates=1750, lr=2.80269e-05, gnorm=0.853, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1994
2023-02-19 18:09:11 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.89, wpb=112.9, bsz=40, num_updates=1760, lr=2.81871e-05, gnorm=0.751, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2005
2023-02-19 18:09:23 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112.1, bsz=40, num_updates=1770, lr=2.83472e-05, gnorm=0.812, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2016
2023-02-19 18:09:34 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.88, wpb=112.5, bsz=40, num_updates=1780, lr=2.85074e-05, gnorm=0.902, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2027
2023-02-19 18:09:45 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=111.7, bsz=40, num_updates=1790, lr=2.86675e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2039
2023-02-19 18:09:57 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.88, wpb=113.4, bsz=40, num_updates=1800, lr=2.88277e-05, gnorm=0.822, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2050
2023-02-19 18:10:08 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.2, ups=0.86, wpb=112.1, bsz=40, num_updates=1810, lr=2.89878e-05, gnorm=0.75, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2062
2023-02-19 18:10:19 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.6, ups=0.91, wpb=112.8, bsz=40, num_updates=1820, lr=2.9148e-05, gnorm=0.634, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2073
2023-02-19 18:10:31 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.88, wpb=112.4, bsz=40, num_updates=1830, lr=2.93081e-05, gnorm=0.829, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2084
2023-02-19 18:10:42 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 11564 loss=0.243, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.89, wpb=110.7, bsz=40, num_updates=1840, lr=2.94683e-05, gnorm=0.957, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2095
2023-02-19 18:10:53 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.89, wpb=111.9, bsz=40, num_updates=1850, lr=2.96284e-05, gnorm=0.859, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2107
2023-02-19 18:11:05 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.88, wpb=111.9, bsz=40, num_updates=1860, lr=2.97886e-05, gnorm=0.834, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2118
2023-02-19 18:11:16 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=94.6, ups=0.85, wpb=110.8, bsz=40, num_updates=1870, lr=2.99488e-05, gnorm=0.856, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2130
2023-02-19 18:11:28 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.5, ups=0.89, wpb=111.2, bsz=40, num_updates=1880, lr=3.01089e-05, gnorm=0.996, clip=30, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=2141
2023-02-19 18:11:39 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=114.5, nsentences=40, sample_size=114.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.1, ups=0.89, wpb=114.5, bsz=40, num_updates=1890, lr=3.02691e-05, gnorm=0.734, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2152
2023-02-19 18:11:50 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.4, ups=0.87, wpb=112.1, bsz=40, num_updates=1900, lr=3.04292e-05, gnorm=0.914, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=2164
2023-02-19 18:12:02 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.88, wpb=110.8, bsz=40, num_updates=1910, lr=3.05894e-05, gnorm=0.896, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2175
2023-02-19 18:12:13 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 11564 loss=0.244, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.4, ups=0.89, wpb=110.8, bsz=40, num_updates=1920, lr=3.07495e-05, gnorm=1.066, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2186
2023-02-19 18:12:24 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.9, wpb=112.5, bsz=40, num_updates=1930, lr=3.09097e-05, gnorm=0.819, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2198
2023-02-19 18:12:36 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.88, wpb=112.2, bsz=40, num_updates=1940, lr=3.10698e-05, gnorm=0.925, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2209
2023-02-19 18:12:46 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.1, ups=0.92, wpb=112.3, bsz=40, num_updates=1950, lr=3.123e-05, gnorm=0.751, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2220
2023-02-19 18:12:57 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.2, ups=0.91, wpb=113.4, bsz=40, num_updates=1960, lr=3.13901e-05, gnorm=0.978, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2231
2023-02-19 18:13:09 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.88, wpb=112, bsz=40, num_updates=1970, lr=3.15503e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2242
2023-02-19 18:13:20 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.5, ups=0.88, wpb=111.5, bsz=40, num_updates=1980, lr=3.17104e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=2254
2023-02-19 18:13:32 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.87, wpb=111.7, bsz=40, num_updates=1990, lr=3.18706e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2265
2023-02-19 18:13:43 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.91, wpb=113.2, bsz=40, num_updates=2000, lr=3.20307e-05, gnorm=0.862, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2276
2023-02-19 18:13:43 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-19 18:13:43 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-02-19 18:13:44 - train.py[line:549] - INFO: 0 / 6234
2023-02-19 18:13:44 - train.py[line:551] - INFO: load:0.94 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-19 18:15:51 - train.py[line:549] - INFO: 200 / 6234
2023-02-19 18:15:51 - train.py[line:551] - INFO: load:0.97 valid_run:126.50 task_valid:121.73 collect_output:3.51
2023-02-19 18:17:52 - train.py[line:549] - INFO: 400 / 6234
2023-02-19 18:17:52 - train.py[line:551] - INFO: load:1.00 valid_run:248.29 task_valid:239.10 collect_output:6.71
2023-02-19 18:19:56 - train.py[line:549] - INFO: 600 / 6234
2023-02-19 18:19:56 - train.py[line:551] - INFO: load:1.03 valid_run:372.33 task_valid:357.05 collect_output:11.63
2023-02-19 18:21:58 - train.py[line:549] - INFO: 800 / 6234
2023-02-19 18:21:58 - train.py[line:551] - INFO: load:1.05 valid_run:494.23 task_valid:471.41 collect_output:18.02
2023-02-19 18:23:59 - train.py[line:549] - INFO: 1000 / 6234
2023-02-19 18:23:59 - train.py[line:551] - INFO: load:1.08 valid_run:614.60 task_valid:588.97 collect_output:19.74
2023-02-19 18:26:03 - train.py[line:549] - INFO: 1200 / 6234
2023-02-19 18:26:03 - train.py[line:551] - INFO: load:1.11 valid_run:738.78 task_valid:708.68 collect_output:22.98
2023-02-19 18:28:08 - train.py[line:549] - INFO: 1400 / 6234
2023-02-19 18:28:08 - train.py[line:551] - INFO: load:1.15 valid_run:863.28 task_valid:828.19 collect_output:26.73
2023-02-19 18:30:11 - train.py[line:549] - INFO: 1600 / 6234
2023-02-19 18:30:11 - train.py[line:551] - INFO: load:1.18 valid_run:986.87 task_valid:945.86 collect_output:31.44
2023-02-19 18:32:16 - train.py[line:549] - INFO: 1800 / 6234
2023-02-19 18:32:16 - train.py[line:551] - INFO: load:1.21 valid_run:1111.74 task_valid:1064.42 collect_output:36.49
2023-02-19 18:34:19 - train.py[line:549] - INFO: 2000 / 6234
2023-02-19 18:34:19 - train.py[line:551] - INFO: load:1.24 valid_run:1234.47 task_valid:1178.23 collect_output:44.19
2023-02-19 18:36:21 - train.py[line:549] - INFO: 2200 / 6234
2023-02-19 18:36:21 - train.py[line:551] - INFO: load:1.27 valid_run:1356.19 task_valid:1294.87 collect_output:48.09
2023-02-19 18:38:24 - train.py[line:549] - INFO: 2400 / 6234
2023-02-19 18:38:24 - train.py[line:551] - INFO: load:1.30 valid_run:1479.34 task_valid:1412.83 collect_output:52.08
2023-02-19 18:40:24 - train.py[line:549] - INFO: 2600 / 6234
2023-02-19 18:40:24 - train.py[line:551] - INFO: load:1.33 valid_run:1599.55 task_valid:1527.49 collect_output:56.43
2023-02-19 18:42:27 - train.py[line:549] - INFO: 2800 / 6234
2023-02-19 18:42:27 - train.py[line:551] - INFO: load:1.36 valid_run:1721.84 task_valid:1646.24 collect_output:58.79
2023-02-19 18:44:30 - train.py[line:549] - INFO: 3000 / 6234
2023-02-19 18:44:30 - train.py[line:551] - INFO: load:1.39 valid_run:1844.56 task_valid:1763.45 collect_output:63.08
2023-02-19 18:46:32 - train.py[line:549] - INFO: 3200 / 6234
2023-02-19 18:46:32 - train.py[line:551] - INFO: load:1.42 valid_run:1966.94 task_valid:1878.43 collect_output:69.28
2023-02-19 18:48:35 - train.py[line:549] - INFO: 3400 / 6234
2023-02-19 18:48:35 - train.py[line:551] - INFO: load:1.45 valid_run:2090.14 task_valid:1996.22 collect_output:73.42
2023-02-19 18:50:37 - train.py[line:549] - INFO: 3600 / 6234
2023-02-19 18:50:37 - train.py[line:551] - INFO: load:1.48 valid_run:2212.22 task_valid:2115.03 collect_output:75.45
2023-02-19 18:52:40 - train.py[line:549] - INFO: 3800 / 6234
2023-02-19 18:52:40 - train.py[line:551] - INFO: load:1.51 valid_run:2335.04 task_valid:2233.22 collect_output:78.89
2023-02-19 18:54:42 - train.py[line:549] - INFO: 4000 / 6234
2023-02-19 18:54:42 - train.py[line:551] - INFO: load:1.55 valid_run:2456.35 task_valid:2350.61 collect_output:81.68
2023-02-19 18:56:45 - train.py[line:549] - INFO: 4200 / 6234
2023-02-19 18:56:45 - train.py[line:551] - INFO: load:1.58 valid_run:2579.78 task_valid:2468.45 collect_output:86.02
2023-02-19 18:58:49 - train.py[line:549] - INFO: 4400 / 6234
2023-02-19 18:58:49 - train.py[line:551] - INFO: load:1.61 valid_run:2703.64 task_valid:2588.67 collect_output:88.42
2023-02-19 19:00:52 - train.py[line:549] - INFO: 4600 / 6234
2023-02-19 19:00:52 - train.py[line:551] - INFO: load:1.64 valid_run:2826.04 task_valid:2704.51 collect_output:93.75
2023-02-19 19:02:53 - train.py[line:549] - INFO: 4800 / 6234
2023-02-19 19:02:53 - train.py[line:551] - INFO: load:1.67 valid_run:2947.69 task_valid:2822.13 collect_output:96.54
2023-02-19 19:04:57 - train.py[line:549] - INFO: 5000 / 6234
2023-02-19 19:04:57 - train.py[line:551] - INFO: load:1.70 valid_run:3071.15 task_valid:2939.71 collect_output:101.19
2023-02-19 19:07:02 - train.py[line:549] - INFO: 5200 / 6234
2023-02-19 19:07:02 - train.py[line:551] - INFO: load:1.73 valid_run:3195.99 task_valid:3057.18 collect_output:107.33
2023-02-19 19:09:03 - train.py[line:549] - INFO: 5400 / 6234
2023-02-19 19:09:03 - train.py[line:551] - INFO: load:1.77 valid_run:3317.31 task_valid:3172.84 collect_output:111.77
2023-02-19 19:11:07 - train.py[line:549] - INFO: 5600 / 6234
2023-02-19 19:11:07 - train.py[line:551] - INFO: load:1.80 valid_run:3440.99 task_valid:3293.75 collect_output:113.33
2023-02-19 19:13:10 - train.py[line:549] - INFO: 5800 / 6234
2023-02-19 19:13:10 - train.py[line:551] - INFO: load:1.84 valid_run:3564.05 task_valid:3410.82 collect_output:118.01
2023-02-19 19:15:14 - train.py[line:549] - INFO: 6000 / 6234
2023-02-19 19:15:14 - train.py[line:551] - INFO: load:1.87 valid_run:3688.06 task_valid:3530.79 collect_output:120.84
2023-02-19 19:17:17 - train.py[line:549] - INFO: 6200 / 6234
2023-02-19 19:17:17 - train.py[line:551] - INFO: load:1.90 valid_run:3810.86 task_valid:3650.68 collect_output:122.53

====================================================================================================
SGG eval:     R @ 50: 0.3652;     R @ 100: 0.4159;     R @ 500: 0.4789;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1610;    mR @ 100: 0.2218;    mR @ 500: 0.2587;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.1707) (covered in:0.0000) (covering:0.1429) (eating:0.4706) (flying in:0.5000) (growing on:0.1250) (hanging from:0.4968) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.3854) (playing:0.0000) (riding:0.5353) (says:0.0000) (sitting on:0.5057) (standing on:0.5208) (using:0.2000) (walking in:0.0000) (walking on:0.2162) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.3652;     R @ 100: 0.4159;     R @ 500: 0.4789;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1610;    mR @ 100: 0.2218;    mR @ 500: 0.2587;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.1707) (covered in:0.0000) (covering:0.1429) (eating:0.4706) (flying in:0.5000) (growing on:0.1250) (hanging from:0.4968) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.3854) (playing:0.0000) (riding:0.5353) (says:0.0000) (sitting on:0.5057) (standing on:0.5208) (using:0.2000) (walking in:0.0000) (walking on:0.2162) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-02-19 19:17:48 - train.py[line:487] - INFO: 0.4159333333333333
2023-02-19 19:17:48 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-19 19:17:48 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.282 | loss_v1 0 | loss_v2 0 | nll_loss 0.119 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.415933 | ppl 1.09 | vqa_score 0.1858 | wps 116.8 | wpb 72 | bsz 24 | num_updates 2000
2023-02-19 19:17:48 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2023-02-19 19:17:48 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/temp_hold_cardoptNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_2000.pt
2023-02-19 19:17:55 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/temp_hold_cardoptNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_2000.pt
2023-02-19 19:18:03 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/temp_hold_cardoptNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.4159333333333333) (writing took 14.111154755577445 seconds)
2023-02-19 19:18:14 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=0.3, ups=0, wpb=111, bsz=40, num_updates=2010, lr=3.21909e-05, gnorm=0.833, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6148
2023-02-19 19:18:25 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.9, wpb=112, bsz=40, num_updates=2020, lr=3.23511e-05, gnorm=0.793, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6159
2023-02-19 19:18:37 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.88, wpb=111.4, bsz=40, num_updates=2030, lr=3.25112e-05, gnorm=0.88, clip=30, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=6170
2023-02-19 19:18:49 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96, ups=0.85, wpb=113.2, bsz=40, num_updates=2040, lr=3.26714e-05, gnorm=0.716, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6182
2023-02-19 19:19:00 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.3, ups=0.87, wpb=111.5, bsz=40, num_updates=2050, lr=3.28315e-05, gnorm=0.857, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6193
2023-02-19 19:19:11 - progress_bar.py[line:274] - INFO: epoch 001:   2061 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.6, ups=0.88, wpb=110.4, bsz=40, num_updates=2060, lr=3.29917e-05, gnorm=0.892, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6205
2023-02-19 19:19:23 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.4, ups=0.87, wpb=111.1, bsz=40, num_updates=2070, lr=3.31518e-05, gnorm=0.945, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6216
2023-02-19 19:19:34 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.88, wpb=112.2, bsz=40, num_updates=2080, lr=3.3312e-05, gnorm=0.756, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6228
2023-02-19 19:19:46 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.89, wpb=112.9, bsz=40, num_updates=2090, lr=3.34721e-05, gnorm=0.835, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6239
2023-02-19 19:19:57 - progress_bar.py[line:274] - INFO: epoch 001:   2101 / 11564 loss=0.251, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=98.1, ups=0.89, wpb=109.8, bsz=40, num_updates=2100, lr=3.36323e-05, gnorm=1.073, clip=50, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6250
2023-02-19 19:20:08 - progress_bar.py[line:274] - INFO: epoch 001:   2111 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.89, wpb=112, bsz=40, num_updates=2110, lr=3.37924e-05, gnorm=0.896, clip=40, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6261
2023-02-19 19:20:19 - progress_bar.py[line:274] - INFO: epoch 001:   2121 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.89, wpb=113.1, bsz=40, num_updates=2120, lr=3.39526e-05, gnorm=0.839, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6273
2023-02-19 19:20:31 - progress_bar.py[line:274] - INFO: epoch 001:   2131 / 11564 loss=0.242, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, wps=97.9, ups=0.87, wpb=112.4, bsz=40, num_updates=2130, lr=3.41127e-05, gnorm=0.822, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6284
2023-02-19 19:20:42 - progress_bar.py[line:274] - INFO: epoch 001:   2141 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.8, ups=0.9, wpb=111.1, bsz=40, num_updates=2140, lr=3.42729e-05, gnorm=0.841, clip=30, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6295
2023-02-19 19:20:53 - progress_bar.py[line:274] - INFO: epoch 001:   2151 / 11564 loss=0.245, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.89, wpb=111.8, bsz=40, num_updates=2150, lr=3.44331e-05, gnorm=0.942, clip=40, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=6307
2023-02-19 19:21:04 - progress_bar.py[line:274] - INFO: epoch 001:   2161 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.88, wpb=112.3, bsz=40, num_updates=2160, lr=3.45932e-05, gnorm=0.752, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6318
2023-02-19 19:21:16 - progress_bar.py[line:274] - INFO: epoch 001:   2171 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.4, ups=0.9, wpb=112.5, bsz=40, num_updates=2170, lr=3.47534e-05, gnorm=0.802, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6329
2023-02-19 19:21:27 - progress_bar.py[line:274] - INFO: epoch 001:   2181 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.2, ups=0.91, wpb=110.7, bsz=40, num_updates=2180, lr=3.49135e-05, gnorm=0.926, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6340
2023-02-19 19:21:38 - progress_bar.py[line:274] - INFO: epoch 001:   2191 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.88, wpb=112.4, bsz=40, num_updates=2190, lr=3.50737e-05, gnorm=0.597, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6351
2023-02-19 19:21:49 - progress_bar.py[line:274] - INFO: epoch 001:   2201 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=111, bsz=40, num_updates=2200, lr=3.52338e-05, gnorm=0.698, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6363
2023-02-19 19:22:01 - progress_bar.py[line:274] - INFO: epoch 001:   2211 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.5, ups=0.88, wpb=112.9, bsz=40, num_updates=2210, lr=3.5394e-05, gnorm=0.933, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6374
2023-02-19 19:22:12 - progress_bar.py[line:274] - INFO: epoch 001:   2221 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.9, wpb=111.6, bsz=40, num_updates=2220, lr=3.55541e-05, gnorm=0.85, clip=50, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6385
2023-02-19 19:22:23 - progress_bar.py[line:274] - INFO: epoch 001:   2231 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.7, ups=0.89, wpb=111.4, bsz=40, num_updates=2230, lr=3.57143e-05, gnorm=0.749, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6396
2023-02-19 19:22:34 - progress_bar.py[line:274] - INFO: epoch 001:   2241 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.7, ups=0.9, wpb=111.1, bsz=40, num_updates=2240, lr=3.58744e-05, gnorm=0.858, clip=40, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6408
2023-02-19 19:22:45 - progress_bar.py[line:274] - INFO: epoch 001:   2251 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.1, ups=0.92, wpb=112.6, bsz=40, num_updates=2250, lr=3.60346e-05, gnorm=0.765, clip=20, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=6418
2023-02-19 19:22:57 - progress_bar.py[line:274] - INFO: epoch 001:   2261 / 11564 loss=0.232, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.87, wpb=111.5, bsz=40, num_updates=2260, lr=3.61947e-05, gnorm=0.825, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6430
2023-02-19 19:23:08 - progress_bar.py[line:274] - INFO: epoch 001:   2271 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=112.7, bsz=40, num_updates=2270, lr=3.63549e-05, gnorm=0.749, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6441
2023-02-19 19:23:19 - progress_bar.py[line:274] - INFO: epoch 001:   2281 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101, ups=0.91, wpb=110.9, bsz=40, num_updates=2280, lr=3.65151e-05, gnorm=0.888, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6452
2023-02-19 19:23:30 - progress_bar.py[line:274] - INFO: epoch 001:   2291 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.4, ups=0.88, wpb=109.8, bsz=40, num_updates=2290, lr=3.66752e-05, gnorm=0.694, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6464
2023-02-19 19:23:42 - progress_bar.py[line:274] - INFO: epoch 001:   2301 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.8, ups=0.89, wpb=111.8, bsz=40, num_updates=2300, lr=3.68354e-05, gnorm=0.664, clip=10, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=6475
2023-02-19 19:23:53 - progress_bar.py[line:274] - INFO: epoch 001:   2311 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.88, wpb=111.8, bsz=40, num_updates=2310, lr=3.69955e-05, gnorm=0.727, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6486
2023-02-19 19:24:04 - progress_bar.py[line:274] - INFO: epoch 001:   2321 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.3, ups=0.92, wpb=111.6, bsz=40, num_updates=2320, lr=3.71557e-05, gnorm=0.579, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6497
2023-02-19 19:24:15 - progress_bar.py[line:274] - INFO: epoch 001:   2331 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.88, wpb=112.1, bsz=40, num_updates=2330, lr=3.73158e-05, gnorm=0.805, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6509
2023-02-19 19:24:27 - progress_bar.py[line:274] - INFO: epoch 001:   2341 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.5, ups=0.88, wpb=111.4, bsz=40, num_updates=2340, lr=3.7476e-05, gnorm=0.758, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6520
2023-02-19 19:24:38 - progress_bar.py[line:274] - INFO: epoch 001:   2351 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=112.2, bsz=40, num_updates=2350, lr=3.76361e-05, gnorm=0.764, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6531
2023-02-19 19:24:49 - progress_bar.py[line:274] - INFO: epoch 001:   2361 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100, ups=0.89, wpb=111.9, bsz=40, num_updates=2360, lr=3.77963e-05, gnorm=0.917, clip=50, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6543
2023-02-19 19:25:00 - progress_bar.py[line:274] - INFO: epoch 001:   2371 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.8, ups=0.92, wpb=111.9, bsz=40, num_updates=2370, lr=3.79564e-05, gnorm=0.809, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6553
2023-02-19 19:25:11 - progress_bar.py[line:274] - INFO: epoch 001:   2381 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103, ups=0.92, wpb=112.1, bsz=40, num_updates=2380, lr=3.81166e-05, gnorm=0.835, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6564
2023-02-19 19:25:22 - progress_bar.py[line:274] - INFO: epoch 001:   2391 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.89, wpb=112.9, bsz=40, num_updates=2390, lr=3.82767e-05, gnorm=0.883, clip=30, loss_scale=1024, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=6576
2023-02-19 19:25:34 - progress_bar.py[line:274] - INFO: epoch 001:   2401 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=96.6, ups=0.86, wpb=112.4, bsz=40, num_updates=2400, lr=3.84369e-05, gnorm=0.789, clip=20, loss_scale=1024, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=6587
2023-02-19 19:25:45 - progress_bar.py[line:274] - INFO: epoch 001:   2411 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.9, wpb=111.7, bsz=40, num_updates=2410, lr=3.85971e-05, gnorm=0.821, clip=20, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=6598
2023-02-19 19:25:56 - progress_bar.py[line:274] - INFO: epoch 001:   2421 / 11564 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.2, ups=0.9, wpb=110.7, bsz=40, num_updates=2420, lr=3.87572e-05, gnorm=0.897, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6609
2023-02-19 19:26:08 - progress_bar.py[line:274] - INFO: epoch 001:   2431 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.4, ups=0.86, wpb=112.1, bsz=40, num_updates=2430, lr=3.89174e-05, gnorm=0.68, clip=0, loss_scale=1024, train_wall=12, gb_free=11.3, ema_decay=0.9999, wall=6621
2023-02-19 19:26:19 - progress_bar.py[line:274] - INFO: epoch 001:   2441 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.9, wpb=111.3, bsz=40, num_updates=2440, lr=3.90775e-05, gnorm=0.691, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6632
2023-02-19 19:26:30 - progress_bar.py[line:274] - INFO: epoch 001:   2451 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.91, wpb=110.8, bsz=40, num_updates=2450, lr=3.92377e-05, gnorm=0.552, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6643
2023-02-19 19:26:41 - progress_bar.py[line:274] - INFO: epoch 001:   2461 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.89, wpb=111.5, bsz=40, num_updates=2460, lr=3.93978e-05, gnorm=0.633, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6655
2023-02-19 19:26:52 - progress_bar.py[line:274] - INFO: epoch 001:   2471 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=104.1, ups=0.92, wpb=112.5, bsz=40, num_updates=2470, lr=3.9558e-05, gnorm=0.837, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6665
2023-02-19 19:27:04 - progress_bar.py[line:274] - INFO: epoch 001:   2481 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.3, ups=0.86, wpb=110.9, bsz=40, num_updates=2480, lr=3.97181e-05, gnorm=0.592, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6677
2023-02-19 19:27:15 - progress_bar.py[line:274] - INFO: epoch 001:   2491 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.89, wpb=112.6, bsz=40, num_updates=2490, lr=3.98783e-05, gnorm=0.802, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6688
2023-02-19 19:27:26 - progress_bar.py[line:274] - INFO: epoch 001:   2501 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.104, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103, ups=0.92, wpb=112, bsz=40, num_updates=2500, lr=4.00384e-05, gnorm=0.801, clip=20, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6699
2023-02-19 19:27:37 - progress_bar.py[line:274] - INFO: epoch 001:   2511 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.88, wpb=113.4, bsz=40, num_updates=2510, lr=4.01986e-05, gnorm=0.666, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6711
2023-02-19 19:27:48 - progress_bar.py[line:274] - INFO: epoch 001:   2521 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.3, ups=0.88, wpb=110.6, bsz=40, num_updates=2520, lr=4.03587e-05, gnorm=0.826, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6722
2023-02-19 19:28:00 - progress_bar.py[line:274] - INFO: epoch 001:   2531 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.88, wpb=112.3, bsz=40, num_updates=2530, lr=4.05189e-05, gnorm=0.749, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6733
2023-02-19 19:28:11 - progress_bar.py[line:274] - INFO: epoch 001:   2541 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.8, ups=0.89, wpb=112.1, bsz=40, num_updates=2540, lr=4.06791e-05, gnorm=0.746, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6744
2023-02-19 19:28:22 - progress_bar.py[line:274] - INFO: epoch 001:   2551 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.1, ups=0.9, wpb=110.4, bsz=40, num_updates=2550, lr=4.08392e-05, gnorm=0.694, clip=10, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=6756
2023-02-19 19:28:33 - progress_bar.py[line:274] - INFO: epoch 001:   2561 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.2, ups=0.9, wpb=113.6, bsz=40, num_updates=2560, lr=4.09994e-05, gnorm=0.751, clip=30, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6767
2023-02-19 19:28:44 - progress_bar.py[line:274] - INFO: epoch 001:   2571 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.3, ups=0.9, wpb=111.2, bsz=40, num_updates=2570, lr=4.11595e-05, gnorm=0.617, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6778
2023-02-19 19:28:56 - progress_bar.py[line:274] - INFO: epoch 001:   2581 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.7, ups=0.89, wpb=112.3, bsz=40, num_updates=2580, lr=4.13197e-05, gnorm=0.575, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6789
2023-02-19 19:29:07 - progress_bar.py[line:274] - INFO: epoch 001:   2591 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.89, wpb=111.6, bsz=40, num_updates=2590, lr=4.14798e-05, gnorm=0.571, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6800
2023-02-19 19:29:19 - progress_bar.py[line:274] - INFO: epoch 001:   2601 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.8, ups=0.86, wpb=112.2, bsz=40, num_updates=2600, lr=4.164e-05, gnorm=0.592, clip=10, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6812
2023-02-19 19:29:30 - progress_bar.py[line:274] - INFO: epoch 001:   2611 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.89, wpb=111.3, bsz=40, num_updates=2610, lr=4.18001e-05, gnorm=0.672, clip=20, loss_scale=2048, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6823
2023-02-19 19:29:32 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-19 19:29:42 - progress_bar.py[line:274] - INFO: epoch 001:   2622 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=89.1, ups=0.8, wpb=111.1, bsz=40, num_updates=2620, lr=4.19603e-05, gnorm=0.776, clip=20, loss_scale=1024, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=6836
2023-02-19 19:29:54 - progress_bar.py[line:274] - INFO: epoch 001:   2632 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.1, ups=0.87, wpb=111.1, bsz=40, num_updates=2630, lr=4.21204e-05, gnorm=0.839, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6847
2023-02-19 19:30:05 - progress_bar.py[line:274] - INFO: epoch 001:   2642 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.9, ups=0.86, wpb=112.7, bsz=40, num_updates=2640, lr=4.22806e-05, gnorm=0.632, clip=10, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6859
2023-02-19 19:30:17 - progress_bar.py[line:274] - INFO: epoch 001:   2652 / 11564 loss=0.249, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.88, wpb=111.9, bsz=40, num_updates=2650, lr=4.24407e-05, gnorm=0.86, clip=30, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6870
2023-02-19 19:30:28 - progress_bar.py[line:274] - INFO: epoch 001:   2662 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.8, ups=0.86, wpb=111.2, bsz=40, num_updates=2660, lr=4.26009e-05, gnorm=0.6, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6882
2023-02-19 19:30:40 - progress_bar.py[line:274] - INFO: epoch 001:   2672 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.89, wpb=111.7, bsz=40, num_updates=2670, lr=4.27611e-05, gnorm=0.676, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6893
2023-02-19 19:30:51 - progress_bar.py[line:274] - INFO: epoch 001:   2682 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.7, ups=0.91, wpb=111.4, bsz=40, num_updates=2680, lr=4.29212e-05, gnorm=0.475, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6904
2023-02-19 19:31:01 - progress_bar.py[line:274] - INFO: epoch 001:   2692 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=103.2, ups=0.92, wpb=112.8, bsz=40, num_updates=2690, lr=4.30814e-05, gnorm=0.769, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6915
2023-02-19 19:31:13 - progress_bar.py[line:274] - INFO: epoch 001:   2702 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.2, ups=0.85, wpb=112.6, bsz=40, num_updates=2700, lr=4.32415e-05, gnorm=0.614, clip=10, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=6927
2023-02-19 19:31:24 - progress_bar.py[line:274] - INFO: epoch 001:   2712 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.1, ups=0.92, wpb=112.5, bsz=40, num_updates=2710, lr=4.34017e-05, gnorm=0.806, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6938
2023-02-19 19:31:35 - progress_bar.py[line:274] - INFO: epoch 001:   2722 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.2, ups=0.89, wpb=111.6, bsz=40, num_updates=2720, lr=4.35618e-05, gnorm=0.63, clip=20, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6949
2023-02-19 19:31:47 - progress_bar.py[line:274] - INFO: epoch 001:   2732 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.88, wpb=111.1, bsz=40, num_updates=2730, lr=4.3722e-05, gnorm=0.676, clip=20, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=6960
2023-02-19 19:31:58 - progress_bar.py[line:274] - INFO: epoch 001:   2742 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.1, ups=0.91, wpb=110.8, bsz=40, num_updates=2740, lr=4.38821e-05, gnorm=0.721, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6971
2023-02-19 19:32:09 - progress_bar.py[line:274] - INFO: epoch 001:   2752 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.89, wpb=112.9, bsz=40, num_updates=2750, lr=4.40423e-05, gnorm=0.639, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6982
2023-02-19 19:32:20 - progress_bar.py[line:274] - INFO: epoch 001:   2762 / 11564 loss=0.241, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.2, ups=0.87, wpb=112.5, bsz=40, num_updates=2760, lr=4.42024e-05, gnorm=0.84, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=6994
2023-02-19 19:32:32 - progress_bar.py[line:274] - INFO: epoch 001:   2772 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.87, wpb=112.4, bsz=40, num_updates=2770, lr=4.43626e-05, gnorm=0.62, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7005
2023-02-19 19:32:43 - progress_bar.py[line:274] - INFO: epoch 001:   2782 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=100.3, ups=0.89, wpb=112.1, bsz=40, num_updates=2780, lr=4.45227e-05, gnorm=0.712, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7017
2023-02-19 19:32:54 - progress_bar.py[line:274] - INFO: epoch 001:   2792 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.3, ups=0.88, wpb=112.4, bsz=40, num_updates=2790, lr=4.46829e-05, gnorm=0.589, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7028
2023-02-19 19:33:06 - progress_bar.py[line:274] - INFO: epoch 001:   2802 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=109.7, nsentences=40, sample_size=109.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.3, ups=0.88, wpb=109.7, bsz=40, num_updates=2800, lr=4.4843e-05, gnorm=0.831, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7039
2023-02-19 19:33:17 - progress_bar.py[line:274] - INFO: epoch 001:   2812 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.89, wpb=111.4, bsz=40, num_updates=2810, lr=4.50032e-05, gnorm=0.658, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7051
2023-02-19 19:33:28 - progress_bar.py[line:274] - INFO: epoch 001:   2822 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.3, ups=0.93, wpb=112.5, bsz=40, num_updates=2820, lr=4.51634e-05, gnorm=0.58, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7061
2023-02-19 19:33:39 - progress_bar.py[line:274] - INFO: epoch 001:   2832 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100, ups=0.9, wpb=111.7, bsz=40, num_updates=2830, lr=4.53235e-05, gnorm=0.643, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7073
2023-02-19 19:33:50 - progress_bar.py[line:274] - INFO: epoch 001:   2842 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.8, ups=0.92, wpb=112.6, bsz=40, num_updates=2840, lr=4.54837e-05, gnorm=0.561, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7083
2023-02-19 19:34:01 - progress_bar.py[line:274] - INFO: epoch 001:   2852 / 11564 loss=0.239, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.89, wpb=111.6, bsz=40, num_updates=2850, lr=4.56438e-05, gnorm=0.791, clip=30, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=7095
2023-02-19 19:34:12 - progress_bar.py[line:274] - INFO: epoch 001:   2862 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.7, bsz=40, num_updates=2860, lr=4.5804e-05, gnorm=0.549, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7106
2023-02-19 19:34:23 - progress_bar.py[line:274] - INFO: epoch 001:   2872 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.91, wpb=110.8, bsz=40, num_updates=2870, lr=4.59641e-05, gnorm=0.79, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7117
2023-02-19 19:34:35 - progress_bar.py[line:274] - INFO: epoch 001:   2882 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.9, ups=0.88, wpb=112.8, bsz=40, num_updates=2880, lr=4.61243e-05, gnorm=0.857, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7128
2023-02-19 19:34:46 - progress_bar.py[line:274] - INFO: epoch 001:   2892 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.4, ups=0.89, wpb=112.1, bsz=40, num_updates=2890, lr=4.62844e-05, gnorm=0.656, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7139
2023-02-19 19:34:57 - progress_bar.py[line:274] - INFO: epoch 001:   2902 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.9, wpb=112.3, bsz=40, num_updates=2900, lr=4.64446e-05, gnorm=0.613, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7151
2023-02-19 19:35:08 - progress_bar.py[line:274] - INFO: epoch 001:   2912 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.88, wpb=112, bsz=40, num_updates=2910, lr=4.66047e-05, gnorm=0.765, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7162
2023-02-19 19:35:20 - progress_bar.py[line:274] - INFO: epoch 001:   2922 / 11564 loss=0.238, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101, ups=0.9, wpb=112.2, bsz=40, num_updates=2920, lr=4.67649e-05, gnorm=0.767, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7173
2023-02-19 19:35:31 - progress_bar.py[line:274] - INFO: epoch 001:   2932 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.5, ups=0.9, wpb=112.6, bsz=40, num_updates=2930, lr=4.6925e-05, gnorm=0.593, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7184
2023-02-19 19:35:42 - progress_bar.py[line:274] - INFO: epoch 001:   2942 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.87, wpb=112.9, bsz=40, num_updates=2940, lr=4.70852e-05, gnorm=0.652, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7196
2023-02-19 19:35:54 - progress_bar.py[line:274] - INFO: epoch 001:   2952 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.4, ups=0.87, wpb=111.3, bsz=40, num_updates=2950, lr=4.72454e-05, gnorm=0.671, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=7207
2023-02-19 19:36:05 - progress_bar.py[line:274] - INFO: epoch 001:   2962 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.9, wpb=112.4, bsz=40, num_updates=2960, lr=4.74055e-05, gnorm=0.639, clip=10, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7218
2023-02-19 19:36:08 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-19 19:36:17 - progress_bar.py[line:274] - INFO: epoch 001:   2973 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=91.6, ups=0.81, wpb=112.8, bsz=40, num_updates=2970, lr=4.75657e-05, gnorm=0.608, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=7231
2023-02-19 19:36:28 - progress_bar.py[line:274] - INFO: epoch 001:   2983 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.7, ups=0.89, wpb=112.5, bsz=40, num_updates=2980, lr=4.77258e-05, gnorm=0.595, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7242
2023-02-19 19:36:40 - progress_bar.py[line:274] - INFO: epoch 001:   2993 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.87, wpb=113.3, bsz=40, num_updates=2990, lr=4.7886e-05, gnorm=0.702, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7253
2023-02-19 19:36:51 - progress_bar.py[line:274] - INFO: epoch 001:   3003 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.89, wpb=110.7, bsz=40, num_updates=3000, lr=4.80461e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7265
2023-02-19 19:37:02 - progress_bar.py[line:274] - INFO: epoch 001:   3013 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.2, ups=0.91, wpb=111.4, bsz=40, num_updates=3010, lr=4.82063e-05, gnorm=0.698, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7276
2023-02-19 19:37:14 - progress_bar.py[line:274] - INFO: epoch 001:   3023 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97, ups=0.87, wpb=111.7, bsz=40, num_updates=3020, lr=4.83664e-05, gnorm=0.649, clip=10, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=7287
2023-02-19 19:37:25 - progress_bar.py[line:274] - INFO: epoch 001:   3033 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.9, wpb=111.2, bsz=40, num_updates=3030, lr=4.85266e-05, gnorm=0.494, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7298
2023-02-19 19:37:36 - progress_bar.py[line:274] - INFO: epoch 001:   3043 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=98.7, ups=0.87, wpb=112.9, bsz=40, num_updates=3040, lr=4.86867e-05, gnorm=0.672, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7310
2023-02-19 19:37:48 - progress_bar.py[line:274] - INFO: epoch 001:   3053 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.9, ups=0.87, wpb=112, bsz=40, num_updates=3050, lr=4.88469e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7321
2023-02-19 19:37:59 - progress_bar.py[line:274] - INFO: epoch 001:   3063 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99, ups=0.89, wpb=111.6, bsz=40, num_updates=3060, lr=4.9007e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7332
2023-02-19 19:38:10 - progress_bar.py[line:274] - INFO: epoch 001:   3073 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.8, ups=0.89, wpb=111, bsz=40, num_updates=3070, lr=4.91672e-05, gnorm=0.773, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=7344
2023-02-19 19:38:21 - progress_bar.py[line:274] - INFO: epoch 001:   3083 / 11564 loss=0.237, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.5, ups=0.89, wpb=112.3, bsz=40, num_updates=3080, lr=4.93274e-05, gnorm=0.716, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7355
2023-02-19 19:38:32 - progress_bar.py[line:274] - INFO: epoch 001:   3093 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.1, ups=0.92, wpb=112.3, bsz=40, num_updates=3090, lr=4.94875e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7366
2023-02-19 19:38:44 - progress_bar.py[line:274] - INFO: epoch 001:   3103 / 11564 loss=0.25, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=97.4, ups=0.87, wpb=111.6, bsz=40, num_updates=3100, lr=4.96477e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7377
2023-02-19 19:38:55 - progress_bar.py[line:274] - INFO: epoch 001:   3113 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.6, ups=0.9, wpb=113.3, bsz=40, num_updates=3110, lr=4.98078e-05, gnorm=0.593, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7388
2023-02-19 19:39:06 - progress_bar.py[line:274] - INFO: epoch 001:   3123 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.87, wpb=112.1, bsz=40, num_updates=3120, lr=4.9968e-05, gnorm=0.572, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7400
2023-02-19 19:39:18 - progress_bar.py[line:274] - INFO: epoch 001:   3133 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.89, wpb=112.3, bsz=40, num_updates=3130, lr=4.99964e-05, gnorm=0.699, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7411
2023-02-19 19:39:29 - progress_bar.py[line:274] - INFO: epoch 001:   3143 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.4, ups=0.9, wpb=111.7, bsz=40, num_updates=3140, lr=4.9992e-05, gnorm=0.676, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=7422
2023-02-19 19:39:40 - progress_bar.py[line:274] - INFO: epoch 001:   3153 / 11564 loss=0.236, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.9, ups=0.87, wpb=110.5, bsz=40, num_updates=3150, lr=4.99876e-05, gnorm=0.709, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7434
2023-02-19 19:39:52 - progress_bar.py[line:274] - INFO: epoch 001:   3163 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.7, ups=0.9, wpb=112.5, bsz=40, num_updates=3160, lr=4.99831e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7445
2023-02-19 19:40:03 - progress_bar.py[line:274] - INFO: epoch 001:   3173 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.89, wpb=112.8, bsz=40, num_updates=3170, lr=4.99787e-05, gnorm=0.71, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7456
2023-02-19 19:40:14 - progress_bar.py[line:274] - INFO: epoch 001:   3183 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.88, wpb=112.5, bsz=40, num_updates=3180, lr=4.99742e-05, gnorm=0.503, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7468
2023-02-19 19:40:26 - progress_bar.py[line:274] - INFO: epoch 001:   3193 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.2, ups=0.86, wpb=112.6, bsz=40, num_updates=3190, lr=4.99698e-05, gnorm=0.644, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7479
2023-02-19 19:40:37 - progress_bar.py[line:274] - INFO: epoch 001:   3203 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.5, ups=0.91, wpb=112.7, bsz=40, num_updates=3200, lr=4.99653e-05, gnorm=0.528, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7490
2023-02-19 19:40:48 - progress_bar.py[line:274] - INFO: epoch 001:   3213 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.3, ups=0.88, wpb=113, bsz=40, num_updates=3210, lr=4.99609e-05, gnorm=0.591, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7502
2023-02-19 19:40:59 - progress_bar.py[line:274] - INFO: epoch 001:   3223 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.89, wpb=110.7, bsz=40, num_updates=3220, lr=4.99565e-05, gnorm=0.562, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7513
2023-02-19 19:41:11 - progress_bar.py[line:274] - INFO: epoch 001:   3233 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.103, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=95.7, ups=0.86, wpb=111.4, bsz=40, num_updates=3230, lr=4.9952e-05, gnorm=0.602, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=7524
2023-02-19 19:41:23 - progress_bar.py[line:274] - INFO: epoch 001:   3243 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=95.9, ups=0.85, wpb=112.8, bsz=40, num_updates=3240, lr=4.99476e-05, gnorm=0.472, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=7536
2023-02-19 19:41:34 - progress_bar.py[line:274] - INFO: epoch 001:   3253 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=110.5, nsentences=40, sample_size=110.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96, ups=0.87, wpb=110.5, bsz=40, num_updates=3250, lr=4.99431e-05, gnorm=0.701, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7548
2023-02-19 19:41:45 - progress_bar.py[line:274] - INFO: epoch 001:   3263 / 11564 loss=0.231, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.3, ups=0.89, wpb=110.1, bsz=40, num_updates=3260, lr=4.99387e-05, gnorm=0.659, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7559
2023-02-19 19:41:56 - progress_bar.py[line:274] - INFO: epoch 001:   3273 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=103.4, ups=0.92, wpb=111.9, bsz=40, num_updates=3270, lr=4.99342e-05, gnorm=0.623, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7570
2023-02-19 19:42:08 - progress_bar.py[line:274] - INFO: epoch 001:   3283 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.89, wpb=110.1, bsz=40, num_updates=3280, lr=4.99298e-05, gnorm=0.59, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7581
2023-02-19 19:42:19 - progress_bar.py[line:274] - INFO: epoch 001:   3293 / 11564 loss=0.24, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=101.5, ups=0.91, wpb=111.9, bsz=40, num_updates=3290, lr=4.99253e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7592
2023-02-19 19:42:30 - progress_bar.py[line:274] - INFO: epoch 001:   3303 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=96.2, ups=0.86, wpb=111.3, bsz=40, num_updates=3300, lr=4.99209e-05, gnorm=0.614, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7604
2023-02-19 19:42:41 - progress_bar.py[line:274] - INFO: epoch 001:   3313 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.89, wpb=112.9, bsz=40, num_updates=3310, lr=4.99165e-05, gnorm=0.525, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7615
2023-02-19 19:42:53 - progress_bar.py[line:274] - INFO: epoch 001:   3323 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.9, wpb=112.1, bsz=40, num_updates=3320, lr=4.9912e-05, gnorm=0.574, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7626
2023-02-19 19:43:04 - progress_bar.py[line:274] - INFO: epoch 001:   3333 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.3, ups=0.88, wpb=110.8, bsz=40, num_updates=3330, lr=4.99076e-05, gnorm=0.73, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7637
2023-02-19 19:43:15 - progress_bar.py[line:274] - INFO: epoch 001:   3343 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.4, ups=0.89, wpb=111.4, bsz=40, num_updates=3340, lr=4.99031e-05, gnorm=0.661, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7649
2023-02-19 19:43:26 - progress_bar.py[line:274] - INFO: epoch 001:   3353 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=112.4, bsz=40, num_updates=3350, lr=4.98987e-05, gnorm=0.626, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7660
2023-02-19 19:43:37 - progress_bar.py[line:274] - INFO: epoch 001:   3363 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.91, wpb=110.8, bsz=40, num_updates=3360, lr=4.98942e-05, gnorm=0.509, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=7671
2023-02-19 19:43:49 - progress_bar.py[line:274] - INFO: epoch 001:   3373 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.6, ups=0.89, wpb=112.1, bsz=40, num_updates=3370, lr=4.98898e-05, gnorm=0.659, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7682
2023-02-19 19:44:00 - progress_bar.py[line:274] - INFO: epoch 001:   3383 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.88, wpb=112.3, bsz=40, num_updates=3380, lr=4.98854e-05, gnorm=0.628, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7693
2023-02-19 19:44:11 - progress_bar.py[line:274] - INFO: epoch 001:   3393 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.5, ups=0.87, wpb=112.6, bsz=40, num_updates=3390, lr=4.98809e-05, gnorm=0.64, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7705
2023-02-19 19:44:23 - progress_bar.py[line:274] - INFO: epoch 001:   3403 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.89, wpb=112.1, bsz=40, num_updates=3400, lr=4.98765e-05, gnorm=0.605, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7716
2023-02-19 19:44:34 - progress_bar.py[line:274] - INFO: epoch 001:   3413 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.3, ups=0.88, wpb=111, bsz=40, num_updates=3410, lr=4.9872e-05, gnorm=0.599, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7727
2023-02-19 19:44:45 - progress_bar.py[line:274] - INFO: epoch 001:   3423 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.9, ups=0.89, wpb=110.9, bsz=40, num_updates=3420, lr=4.98676e-05, gnorm=0.552, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=7739
2023-02-19 19:44:56 - progress_bar.py[line:274] - INFO: epoch 001:   3433 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.8, ups=0.91, wpb=111.3, bsz=40, num_updates=3430, lr=4.98631e-05, gnorm=0.589, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7750
2023-02-19 19:45:08 - progress_bar.py[line:274] - INFO: epoch 001:   3443 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.3, ups=0.88, wpb=113.5, bsz=40, num_updates=3440, lr=4.98587e-05, gnorm=0.453, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7761
2023-02-19 19:45:19 - progress_bar.py[line:274] - INFO: epoch 001:   3453 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.7, ups=0.87, wpb=112.5, bsz=40, num_updates=3450, lr=4.98542e-05, gnorm=0.493, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7773
2023-02-19 19:45:30 - progress_bar.py[line:274] - INFO: epoch 001:   3463 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=111.7, bsz=40, num_updates=3460, lr=4.98498e-05, gnorm=0.6, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7784
2023-02-19 19:45:42 - progress_bar.py[line:274] - INFO: epoch 001:   3473 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.1, ups=0.88, wpb=111.7, bsz=40, num_updates=3470, lr=4.98454e-05, gnorm=0.516, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7795
2023-02-19 19:45:53 - progress_bar.py[line:274] - INFO: epoch 001:   3483 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.099, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.89, wpb=111, bsz=40, num_updates=3480, lr=4.98409e-05, gnorm=0.83, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7806
2023-02-19 19:46:04 - progress_bar.py[line:274] - INFO: epoch 001:   3493 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.9, ups=0.9, wpb=112.7, bsz=40, num_updates=3490, lr=4.98365e-05, gnorm=0.618, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7818
2023-02-19 19:46:16 - progress_bar.py[line:274] - INFO: epoch 001:   3503 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.7, ups=0.88, wpb=110.7, bsz=40, num_updates=3500, lr=4.9832e-05, gnorm=0.637, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=7829
2023-02-19 19:46:27 - progress_bar.py[line:274] - INFO: epoch 001:   3513 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.7, ups=0.89, wpb=111.2, bsz=40, num_updates=3510, lr=4.98276e-05, gnorm=0.598, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7840
2023-02-19 19:46:37 - progress_bar.py[line:274] - INFO: epoch 001:   3523 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=106.2, ups=0.95, wpb=112.2, bsz=40, num_updates=3520, lr=4.98231e-05, gnorm=0.56, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7851
2023-02-19 19:46:49 - progress_bar.py[line:274] - INFO: epoch 001:   3533 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.86, wpb=112.7, bsz=40, num_updates=3530, lr=4.98187e-05, gnorm=0.504, clip=0, loss_scale=1024, train_wall=12, gb_free=10, ema_decay=0.9999, wall=7862
2023-02-19 19:47:00 - progress_bar.py[line:274] - INFO: epoch 001:   3543 / 11564 loss=0.2, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.4, ups=0.89, wpb=111.2, bsz=40, num_updates=3540, lr=4.98143e-05, gnorm=0.44, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7874
2023-02-19 19:47:12 - progress_bar.py[line:274] - INFO: epoch 001:   3553 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.9, ups=0.89, wpb=111.6, bsz=40, num_updates=3550, lr=4.98098e-05, gnorm=0.614, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7885
2023-02-19 19:47:23 - progress_bar.py[line:274] - INFO: epoch 001:   3563 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.2, ups=0.88, wpb=111.9, bsz=40, num_updates=3560, lr=4.98054e-05, gnorm=0.578, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7896
2023-02-19 19:47:34 - progress_bar.py[line:274] - INFO: epoch 001:   3573 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=111.9, bsz=40, num_updates=3570, lr=4.98009e-05, gnorm=0.584, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7908
2023-02-19 19:47:46 - progress_bar.py[line:274] - INFO: epoch 001:   3583 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.89, wpb=111.3, bsz=40, num_updates=3580, lr=4.97965e-05, gnorm=0.657, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=7919
2023-02-19 19:47:57 - progress_bar.py[line:274] - INFO: epoch 001:   3593 / 11564 loss=0.205, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.5, ups=0.89, wpb=113.5, bsz=40, num_updates=3590, lr=4.9792e-05, gnorm=0.479, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7930
2023-02-19 19:48:08 - progress_bar.py[line:274] - INFO: epoch 001:   3603 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.92, wpb=110.7, bsz=40, num_updates=3600, lr=4.97876e-05, gnorm=0.58, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7941
2023-02-19 19:48:19 - progress_bar.py[line:274] - INFO: epoch 001:   3613 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.9, ups=0.89, wpb=112.7, bsz=40, num_updates=3610, lr=4.97831e-05, gnorm=0.711, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=7952
2023-02-19 19:48:31 - progress_bar.py[line:274] - INFO: epoch 001:   3623 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.1, ups=0.87, wpb=112.4, bsz=40, num_updates=3620, lr=4.97787e-05, gnorm=0.727, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=7964
2023-02-19 19:48:42 - progress_bar.py[line:274] - INFO: epoch 001:   3633 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.096, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.3, ups=0.91, wpb=111.9, bsz=40, num_updates=3630, lr=4.97743e-05, gnorm=0.583, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7975
2023-02-19 19:48:53 - progress_bar.py[line:274] - INFO: epoch 001:   3643 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.8, ups=0.91, wpb=112.3, bsz=40, num_updates=3640, lr=4.97698e-05, gnorm=0.643, clip=20, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=7986
2023-02-19 19:49:04 - progress_bar.py[line:274] - INFO: epoch 001:   3653 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=110.9, bsz=40, num_updates=3650, lr=4.97654e-05, gnorm=0.697, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=7997
2023-02-19 19:49:15 - progress_bar.py[line:274] - INFO: epoch 001:   3663 / 11564 loss=0.233, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.7, ups=0.92, wpb=112.6, bsz=40, num_updates=3660, lr=4.97609e-05, gnorm=0.673, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8008
2023-02-19 19:49:26 - progress_bar.py[line:274] - INFO: epoch 001:   3673 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.1, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.88, wpb=111.4, bsz=40, num_updates=3670, lr=4.97565e-05, gnorm=0.736, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8019
2023-02-19 19:49:37 - progress_bar.py[line:274] - INFO: epoch 001:   3683 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.3, ups=0.9, wpb=113.4, bsz=40, num_updates=3680, lr=4.9752e-05, gnorm=0.582, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8031
2023-02-19 19:49:49 - progress_bar.py[line:274] - INFO: epoch 001:   3693 / 11564 loss=0.234, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=95.2, ups=0.86, wpb=111, bsz=40, num_updates=3690, lr=4.97476e-05, gnorm=0.616, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=8042
2023-02-19 19:50:00 - progress_bar.py[line:274] - INFO: epoch 001:   3703 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.9, ups=0.89, wpb=111.7, bsz=40, num_updates=3700, lr=4.97432e-05, gnorm=0.569, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8053
2023-02-19 19:50:11 - progress_bar.py[line:274] - INFO: epoch 001:   3713 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.079, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.8, ups=0.89, wpb=111.9, bsz=40, num_updates=3710, lr=4.97387e-05, gnorm=0.507, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8065
2023-02-19 19:50:22 - progress_bar.py[line:274] - INFO: epoch 001:   3723 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.075, ntokens=112.9, nsentences=40, sample_size=112.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=101.2, ups=0.9, wpb=112.9, bsz=40, num_updates=3720, lr=4.97343e-05, gnorm=0.524, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8076
2023-02-19 19:50:34 - progress_bar.py[line:274] - INFO: epoch 001:   3733 / 11564 loss=0.223, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.1, ups=0.86, wpb=111.2, bsz=40, num_updates=3730, lr=4.97298e-05, gnorm=0.641, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=8087
2023-02-19 19:50:45 - progress_bar.py[line:274] - INFO: epoch 001:   3743 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102, ups=0.92, wpb=111.3, bsz=40, num_updates=3740, lr=4.97254e-05, gnorm=0.694, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8098
2023-02-19 19:50:56 - progress_bar.py[line:274] - INFO: epoch 001:   3753 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=111.2, bsz=40, num_updates=3750, lr=4.97209e-05, gnorm=0.49, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8110
2023-02-19 19:51:08 - progress_bar.py[line:274] - INFO: epoch 001:   3763 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.073, ntokens=113.9, nsentences=40, sample_size=113.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.86, wpb=113.9, bsz=40, num_updates=3760, lr=4.97165e-05, gnorm=0.439, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=8121
2023-02-19 19:51:19 - progress_bar.py[line:274] - INFO: epoch 001:   3773 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.106, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.1, ups=0.88, wpb=112.8, bsz=40, num_updates=3770, lr=4.9712e-05, gnorm=0.601, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8133
2023-02-19 19:51:30 - progress_bar.py[line:274] - INFO: epoch 001:   3783 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.2, ups=0.9, wpb=111.2, bsz=40, num_updates=3780, lr=4.97076e-05, gnorm=0.59, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8144
2023-02-19 19:51:42 - progress_bar.py[line:274] - INFO: epoch 001:   3793 / 11564 loss=0.224, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.8, ups=0.86, wpb=113.2, bsz=40, num_updates=3790, lr=4.97032e-05, gnorm=0.647, clip=10, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=8155
2023-02-19 19:51:53 - progress_bar.py[line:274] - INFO: epoch 001:   3803 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.91, wpb=112.1, bsz=40, num_updates=3800, lr=4.96987e-05, gnorm=0.512, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8166
2023-02-19 19:52:04 - progress_bar.py[line:274] - INFO: epoch 001:   3813 / 11564 loss=0.23, loss_v1=0, loss_v2=0, nll_loss=0.101, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=96.7, ups=0.87, wpb=111.6, bsz=40, num_updates=3810, lr=4.96943e-05, gnorm=0.662, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8178
2023-02-19 19:52:15 - progress_bar.py[line:274] - INFO: epoch 001:   3823 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.092, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.6, ups=0.91, wpb=112.6, bsz=40, num_updates=3820, lr=4.96898e-05, gnorm=0.568, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8189
2023-02-19 19:52:27 - progress_bar.py[line:274] - INFO: epoch 001:   3833 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.098, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=99.2, ups=0.89, wpb=111.8, bsz=40, num_updates=3830, lr=4.96854e-05, gnorm=0.582, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8200
2023-02-19 19:52:38 - progress_bar.py[line:274] - INFO: epoch 001:   3843 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=97.4, ups=0.87, wpb=111.7, bsz=40, num_updates=3840, lr=4.96809e-05, gnorm=0.527, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=8212
2023-02-19 19:52:49 - progress_bar.py[line:274] - INFO: epoch 001:   3853 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.6, ups=0.91, wpb=110.8, bsz=40, num_updates=3850, lr=4.96765e-05, gnorm=0.458, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8223
2023-02-19 19:53:00 - progress_bar.py[line:274] - INFO: epoch 001:   3863 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.2, ups=0.91, wpb=112.4, bsz=40, num_updates=3860, lr=4.96721e-05, gnorm=0.562, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=8234
2023-02-19 19:53:12 - progress_bar.py[line:274] - INFO: epoch 001:   3873 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98, ups=0.88, wpb=111.6, bsz=40, num_updates=3870, lr=4.96676e-05, gnorm=0.501, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=8245
2023-02-19 19:53:23 - progress_bar.py[line:274] - INFO: epoch 001:   3883 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.6, ups=0.89, wpb=111.2, bsz=40, num_updates=3880, lr=4.96632e-05, gnorm=0.582, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8256
2023-02-19 19:53:34 - progress_bar.py[line:274] - INFO: epoch 001:   3893 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113.2, nsentences=40, sample_size=113.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.87, wpb=113.2, bsz=40, num_updates=3890, lr=4.96587e-05, gnorm=0.477, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8268
2023-02-19 19:53:46 - progress_bar.py[line:274] - INFO: epoch 001:   3903 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.9, ups=0.87, wpb=111.9, bsz=40, num_updates=3900, lr=4.96543e-05, gnorm=0.53, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8279
2023-02-19 19:53:57 - progress_bar.py[line:274] - INFO: epoch 001:   3913 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.88, wpb=111.7, bsz=40, num_updates=3910, lr=4.96498e-05, gnorm=0.678, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8291
2023-02-19 19:54:09 - progress_bar.py[line:274] - INFO: epoch 001:   3923 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.87, wpb=112.8, bsz=40, num_updates=3920, lr=4.96454e-05, gnorm=0.73, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=8302
2023-02-19 19:54:20 - progress_bar.py[line:274] - INFO: epoch 001:   3933 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.071, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.9, ups=0.89, wpb=111.1, bsz=40, num_updates=3930, lr=4.96409e-05, gnorm=0.408, clip=0, loss_scale=1024, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=8314
2023-02-19 19:54:31 - progress_bar.py[line:274] - INFO: epoch 001:   3943 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99, ups=0.89, wpb=111.6, bsz=40, num_updates=3940, lr=4.96365e-05, gnorm=0.429, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8325
2023-02-19 19:54:43 - progress_bar.py[line:274] - INFO: epoch 001:   3953 / 11564 loss=0.206, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.4, ups=0.9, wpb=111.6, bsz=40, num_updates=3950, lr=4.96321e-05, gnorm=0.434, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8336
2023-02-19 19:54:54 - progress_bar.py[line:274] - INFO: epoch 001:   3963 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.3, ups=0.92, wpb=113.5, bsz=40, num_updates=3960, lr=4.96276e-05, gnorm=0.541, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=8347
2023-02-19 19:55:04 - progress_bar.py[line:274] - INFO: epoch 001:   3973 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.6, ups=0.93, wpb=112.3, bsz=40, num_updates=3970, lr=4.96232e-05, gnorm=0.577, clip=10, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8358
2023-02-19 19:55:15 - progress_bar.py[line:274] - INFO: epoch 001:   3983 / 11564 loss=0.229, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.5, ups=0.92, wpb=112.6, bsz=40, num_updates=3980, lr=4.96187e-05, gnorm=0.571, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=8369
2023-02-19 19:55:26 - progress_bar.py[line:274] - INFO: epoch 001:   3993 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.91, wpb=110.4, bsz=40, num_updates=3990, lr=4.96143e-05, gnorm=0.538, clip=10, loss_scale=2048, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=8380
2023-02-19 19:55:37 - progress_bar.py[line:274] - INFO: epoch 001:   4003 / 11564 loss=0.202, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=112.6, nsentences=40, sample_size=112.6, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=103.4, ups=0.92, wpb=112.6, bsz=40, num_updates=4000, lr=4.96098e-05, gnorm=0.397, clip=0, loss_scale=2048, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=8391
2023-02-19 19:55:37 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-02-19 19:55:39 - train.py[line:549] - INFO: 0 / 6234
2023-02-19 19:55:39 - train.py[line:551] - INFO: load:0.80 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-02-19 19:57:42 - train.py[line:549] - INFO: 200 / 6234
2023-02-19 19:57:42 - train.py[line:551] - INFO: load:0.83 valid_run:123.11 task_valid:119.88 collect_output:2.06
2023-02-19 19:59:44 - train.py[line:549] - INFO: 400 / 6234
2023-02-19 19:59:44 - train.py[line:551] - INFO: load:0.86 valid_run:244.38 task_valid:236.71 collect_output:5.21
2023-02-19 20:01:47 - train.py[line:549] - INFO: 600 / 6234
2023-02-19 20:01:47 - train.py[line:551] - INFO: load:0.89 valid_run:368.10 task_valid:354.57 collect_output:9.85
2023-02-19 20:03:51 - train.py[line:549] - INFO: 800 / 6234
2023-02-19 20:03:51 - train.py[line:551] - INFO: load:0.91 valid_run:491.29 task_valid:469.48 collect_output:16.91
2023-02-19 20:05:53 - train.py[line:549] - INFO: 1000 / 6234
2023-02-19 20:05:53 - train.py[line:551] - INFO: load:0.96 valid_run:613.08 task_valid:587.88 collect_output:19.16
2023-02-19 20:07:57 - train.py[line:549] - INFO: 1200 / 6234
2023-02-19 20:07:57 - train.py[line:551] - INFO: load:0.98 valid_run:737.54 task_valid:707.92 collect_output:22.40
2023-02-19 20:10:01 - train.py[line:549] - INFO: 1400 / 6234
2023-02-19 20:10:01 - train.py[line:551] - INFO: load:1.02 valid_run:861.71 task_valid:826.93 collect_output:26.34
2023-02-19 20:12:05 - train.py[line:549] - INFO: 1600 / 6234
2023-02-19 20:12:05 - train.py[line:551] - INFO: load:1.05 valid_run:985.14 task_valid:944.56 collect_output:30.93
2023-02-19 20:14:10 - train.py[line:549] - INFO: 1800 / 6234
2023-02-19 20:14:10 - train.py[line:551] - INFO: load:1.08 valid_run:1110.03 task_valid:1063.03 collect_output:36.12
2023-02-19 20:16:13 - train.py[line:549] - INFO: 2000 / 6234
2023-02-19 20:16:13 - train.py[line:551] - INFO: load:1.11 valid_run:1233.06 task_valid:1176.94 collect_output:44.01
2023-02-19 20:18:15 - train.py[line:549] - INFO: 2200 / 6234
2023-02-19 20:18:15 - train.py[line:551] - INFO: load:1.13 valid_run:1354.98 task_valid:1293.70 collect_output:47.95
2023-02-19 20:20:19 - train.py[line:549] - INFO: 2400 / 6234
2023-02-19 20:20:19 - train.py[line:551] - INFO: load:1.16 valid_run:1478.62 task_valid:1411.86 collect_output:52.19
2023-02-19 20:22:19 - train.py[line:549] - INFO: 2600 / 6234
2023-02-19 20:22:19 - train.py[line:551] - INFO: load:1.19 valid_run:1599.34 task_valid:1526.82 collect_output:56.74
2023-02-19 20:24:22 - train.py[line:549] - INFO: 2800 / 6234
2023-02-19 20:24:22 - train.py[line:551] - INFO: load:1.22 valid_run:1722.01 task_valid:1645.74 collect_output:59.28
2023-02-19 20:26:25 - train.py[line:549] - INFO: 3000 / 6234
2023-02-19 20:26:25 - train.py[line:551] - INFO: load:1.25 valid_run:1844.39 task_valid:1762.98 collect_output:63.21
2023-02-19 20:28:27 - train.py[line:549] - INFO: 3200 / 6234
2023-02-19 20:28:27 - train.py[line:551] - INFO: load:1.29 valid_run:1967.18 task_valid:1878.02 collect_output:69.72
2023-02-19 20:30:30 - train.py[line:549] - INFO: 3400 / 6234
2023-02-19 20:30:30 - train.py[line:551] - INFO: load:1.32 valid_run:2089.84 task_valid:1995.12 collect_output:74.10
2023-02-19 20:32:33 - train.py[line:549] - INFO: 3600 / 6234
2023-02-19 20:32:33 - train.py[line:551] - INFO: load:1.34 valid_run:2212.30 task_valid:2114.57 collect_output:75.96
2023-02-19 20:34:36 - train.py[line:549] - INFO: 3800 / 6234
2023-02-19 20:34:36 - train.py[line:551] - INFO: load:1.39 valid_run:2335.86 task_valid:2233.05 collect_output:79.81
2023-02-19 20:36:38 - train.py[line:549] - INFO: 4000 / 6234
2023-02-19 20:36:38 - train.py[line:551] - INFO: load:1.42 valid_run:2457.69 task_valid:2350.77 collect_output:82.70
2023-02-19 20:38:42 - train.py[line:549] - INFO: 4200 / 6234
2023-02-19 20:38:42 - train.py[line:551] - INFO: load:1.45 valid_run:2581.04 task_valid:2468.64 collect_output:86.94
2023-02-19 20:40:45 - train.py[line:549] - INFO: 4400 / 6234
2023-02-19 20:40:45 - train.py[line:551] - INFO: load:1.48 valid_run:2704.41 task_valid:2588.56 collect_output:89.24
2023-02-19 20:42:47 - train.py[line:549] - INFO: 4600 / 6234
2023-02-19 20:42:47 - train.py[line:551] - INFO: load:1.51 valid_run:2826.64 task_valid:2704.55 collect_output:94.22
2023-02-19 20:44:49 - train.py[line:549] - INFO: 4800 / 6234
2023-02-19 20:44:49 - train.py[line:551] - INFO: load:1.53 valid_run:2948.19 task_valid:2821.94 collect_output:97.20
2023-02-19 20:46:52 - train.py[line:549] - INFO: 5000 / 6234
2023-02-19 20:46:52 - train.py[line:551] - INFO: load:1.56 valid_run:3071.58 task_valid:2939.42 collect_output:101.94
2023-02-19 20:48:57 - train.py[line:549] - INFO: 5200 / 6234
2023-02-19 20:48:57 - train.py[line:551] - INFO: load:1.59 valid_run:3196.27 task_valid:3056.91 collect_output:107.93
2023-02-19 20:50:59 - train.py[line:549] - INFO: 5400 / 6234
2023-02-19 20:50:59 - train.py[line:551] - INFO: load:1.62 valid_run:3317.54 task_valid:3172.43 collect_output:112.45
2023-02-19 20:53:02 - train.py[line:549] - INFO: 5600 / 6234
2023-02-19 20:53:02 - train.py[line:551] - INFO: load:1.65 valid_run:3440.89 task_valid:3292.89 collect_output:114.16
2023-02-19 20:55:05 - train.py[line:549] - INFO: 5800 / 6234
2023-02-19 20:55:05 - train.py[line:551] - INFO: load:1.68 valid_run:3564.18 task_valid:3409.54 collect_output:119.60
2023-02-19 20:57:09 - train.py[line:549] - INFO: 6000 / 6234
2023-02-19 20:57:09 - train.py[line:551] - INFO: load:1.71 valid_run:3687.59 task_valid:3528.95 collect_output:122.41
2023-02-19 20:59:12 - train.py[line:549] - INFO: 6200 / 6234
2023-02-19 20:59:12 - train.py[line:551] - INFO: load:1.74 valid_run:3810.43 task_valid:3648.50 collect_output:124.52

====================================================================================================
SGG eval:     R @ 50: 0.5868;     R @ 100: 0.6364;     R @ 500: 0.6801;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3699;    mR @ 100: 0.4263;    mR @ 500: 0.4673;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6463) (covered in:0.2500) (covering:0.3714) (eating:0.7059) (flying in:0.8636) (growing on:0.3750) (hanging from:0.5194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9167) (playing:0.0000) (riding:0.9565) (says:0.0000) (sitting on:0.6545) (standing on:0.4812) (using:0.3500) (walking in:0.0000) (walking on:0.6216) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5868;     R @ 100: 0.6364;     R @ 500: 0.6801;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3699;    mR @ 100: 0.4263;    mR @ 500: 0.4673;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6463) (covered in:0.2500) (covering:0.3714) (eating:0.7059) (flying in:0.8636) (growing on:0.3750) (hanging from:0.5194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9167) (playing:0.0000) (riding:0.9565) (says:0.0000) (sitting on:0.6545) (standing on:0.4812) (using:0.3500) (walking in:0.0000) (walking on:0.6216) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================

2023-02-19 20:59:43 - train.py[line:487] - INFO: 0.6363979628214921
2023-02-19 20:59:43 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-02-19 20:59:43 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.263 | loss_v1 0 | loss_v2 0 | nll_loss 0.098 | ntokens 71.953 | nsentences 24 | sample_size 71.953 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.636398 | ppl 1.07 | vqa_score 0.3874 | wps 116.7 | wpb 72 | bsz 24 | num_updates 4000 | best_R@100 0.636398
2023-02-19 20:59:43 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4000 updates
2023-02-19 20:59:43 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/temp_hold_cardoptNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_4000.pt
2023-02-19 20:59:50 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/temp_hold_cardoptNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_4000.pt
2023-02-19 20:59:58 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/temp_hold_cardoptNew_caption_trained_visual_DS-k10alpha1.0_/1_B20_A1_E10_0.027_5e-5_480/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 0.6363979628214921) (writing took 15.371232191100717 seconds)
2023-02-19 21:00:04 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1024.0
2023-02-19 21:00:11 - progress_bar.py[line:274] - INFO: epoch 001:   4014 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=0.3, ups=0, wpb=111.5, bsz=40, num_updates=4010, lr=4.96054e-05, gnorm=0.537, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=12264
2023-02-19 21:00:22 - progress_bar.py[line:274] - INFO: epoch 001:   4024 / 11564 loss=0.218, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.4, nsentences=40, sample_size=113.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101, ups=0.89, wpb=113.4, bsz=40, num_updates=4020, lr=4.9601e-05, gnorm=0.658, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12275
2023-02-19 21:00:33 - progress_bar.py[line:274] - INFO: epoch 001:   4034 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=104.8, ups=0.93, wpb=112.4, bsz=40, num_updates=4030, lr=4.95965e-05, gnorm=0.64, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12286
2023-02-19 21:00:44 - progress_bar.py[line:274] - INFO: epoch 001:   4044 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=110.9, nsentences=40, sample_size=110.9, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.89, wpb=110.9, bsz=40, num_updates=4040, lr=4.95921e-05, gnorm=0.529, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12297
2023-02-19 21:00:55 - progress_bar.py[line:274] - INFO: epoch 001:   4054 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.8, ups=0.88, wpb=111.7, bsz=40, num_updates=4050, lr=4.95876e-05, gnorm=0.526, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12309
2023-02-19 21:01:06 - progress_bar.py[line:274] - INFO: epoch 001:   4064 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.09, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.4, ups=0.9, wpb=112.8, bsz=40, num_updates=4060, lr=4.95832e-05, gnorm=0.603, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12320
2023-02-19 21:01:18 - progress_bar.py[line:274] - INFO: epoch 001:   4074 / 11564 loss=0.219, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.5, ups=0.88, wpb=111.4, bsz=40, num_updates=4070, lr=4.95787e-05, gnorm=0.515, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12331
2023-02-19 21:01:29 - progress_bar.py[line:274] - INFO: epoch 001:   4084 / 11564 loss=0.221, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.2, ups=0.9, wpb=112.5, bsz=40, num_updates=4080, lr=4.95743e-05, gnorm=0.509, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12342
2023-02-19 21:01:40 - progress_bar.py[line:274] - INFO: epoch 001:   4094 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.6, ups=0.89, wpb=112.7, bsz=40, num_updates=4090, lr=4.95698e-05, gnorm=0.449, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12354
2023-02-19 21:01:52 - progress_bar.py[line:274] - INFO: epoch 001:   4104 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.085, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=112.8, bsz=40, num_updates=4100, lr=4.95654e-05, gnorm=0.454, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12365
2023-02-19 21:02:03 - progress_bar.py[line:274] - INFO: epoch 001:   4114 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=113.3, nsentences=40, sample_size=113.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.2, ups=0.88, wpb=113.3, bsz=40, num_updates=4110, lr=4.9561e-05, gnorm=0.518, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12376
2023-02-19 21:02:14 - progress_bar.py[line:274] - INFO: epoch 001:   4124 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.7, ups=0.91, wpb=112.2, bsz=40, num_updates=4120, lr=4.95565e-05, gnorm=0.586, clip=10, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12387
2023-02-19 21:02:25 - progress_bar.py[line:274] - INFO: epoch 001:   4134 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.093, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.6, ups=0.88, wpb=112.3, bsz=40, num_updates=4130, lr=4.95521e-05, gnorm=0.641, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12399
2023-02-19 21:02:36 - progress_bar.py[line:274] - INFO: epoch 001:   4144 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.5, ups=0.9, wpb=111.7, bsz=40, num_updates=4140, lr=4.95476e-05, gnorm=0.569, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12410
2023-02-19 21:02:48 - progress_bar.py[line:274] - INFO: epoch 001:   4154 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.072, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.2, ups=0.89, wpb=112, bsz=40, num_updates=4150, lr=4.95432e-05, gnorm=0.486, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12421
2023-02-19 21:02:59 - progress_bar.py[line:274] - INFO: epoch 001:   4164 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=113.1, nsentences=40, sample_size=113.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.3, ups=0.89, wpb=113.1, bsz=40, num_updates=4160, lr=4.95387e-05, gnorm=0.609, clip=10, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12432
2023-02-19 21:03:10 - progress_bar.py[line:274] - INFO: epoch 001:   4174 / 11564 loss=0.204, loss_v1=0, loss_v2=0, nll_loss=0.067, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.88, wpb=112.5, bsz=40, num_updates=4170, lr=4.95343e-05, gnorm=0.5, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12444
2023-02-19 21:03:21 - progress_bar.py[line:274] - INFO: epoch 001:   4184 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=113.5, nsentences=40, sample_size=113.5, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=102.1, ups=0.9, wpb=113.5, bsz=40, num_updates=4180, lr=4.95299e-05, gnorm=0.519, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12455
2023-02-19 21:03:33 - progress_bar.py[line:274] - INFO: epoch 001:   4194 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.1, ups=0.9, wpb=111.3, bsz=40, num_updates=4190, lr=4.95254e-05, gnorm=0.542, clip=10, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12466
2023-02-19 21:03:39 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-02-19 21:03:45 - progress_bar.py[line:274] - INFO: epoch 001:   4205 / 11564 loss=0.228, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=91.3, ups=0.82, wpb=111, bsz=40, num_updates=4200, lr=4.9521e-05, gnorm=0.57, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=12478
2023-02-19 21:03:56 - progress_bar.py[line:274] - INFO: epoch 001:   4215 / 11564 loss=0.225, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=98.4, ups=0.88, wpb=112.3, bsz=40, num_updates=4210, lr=4.95165e-05, gnorm=0.518, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12490
2023-02-19 21:04:07 - progress_bar.py[line:274] - INFO: epoch 001:   4225 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.082, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.3, ups=0.9, wpb=112.2, bsz=40, num_updates=4220, lr=4.95121e-05, gnorm=0.598, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12501
2023-02-19 21:04:19 - progress_bar.py[line:274] - INFO: epoch 001:   4235 / 11564 loss=0.215, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99, ups=0.88, wpb=112.7, bsz=40, num_updates=4230, lr=4.95076e-05, gnorm=0.694, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12512
2023-02-19 21:04:30 - progress_bar.py[line:274] - INFO: epoch 001:   4245 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.1, ups=0.9, wpb=112.4, bsz=40, num_updates=4240, lr=4.95032e-05, gnorm=0.47, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12523
2023-02-19 21:04:41 - progress_bar.py[line:274] - INFO: epoch 001:   4255 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.091, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=103.4, ups=0.92, wpb=111.8, bsz=40, num_updates=4250, lr=4.94987e-05, gnorm=0.643, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12534
2023-02-19 21:04:52 - progress_bar.py[line:274] - INFO: epoch 001:   4265 / 11564 loss=0.235, loss_v1=0, loss_v2=0, nll_loss=0.107, ntokens=110.7, nsentences=40, sample_size=110.7, sample_size_v1=0, sample_size_v2=0, ppl=1.08, wps=99.3, ups=0.9, wpb=110.7, bsz=40, num_updates=4260, lr=4.94943e-05, gnorm=0.767, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12545
2023-02-19 21:05:03 - progress_bar.py[line:274] - INFO: epoch 001:   4275 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=111.1, nsentences=40, sample_size=111.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.91, wpb=111.1, bsz=40, num_updates=4270, lr=4.94899e-05, gnorm=0.547, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12556
2023-02-19 21:05:14 - progress_bar.py[line:274] - INFO: epoch 001:   4285 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.3, nsentences=40, sample_size=112.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=98.1, ups=0.87, wpb=112.3, bsz=40, num_updates=4280, lr=4.94854e-05, gnorm=0.546, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12568
2023-02-19 21:05:25 - progress_bar.py[line:274] - INFO: epoch 001:   4295 / 11564 loss=0.208, loss_v1=0, loss_v2=0, nll_loss=0.069, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=100.8, ups=0.89, wpb=113, bsz=40, num_updates=4290, lr=4.9481e-05, gnorm=0.49, clip=0, loss_scale=512, train_wall=11, gb_free=11.2, ema_decay=0.9999, wall=12579
2023-02-19 21:05:37 - progress_bar.py[line:274] - INFO: epoch 001:   4305 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=99.1, ups=0.89, wpb=111.7, bsz=40, num_updates=4300, lr=4.94765e-05, gnorm=0.512, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12590
2023-02-19 21:05:48 - progress_bar.py[line:274] - INFO: epoch 001:   4315 / 11564 loss=0.226, loss_v1=0, loss_v2=0, nll_loss=0.095, ntokens=111.9, nsentences=40, sample_size=111.9, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100.1, ups=0.89, wpb=111.9, bsz=40, num_updates=4310, lr=4.94721e-05, gnorm=0.51, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12601
2023-02-19 21:05:59 - progress_bar.py[line:274] - INFO: epoch 001:   4325 / 11564 loss=0.207, loss_v1=0, loss_v2=0, nll_loss=0.074, ntokens=111.5, nsentences=40, sample_size=111.5, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=99.1, ups=0.89, wpb=111.5, bsz=40, num_updates=4320, lr=4.94676e-05, gnorm=0.451, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12613
2023-02-19 21:06:10 - progress_bar.py[line:274] - INFO: epoch 001:   4335 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.094, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=101.9, ups=0.9, wpb=113, bsz=40, num_updates=4330, lr=4.94632e-05, gnorm=0.583, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12624
2023-02-19 21:06:22 - progress_bar.py[line:274] - INFO: epoch 001:   4345 / 11564 loss=0.211, loss_v1=0, loss_v2=0, nll_loss=0.08, ntokens=112.8, nsentences=40, sample_size=112.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.3, ups=0.87, wpb=112.8, bsz=40, num_updates=4340, lr=4.94588e-05, gnorm=0.481, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12635
2023-02-19 21:06:33 - progress_bar.py[line:274] - INFO: epoch 001:   4355 / 11564 loss=0.209, loss_v1=0, loss_v2=0, nll_loss=0.077, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102.4, ups=0.91, wpb=112.7, bsz=40, num_updates=4350, lr=4.94543e-05, gnorm=0.522, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12646
2023-02-19 21:06:44 - progress_bar.py[line:274] - INFO: epoch 001:   4365 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.083, ntokens=112.1, nsentences=40, sample_size=112.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=112.1, bsz=40, num_updates=4360, lr=4.94499e-05, gnorm=0.515, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12657
2023-02-19 21:06:56 - progress_bar.py[line:274] - INFO: epoch 001:   4375 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.089, ntokens=112.5, nsentences=40, sample_size=112.5, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=98.4, ups=0.87, wpb=112.5, bsz=40, num_updates=4370, lr=4.94454e-05, gnorm=0.515, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12669
2023-02-19 21:07:07 - progress_bar.py[line:274] - INFO: epoch 001:   4385 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.4, ups=0.9, wpb=111.8, bsz=40, num_updates=4380, lr=4.9441e-05, gnorm=0.378, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=12680
2023-02-19 21:07:18 - progress_bar.py[line:274] - INFO: epoch 001:   4395 / 11564 loss=0.212, loss_v1=0, loss_v2=0, nll_loss=0.081, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=111.7, bsz=40, num_updates=4390, lr=4.94365e-05, gnorm=0.368, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12691
2023-02-19 21:07:29 - progress_bar.py[line:274] - INFO: epoch 001:   4405 / 11564 loss=0.217, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.91, wpb=112, bsz=40, num_updates=4400, lr=4.94321e-05, gnorm=0.544, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=12702
2023-02-19 21:07:40 - progress_bar.py[line:274] - INFO: epoch 001:   4415 / 11564 loss=0.216, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=113.6, nsentences=40, sample_size=113.6, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97.4, ups=0.86, wpb=113.6, bsz=40, num_updates=4410, lr=4.94276e-05, gnorm=0.476, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=12714
2023-02-19 21:07:52 - progress_bar.py[line:274] - INFO: epoch 001:   4425 / 11564 loss=0.21, loss_v1=0, loss_v2=0, nll_loss=0.078, ntokens=112.7, nsentences=40, sample_size=112.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=101.9, ups=0.9, wpb=112.7, bsz=40, num_updates=4420, lr=4.94232e-05, gnorm=0.479, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12725
2023-02-19 21:08:03 - progress_bar.py[line:274] - INFO: epoch 001:   4435 / 11564 loss=0.222, loss_v1=0, loss_v2=0, nll_loss=0.097, ntokens=113, nsentences=40, sample_size=113, sample_size_v1=0, sample_size_v2=0, ppl=1.07, wps=100, ups=0.88, wpb=113, bsz=40, num_updates=4430, lr=4.94188e-05, gnorm=0.514, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12736
2023-02-19 21:08:14 - progress_bar.py[line:274] - INFO: epoch 001:   4445 / 11564 loss=0.22, loss_v1=0, loss_v2=0, nll_loss=0.086, ntokens=110.1, nsentences=40, sample_size=110.1, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=97, ups=0.88, wpb=110.1, bsz=40, num_updates=4440, lr=4.94143e-05, gnorm=0.492, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12748
2023-02-19 21:08:26 - progress_bar.py[line:274] - INFO: epoch 001:   4455 / 11564 loss=0.214, loss_v1=0, loss_v2=0, nll_loss=0.076, ntokens=111.3, nsentences=40, sample_size=111.3, sample_size_v1=0, sample_size_v2=0, ppl=1.05, wps=97.8, ups=0.88, wpb=111.3, bsz=40, num_updates=4450, lr=4.94099e-05, gnorm=0.618, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=12759
2023-02-19 21:08:37 - progress_bar.py[line:274] - INFO: epoch 001:   4465 / 11564 loss=0.227, loss_v1=0, loss_v2=0, nll_loss=0.087, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=102, ups=0.91, wpb=111.8, bsz=40, num_updates=4460, lr=4.94054e-05, gnorm=0.557, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=12770
2023-02-19 21:08:48 - progress_bar.py[line:274] - INFO: epoch 001:   4475 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.084, ntokens=111.7, nsentences=40, sample_size=111.7, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100.8, ups=0.9, wpb=111.7, bsz=40, num_updates=4470, lr=4.9401e-05, gnorm=0.545, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12781
2023-02-19 21:08:59 - progress_bar.py[line:274] - INFO: epoch 001:   4485 / 11564 loss=0.213, loss_v1=0, loss_v2=0, nll_loss=0.088, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.06, wps=100, ups=0.89, wpb=111.8, bsz=40, num_updates=4480, lr=4.93965e-05, gnorm=0.569, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=12792
Traceback (most recent call last):
Traceback (most recent call last):
  File "../../train.py", line 632, in <module>
  File "../../train.py", line 632, in <module>
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 3154157
Killing subprocess 3154158
Main process received SIGINT, exiting
