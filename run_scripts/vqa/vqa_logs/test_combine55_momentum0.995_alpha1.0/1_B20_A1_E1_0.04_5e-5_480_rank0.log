2023-01-09 13:52:00 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-01-09 13:52:00 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-01-09 13:52:00 - utils.py[line:261] - INFO: Start init
2023-01-09 13:52:01 - utils.py[line:261] - INFO: Start init
Retry: 1, with value error <class 'RuntimeError'>
2023-01-09 13:52:01 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/query_2card_card-bsz20_NA1_KB5_caption5_loop10000.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way/query_val_500.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/test_combine55_momentum0.995_alpha1.0', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=15', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=1', '--warmup-ratio=0.04', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=1000', '--validate-interval-updates=1000', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/20_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 255.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 4035293
Killing subprocess 4035294
2023-01-09 13:53:17 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-01-09 13:53:17 - utils.py[line:261] - INFO: Start init
2023-01-09 13:53:17 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-01-09 13:53:17 - utils.py[line:261] - INFO: Start init
2023-01-09 13:53:18 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-01-09 13:53:18 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-01-09 13:53:18 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.2023-01-09 13:53:18 - utils.py[line:274] - INFO: initialized host node4 as rank 0

single-machine distributed training is initialized.
2023-01-09 13:53:27 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/test_combine55_momentum0.995_alpha1.0', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 1000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 15, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 1, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/20_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='15', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/query_2card_card-bsz20_NA1_KB5_caption5_loop10000.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way/query_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=1, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480', save_interval=10, save_interval_updates=1000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/test_combine55_momentum0.995_alpha1.0', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=1000, wandb_project=None, warmup_ratio=0.04, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/query_2card_card-bsz20_NA1_KB5_caption5_loop10000.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/20_way/query_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/20_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.04, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-01-09 13:53:27 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-01-09 13:53:27 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-01-09 13:53:31 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-01-09 13:53:31 - train.py[line:118] - INFO: task: VqaGenTask
2023-01-09 13:53:31 - train.py[line:119] - INFO: model: OFAModel
2023-01-09 13:53:31 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-01-09 13:53:31 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-01-09 13:53:31 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way/query_val_500.tsv slice_id 0 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-01-09 13:53:31 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way/query_val_500.tsv slice_id 1 row count 74807 total row count 149614
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-01-09 13:53:32 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-01-09 13:53:33 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-01-09 13:53:34 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-01-09 13:53:34 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-01-09 13:53:34 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-01-09 13:53:34 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.995 cuda cpu, cpu
Done 0.995 cuda cpu, cpu
2023-01-09 13:53:35 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-01-09 13:53:35 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-01-09 13:53:35 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-01-09 13:53:44 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-01-09 13:53:44 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-01-09 13:53:44 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-01-09 13:53:44 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-01-09 13:53:45 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-01-09 13:53:45 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-01-09 13:53:45 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/query_2card_card-bsz20_NA1_KB5_caption5_loop10000.tsv slice_id 1 row count 2000000 total row count 4000000
file /data/private/yutianyu/datasets/OFA_data/sgg/20_way_combine/query_2card_card-bsz20_NA1_KB5_caption5_loop10000.tsv slice_id 0 row count 2000000 total row count 4000000
2023-01-09 13:53:48 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 100000, warmup steps 4000, warmup_factor 0.00025
Total steps 100000, warmup steps 4000, warmup_factor 0.00025
2023-01-09 13:53:49 - trainer.py[line:758] - INFO: begin training epoch 1
2023-01-09 13:53:49 - train.py[line:312] - INFO: Start iterating over samples
From cpu to cuda:1From cpu to cuda:0

2023-01-09 13:54:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:20 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 100000 loss=0.956, loss_v1=0, loss_v2=0, nll_loss=0.808, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.75, vqa_score=0.033, wps=71.8, ups=0.41, wpb=111.3, bsz=40, num_updates=10, lr=1.25e-07, gnorm=10.36, clip=100, loss_scale=128, train_wall=28, gb_free=10.3, ema_decay=0.9999, wall=46
2023-01-09 13:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:37 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 100000 loss=1.002, loss_v1=0, loss_v2=0, nll_loss=0.854, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.81, vqa_score=0.0312, wps=96.4, ups=0.58, wpb=111.1, bsz=40, num_updates=20, lr=2.5e-07, gnorm=10.586, clip=100, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=64
2023-01-09 13:54:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:54:55 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 100000 loss=1.074, loss_v1=0, loss_v2=0, nll_loss=0.931, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.91, vqa_score=0.0297, wps=96.9, ups=0.59, wpb=109.7, bsz=40, num_updates=30, lr=3.75e-07, gnorm=11.186, clip=100, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=81
2023-01-09 13:55:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:12 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 100000 loss=1.03, loss_v1=0, loss_v2=0, nll_loss=0.884, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.85, vqa_score=0.0202, wps=96.8, ups=0.58, wpb=111.2, bsz=40, num_updates=40, lr=5e-07, gnorm=10.536, clip=100, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=98
2023-01-09 13:55:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:30 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 100000 loss=1.066, loss_v1=0, loss_v2=0, nll_loss=0.926, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.9, vqa_score=0.0417, wps=92.4, ups=0.56, wpb=110.3, bsz=40, num_updates=50, lr=6.25e-07, gnorm=11.104, clip=100, loss_scale=128, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=116
2023-01-09 13:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:46 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 100000 loss=0.932, loss_v1=0, loss_v2=0, nll_loss=0.785, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.72, vqa_score=0.051, wps=103, ups=0.62, wpb=110.1, bsz=40, num_updates=60, lr=7.5e-07, gnorm=8.44, clip=100, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=133
2023-01-09 13:55:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:55:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:04 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 100000 loss=0.903, loss_v1=0, loss_v2=0, nll_loss=0.763, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.7, vqa_score=0.0556, wps=99.3, ups=0.6, wpb=110.6, bsz=40, num_updates=70, lr=8.75e-07, gnorm=9.706, clip=100, loss_scale=128, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=150
2023-01-09 13:56:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:20 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 100000 loss=0.972, loss_v1=0, loss_v2=0, nll_loss=0.847, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.8, vqa_score=0, wps=99.9, ups=0.62, wpb=108, bsz=40, num_updates=80, lr=1e-06, gnorm=8.848, clip=100, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=166
2023-01-09 13:56:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:37 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 100000 loss=0.867, loss_v1=0, loss_v2=0, nll_loss=0.743, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.67, vqa_score=0, wps=100.3, ups=0.61, wpb=109.2, bsz=40, num_updates=90, lr=1.125e-06, gnorm=6.618, clip=100, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=183
2023-01-09 13:56:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:56:54 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 100000 loss=0.815, loss_v1=0, loss_v2=0, nll_loss=0.694, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.62, vqa_score=0.0426, wps=98.4, ups=0.59, wpb=110.3, bsz=40, num_updates=100, lr=1.25e-06, gnorm=6.12, clip=100, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=200
2023-01-09 13:56:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:10 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 100000 loss=0.853, loss_v1=0, loss_v2=0, nll_loss=0.746, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.68, vqa_score=0.0636, wps=101, ups=0.62, wpb=109.1, bsz=40, num_updates=110, lr=1.375e-06, gnorm=5.804, clip=100, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=216
2023-01-09 13:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:27 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 100000 loss=0.763, loss_v1=0, loss_v2=0, nll_loss=0.651, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.57, vqa_score=0.0286, wps=100.5, ups=0.61, wpb=109.5, bsz=40, num_updates=120, lr=1.5e-06, gnorm=4.791, clip=100, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=233
2023-01-09 13:57:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:44 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 100000 loss=0.708, loss_v1=0, loss_v2=0, nll_loss=0.592, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.51, vqa_score=0.0108, wps=99.5, ups=0.6, wpb=110.2, bsz=40, num_updates=130, lr=1.625e-06, gnorm=4.471, clip=100, loss_scale=128, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=250
2023-01-09 13:57:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:57:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:01 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 100000 loss=0.756, loss_v1=0, loss_v2=0, nll_loss=0.648, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.57, vqa_score=0.0566, wps=96.3, ups=0.58, wpb=110.1, bsz=40, num_updates=140, lr=1.75e-06, gnorm=4.32, clip=100, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=267
2023-01-09 13:58:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:17 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 100000 loss=0.777, loss_v1=0, loss_v2=0, nll_loss=0.674, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.6, vqa_score=0, wps=100.3, ups=0.62, wpb=107.6, bsz=40, num_updates=150, lr=1.875e-06, gnorm=4.077, clip=100, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=284
2023-01-09 13:58:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:35 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0505, wps=95.4, ups=0.58, wpb=108.9, bsz=40, num_updates=160, lr=2e-06, gnorm=3.673, clip=100, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=301
2023-01-09 13:58:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:51 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 100000 loss=0.701, loss_v1=0, loss_v2=0, nll_loss=0.594, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.51, vqa_score=0.0198, wps=99.8, ups=0.61, wpb=109.1, bsz=40, num_updates=170, lr=2.125e-06, gnorm=3.66, clip=100, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=318
2023-01-09 13:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:58:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:08 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 100000 loss=0.697, loss_v1=0, loss_v2=0, nll_loss=0.599, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.51, vqa_score=0.0273, wps=100.7, ups=0.61, wpb=110.4, bsz=40, num_updates=180, lr=2.25e-06, gnorm=3.457, clip=100, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=334
2023-01-09 13:59:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:25 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 100000 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.514, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.43, vqa_score=0.0633, wps=101.5, ups=0.61, wpb=111.8, bsz=40, num_updates=190, lr=2.375e-06, gnorm=3.443, clip=100, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=351
2023-01-09 13:59:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:43 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 100000 loss=0.675, loss_v1=0, loss_v2=0, nll_loss=0.564, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.48, vqa_score=0, wps=91.6, ups=0.56, wpb=109.6, bsz=40, num_updates=200, lr=2.5e-06, gnorm=2.975, clip=100, loss_scale=128, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=369
2023-01-09 13:59:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 13:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:02 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 100000 loss=0.717, loss_v1=0, loss_v2=0, nll_loss=0.622, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.54, vqa_score=0.0172, wps=86.4, ups=0.53, wpb=108, bsz=40, num_updates=210, lr=2.625e-06, gnorm=3.416, clip=100, loss_scale=128, train_wall=19, gb_free=10.2, ema_decay=0.9999, wall=388
2023-01-09 14:00:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:18 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.067, nsentences=40, sample_size=107.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0351, wps=99.7, ups=0.62, wpb=107.1, bsz=40, num_updates=220, lr=2.75e-06, gnorm=3.092, clip=100, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=404
2023-01-09 14:00:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:35 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 100000 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.523, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.44, vqa_score=0.0103, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=230, lr=2.875e-06, gnorm=3.054, clip=100, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=421
2023-01-09 14:00:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:51 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0392, wps=100.3, ups=0.61, wpb=108.8, bsz=40, num_updates=240, lr=3e-06, gnorm=3.17, clip=100, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=438
2023-01-09 14:00:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:00:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:08 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0297, wps=100.5, ups=0.61, wpb=109.7, bsz=40, num_updates=250, lr=3.125e-06, gnorm=2.498, clip=100, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=454
2023-01-09 14:01:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:25 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 100000 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.53, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.44, vqa_score=0.0222, wps=99.6, ups=0.6, wpb=109.9, bsz=40, num_updates=260, lr=3.25e-06, gnorm=2.776, clip=100, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=471
2023-01-09 14:01:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:41 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 100000 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.508, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.42, vqa_score=0.0619, wps=100.1, ups=0.61, wpb=109, bsz=40, num_updates=270, lr=3.375e-06, gnorm=2.659, clip=100, loss_scale=128, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=488
2023-01-09 14:01:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:01:58 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 100000 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.523, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.44, vqa_score=0.02, wps=102.7, ups=0.62, wpb=109.9, bsz=40, num_updates=280, lr=3.5e-06, gnorm=2.423, clip=100, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=504
2023-01-09 14:02:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:14 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0385, wps=103, ups=0.62, wpb=110.1, bsz=40, num_updates=290, lr=3.625e-06, gnorm=2.68, clip=100, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=520
2023-01-09 14:02:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:30 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 100000 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.519, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.43, vqa_score=0.0306, wps=103, ups=0.63, wpb=109.7, bsz=40, num_updates=300, lr=3.75e-06, gnorm=2.691, clip=100, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=536
2023-01-09 14:02:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:47 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 100000 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.51, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.42, vqa_score=0.0198, wps=100.2, ups=0.61, wpb=110.3, bsz=40, num_updates=310, lr=3.875e-06, gnorm=2.311, clip=100, loss_scale=128, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=553
2023-01-09 14:02:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:03 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 100000 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.522, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.44, vqa_score=0.027, wps=100.8, ups=0.61, wpb=109.8, bsz=40, num_updates=320, lr=4e-06, gnorm=2.772, clip=100, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=570
2023-01-09 14:03:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:20 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0297, wps=101.5, ups=0.62, wpb=109.1, bsz=40, num_updates=330, lr=4.125e-06, gnorm=2.53, clip=100, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=586
2023-01-09 14:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:36 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 100000 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, vqa_score=0.051, wps=99.8, ups=0.61, wpb=109.6, bsz=40, num_updates=340, lr=4.25e-06, gnorm=2.488, clip=100, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=603
2023-01-09 14:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:03:53 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 100000 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.522, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.44, vqa_score=0.0093, wps=101.9, ups=0.62, wpb=109.9, bsz=40, num_updates=350, lr=4.375e-06, gnorm=2.474, clip=100, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=619
2023-01-09 14:03:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:10 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 100000 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.37, vqa_score=0.0435, wps=99.7, ups=0.6, wpb=110.9, bsz=40, num_updates=360, lr=4.5e-06, gnorm=2.342, clip=100, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=636
2023-01-09 14:04:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:26 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 100000 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.37, vqa_score=0, wps=102.5, ups=0.61, wpb=111.9, bsz=40, num_updates=370, lr=4.625e-06, gnorm=2.192, clip=100, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=652
2023-01-09 14:04:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:43 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 100000 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.491, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.41, vqa_score=0.0202, wps=101.8, ups=0.62, wpb=109.5, bsz=40, num_updates=380, lr=4.75e-06, gnorm=2.605, clip=100, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=669
2023-01-09 14:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:04:59 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 100000 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.544, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.46, vqa_score=0.0085, wps=100.1, ups=0.62, wpb=108.2, bsz=40, num_updates=390, lr=4.875e-06, gnorm=2.427, clip=100, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=685
2023-01-09 14:05:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:15 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 100000 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, vqa_score=0.0374, wps=102.7, ups=0.62, wpb=109.8, bsz=40, num_updates=400, lr=5e-06, gnorm=2.155, clip=100, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=702
2023-01-09 14:05:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:32 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 100000 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.39, vqa_score=0, wps=99.4, ups=0.6, wpb=110.7, bsz=40, num_updates=410, lr=5.125e-06, gnorm=2.437, clip=100, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=718
2023-01-09 14:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:49 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 100000 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.472, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.39, vqa_score=0.0316, wps=103, ups=0.62, wpb=111, bsz=40, num_updates=420, lr=5.25e-06, gnorm=2.442, clip=100, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=735
2023-01-09 14:05:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:05:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:06 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 100000 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.38, vqa_score=0.0098, wps=96.5, ups=0.58, wpb=110.5, bsz=40, num_updates=430, lr=5.375e-06, gnorm=2.28, clip=100, loss_scale=128, train_wall=17, gb_free=9.4, ema_decay=0.9999, wall=752
2023-01-09 14:06:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:22 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 100000 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, vqa_score=0.01, wps=102, ups=0.62, wpb=110.2, bsz=40, num_updates=440, lr=5.5e-06, gnorm=2.188, clip=100, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=769
2023-01-09 14:06:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:39 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 100000 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.36, vqa_score=0.0194, wps=99.7, ups=0.6, wpb=110.3, bsz=40, num_updates=450, lr=5.625e-06, gnorm=2.153, clip=100, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=785
2023-01-09 14:06:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:06:56 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 100000 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.36, vqa_score=0.0306, wps=101.3, ups=0.61, wpb=110.3, bsz=40, num_updates=460, lr=5.75e-06, gnorm=2.301, clip=100, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=802
2023-01-09 14:07:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:12 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 100000 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, vqa_score=0, wps=105.7, ups=0.64, wpb=110.4, bsz=40, num_updates=470, lr=5.875e-06, gnorm=2.088, clip=100, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=818
2023-01-09 14:07:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:29 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 100000 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.37, vqa_score=0.0098, wps=97.9, ups=0.6, wpb=109.1, bsz=40, num_updates=480, lr=6e-06, gnorm=2.296, clip=100, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=835
2023-01-09 14:07:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:45 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 100000 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.0532, wps=100.8, ups=0.61, wpb=110.5, bsz=40, num_updates=490, lr=6.125e-06, gnorm=2.267, clip=100, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=851
2023-01-09 14:07:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:07:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:02 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 100000 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.34, vqa_score=0.049, wps=101.4, ups=0.62, wpb=109.5, bsz=40, num_updates=500, lr=6.25e-06, gnorm=1.964, clip=90, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=868
2023-01-09 14:08:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:18 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0495, wps=100.7, ups=0.61, wpb=109.7, bsz=40, num_updates=510, lr=6.375e-06, gnorm=2.227, clip=100, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=884
2023-01-09 14:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:35 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 100000 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.36, vqa_score=0.0495, wps=101.6, ups=0.62, wpb=109.9, bsz=40, num_updates=520, lr=6.5e-06, gnorm=2.189, clip=100, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=901
2023-01-09 14:08:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:51 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 100000 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.0104, wps=104, ups=0.63, wpb=110.9, bsz=40, num_updates=530, lr=6.625e-06, gnorm=2.038, clip=100, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=917
2023-01-09 14:08:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:08:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:07 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 100000 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.37, vqa_score=0, wps=101.9, ups=0.62, wpb=109.9, bsz=40, num_updates=540, lr=6.75e-06, gnorm=2.378, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=933
2023-01-09 14:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:24 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 100000 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.482, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.4, vqa_score=0.0481, wps=97.7, ups=0.6, wpb=109.1, bsz=40, num_updates=550, lr=6.875e-06, gnorm=2.033, clip=100, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=950
2023-01-09 14:09:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:40 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 100000 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.36, vqa_score=0.0309, wps=102, ups=0.62, wpb=109, bsz=40, num_updates=560, lr=7e-06, gnorm=2.031, clip=100, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=967
2023-01-09 14:09:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:09:57 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 100000 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.35, vqa_score=0.028, wps=103.1, ups=0.63, wpb=109.5, bsz=40, num_updates=570, lr=7.125e-06, gnorm=2.024, clip=90, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=983
2023-01-09 14:10:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:13 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 100000 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.0435, wps=105.2, ups=0.63, wpb=111, bsz=40, num_updates=580, lr=7.25e-06, gnorm=2.033, clip=100, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=999
2023-01-09 14:10:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:29 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0187, wps=102.3, ups=0.62, wpb=110.3, bsz=40, num_updates=590, lr=7.375e-06, gnorm=2.229, clip=100, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=1015
2023-01-09 14:10:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:45 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 100000 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.35, vqa_score=0.0202, wps=100.5, ups=0.61, wpb=109, bsz=40, num_updates=600, lr=7.5e-06, gnorm=2.032, clip=100, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=1032
2023-01-09 14:10:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:10:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:02 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 100000 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.01, wps=101.3, ups=0.61, wpb=111.1, bsz=40, num_updates=610, lr=7.625e-06, gnorm=1.787, clip=100, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=1048
2023-01-09 14:11:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:19 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 100000 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.36, vqa_score=0.05, wps=100.7, ups=0.61, wpb=110, bsz=40, num_updates=620, lr=7.75e-06, gnorm=2.058, clip=100, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=1065
2023-01-09 14:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:36 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.01, wps=97.4, ups=0.59, wpb=109.3, bsz=40, num_updates=630, lr=7.875e-06, gnorm=1.728, clip=100, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=1082
2023-01-09 14:11:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:11:52 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 100000 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.35, vqa_score=0.0202, wps=101.7, ups=0.62, wpb=109.9, bsz=40, num_updates=640, lr=8e-06, gnorm=1.807, clip=100, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=1098
2023-01-09 14:11:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:09 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 100000 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.35, vqa_score=0.0185, wps=99.4, ups=0.61, wpb=108.9, bsz=40, num_updates=650, lr=8.125e-06, gnorm=1.848, clip=100, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=1115
2023-01-09 14:12:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:25 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0286, wps=100.9, ups=0.61, wpb=109.8, bsz=40, num_updates=660, lr=8.25e-06, gnorm=1.868, clip=100, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=1132
2023-01-09 14:12:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:42 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 100000 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.03, wps=103, ups=0.62, wpb=110.6, bsz=40, num_updates=670, lr=8.375e-06, gnorm=1.583, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1148
2023-01-09 14:12:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:12:58 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0196, wps=100.5, ups=0.6, wpb=110.9, bsz=40, num_updates=680, lr=8.5e-06, gnorm=1.736, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1165
2023-01-09 14:13:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:15 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 100000 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.03, wps=99.7, ups=0.61, wpb=109.7, bsz=40, num_updates=690, lr=8.625e-06, gnorm=1.866, clip=90, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=1181
2023-01-09 14:13:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:32 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 100000 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.31, vqa_score=0.0101, wps=100.4, ups=0.61, wpb=109.5, bsz=40, num_updates=700, lr=8.75e-06, gnorm=1.708, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1198
2023-01-09 14:13:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:48 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 100000 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.31, vqa_score=0.0099, wps=104, ups=0.63, wpb=110, bsz=40, num_updates=710, lr=8.875e-06, gnorm=1.729, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1214
2023-01-09 14:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:13:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:04 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 100000 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, vqa_score=0.0381, wps=103.8, ups=0.62, wpb=111.2, bsz=40, num_updates=720, lr=9e-06, gnorm=1.932, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1230
2023-01-09 14:14:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:21 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 100000 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.0217, wps=99.8, ups=0.61, wpb=108.6, bsz=40, num_updates=730, lr=9.125e-06, gnorm=1.845, clip=100, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=1247
2023-01-09 14:14:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:37 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 100000 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0485, wps=101.5, ups=0.62, wpb=108.8, bsz=40, num_updates=740, lr=9.25e-06, gnorm=1.661, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1263
2023-01-09 14:14:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:14:53 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 100000 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.044, wps=102.6, ups=0.62, wpb=110.9, bsz=40, num_updates=750, lr=9.375e-06, gnorm=1.786, clip=100, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=1280
2023-01-09 14:14:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:09 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 100000 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0326, wps=102.6, ups=0.63, wpb=109.3, bsz=40, num_updates=760, lr=9.5e-06, gnorm=1.796, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1296
2023-01-09 14:15:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:26 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 100000 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.35, vqa_score=0.0536, wps=99.6, ups=0.62, wpb=107.9, bsz=40, num_updates=770, lr=9.625e-06, gnorm=1.817, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1312
2023-01-09 14:15:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:42 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 100000 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0549, wps=105.5, ups=0.63, wpb=111.7, bsz=40, num_updates=780, lr=9.75e-06, gnorm=1.918, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1328
2023-01-09 14:15:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:15:59 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 100000 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, vqa_score=0.0323, wps=98.3, ups=0.59, wpb=110.4, bsz=40, num_updates=790, lr=9.875e-06, gnorm=1.822, clip=100, loss_scale=256, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=1345
2023-01-09 14:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:15 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 100000 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0326, wps=103.1, ups=0.62, wpb=110.5, bsz=40, num_updates=800, lr=1e-05, gnorm=1.749, clip=90, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=1362
2023-01-09 14:16:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:32 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.04, wps=100.5, ups=0.61, wpb=110.2, bsz=40, num_updates=810, lr=1.0125e-05, gnorm=1.752, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1378
2023-01-09 14:16:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:49 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 100000 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0, wps=101.4, ups=0.61, wpb=110.5, bsz=40, num_updates=820, lr=1.025e-05, gnorm=1.337, clip=90, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=1395
2023-01-09 14:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:16:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:05 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 100000 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.0183, wps=100, ups=0.61, wpb=110.1, bsz=40, num_updates=830, lr=1.0375e-05, gnorm=1.691, clip=100, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=1412
2023-01-09 14:17:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:22 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 100000 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.036, wps=99, ups=0.61, wpb=108.1, bsz=40, num_updates=840, lr=1.05e-05, gnorm=1.689, clip=90, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1428
2023-01-09 14:17:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:38 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 100000 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0404, wps=102.7, ups=0.62, wpb=110.7, bsz=40, num_updates=850, lr=1.0625e-05, gnorm=1.782, clip=100, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=1444
2023-01-09 14:17:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:17:55 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 100000 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.05, wps=99.4, ups=0.6, wpb=109.8, bsz=40, num_updates=860, lr=1.075e-05, gnorm=1.557, clip=100, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=1461
2023-01-09 14:18:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:12 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 100000 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0105, wps=101.2, ups=0.61, wpb=109.8, bsz=40, num_updates=870, lr=1.0875e-05, gnorm=1.718, clip=100, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=1478
2023-01-09 14:18:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:28 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 100000 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0106, wps=101.2, ups=0.61, wpb=111.5, bsz=40, num_updates=880, lr=1.1e-05, gnorm=1.852, clip=100, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=1494
2023-01-09 14:18:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:45 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 100000 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.0446, wps=100.8, ups=0.62, wpb=108.9, bsz=40, num_updates=890, lr=1.1125e-05, gnorm=1.539, clip=100, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=1511
2023-01-09 14:18:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:18:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:01 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 100000 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0426, wps=99.5, ups=0.61, wpb=109.5, bsz=40, num_updates=900, lr=1.125e-05, gnorm=1.645, clip=90, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1528
2023-01-09 14:19:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:18 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 100000 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.32, vqa_score=0.036, wps=102.6, ups=0.62, wpb=109.7, bsz=40, num_updates=910, lr=1.1375e-05, gnorm=1.974, clip=100, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=1544
2023-01-09 14:19:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:34 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 100000 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, vqa_score=0.0183, wps=100.1, ups=0.61, wpb=108.6, bsz=40, num_updates=920, lr=1.15e-05, gnorm=1.772, clip=100, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=1560
2023-01-09 14:19:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:51 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 100000 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, vqa_score=0.0198, wps=100.9, ups=0.61, wpb=110.6, bsz=40, num_updates=930, lr=1.1625e-05, gnorm=1.636, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1577
2023-01-09 14:19:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:19:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:08 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0098, wps=99.8, ups=0.6, wpb=110.4, bsz=40, num_updates=940, lr=1.175e-05, gnorm=1.72, clip=100, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=1594
2023-01-09 14:20:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:25 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 100000 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.31, vqa_score=0.0291, wps=98.6, ups=0.6, wpb=110.1, bsz=40, num_updates=950, lr=1.1875e-05, gnorm=1.747, clip=100, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=1611
2023-01-09 14:20:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:41 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 100000 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0108, wps=105.8, ups=0.63, wpb=111.9, bsz=40, num_updates=960, lr=1.2e-05, gnorm=1.634, clip=90, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=1627
2023-01-09 14:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:20:57 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 100000 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.05, wps=102.6, ups=0.61, wpb=111.9, bsz=40, num_updates=970, lr=1.2125e-05, gnorm=1.562, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1643
2023-01-09 14:21:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:14 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 100000 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0294, wps=100.3, ups=0.61, wpb=109.5, bsz=40, num_updates=980, lr=1.225e-05, gnorm=1.61, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=1660
2023-01-09 14:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:30 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 100000 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0194, wps=103.6, ups=0.63, wpb=109.3, bsz=40, num_updates=990, lr=1.2375e-05, gnorm=1.611, clip=100, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=1676
2023-01-09 14:21:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 14:21:46 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 100000 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.023, wps=102.2, ups=0.61, wpb=112.2, bsz=40, num_updates=1000, lr=1.25e-05, gnorm=1.697, clip=100, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=1693
2023-01-09 14:21:46 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-09 14:21:46 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-01-09 14:21:48 - train.py[line:549] - INFO: 0 / 4988
2023-01-09 14:21:48 - train.py[line:551] - INFO: load:1.17 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-09 14:24:22 - train.py[line:549] - INFO: 200 / 4988
2023-01-09 14:24:22 - train.py[line:551] - INFO: load:1.20 valid_run:153.63 task_valid:150.20 collect_output:2.38
2023-01-09 14:26:50 - train.py[line:549] - INFO: 400 / 4988
2023-01-09 14:26:50 - train.py[line:551] - INFO: load:1.22 valid_run:301.68 task_valid:293.41 collect_output:6.21
2023-01-09 14:29:21 - train.py[line:549] - INFO: 600 / 4988
2023-01-09 14:29:21 - train.py[line:551] - INFO: load:1.25 valid_run:452.91 task_valid:436.42 collect_output:13.40
2023-01-09 14:31:50 - train.py[line:549] - INFO: 800 / 4988
2023-01-09 14:31:50 - train.py[line:551] - INFO: load:1.28 valid_run:601.33 task_valid:581.27 collect_output:15.93
2023-01-09 14:34:22 - train.py[line:549] - INFO: 1000 / 4988
2023-01-09 14:34:22 - train.py[line:551] - INFO: load:1.30 valid_run:753.17 task_valid:728.64 collect_output:19.34
2023-01-09 14:36:53 - train.py[line:549] - INFO: 1200 / 4988
2023-01-09 14:36:53 - train.py[line:551] - INFO: load:1.33 valid_run:904.46 task_valid:874.42 collect_output:23.83
2023-01-09 14:39:25 - train.py[line:549] - INFO: 1400 / 4988
2023-01-09 14:39:25 - train.py[line:551] - INFO: load:1.36 valid_run:1056.66 task_valid:1020.59 collect_output:28.80
2023-01-09 14:41:55 - train.py[line:549] - INFO: 1600 / 4988
2023-01-09 14:41:55 - train.py[line:551] - INFO: load:1.38 valid_run:1206.60 task_valid:1161.60 collect_output:36.71
2023-01-09 14:44:24 - train.py[line:549] - INFO: 1800 / 4988
2023-01-09 14:44:24 - train.py[line:551] - INFO: load:1.41 valid_run:1355.24 task_valid:1306.21 collect_output:39.72
2023-01-09 14:46:52 - train.py[line:549] - INFO: 2000 / 4988
2023-01-09 14:46:52 - train.py[line:551] - INFO: load:1.44 valid_run:1503.21 task_valid:1449.59 collect_output:43.29
2023-01-09 14:49:22 - train.py[line:549] - INFO: 2200 / 4988
2023-01-09 14:49:22 - train.py[line:551] - INFO: load:1.47 valid_run:1652.67 task_valid:1594.88 collect_output:46.43
2023-01-09 14:51:51 - train.py[line:549] - INFO: 2400 / 4988
2023-01-09 14:51:51 - train.py[line:551] - INFO: load:1.49 valid_run:1802.30 task_valid:1740.23 collect_output:49.65
2023-01-09 14:54:20 - train.py[line:549] - INFO: 2600 / 4988
2023-01-09 14:54:20 - train.py[line:551] - INFO: load:1.52 valid_run:1951.33 task_valid:1882.09 collect_output:55.77
2023-01-09 14:56:51 - train.py[line:549] - INFO: 2800 / 4988
2023-01-09 14:56:51 - train.py[line:551] - INFO: load:1.55 valid_run:2101.92 task_valid:2027.97 collect_output:59.45
2023-01-09 14:59:21 - train.py[line:549] - INFO: 3000 / 4988
2023-01-09 14:59:21 - train.py[line:551] - INFO: load:1.57 valid_run:2251.85 task_valid:2174.73 collect_output:61.56
2023-01-09 15:01:51 - train.py[line:549] - INFO: 3200 / 4988
2023-01-09 15:01:51 - train.py[line:551] - INFO: load:1.60 valid_run:2401.62 task_valid:2319.40 collect_output:65.62
2023-01-09 15:04:22 - train.py[line:549] - INFO: 3400 / 4988
2023-01-09 15:04:22 - train.py[line:551] - INFO: load:1.63 valid_run:2552.35 task_valid:2465.29 collect_output:69.44
2023-01-09 15:06:53 - train.py[line:549] - INFO: 3600 / 4988
2023-01-09 15:06:53 - train.py[line:551] - INFO: load:1.66 valid_run:2703.01 task_valid:2612.85 collect_output:71.51
2023-01-09 15:09:20 - train.py[line:549] - INFO: 3800 / 4988
2023-01-09 15:09:20 - train.py[line:551] - INFO: load:1.68 valid_run:2850.50 task_valid:2754.72 collect_output:76.10
2023-01-09 15:11:50 - train.py[line:549] - INFO: 4000 / 4988
2023-01-09 15:11:50 - train.py[line:551] - INFO: load:1.71 valid_run:3000.01 task_valid:2900.09 collect_output:79.19
2023-01-09 15:14:20 - train.py[line:549] - INFO: 4200 / 4988
2023-01-09 15:14:20 - train.py[line:551] - INFO: load:1.74 valid_run:3150.64 task_valid:3044.70 collect_output:84.22
2023-01-09 15:16:49 - train.py[line:549] - INFO: 4400 / 4988
2023-01-09 15:16:50 - train.py[line:551] - INFO: load:1.76 valid_run:3299.66 task_valid:3189.66 collect_output:87.24
2023-01-09 15:19:20 - train.py[line:549] - INFO: 4600 / 4988
2023-01-09 15:19:20 - train.py[line:551] - INFO: load:1.79 valid_run:3450.03 task_valid:3336.16 collect_output:90.08
2023-01-09 15:21:51 - train.py[line:549] - INFO: 4800 / 4988
2023-01-09 15:21:51 - train.py[line:551] - INFO: load:1.82 valid_run:3600.79 task_valid:3482.96 collect_output:93.02

====================================================================================================
SGG eval:     R @ 50: 0.1791;     R @ 100: 0.2608;     R @ 500: 0.3463;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0835;    mR @ 100: 0.1414;    mR @ 500: 0.1910;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0293) (covered in:0.0000) (covering:0.1429) (eating:0.2353) (flying in:0.5000) (growing on:0.1250) (hanging from:0.3935) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.1250) (playing:0.0000) (riding:0.2059) (says:0.0000) (sitting on:0.3078) (standing on:0.5033) (using:0.1500) (walking in:0.0000) (walking on:0.0270) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.1791;     R @ 100: 0.2608;     R @ 500: 0.3463;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0835;    mR @ 100: 0.1414;    mR @ 500: 0.1910;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0293) (covered in:0.0000) (covering:0.1429) (eating:0.2353) (flying in:0.5000) (growing on:0.1250) (hanging from:0.3935) (lying on:0.0000) (mounted on:0.0000) (painted on:0.0833) (parked on:0.1250) (playing:0.0000) (riding:0.2059) (says:0.0000) (sitting on:0.3078) (standing on:0.5033) (using:0.1500) (walking in:0.0000) (walking on:0.0270) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-01-09 15:24:21 - train.py[line:487] - INFO: 0.26080000000000003
2023-01-09 15:24:22 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-09 15:24:22 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.267 | loss_v1 0 | loss_v2 0 | nll_loss 0.102 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.2608 | ppl 1.07 | vqa_score 0.1779 | wps 119.6 | wpb 89.9 | bsz 30 | num_updates 1000
2023-01-09 15:24:22 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 1000 updates
2023-01-09 15:24:22 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_1000.pt
2023-01-09 15:24:58 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_1000.pt
2023-01-09 15:27:36 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_1000.pt (epoch 1 @ 1000 updates, score 0.26080000000000003) (writing took 194.0191730055958 seconds)
2023-01-09 15:27:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:27:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:27:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:27:52 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0367, wps=0.4, ups=0, wpb=109.3, bsz=40, num_updates=1010, lr=1.2625e-05, gnorm=1.692, clip=100, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=5658
2023-01-09 15:27:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:09 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 100000 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.0294, wps=98.9, ups=0.6, wpb=109.9, bsz=40, num_updates=1020, lr=1.275e-05, gnorm=2.058, clip=100, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=5675
2023-01-09 15:28:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:26 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 100000 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0187, wps=98.2, ups=0.6, wpb=108.9, bsz=40, num_updates=1030, lr=1.2875e-05, gnorm=1.586, clip=100, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=5692
2023-01-09 15:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:42 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 100000 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0283, wps=102, ups=0.63, wpb=108.3, bsz=40, num_updates=1040, lr=1.3e-05, gnorm=1.515, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=5708
2023-01-09 15:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:28:59 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0096, wps=98.2, ups=0.6, wpb=109.5, bsz=40, num_updates=1050, lr=1.3125e-05, gnorm=1.548, clip=90, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=5725
2023-01-09 15:29:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:16 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0385, wps=100.3, ups=0.6, wpb=110.7, bsz=40, num_updates=1060, lr=1.325e-05, gnorm=1.691, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=5742
2023-01-09 15:29:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:32 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 100000 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0115, wps=103.8, ups=0.63, wpb=110.1, bsz=40, num_updates=1070, lr=1.3375e-05, gnorm=1.45, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=5758
2023-01-09 15:29:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:48 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 100000 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0495, wps=101.1, ups=0.62, wpb=108.9, bsz=40, num_updates=1080, lr=1.35e-05, gnorm=1.561, clip=100, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=5774
2023-01-09 15:29:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:29:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:04 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 100000 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0283, wps=104.3, ups=0.63, wpb=110.8, bsz=40, num_updates=1090, lr=1.3625e-05, gnorm=1.499, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=5791
2023-01-09 15:30:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:21 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 100000 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0102, wps=100.3, ups=0.6, wpb=110.8, bsz=40, num_updates=1100, lr=1.375e-05, gnorm=1.495, clip=100, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=5807
2023-01-09 15:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:38 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 100000 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0, wps=98.1, ups=0.6, wpb=109.3, bsz=40, num_updates=1110, lr=1.3875e-05, gnorm=1.709, clip=90, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=5824
2023-01-09 15:30:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:30:55 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 100000 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0353, wps=101.2, ups=0.61, wpb=110.5, bsz=40, num_updates=1120, lr=1.4e-05, gnorm=1.573, clip=100, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=5841
2023-01-09 15:31:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:11 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 100000 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0204, wps=100.3, ups=0.61, wpb=109.5, bsz=40, num_updates=1130, lr=1.4125e-05, gnorm=1.564, clip=90, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=5858
2023-01-09 15:31:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:28 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 100000 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, vqa_score=0.0495, wps=101, ups=0.61, wpb=110.2, bsz=40, num_updates=1140, lr=1.425e-05, gnorm=1.908, clip=100, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=5874
2023-01-09 15:31:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:44 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0309, wps=99.9, ups=0.62, wpb=107.9, bsz=40, num_updates=1150, lr=1.4375e-05, gnorm=1.603, clip=100, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=5891
2023-01-09 15:31:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:31:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:01 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 100000 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0495, wps=103.7, ups=0.63, wpb=110.1, bsz=40, num_updates=1160, lr=1.45e-05, gnorm=1.489, clip=90, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=5907
2023-01-09 15:32:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:17 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 100000 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0388, wps=102.8, ups=0.62, wpb=109.8, bsz=40, num_updates=1170, lr=1.4625e-05, gnorm=1.433, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=5923
2023-01-09 15:32:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:33 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 100000 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=112.467, nsentences=40, sample_size=112.467, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0103, wps=104.8, ups=0.62, wpb=112.5, bsz=40, num_updates=1180, lr=1.475e-05, gnorm=1.618, clip=100, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=5939
2023-01-09 15:32:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:50 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0392, wps=99.9, ups=0.6, wpb=110.5, bsz=40, num_updates=1190, lr=1.4875e-05, gnorm=1.496, clip=100, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=5956
2023-01-09 15:32:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:32:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:07 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0103, wps=99.5, ups=0.6, wpb=109.8, bsz=40, num_updates=1200, lr=1.5e-05, gnorm=1.575, clip=100, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=5973
2023-01-09 15:33:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:23 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 100000 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0377, wps=102.7, ups=0.62, wpb=110, bsz=40, num_updates=1210, lr=1.5125e-05, gnorm=1.495, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=5989
2023-01-09 15:33:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:39 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 100000 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0636, wps=100.4, ups=0.62, wpb=108.4, bsz=40, num_updates=1220, lr=1.525e-05, gnorm=1.916, clip=100, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=6006
2023-01-09 15:33:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:33:56 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 100000 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0204, wps=103.3, ups=0.62, wpb=110.4, bsz=40, num_updates=1230, lr=1.5375e-05, gnorm=1.44, clip=100, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=6022
2023-01-09 15:34:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:12 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 100000 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0323, wps=104.2, ups=0.63, wpb=110.9, bsz=40, num_updates=1240, lr=1.55e-05, gnorm=1.58, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6038
2023-01-09 15:34:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:28 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0211, wps=101.7, ups=0.61, wpb=110.3, bsz=40, num_updates=1250, lr=1.5625e-05, gnorm=1.648, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6054
2023-01-09 15:34:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:45 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 100000 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0106, wps=99.1, ups=0.6, wpb=110.5, bsz=40, num_updates=1260, lr=1.575e-05, gnorm=1.53, clip=100, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=6071
2023-01-09 15:34:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:34:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:02 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0769, wps=101.6, ups=0.61, wpb=111.8, bsz=40, num_updates=1270, lr=1.5875e-05, gnorm=1.572, clip=100, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=6088
2023-01-09 15:35:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:18 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 100000 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0412, wps=102.2, ups=0.62, wpb=110.1, bsz=40, num_updates=1280, lr=1.6e-05, gnorm=1.875, clip=100, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=6105
2023-01-09 15:35:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:34 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 100000 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0187, wps=103.8, ups=0.63, wpb=109.1, bsz=40, num_updates=1290, lr=1.6125e-05, gnorm=1.332, clip=100, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=6121
2023-01-09 15:35:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:51 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0194, wps=99, ups=0.61, wpb=107.6, bsz=40, num_updates=1300, lr=1.625e-05, gnorm=1.772, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6137
2023-01-09 15:35:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:35:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:07 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 100000 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0103, wps=102.4, ups=0.63, wpb=108.9, bsz=40, num_updates=1310, lr=1.6375e-05, gnorm=1.673, clip=100, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=6153
2023-01-09 15:36:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:24 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 100000 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0337, wps=101.9, ups=0.61, wpb=110.6, bsz=40, num_updates=1320, lr=1.65e-05, gnorm=1.479, clip=100, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=6170
2023-01-09 15:36:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:40 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 100000 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0097, wps=102.1, ups=0.62, wpb=109.8, bsz=40, num_updates=1330, lr=1.6625e-05, gnorm=1.828, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6186
2023-01-09 15:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:36:56 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 100000 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0385, wps=101.6, ups=0.61, wpb=110.3, bsz=40, num_updates=1340, lr=1.675e-05, gnorm=1.685, clip=100, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=6203
2023-01-09 15:37:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:12 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 100000 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0309, wps=104.5, ups=0.63, wpb=109.7, bsz=40, num_updates=1350, lr=1.6875e-05, gnorm=1.664, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6219
2023-01-09 15:37:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:29 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 100000 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0619, wps=101.4, ups=0.61, wpb=110.2, bsz=40, num_updates=1360, lr=1.7e-05, gnorm=1.517, clip=100, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=6235
2023-01-09 15:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:45 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 100000 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0541, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=1370, lr=1.7125e-05, gnorm=1.551, clip=100, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=6252
2023-01-09 15:37:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:37:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:02 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 100000 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0444, wps=102.2, ups=0.61, wpb=111.1, bsz=40, num_updates=1380, lr=1.725e-05, gnorm=1.345, clip=70, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=6268
2023-01-09 15:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:19 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 100000 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0105, wps=98.1, ups=0.6, wpb=109.1, bsz=40, num_updates=1390, lr=1.7375e-05, gnorm=1.572, clip=100, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=6285
2023-01-09 15:38:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:35 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 100000 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0108, wps=100.5, ups=0.61, wpb=109.5, bsz=40, num_updates=1400, lr=1.75e-05, gnorm=1.681, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6302
2023-01-09 15:38:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:38:52 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 100000 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0198, wps=102.1, ups=0.62, wpb=109.7, bsz=40, num_updates=1410, lr=1.7625e-05, gnorm=1.755, clip=100, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=6318
2023-01-09 15:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:09 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 100000 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0103, wps=99, ups=0.6, wpb=109.9, bsz=40, num_updates=1420, lr=1.775e-05, gnorm=1.312, clip=80, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=6335
2023-01-09 15:39:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:25 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 100000 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.0385, wps=101.1, ups=0.62, wpb=109, bsz=40, num_updates=1430, lr=1.7875e-05, gnorm=1.426, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6351
2023-01-09 15:39:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:41 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0312, wps=103.6, ups=0.63, wpb=109.8, bsz=40, num_updates=1440, lr=1.8e-05, gnorm=1.774, clip=100, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=6367
2023-01-09 15:39:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:39:58 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 100000 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0309, wps=101.1, ups=0.6, wpb=111.7, bsz=40, num_updates=1450, lr=1.8125e-05, gnorm=1.464, clip=80, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=6384
2023-01-09 15:40:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:14 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 100000 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.3, vqa_score=0.028, wps=100.1, ups=0.62, wpb=108.3, bsz=40, num_updates=1460, lr=1.825e-05, gnorm=1.815, clip=100, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=6401
2023-01-09 15:40:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:31 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 100000 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0099, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=1470, lr=1.8375e-05, gnorm=1.901, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6417
2023-01-09 15:40:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:47 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 100000 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0, wps=105.3, ups=0.64, wpb=110, bsz=40, num_updates=1480, lr=1.85e-05, gnorm=1.475, clip=90, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=6433
2023-01-09 15:40:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:40:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:04 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0714, wps=99.4, ups=0.6, wpb=110.9, bsz=40, num_updates=1490, lr=1.8625e-05, gnorm=1.485, clip=90, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=6450
2023-01-09 15:41:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:21 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 100000 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0404, wps=99.1, ups=0.6, wpb=109.4, bsz=40, num_updates=1500, lr=1.875e-05, gnorm=1.513, clip=90, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=6467
2023-01-09 15:41:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:37 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 100000 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0114, wps=102.6, ups=0.62, wpb=110.9, bsz=40, num_updates=1510, lr=1.8875e-05, gnorm=1.415, clip=100, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=6483
2023-01-09 15:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:41:54 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0222, wps=100.7, ups=0.61, wpb=110.5, bsz=40, num_updates=1520, lr=1.9e-05, gnorm=1.578, clip=90, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=6500
2023-01-09 15:41:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:11 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 100000 loss=0.473, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0189, wps=99.4, ups=0.6, wpb=110, bsz=40, num_updates=1530, lr=1.9125e-05, gnorm=1.414, clip=90, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=6517
2023-01-09 15:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:27 - progress_bar.py[line:274] - INFO: epoch 001:   1540 / 100000 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0333, wps=100.5, ups=0.6, wpb=110.8, bsz=40, num_updates=1540, lr=1.925e-05, gnorm=1.717, clip=100, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6533
2023-01-09 15:42:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:44 - progress_bar.py[line:274] - INFO: epoch 001:   1550 / 100000 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0309, wps=102, ups=0.61, wpb=110.7, bsz=40, num_updates=1550, lr=1.9375e-05, gnorm=1.587, clip=100, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=6550
2023-01-09 15:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:42:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:00 - progress_bar.py[line:274] - INFO: epoch 001:   1560 / 100000 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.33, vqa_score=0.009, wps=103, ups=0.63, wpb=109.5, bsz=40, num_updates=1560, lr=1.95e-05, gnorm=2.435, clip=100, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6566
2023-01-09 15:43:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:16 - progress_bar.py[line:274] - INFO: epoch 001:   1570 / 100000 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.05, wps=104.5, ups=0.63, wpb=110.7, bsz=40, num_updates=1570, lr=1.9625e-05, gnorm=1.466, clip=100, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6582
2023-01-09 15:43:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:33 - progress_bar.py[line:274] - INFO: epoch 001:   1580 / 100000 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0549, wps=101.7, ups=0.62, wpb=109.9, bsz=40, num_updates=1580, lr=1.975e-05, gnorm=1.446, clip=90, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=6599
2023-01-09 15:43:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:49 - progress_bar.py[line:274] - INFO: epoch 001:   1590 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0265, wps=100, ups=0.61, wpb=109.3, bsz=40, num_updates=1590, lr=1.9875e-05, gnorm=1.678, clip=100, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=6615
2023-01-09 15:43:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:43:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:05 - progress_bar.py[line:274] - INFO: epoch 001:   1600 / 100000 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.29, vqa_score=0.0174, wps=103.1, ups=0.63, wpb=109.3, bsz=40, num_updates=1600, lr=2e-05, gnorm=1.663, clip=100, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6631
2023-01-09 15:44:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:13 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-09 15:44:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:23 - progress_bar.py[line:274] - INFO: epoch 001:   1611 / 100000 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0198, wps=92.4, ups=0.56, wpb=109.6, bsz=40, num_updates=1610, lr=2.0125e-05, gnorm=1.507, clip=100, loss_scale=512, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=6650
2023-01-09 15:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:40 - progress_bar.py[line:274] - INFO: epoch 001:   1621 / 100000 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0196, wps=102.3, ups=0.62, wpb=109.7, bsz=40, num_updates=1620, lr=2.025e-05, gnorm=1.366, clip=90, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=6666
2023-01-09 15:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:44:56 - progress_bar.py[line:274] - INFO: epoch 001:   1631 / 100000 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0225, wps=100.6, ups=0.61, wpb=110, bsz=40, num_updates=1630, lr=2.0375e-05, gnorm=1.218, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6683
2023-01-09 15:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:13 - progress_bar.py[line:274] - INFO: epoch 001:   1641 / 100000 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0306, wps=101, ups=0.61, wpb=110.9, bsz=40, num_updates=1640, lr=2.05e-05, gnorm=1.492, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6699
2023-01-09 15:45:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:29 - progress_bar.py[line:274] - INFO: epoch 001:   1651 / 100000 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0104, wps=101.8, ups=0.62, wpb=108.7, bsz=40, num_updates=1650, lr=2.0625e-05, gnorm=1.636, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6716
2023-01-09 15:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:46 - progress_bar.py[line:274] - INFO: epoch 001:   1661 / 100000 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0099, wps=98.9, ups=0.6, wpb=109.2, bsz=40, num_updates=1660, lr=2.075e-05, gnorm=1.585, clip=90, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=6732
2023-01-09 15:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:45:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:03 - progress_bar.py[line:274] - INFO: epoch 001:   1671 / 100000 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0291, wps=99.1, ups=0.6, wpb=109.9, bsz=40, num_updates=1670, lr=2.0875e-05, gnorm=1.464, clip=90, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=6749
2023-01-09 15:46:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:19 - progress_bar.py[line:274] - INFO: epoch 001:   1681 / 100000 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0444, wps=102.1, ups=0.62, wpb=110.5, bsz=40, num_updates=1680, lr=2.1e-05, gnorm=1.295, clip=70, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=6766
2023-01-09 15:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:36 - progress_bar.py[line:274] - INFO: epoch 001:   1691 / 100000 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0412, wps=103.4, ups=0.62, wpb=111.1, bsz=40, num_updates=1690, lr=2.1125e-05, gnorm=1.423, clip=90, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=6782
2023-01-09 15:46:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:52 - progress_bar.py[line:274] - INFO: epoch 001:   1701 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0, wps=101.6, ups=0.62, wpb=110.1, bsz=40, num_updates=1700, lr=2.125e-05, gnorm=1.536, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6798
2023-01-09 15:46:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:46:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:09 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 100000 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0638, wps=103.4, ups=0.62, wpb=110.9, bsz=40, num_updates=1710, lr=2.1375e-05, gnorm=1.566, clip=90, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=6815
2023-01-09 15:47:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:25 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0275, wps=100.5, ups=0.62, wpb=108.7, bsz=40, num_updates=1720, lr=2.15e-05, gnorm=1.771, clip=80, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=6831
2023-01-09 15:47:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:41 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 100000 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0, wps=102.9, ups=0.63, wpb=109.4, bsz=40, num_updates=1730, lr=2.1625e-05, gnorm=1.549, clip=80, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=6847
2023-01-09 15:47:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:47:57 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 100000 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0095, wps=102.3, ups=0.63, wpb=108.4, bsz=40, num_updates=1740, lr=2.175e-05, gnorm=1.858, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6864
2023-01-09 15:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:14 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0099, wps=98.2, ups=0.6, wpb=108.7, bsz=40, num_updates=1750, lr=2.1875e-05, gnorm=1.72, clip=90, loss_scale=512, train_wall=17, gb_free=9.8, ema_decay=0.9999, wall=6880
2023-01-09 15:48:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:31 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 100000 loss=0.458, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0426, wps=101.4, ups=0.62, wpb=109.6, bsz=40, num_updates=1760, lr=2.2e-05, gnorm=1.767, clip=100, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=6897
2023-01-09 15:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:47 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 100000 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0412, wps=101.8, ups=0.61, wpb=110.7, bsz=40, num_updates=1770, lr=2.2125e-05, gnorm=1.758, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6913
2023-01-09 15:48:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:48:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:04 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 100000 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0326, wps=100.7, ups=0.61, wpb=110.7, bsz=40, num_updates=1780, lr=2.225e-05, gnorm=1.622, clip=100, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=6930
2023-01-09 15:49:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:20 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 100000 loss=0.465, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0309, wps=105.5, ups=0.64, wpb=110, bsz=40, num_updates=1790, lr=2.2375e-05, gnorm=1.731, clip=100, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=6946
2023-01-09 15:49:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:36 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 100000 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0179, wps=98.9, ups=0.6, wpb=109, bsz=40, num_updates=1800, lr=2.25e-05, gnorm=1.467, clip=90, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=6963
2023-01-09 15:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:49:53 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 100000 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0421, wps=101.6, ups=0.62, wpb=109.4, bsz=40, num_updates=1810, lr=2.2625e-05, gnorm=1.63, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=6979
2023-01-09 15:49:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:10 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 100000 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0288, wps=99.3, ups=0.6, wpb=110.1, bsz=40, num_updates=1820, lr=2.275e-05, gnorm=1.91, clip=100, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=6996
2023-01-09 15:50:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:26 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 100000 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0294, wps=101.7, ups=0.63, wpb=108.3, bsz=40, num_updates=1830, lr=2.2875e-05, gnorm=1.811, clip=100, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=7012
2023-01-09 15:50:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:42 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 100000 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0095, wps=103.8, ups=0.63, wpb=109.3, bsz=40, num_updates=1840, lr=2.3e-05, gnorm=1.45, clip=90, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=7028
2023-01-09 15:50:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:50:59 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0306, wps=99.9, ups=0.61, wpb=109.1, bsz=40, num_updates=1850, lr=2.3125e-05, gnorm=1.503, clip=70, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=7045
2023-01-09 15:51:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:15 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 100000 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0515, wps=101.5, ups=0.62, wpb=109.5, bsz=40, num_updates=1860, lr=2.325e-05, gnorm=1.25, clip=90, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=7061
2023-01-09 15:51:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:32 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 100000 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0095, wps=98.2, ups=0.6, wpb=108.4, bsz=40, num_updates=1870, lr=2.3375e-05, gnorm=1.469, clip=90, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=7078
2023-01-09 15:51:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:49 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 100000 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0316, wps=99.6, ups=0.6, wpb=110.4, bsz=40, num_updates=1880, lr=2.35e-05, gnorm=1.223, clip=90, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=7095
2023-01-09 15:51:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:51:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:05 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 100000 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0115, wps=103.3, ups=0.62, wpb=111.3, bsz=40, num_updates=1890, lr=2.3625e-05, gnorm=1.479, clip=90, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=7111
2023-01-09 15:52:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:22 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 100000 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0196, wps=104.3, ups=0.62, wpb=111.5, bsz=40, num_updates=1900, lr=2.375e-05, gnorm=1.51, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=7128
2023-01-09 15:52:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:38 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0263, wps=101.7, ups=0.62, wpb=108.9, bsz=40, num_updates=1910, lr=2.3875e-05, gnorm=1.695, clip=100, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=7144
2023-01-09 15:52:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:52:54 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 100000 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0204, wps=104.3, ups=0.64, wpb=109.1, bsz=40, num_updates=1920, lr=2.4e-05, gnorm=1.616, clip=100, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=7160
2023-01-09 15:52:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:10 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.037, wps=101.5, ups=0.62, wpb=109, bsz=40, num_updates=1930, lr=2.4125e-05, gnorm=1.644, clip=80, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=7176
2023-01-09 15:53:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:27 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0208, wps=100.4, ups=0.61, wpb=109.1, bsz=40, num_updates=1940, lr=2.425e-05, gnorm=1.409, clip=80, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=7193
2023-01-09 15:53:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:43 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 100000 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.028, wps=103, ups=0.63, wpb=109.5, bsz=40, num_updates=1950, lr=2.4375e-05, gnorm=1.383, clip=80, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=7209
2023-01-09 15:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:53:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:00 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 100000 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0096, wps=99.9, ups=0.6, wpb=110.3, bsz=40, num_updates=1960, lr=2.45e-05, gnorm=1.518, clip=90, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=7226
2023-01-09 15:54:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:16 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 100000 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0273, wps=101, ups=0.62, wpb=109.3, bsz=40, num_updates=1970, lr=2.4625e-05, gnorm=1.4, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=7242
2023-01-09 15:54:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:33 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 100000 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0566, wps=101.8, ups=0.62, wpb=109.7, bsz=40, num_updates=1980, lr=2.475e-05, gnorm=1.412, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=7259
2023-01-09 15:54:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:49 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 100000 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0192, wps=103.2, ups=0.63, wpb=109.5, bsz=40, num_updates=1990, lr=2.4875e-05, gnorm=1.823, clip=100, loss_scale=512, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=7275
2023-01-09 15:54:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:54:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:55:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:55:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 15:55:05 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 100000 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0476, wps=101.9, ups=0.61, wpb=110.9, bsz=40, num_updates=2000, lr=2.5e-05, gnorm=1.477, clip=90, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=7292
2023-01-09 15:55:05 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-09 15:55:07 - train.py[line:549] - INFO: 0 / 4988
2023-01-09 15:55:07 - train.py[line:551] - INFO: load:1.15 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-09 15:57:38 - train.py[line:549] - INFO: 200 / 4988
2023-01-09 15:57:38 - train.py[line:551] - INFO: load:1.18 valid_run:151.36 task_valid:148.41 collect_output:1.87
2023-01-09 16:00:06 - train.py[line:549] - INFO: 400 / 4988
2023-01-09 16:00:06 - train.py[line:551] - INFO: load:1.20 valid_run:299.33 task_valid:291.61 collect_output:5.56
2023-01-09 16:02:38 - train.py[line:549] - INFO: 600 / 4988
2023-01-09 16:02:38 - train.py[line:551] - INFO: load:1.23 valid_run:450.45 task_valid:434.70 collect_output:12.55
2023-01-09 16:05:06 - train.py[line:549] - INFO: 800 / 4988
2023-01-09 16:05:06 - train.py[line:551] - INFO: load:1.26 valid_run:598.97 task_valid:579.63 collect_output:15.11
2023-01-09 16:07:38 - train.py[line:549] - INFO: 1000 / 4988
2023-01-09 16:07:38 - train.py[line:551] - INFO: load:1.28 valid_run:750.59 task_valid:727.02 collect_output:18.31
2023-01-09 16:10:09 - train.py[line:549] - INFO: 1200 / 4988
2023-01-09 16:10:09 - train.py[line:551] - INFO: load:1.31 valid_run:901.85 task_valid:872.74 collect_output:22.83
2023-01-09 16:12:41 - train.py[line:549] - INFO: 1400 / 4988
2023-01-09 16:12:41 - train.py[line:551] - INFO: load:1.34 valid_run:1054.02 task_valid:1018.82 collect_output:27.85
2023-01-09 16:15:12 - train.py[line:549] - INFO: 1600 / 4988
2023-01-09 16:15:12 - train.py[line:551] - INFO: load:1.36 valid_run:1204.31 task_valid:1159.97 collect_output:35.94
2023-01-09 16:17:41 - train.py[line:549] - INFO: 1800 / 4988
2023-01-09 16:17:41 - train.py[line:551] - INFO: load:1.39 valid_run:1353.00 task_valid:1304.61 collect_output:38.99
2023-01-09 16:20:09 - train.py[line:549] - INFO: 2000 / 4988
2023-01-09 16:20:09 - train.py[line:551] - INFO: load:1.42 valid_run:1501.08 task_valid:1447.93 collect_output:42.72
2023-01-09 16:22:38 - train.py[line:549] - INFO: 2200 / 4988
2023-01-09 16:22:38 - train.py[line:551] - INFO: load:1.45 valid_run:1650.11 task_valid:1592.77 collect_output:45.91
2023-01-09 16:25:07 - train.py[line:549] - INFO: 2400 / 4988
2023-01-09 16:25:07 - train.py[line:551] - INFO: load:1.47 valid_run:1799.35 task_valid:1737.85 collect_output:49.03
2023-01-09 16:27:36 - train.py[line:549] - INFO: 2600 / 4988
2023-01-09 16:27:36 - train.py[line:551] - INFO: load:1.50 valid_run:1948.33 task_valid:1879.86 collect_output:54.97
2023-01-09 16:30:06 - train.py[line:549] - INFO: 2800 / 4988
2023-01-09 16:30:06 - train.py[line:551] - INFO: load:1.53 valid_run:2098.50 task_valid:2025.44 collect_output:58.55
2023-01-09 16:32:36 - train.py[line:549] - INFO: 3000 / 4988
2023-01-09 16:32:36 - train.py[line:551] - INFO: load:1.55 valid_run:2248.40 task_valid:2172.23 collect_output:60.62
2023-01-09 16:35:06 - train.py[line:549] - INFO: 3200 / 4988
2023-01-09 16:35:06 - train.py[line:551] - INFO: load:1.58 valid_run:2397.77 task_valid:2316.50 collect_output:64.68
2023-01-09 16:37:37 - train.py[line:549] - INFO: 3400 / 4988
2023-01-09 16:37:37 - train.py[line:551] - INFO: load:1.60 valid_run:2548.38 task_valid:2462.16 collect_output:68.55
2023-01-09 16:40:07 - train.py[line:549] - INFO: 3600 / 4988
2023-01-09 16:40:07 - train.py[line:551] - INFO: load:1.63 valid_run:2698.60 task_valid:2609.15 collect_output:70.76
2023-01-09 16:42:34 - train.py[line:549] - INFO: 3800 / 4988
2023-01-09 16:42:34 - train.py[line:551] - INFO: load:1.65 valid_run:2845.95 task_valid:2750.92 collect_output:75.29
2023-01-09 16:45:04 - train.py[line:549] - INFO: 4000 / 4988
2023-01-09 16:45:04 - train.py[line:551] - INFO: load:1.68 valid_run:2995.39 task_valid:2896.18 collect_output:78.47
2023-01-09 16:47:35 - train.py[line:549] - INFO: 4200 / 4988
2023-01-09 16:47:35 - train.py[line:551] - INFO: load:1.70 valid_run:3146.05 task_valid:3040.98 collect_output:83.28
2023-01-09 16:50:03 - train.py[line:549] - INFO: 4400 / 4988
2023-01-09 16:50:03 - train.py[line:551] - INFO: load:1.73 valid_run:3294.82 task_valid:3185.61 collect_output:86.37
2023-01-09 16:52:34 - train.py[line:549] - INFO: 4600 / 4988
2023-01-09 16:52:34 - train.py[line:551] - INFO: load:1.76 valid_run:3445.02 task_valid:3331.98 collect_output:89.15
2023-01-09 16:55:04 - train.py[line:549] - INFO: 4800 / 4988
2023-01-09 16:55:04 - train.py[line:551] - INFO: load:1.78 valid_run:3595.52 task_valid:3478.52 collect_output:92.11

====================================================================================================
SGG eval:     R @ 50: 0.3508;     R @ 100: 0.3939;     R @ 500: 0.4609;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1590;    mR @ 100: 0.2096;    mR @ 500: 0.2490;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0732) (covered in:0.0000) (covering:0.1429) (eating:0.5000) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5161) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.2604) (playing:0.0000) (riding:0.5098) (says:0.0000) (sitting on:0.4785) (standing on:0.5033) (using:0.2000) (walking in:0.0000) (walking on:0.2162) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-01-09 16:57:35 - train.py[line:487] - INFO: 0.39393333333333336

====================================================================================================
SGG eval:     R @ 50: 0.3508;     R @ 100: 0.3939;     R @ 500: 0.4609;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1590;    mR @ 100: 0.2096;    mR @ 500: 0.2490;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.0732) (covered in:0.0000) (covering:0.1429) (eating:0.5000) (flying in:0.5000) (growing on:0.1250) (hanging from:0.5161) (lying on:0.0000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.2604) (playing:0.0000) (riding:0.5098) (says:0.0000) (sitting on:0.4785) (standing on:0.5033) (using:0.2000) (walking in:0.0000) (walking on:0.2162) (watching:0.0000) 
--------------------------------------------------------
====================================================================================================

2023-01-09 16:57:35 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-09 16:57:35 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.276 | loss_v1 0 | loss_v2 0 | nll_loss 0.116 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.393933 | ppl 1.08 | vqa_score 0.1926 | wps 119.7 | wpb 89.9 | bsz 30 | num_updates 2000 | best_R@100 0.393933
2023-01-09 16:57:35 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 2000 updates
2023-01-09 16:57:35 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt
2023-01-09 16:58:12 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt
2023-01-09 17:00:57 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_2000.pt (epoch 1 @ 2000 updates, score 0.39393333333333336) (writing took 201.92621322721243 seconds)
2023-01-09 17:01:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:14 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 100000 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0303, wps=0.4, ups=0, wpb=109.9, bsz=40, num_updates=2010, lr=2.5125e-05, gnorm=1.562, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11260
2023-01-09 17:01:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:31 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 100000 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0, wps=95.1, ups=0.58, wpb=109.9, bsz=40, num_updates=2020, lr=2.525e-05, gnorm=1.503, clip=90, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=11277
2023-01-09 17:01:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:47 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 100000 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0105, wps=106.2, ups=0.64, wpb=111.4, bsz=40, num_updates=2030, lr=2.5375e-05, gnorm=1.504, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11293
2023-01-09 17:01:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:01:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:03 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 100000 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0095, wps=103.1, ups=0.62, wpb=110.3, bsz=40, num_updates=2040, lr=2.55e-05, gnorm=1.58, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11310
2023-01-09 17:02:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:20 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 100000 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0, wps=99.8, ups=0.62, wpb=108.1, bsz=40, num_updates=2050, lr=2.5625e-05, gnorm=1.787, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11326
2023-01-09 17:02:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:37 - progress_bar.py[line:274] - INFO: epoch 001:   2061 / 100000 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0208, wps=99.6, ups=0.61, wpb=109.7, bsz=40, num_updates=2060, lr=2.575e-05, gnorm=1.477, clip=90, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=11343
2023-01-09 17:02:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:53 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0309, wps=102.7, ups=0.62, wpb=110.7, bsz=40, num_updates=2070, lr=2.5875e-05, gnorm=1.578, clip=90, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=11359
2023-01-09 17:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:10 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 100000 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0446, wps=99.7, ups=0.61, wpb=109.1, bsz=40, num_updates=2080, lr=2.6e-05, gnorm=2.135, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11376
2023-01-09 17:03:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:26 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 100000 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0098, wps=102.5, ups=0.62, wpb=111.1, bsz=40, num_updates=2090, lr=2.6125e-05, gnorm=1.421, clip=90, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=11392
2023-01-09 17:03:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:43 - progress_bar.py[line:274] - INFO: epoch 001:   2101 / 100000 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0294, wps=101.6, ups=0.62, wpb=109.6, bsz=40, num_updates=2100, lr=2.625e-05, gnorm=2.004, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11409
2023-01-09 17:03:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:03:59 - progress_bar.py[line:274] - INFO: epoch 001:   2111 / 100000 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0288, wps=104.1, ups=0.63, wpb=109.4, bsz=40, num_updates=2110, lr=2.6375e-05, gnorm=1.509, clip=100, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=11425
2023-01-09 17:04:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:15 - progress_bar.py[line:274] - INFO: epoch 001:   2121 / 100000 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0115, wps=103.5, ups=0.62, wpb=111.5, bsz=40, num_updates=2120, lr=2.65e-05, gnorm=1.349, clip=90, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=11441
2023-01-09 17:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:32 - progress_bar.py[line:274] - INFO: epoch 001:   2131 / 100000 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0211, wps=99.8, ups=0.61, wpb=109.4, bsz=40, num_updates=2130, lr=2.6625e-05, gnorm=1.475, clip=100, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=11458
2023-01-09 17:04:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:48 - progress_bar.py[line:274] - INFO: epoch 001:   2141 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0532, wps=100.1, ups=0.6, wpb=110.9, bsz=40, num_updates=2140, lr=2.675e-05, gnorm=1.418, clip=90, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=11475
2023-01-09 17:04:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:04:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:05 - progress_bar.py[line:274] - INFO: epoch 001:   2151 / 100000 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0102, wps=100.7, ups=0.61, wpb=110.5, bsz=40, num_updates=2150, lr=2.6875e-05, gnorm=1.753, clip=100, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=11491
2023-01-09 17:05:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:21 - progress_bar.py[line:274] - INFO: epoch 001:   2161 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0312, wps=103.6, ups=0.63, wpb=109.7, bsz=40, num_updates=2160, lr=2.7e-05, gnorm=1.434, clip=80, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=11507
2023-01-09 17:05:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:38 - progress_bar.py[line:274] - INFO: epoch 001:   2171 / 100000 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0381, wps=100.2, ups=0.6, wpb=110.4, bsz=40, num_updates=2170, lr=2.7125e-05, gnorm=1.61, clip=80, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=11524
2023-01-09 17:05:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:05:54 - progress_bar.py[line:274] - INFO: epoch 001:   2181 / 100000 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0421, wps=101.7, ups=0.62, wpb=109.8, bsz=40, num_updates=2180, lr=2.725e-05, gnorm=1.327, clip=80, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=11541
2023-01-09 17:05:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:11 - progress_bar.py[line:274] - INFO: epoch 001:   2191 / 100000 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0215, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=2190, lr=2.7375e-05, gnorm=1.53, clip=90, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=11557
2023-01-09 17:06:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:28 - progress_bar.py[line:274] - INFO: epoch 001:   2201 / 100000 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0306, wps=101.4, ups=0.61, wpb=110.3, bsz=40, num_updates=2200, lr=2.75e-05, gnorm=1.633, clip=90, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=11574
2023-01-09 17:06:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:43 - progress_bar.py[line:274] - INFO: epoch 001:   2211 / 100000 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0204, wps=105.2, ups=0.64, wpb=110, bsz=40, num_updates=2210, lr=2.7625e-05, gnorm=1.578, clip=90, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=11590
2023-01-09 17:06:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:06:59 - progress_bar.py[line:274] - INFO: epoch 001:   2221 / 100000 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0, wps=106.5, ups=0.64, wpb=111.7, bsz=40, num_updates=2220, lr=2.775e-05, gnorm=1.438, clip=90, loss_scale=1024, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=11606
2023-01-09 17:07:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:16 - progress_bar.py[line:274] - INFO: epoch 001:   2231 / 100000 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0273, wps=98.6, ups=0.61, wpb=107.7, bsz=40, num_updates=2230, lr=2.7875e-05, gnorm=1.797, clip=80, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=11622
2023-01-09 17:07:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:33 - progress_bar.py[line:274] - INFO: epoch 001:   2241 / 100000 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0, wps=99.6, ups=0.61, wpb=109.1, bsz=40, num_updates=2240, lr=2.8e-05, gnorm=1.697, clip=100, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11639
2023-01-09 17:07:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:49 - progress_bar.py[line:274] - INFO: epoch 001:   2251 / 100000 loss=0.464, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0202, wps=102.1, ups=0.61, wpb=110.8, bsz=40, num_updates=2250, lr=2.8125e-05, gnorm=2.018, clip=100, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11655
2023-01-09 17:07:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:07:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:06 - progress_bar.py[line:274] - INFO: epoch 001:   2261 / 100000 loss=0.452, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0194, wps=100.5, ups=0.61, wpb=110.7, bsz=40, num_updates=2260, lr=2.825e-05, gnorm=1.496, clip=90, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11672
2023-01-09 17:08:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:22 - progress_bar.py[line:274] - INFO: epoch 001:   2271 / 100000 loss=0.478, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0, wps=100.8, ups=0.61, wpb=109.9, bsz=40, num_updates=2270, lr=2.8375e-05, gnorm=1.574, clip=90, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=11689
2023-01-09 17:08:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:39 - progress_bar.py[line:274] - INFO: epoch 001:   2281 / 100000 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0, wps=98.9, ups=0.6, wpb=110.1, bsz=40, num_updates=2280, lr=2.85e-05, gnorm=1.397, clip=70, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=11706
2023-01-09 17:08:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:08:56 - progress_bar.py[line:274] - INFO: epoch 001:   2291 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0192, wps=102, ups=0.62, wpb=108.8, bsz=40, num_updates=2290, lr=2.8625e-05, gnorm=1.511, clip=70, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11722
2023-01-09 17:09:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:12 - progress_bar.py[line:274] - INFO: epoch 001:   2301 / 100000 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0404, wps=104.3, ups=0.63, wpb=110.3, bsz=40, num_updates=2300, lr=2.875e-05, gnorm=1.474, clip=80, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11738
2023-01-09 17:09:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:29 - progress_bar.py[line:274] - INFO: epoch 001:   2311 / 100000 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0185, wps=100, ups=0.62, wpb=108.3, bsz=40, num_updates=2310, lr=2.8875e-05, gnorm=1.485, clip=90, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=11755
2023-01-09 17:09:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:45 - progress_bar.py[line:274] - INFO: epoch 001:   2321 / 100000 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0588, wps=101.2, ups=0.61, wpb=110.1, bsz=40, num_updates=2320, lr=2.9e-05, gnorm=1.389, clip=80, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=11771
2023-01-09 17:09:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:02 - progress_bar.py[line:274] - INFO: epoch 001:   2331 / 100000 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.051, wps=101.9, ups=0.62, wpb=110.2, bsz=40, num_updates=2330, lr=2.9125e-05, gnorm=1.306, clip=70, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11788
2023-01-09 17:10:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:19 - progress_bar.py[line:274] - INFO: epoch 001:   2341 / 100000 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0213, wps=100.4, ups=0.61, wpb=110.5, bsz=40, num_updates=2340, lr=2.925e-05, gnorm=1.578, clip=100, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11805
2023-01-09 17:10:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:27 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-09 17:10:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:37 - progress_bar.py[line:274] - INFO: epoch 001:   2352 / 100000 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0286, wps=93.9, ups=0.57, wpb=109.7, bsz=40, num_updates=2350, lr=2.9375e-05, gnorm=1.515, clip=100, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=11823
2023-01-09 17:10:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:10:54 - progress_bar.py[line:274] - INFO: epoch 001:   2362 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0288, wps=97.2, ups=0.59, wpb=108.9, bsz=40, num_updates=2360, lr=2.95e-05, gnorm=1.34, clip=80, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=11840
2023-01-09 17:10:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:11 - progress_bar.py[line:274] - INFO: epoch 001:   2372 / 100000 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0196, wps=100.8, ups=0.61, wpb=110, bsz=40, num_updates=2370, lr=2.9625e-05, gnorm=1.616, clip=90, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=11857
2023-01-09 17:11:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:28 - progress_bar.py[line:274] - INFO: epoch 001:   2382 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0189, wps=99.5, ups=0.61, wpb=109.2, bsz=40, num_updates=2380, lr=2.975e-05, gnorm=1.34, clip=100, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=11874
2023-01-09 17:11:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:45 - progress_bar.py[line:274] - INFO: epoch 001:   2392 / 100000 loss=0.469, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0388, wps=100.3, ups=0.62, wpb=108.4, bsz=40, num_updates=2390, lr=2.9875e-05, gnorm=1.682, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=11891
2023-01-09 17:11:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:11:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:02 - progress_bar.py[line:274] - INFO: epoch 001:   2402 / 100000 loss=0.468, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0, wps=99.9, ups=0.61, wpb=109, bsz=40, num_updates=2400, lr=3e-05, gnorm=1.406, clip=70, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=11908
2023-01-09 17:12:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:18 - progress_bar.py[line:274] - INFO: epoch 001:   2412 / 100000 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.03, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=2410, lr=3.0125e-05, gnorm=1.437, clip=90, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=11924
2023-01-09 17:12:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:35 - progress_bar.py[line:274] - INFO: epoch 001:   2422 / 100000 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0103, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=2420, lr=3.025e-05, gnorm=1.344, clip=90, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=11941
2023-01-09 17:12:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:51 - progress_bar.py[line:274] - INFO: epoch 001:   2432 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.05, wps=100.7, ups=0.62, wpb=109, bsz=40, num_updates=2430, lr=3.0375e-05, gnorm=1.22, clip=80, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=11957
2023-01-09 17:12:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:12:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:08 - progress_bar.py[line:274] - INFO: epoch 001:   2442 / 100000 loss=0.477, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0.0095, wps=101, ups=0.62, wpb=109.5, bsz=40, num_updates=2440, lr=3.05e-05, gnorm=1.462, clip=90, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=11974
2023-01-09 17:13:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:24 - progress_bar.py[line:274] - INFO: epoch 001:   2452 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0294, wps=100.8, ups=0.61, wpb=110.4, bsz=40, num_updates=2450, lr=3.0625e-05, gnorm=1.354, clip=80, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=11990
2023-01-09 17:13:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:41 - progress_bar.py[line:274] - INFO: epoch 001:   2462 / 100000 loss=0.47, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0194, wps=99.9, ups=0.61, wpb=109.3, bsz=40, num_updates=2460, lr=3.075e-05, gnorm=1.114, clip=80, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=12007
2023-01-09 17:13:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:13:57 - progress_bar.py[line:274] - INFO: epoch 001:   2472 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.022, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=2470, lr=3.0875e-05, gnorm=1.327, clip=70, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=12024
2023-01-09 17:14:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:14 - progress_bar.py[line:274] - INFO: epoch 001:   2482 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0103, wps=102.1, ups=0.62, wpb=109, bsz=40, num_updates=2480, lr=3.1e-05, gnorm=1.366, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12040
2023-01-09 17:14:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:31 - progress_bar.py[line:274] - INFO: epoch 001:   2492 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0215, wps=98.8, ups=0.6, wpb=110.4, bsz=40, num_updates=2490, lr=3.1125e-05, gnorm=1.513, clip=90, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=12057
2023-01-09 17:14:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:47 - progress_bar.py[line:274] - INFO: epoch 001:   2502 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0377, wps=102.9, ups=0.62, wpb=109.9, bsz=40, num_updates=2500, lr=3.125e-05, gnorm=1.527, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12073
2023-01-09 17:14:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:14:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:04 - progress_bar.py[line:274] - INFO: epoch 001:   2512 / 100000 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0412, wps=100.7, ups=0.61, wpb=109.5, bsz=40, num_updates=2510, lr=3.1375e-05, gnorm=1.482, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12090
2023-01-09 17:15:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:20 - progress_bar.py[line:274] - INFO: epoch 001:   2522 / 100000 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=2520, lr=3.15e-05, gnorm=1.25, clip=60, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=12106
2023-01-09 17:15:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:36 - progress_bar.py[line:274] - INFO: epoch 001:   2532 / 100000 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.023, wps=102.7, ups=0.62, wpb=110.7, bsz=40, num_updates=2530, lr=3.1625e-05, gnorm=1.471, clip=90, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=12123
2023-01-09 17:15:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:53 - progress_bar.py[line:274] - INFO: epoch 001:   2542 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0103, wps=100.9, ups=0.61, wpb=110.5, bsz=40, num_updates=2540, lr=3.175e-05, gnorm=1.398, clip=90, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=12139
2023-01-09 17:15:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:15:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:10 - progress_bar.py[line:274] - INFO: epoch 001:   2552 / 100000 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0215, wps=99.3, ups=0.6, wpb=109.7, bsz=40, num_updates=2550, lr=3.1875e-05, gnorm=1.283, clip=90, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=12156
2023-01-09 17:16:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:26 - progress_bar.py[line:274] - INFO: epoch 001:   2562 / 100000 loss=0.466, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0495, wps=102.5, ups=0.62, wpb=110.9, bsz=40, num_updates=2560, lr=3.2e-05, gnorm=1.345, clip=60, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=12173
2023-01-09 17:16:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:43 - progress_bar.py[line:274] - INFO: epoch 001:   2572 / 100000 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0698, wps=101.2, ups=0.61, wpb=110.6, bsz=40, num_updates=2570, lr=3.2125e-05, gnorm=1.001, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12189
2023-01-09 17:16:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:16:59 - progress_bar.py[line:274] - INFO: epoch 001:   2582 / 100000 loss=0.461, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0202, wps=101.3, ups=0.62, wpb=109.3, bsz=40, num_updates=2580, lr=3.225e-05, gnorm=1.257, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12206
2023-01-09 17:17:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:16 - progress_bar.py[line:274] - INFO: epoch 001:   2592 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0185, wps=98.8, ups=0.6, wpb=108.9, bsz=40, num_updates=2590, lr=3.2375e-05, gnorm=1.296, clip=80, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=12222
2023-01-09 17:17:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:33 - progress_bar.py[line:274] - INFO: epoch 001:   2602 / 100000 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.01, wps=101.7, ups=0.62, wpb=109.3, bsz=40, num_updates=2600, lr=3.25e-05, gnorm=1.644, clip=90, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=12239
2023-01-09 17:17:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:49 - progress_bar.py[line:274] - INFO: epoch 001:   2612 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0196, wps=104.2, ups=0.63, wpb=110, bsz=40, num_updates=2610, lr=3.2625e-05, gnorm=1.361, clip=80, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=12255
2023-01-09 17:17:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:17:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:06 - progress_bar.py[line:274] - INFO: epoch 001:   2622 / 100000 loss=0.438, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.044, wps=99.2, ups=0.6, wpb=110.7, bsz=40, num_updates=2620, lr=3.275e-05, gnorm=1.42, clip=80, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=12272
2023-01-09 17:18:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:22 - progress_bar.py[line:274] - INFO: epoch 001:   2632 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.03, wps=100.2, ups=0.61, wpb=109.3, bsz=40, num_updates=2630, lr=3.2875e-05, gnorm=1.312, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12289
2023-01-09 17:18:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:39 - progress_bar.py[line:274] - INFO: epoch 001:   2642 / 100000 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0215, wps=102.7, ups=0.62, wpb=110, bsz=40, num_updates=2640, lr=3.3e-05, gnorm=1.193, clip=60, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=12305
2023-01-09 17:18:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:18:55 - progress_bar.py[line:274] - INFO: epoch 001:   2652 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.01, wps=101.2, ups=0.62, wpb=109.5, bsz=40, num_updates=2650, lr=3.3125e-05, gnorm=1.507, clip=90, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=12321
2023-01-09 17:18:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:12 - progress_bar.py[line:274] - INFO: epoch 001:   2662 / 100000 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0294, wps=101.6, ups=0.62, wpb=109.9, bsz=40, num_updates=2660, lr=3.325e-05, gnorm=1.298, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12338
2023-01-09 17:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:28 - progress_bar.py[line:274] - INFO: epoch 001:   2672 / 100000 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.009, wps=99.4, ups=0.61, wpb=109.1, bsz=40, num_updates=2670, lr=3.3375e-05, gnorm=1.352, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12354
2023-01-09 17:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:45 - progress_bar.py[line:274] - INFO: epoch 001:   2682 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0273, wps=98, ups=0.6, wpb=108.2, bsz=40, num_updates=2680, lr=3.35e-05, gnorm=1.332, clip=80, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=12371
2023-01-09 17:19:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:19:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:02 - progress_bar.py[line:274] - INFO: epoch 001:   2692 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0319, wps=99.9, ups=0.61, wpb=108.6, bsz=40, num_updates=2690, lr=3.3625e-05, gnorm=1.815, clip=90, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=12388
2023-01-09 17:20:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:18 - progress_bar.py[line:274] - INFO: epoch 001:   2702 / 100000 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, vqa_score=0, wps=100.6, ups=0.61, wpb=109.6, bsz=40, num_updates=2700, lr=3.375e-05, gnorm=1.718, clip=90, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=12404
2023-01-09 17:20:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:34 - progress_bar.py[line:274] - INFO: epoch 001:   2712 / 100000 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.03, wps=102, ups=0.62, wpb=109.4, bsz=40, num_updates=2710, lr=3.3875e-05, gnorm=1.569, clip=100, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12421
2023-01-09 17:20:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:51 - progress_bar.py[line:274] - INFO: epoch 001:   2722 / 100000 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0097, wps=101.1, ups=0.62, wpb=108.1, bsz=40, num_updates=2720, lr=3.4e-05, gnorm=1.657, clip=90, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=12437
2023-01-09 17:20:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:20:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:07 - progress_bar.py[line:274] - INFO: epoch 001:   2732 / 100000 loss=0.453, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0183, wps=101.5, ups=0.62, wpb=109.5, bsz=40, num_updates=2730, lr=3.4125e-05, gnorm=1.485, clip=90, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=12453
2023-01-09 17:21:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:24 - progress_bar.py[line:274] - INFO: epoch 001:   2742 / 100000 loss=0.455, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0291, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=2740, lr=3.425e-05, gnorm=1.517, clip=80, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=12470
2023-01-09 17:21:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:40 - progress_bar.py[line:274] - INFO: epoch 001:   2752 / 100000 loss=0.44, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0297, wps=101, ups=0.61, wpb=110.5, bsz=40, num_updates=2750, lr=3.4375e-05, gnorm=1.258, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12487
2023-01-09 17:21:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:21:57 - progress_bar.py[line:274] - INFO: epoch 001:   2762 / 100000 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0, wps=98.7, ups=0.6, wpb=109.1, bsz=40, num_updates=2760, lr=3.45e-05, gnorm=1.521, clip=80, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=12503
2023-01-09 17:22:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:14 - progress_bar.py[line:274] - INFO: epoch 001:   2772 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0, wps=101.3, ups=0.61, wpb=110.5, bsz=40, num_updates=2770, lr=3.4625e-05, gnorm=1.209, clip=70, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=12520
2023-01-09 17:22:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:30 - progress_bar.py[line:274] - INFO: epoch 001:   2782 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0, wps=103.3, ups=0.63, wpb=110.1, bsz=40, num_updates=2780, lr=3.475e-05, gnorm=1.59, clip=80, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=12536
2023-01-09 17:22:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:35 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-09 17:22:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:48 - progress_bar.py[line:274] - INFO: epoch 001:   2793 / 100000 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0472, wps=94.9, ups=0.57, wpb=110.4, bsz=40, num_updates=2790, lr=3.4875e-05, gnorm=1.624, clip=90, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=12554
2023-01-09 17:22:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:22:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:04 - progress_bar.py[line:274] - INFO: epoch 001:   2803 / 100000 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.28, vqa_score=0.0175, wps=100.4, ups=0.62, wpb=108.7, bsz=40, num_updates=2800, lr=3.5e-05, gnorm=3.333, clip=90, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=12570
2023-01-09 17:23:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:21 - progress_bar.py[line:274] - INFO: epoch 001:   2813 / 100000 loss=0.454, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0, wps=98.9, ups=0.6, wpb=109.7, bsz=40, num_updates=2810, lr=3.5125e-05, gnorm=2.314, clip=90, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=12587
2023-01-09 17:23:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:38 - progress_bar.py[line:274] - INFO: epoch 001:   2823 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0179, wps=96.8, ups=0.59, wpb=108.5, bsz=40, num_updates=2820, lr=3.525e-05, gnorm=1.487, clip=70, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=12604
2023-01-09 17:23:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:54 - progress_bar.py[line:274] - INFO: epoch 001:   2833 / 100000 loss=0.456, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0388, wps=101.3, ups=0.61, wpb=110.3, bsz=40, num_updates=2830, lr=3.5375e-05, gnorm=1.664, clip=90, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=12621
2023-01-09 17:23:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:23:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:11 - progress_bar.py[line:274] - INFO: epoch 001:   2843 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0345, wps=102.9, ups=0.62, wpb=110.5, bsz=40, num_updates=2840, lr=3.55e-05, gnorm=1.397, clip=80, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=12637
2023-01-09 17:24:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:27 - progress_bar.py[line:274] - INFO: epoch 001:   2853 / 100000 loss=0.463, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0288, wps=102.2, ups=0.62, wpb=109.7, bsz=40, num_updates=2850, lr=3.5625e-05, gnorm=2.067, clip=100, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12653
2023-01-09 17:24:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:44 - progress_bar.py[line:274] - INFO: epoch 001:   2863 / 100000 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0098, wps=100.7, ups=0.61, wpb=110.2, bsz=40, num_updates=2860, lr=3.575e-05, gnorm=1.253, clip=80, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=12670
2023-01-09 17:24:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:24:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:00 - progress_bar.py[line:274] - INFO: epoch 001:   2873 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0104, wps=102.1, ups=0.62, wpb=109.8, bsz=40, num_updates=2870, lr=3.5875e-05, gnorm=1.7, clip=90, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=12686
2023-01-09 17:25:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:17 - progress_bar.py[line:274] - INFO: epoch 001:   2883 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0303, wps=102.7, ups=0.62, wpb=110.4, bsz=40, num_updates=2880, lr=3.6e-05, gnorm=1.942, clip=90, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=12703
2023-01-09 17:25:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:33 - progress_bar.py[line:274] - INFO: epoch 001:   2893 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0104, wps=102.5, ups=0.62, wpb=109.6, bsz=40, num_updates=2890, lr=3.6125e-05, gnorm=1.925, clip=90, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=12719
2023-01-09 17:25:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:48 - progress_bar.py[line:274] - INFO: epoch 001:   2903 / 100000 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0194, wps=107.6, ups=0.65, wpb=110.8, bsz=40, num_updates=2900, lr=3.625e-05, gnorm=1.553, clip=70, loss_scale=256, train_wall=15, gb_free=10.4, ema_decay=0.9999, wall=12735
2023-01-09 17:25:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:25:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:05 - progress_bar.py[line:274] - INFO: epoch 001:   2913 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0465, wps=101.8, ups=0.61, wpb=111.7, bsz=40, num_updates=2910, lr=3.6375e-05, gnorm=1.062, clip=50, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=12751
2023-01-09 17:26:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:21 - progress_bar.py[line:274] - INFO: epoch 001:   2923 / 100000 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0435, wps=103.5, ups=0.62, wpb=110.4, bsz=40, num_updates=2920, lr=3.65e-05, gnorm=1.625, clip=80, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12768
2023-01-09 17:26:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:38 - progress_bar.py[line:274] - INFO: epoch 001:   2933 / 100000 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0198, wps=102.8, ups=0.62, wpb=111, bsz=40, num_updates=2930, lr=3.6625e-05, gnorm=1.189, clip=70, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=12784
2023-01-09 17:26:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:55 - progress_bar.py[line:274] - INFO: epoch 001:   2943 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0531, wps=100.4, ups=0.61, wpb=110.7, bsz=40, num_updates=2940, lr=3.675e-05, gnorm=1.147, clip=60, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=12801
2023-01-09 17:26:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:26:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:11 - progress_bar.py[line:274] - INFO: epoch 001:   2953 / 100000 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0294, wps=101.4, ups=0.61, wpb=110.2, bsz=40, num_updates=2950, lr=3.6875e-05, gnorm=1.356, clip=70, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=12817
2023-01-09 17:27:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:27 - progress_bar.py[line:274] - INFO: epoch 001:   2963 / 100000 loss=0.459, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, vqa_score=0.0404, wps=103.6, ups=0.62, wpb=110.8, bsz=40, num_updates=2960, lr=3.7e-05, gnorm=1.918, clip=90, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=12834
2023-01-09 17:27:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:43 - progress_bar.py[line:274] - INFO: epoch 001:   2973 / 100000 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0192, wps=103.2, ups=0.63, wpb=109.7, bsz=40, num_updates=2970, lr=3.7125e-05, gnorm=1.227, clip=80, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12850
2023-01-09 17:27:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:27:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:28:00 - progress_bar.py[line:274] - INFO: epoch 001:   2983 / 100000 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0421, wps=101, ups=0.61, wpb=111.2, bsz=40, num_updates=2980, lr=3.725e-05, gnorm=1.335, clip=90, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=12866
2023-01-09 17:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:28:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:28:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:28:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:28:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:28:17 - progress_bar.py[line:274] - INFO: epoch 001:   2993 / 100000 loss=0.439, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0189, wps=99.1, ups=0.6, wpb=109.8, bsz=40, num_updates=2990, lr=3.7375e-05, gnorm=1.41, clip=90, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=12883
2023-01-09 17:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:28:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:28:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:28:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 17:28:34 - progress_bar.py[line:274] - INFO: epoch 001:   3003 / 100000 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0098, wps=101.5, ups=0.62, wpb=109.9, bsz=40, num_updates=3000, lr=3.75e-05, gnorm=1.256, clip=60, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=12900
2023-01-09 17:28:34 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-09 17:28:35 - train.py[line:549] - INFO: 0 / 4988
2023-01-09 17:28:35 - train.py[line:551] - INFO: load:1.17 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-09 17:31:07 - train.py[line:549] - INFO: 200 / 4988
2023-01-09 17:31:07 - train.py[line:551] - INFO: load:1.19 valid_run:152.15 task_valid:148.62 collect_output:2.40
2023-01-09 17:33:35 - train.py[line:549] - INFO: 400 / 4988
2023-01-09 17:33:35 - train.py[line:551] - INFO: load:1.22 valid_run:300.25 task_valid:291.89 collect_output:6.20
2023-01-09 17:36:07 - train.py[line:549] - INFO: 600 / 4988
2023-01-09 17:36:07 - train.py[line:551] - INFO: load:1.24 valid_run:452.26 task_valid:435.39 collect_output:13.67
2023-01-09 17:38:36 - train.py[line:549] - INFO: 800 / 4988
2023-01-09 17:38:36 - train.py[line:551] - INFO: load:1.27 valid_run:600.53 task_valid:580.25 collect_output:16.08
2023-01-09 17:41:07 - train.py[line:549] - INFO: 1000 / 4988
2023-01-09 17:41:07 - train.py[line:551] - INFO: load:1.29 valid_run:751.90 task_valid:727.43 collect_output:19.27
2023-01-09 17:43:38 - train.py[line:549] - INFO: 1200 / 4988
2023-01-09 17:43:38 - train.py[line:551] - INFO: load:1.31 valid_run:902.90 task_valid:872.97 collect_output:23.73
2023-01-09 17:46:10 - train.py[line:549] - INFO: 1400 / 4988
2023-01-09 17:46:10 - train.py[line:551] - INFO: load:1.34 valid_run:1054.90 task_valid:1018.81 collect_output:28.87
2023-01-09 17:48:40 - train.py[line:549] - INFO: 1600 / 4988
2023-01-09 17:48:40 - train.py[line:551] - INFO: load:1.36 valid_run:1204.77 task_valid:1159.79 collect_output:36.75
2023-01-09 17:51:09 - train.py[line:549] - INFO: 1800 / 4988
2023-01-09 17:51:09 - train.py[line:551] - INFO: load:1.39 valid_run:1353.44 task_valid:1304.45 collect_output:39.74
2023-01-09 17:53:37 - train.py[line:549] - INFO: 2000 / 4988
2023-01-09 17:53:37 - train.py[line:551] - INFO: load:1.41 valid_run:1501.29 task_valid:1447.64 collect_output:43.41
2023-01-09 17:56:06 - train.py[line:549] - INFO: 2200 / 4988
2023-01-09 17:56:06 - train.py[line:551] - INFO: load:1.44 valid_run:1650.10 task_valid:1592.38 collect_output:46.47
2023-01-09 17:58:35 - train.py[line:549] - INFO: 2400 / 4988
2023-01-09 17:58:35 - train.py[line:551] - INFO: load:1.46 valid_run:1799.03 task_valid:1737.06 collect_output:49.73
2023-01-09 18:01:04 - train.py[line:549] - INFO: 2600 / 4988
2023-01-09 18:01:04 - train.py[line:551] - INFO: load:1.49 valid_run:1947.63 task_valid:1878.68 collect_output:55.71
2023-01-09 18:03:34 - train.py[line:549] - INFO: 2800 / 4988
2023-01-09 18:03:34 - train.py[line:551] - INFO: load:1.51 valid_run:2097.59 task_valid:2024.25 collect_output:59.07
2023-01-09 18:06:03 - train.py[line:549] - INFO: 3000 / 4988
2023-01-09 18:06:03 - train.py[line:551] - INFO: load:1.53 valid_run:2247.03 task_valid:2170.74 collect_output:61.02
2023-01-09 18:08:32 - train.py[line:549] - INFO: 3200 / 4988
2023-01-09 18:08:32 - train.py[line:551] - INFO: load:1.56 valid_run:2396.05 task_valid:2314.81 collect_output:64.97
2023-01-09 18:11:03 - train.py[line:549] - INFO: 3400 / 4988
2023-01-09 18:11:03 - train.py[line:551] - INFO: load:1.58 valid_run:2546.31 task_valid:2460.18 collect_output:68.85
2023-01-09 18:13:33 - train.py[line:549] - INFO: 3600 / 4988
2023-01-09 18:13:33 - train.py[line:551] - INFO: load:1.61 valid_run:2696.50 task_valid:2607.28 collect_output:70.93
2023-01-09 18:16:00 - train.py[line:549] - INFO: 3800 / 4988
2023-01-09 18:16:00 - train.py[line:551] - INFO: load:1.63 valid_run:2843.65 task_valid:2748.77 collect_output:75.58
2023-01-09 18:18:30 - train.py[line:549] - INFO: 4000 / 4988
2023-01-09 18:18:30 - train.py[line:551] - INFO: load:1.66 valid_run:2992.95 task_valid:2893.84 collect_output:78.79
2023-01-09 18:21:00 - train.py[line:549] - INFO: 4200 / 4988
2023-01-09 18:21:00 - train.py[line:551] - INFO: load:1.68 valid_run:3143.47 task_valid:3038.51 collect_output:83.64
2023-01-09 18:23:29 - train.py[line:549] - INFO: 4400 / 4988
2023-01-09 18:23:29 - train.py[line:551] - INFO: load:1.71 valid_run:3292.37 task_valid:3183.30 collect_output:86.74
2023-01-09 18:26:00 - train.py[line:549] - INFO: 4600 / 4988
2023-01-09 18:26:00 - train.py[line:551] - INFO: load:1.73 valid_run:3442.66 task_valid:3329.77 collect_output:89.53
2023-01-09 18:28:30 - train.py[line:549] - INFO: 4800 / 4988
2023-01-09 18:28:30 - train.py[line:551] - INFO: load:1.76 valid_run:3593.19 task_valid:3476.42 collect_output:92.41

====================================================================================================
SGG eval:     R @ 50: 0.4770;     R @ 100: 0.5313;     R @ 500: 0.5846;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2394;    mR @ 100: 0.3151;    mR @ 500: 0.3763;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.3366) (covered in:0.0625) (covering:0.5143) (eating:0.7059) (flying in:0.6364) (growing on:0.2500) (hanging from:0.4839) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.4583) (playing:0.0000) (riding:0.7676) (says:0.0000) (sitting on:0.6409) (standing on:0.5250) (using:0.3000) (walking in:0.0000) (walking on:0.2838) (watching:0.0694) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4770;     R @ 100: 0.5313;     R @ 500: 0.5846;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2394;    mR @ 100: 0.3151;    mR @ 500: 0.3763;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.3366) (covered in:0.0625) (covering:0.5143) (eating:0.7059) (flying in:0.6364) (growing on:0.2500) (hanging from:0.4839) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.4583) (playing:0.0000) (riding:0.7676) (says:0.0000) (sitting on:0.6409) (standing on:0.5250) (using:0.3000) (walking in:0.0000) (walking on:0.2838) (watching:0.0694) 
--------------------------------------------------------
====================================================================================================

2023-01-09 18:31:01 - train.py[line:487] - INFO: 0.5313025974025973
2023-01-09 18:31:01 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-09 18:31:02 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.295 | loss_v1 0 | loss_v2 0 | nll_loss 0.139 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.531303 | ppl 1.1 | vqa_score 0.2691 | wps 119.7 | wpb 89.9 | bsz 30 | num_updates 3000 | best_R@100 0.531303
2023-01-09 18:31:02 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 3000 updates
2023-01-09 18:31:02 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_3000.pt
2023-01-09 18:31:45 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_3000.pt
2023-01-09 18:34:37 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_3000.pt (epoch 1 @ 3000 updates, score 0.5313025974025973) (writing took 215.36488271132112 seconds)
2023-01-09 18:34:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:34:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:34:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:34:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:34:54 - progress_bar.py[line:274] - INFO: epoch 001:   3013 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0632, wps=0.4, ups=0, wpb=109.8, bsz=40, num_updates=3010, lr=3.7625e-05, gnorm=1.11, clip=50, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=16880
2023-01-09 18:34:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:34:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:11 - progress_bar.py[line:274] - INFO: epoch 001:   3023 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0297, wps=101.2, ups=0.61, wpb=109.9, bsz=40, num_updates=3020, lr=3.775e-05, gnorm=1.089, clip=60, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=16897
2023-01-09 18:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:28 - progress_bar.py[line:274] - INFO: epoch 001:   3033 / 100000 loss=0.451, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.01, wps=98.7, ups=0.6, wpb=109.3, bsz=40, num_updates=3030, lr=3.7875e-05, gnorm=1.306, clip=80, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=16914
2023-01-09 18:35:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:45 - progress_bar.py[line:274] - INFO: epoch 001:   3043 / 100000 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0099, wps=103.8, ups=0.63, wpb=110.4, bsz=40, num_updates=3040, lr=3.8e-05, gnorm=1.459, clip=80, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=16931
2023-01-09 18:35:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:35:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:01 - progress_bar.py[line:274] - INFO: epoch 001:   3053 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0561, wps=102.9, ups=0.63, wpb=109.5, bsz=40, num_updates=3050, lr=3.8125e-05, gnorm=1.257, clip=60, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=16947
2023-01-09 18:36:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:18 - progress_bar.py[line:274] - INFO: epoch 001:   3063 / 100000 loss=0.423, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0294, wps=100.9, ups=0.61, wpb=109.7, bsz=40, num_updates=3060, lr=3.825e-05, gnorm=1.312, clip=80, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=16964
2023-01-09 18:36:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:35 - progress_bar.py[line:274] - INFO: epoch 001:   3073 / 100000 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0309, wps=102.1, ups=0.61, wpb=111.1, bsz=40, num_updates=3070, lr=3.8375e-05, gnorm=1.223, clip=80, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=16981
2023-01-09 18:36:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:51 - progress_bar.py[line:274] - INFO: epoch 001:   3083 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107, nsentences=40, sample_size=107, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0303, wps=98, ups=0.61, wpb=107, bsz=40, num_updates=3080, lr=3.85e-05, gnorm=1.491, clip=60, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=16997
2023-01-09 18:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:36:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:09 - progress_bar.py[line:274] - INFO: epoch 001:   3093 / 100000 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0114, wps=99.8, ups=0.6, wpb=110.9, bsz=40, num_updates=3090, lr=3.8625e-05, gnorm=1.053, clip=50, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=17014
2023-01-09 18:37:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:25 - progress_bar.py[line:274] - INFO: epoch 001:   3103 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0505, wps=98.7, ups=0.61, wpb=108.1, bsz=40, num_updates=3100, lr=3.875e-05, gnorm=1.219, clip=70, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=17031
2023-01-09 18:37:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:42 - progress_bar.py[line:274] - INFO: epoch 001:   3113 / 100000 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0227, wps=101.2, ups=0.6, wpb=111.7, bsz=40, num_updates=3110, lr=3.8875e-05, gnorm=1.478, clip=100, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=17048
2023-01-09 18:37:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:37:59 - progress_bar.py[line:274] - INFO: epoch 001:   3123 / 100000 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0208, wps=101, ups=0.61, wpb=109.5, bsz=40, num_updates=3120, lr=3.9e-05, gnorm=1.343, clip=80, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=17065
2023-01-09 18:38:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:16 - progress_bar.py[line:274] - INFO: epoch 001:   3133 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0306, wps=98.5, ups=0.6, wpb=109.6, bsz=40, num_updates=3130, lr=3.9125e-05, gnorm=1.174, clip=50, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=17082
2023-01-09 18:38:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:33 - progress_bar.py[line:274] - INFO: epoch 001:   3143 / 100000 loss=0.449, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0526, wps=98.4, ups=0.6, wpb=109.7, bsz=40, num_updates=3140, lr=3.925e-05, gnorm=1.285, clip=80, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=17099
2023-01-09 18:38:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:50 - progress_bar.py[line:274] - INFO: epoch 001:   3153 / 100000 loss=0.446, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0306, wps=99.5, ups=0.6, wpb=110.1, bsz=40, num_updates=3150, lr=3.9375e-05, gnorm=1.305, clip=90, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=17116
2023-01-09 18:38:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:38:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:07 - progress_bar.py[line:274] - INFO: epoch 001:   3163 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0449, wps=101.8, ups=0.61, wpb=111.9, bsz=40, num_updates=3160, lr=3.95e-05, gnorm=1.394, clip=80, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=17133
2023-01-09 18:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:23 - progress_bar.py[line:274] - INFO: epoch 001:   3173 / 100000 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0309, wps=103, ups=0.62, wpb=111.3, bsz=40, num_updates=3170, lr=3.9625e-05, gnorm=1.443, clip=80, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=17149
2023-01-09 18:39:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:40 - progress_bar.py[line:274] - INFO: epoch 001:   3183 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.011, wps=103.1, ups=0.62, wpb=110.7, bsz=40, num_updates=3180, lr=3.975e-05, gnorm=1.875, clip=80, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=17166
2023-01-09 18:39:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:39:56 - progress_bar.py[line:274] - INFO: epoch 001:   3193 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0396, wps=103.5, ups=0.63, wpb=109.7, bsz=40, num_updates=3190, lr=3.9875e-05, gnorm=1.638, clip=80, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=17182
2023-01-09 18:39:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:12 - progress_bar.py[line:274] - INFO: epoch 001:   3203 / 100000 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.023, wps=101.7, ups=0.62, wpb=110.1, bsz=40, num_updates=3200, lr=4e-05, gnorm=1.327, clip=80, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=17198
2023-01-09 18:40:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:28 - progress_bar.py[line:274] - INFO: epoch 001:   3213 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0092, wps=102, ups=0.63, wpb=108.7, bsz=40, num_updates=3210, lr=4.0125e-05, gnorm=1.355, clip=90, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=17215
2023-01-09 18:40:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:45 - progress_bar.py[line:274] - INFO: epoch 001:   3223 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0396, wps=98.9, ups=0.61, wpb=108.5, bsz=40, num_updates=3220, lr=4.025e-05, gnorm=1.257, clip=80, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17231
2023-01-09 18:40:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:40:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:02 - progress_bar.py[line:274] - INFO: epoch 001:   3233 / 100000 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.022, wps=102, ups=0.62, wpb=110.5, bsz=40, num_updates=3230, lr=4.0375e-05, gnorm=1.504, clip=90, loss_scale=256, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=17248
2023-01-09 18:41:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:18 - progress_bar.py[line:274] - INFO: epoch 001:   3243 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.055, wps=102.5, ups=0.62, wpb=110.7, bsz=40, num_updates=3240, lr=4.05e-05, gnorm=1.475, clip=80, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17264
2023-01-09 18:41:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:35 - progress_bar.py[line:274] - INFO: epoch 001:   3253 / 100000 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0215, wps=101.1, ups=0.61, wpb=110.8, bsz=40, num_updates=3250, lr=4.0625e-05, gnorm=1.063, clip=60, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=17281
2023-01-09 18:41:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:51 - progress_bar.py[line:274] - INFO: epoch 001:   3263 / 100000 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0204, wps=103.1, ups=0.64, wpb=108.3, bsz=40, num_updates=3260, lr=4.075e-05, gnorm=1.017, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=17297
2023-01-09 18:41:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:41:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:07 - progress_bar.py[line:274] - INFO: epoch 001:   3273 / 100000 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.037, wps=101.7, ups=0.62, wpb=109.7, bsz=40, num_updates=3270, lr=4.0875e-05, gnorm=1.002, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=17313
2023-01-09 18:42:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:24 - progress_bar.py[line:274] - INFO: epoch 001:   3283 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0583, wps=100.9, ups=0.61, wpb=110.2, bsz=40, num_updates=3280, lr=4.1e-05, gnorm=1.184, clip=40, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=17330
2023-01-09 18:42:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:41 - progress_bar.py[line:274] - INFO: epoch 001:   3293 / 100000 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0105, wps=100.7, ups=0.6, wpb=111, bsz=40, num_updates=3290, lr=4.1125e-05, gnorm=1.179, clip=70, loss_scale=256, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=17347
2023-01-09 18:42:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:42:57 - progress_bar.py[line:274] - INFO: epoch 001:   3303 / 100000 loss=0.448, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0515, wps=101.9, ups=0.61, wpb=111.4, bsz=40, num_updates=3300, lr=4.125e-05, gnorm=1.681, clip=80, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=17363
2023-01-09 18:42:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:14 - progress_bar.py[line:274] - INFO: epoch 001:   3313 / 100000 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0194, wps=98.2, ups=0.6, wpb=108.9, bsz=40, num_updates=3310, lr=4.1375e-05, gnorm=1.943, clip=60, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=17380
2023-01-09 18:43:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:30 - progress_bar.py[line:274] - INFO: epoch 001:   3323 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0465, wps=105.3, ups=0.63, wpb=112.2, bsz=40, num_updates=3320, lr=4.15e-05, gnorm=1.075, clip=60, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=17396
2023-01-09 18:43:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:46 - progress_bar.py[line:274] - INFO: epoch 001:   3333 / 100000 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0571, wps=105.7, ups=0.64, wpb=110.6, bsz=40, num_updates=3330, lr=4.1625e-05, gnorm=1.228, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17412
2023-01-09 18:43:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:43:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:02 - progress_bar.py[line:274] - INFO: epoch 001:   3343 / 100000 loss=0.432, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0189, wps=104, ups=0.63, wpb=110.9, bsz=40, num_updates=3340, lr=4.175e-05, gnorm=1.416, clip=70, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=17429
2023-01-09 18:44:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:19 - progress_bar.py[line:274] - INFO: epoch 001:   3353 / 100000 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.04, wps=102.4, ups=0.62, wpb=109.6, bsz=40, num_updates=3350, lr=4.1875e-05, gnorm=1.284, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17445
2023-01-09 18:44:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:35 - progress_bar.py[line:274] - INFO: epoch 001:   3363 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.05, wps=103.9, ups=0.62, wpb=111.9, bsz=40, num_updates=3360, lr=4.2e-05, gnorm=1.444, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17461
2023-01-09 18:44:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:52 - progress_bar.py[line:274] - INFO: epoch 001:   3373 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0103, wps=100.8, ups=0.6, wpb=111.1, bsz=40, num_updates=3370, lr=4.2125e-05, gnorm=1.488, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17478
2023-01-09 18:44:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:09 - progress_bar.py[line:274] - INFO: epoch 001:   3383 / 100000 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0096, wps=101.3, ups=0.62, wpb=109.7, bsz=40, num_updates=3380, lr=4.225e-05, gnorm=0.983, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17495
2023-01-09 18:45:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:25 - progress_bar.py[line:274] - INFO: epoch 001:   3393 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0, wps=98.9, ups=0.61, wpb=107.9, bsz=40, num_updates=3390, lr=4.2375e-05, gnorm=1.161, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17511
2023-01-09 18:45:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:42 - progress_bar.py[line:274] - INFO: epoch 001:   3403 / 100000 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0568, wps=101.2, ups=0.61, wpb=111.3, bsz=40, num_updates=3400, lr=4.25e-05, gnorm=1.311, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=17528
2023-01-09 18:45:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:45:59 - progress_bar.py[line:274] - INFO: epoch 001:   3413 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0526, wps=99.1, ups=0.6, wpb=109.6, bsz=40, num_updates=3410, lr=4.2625e-05, gnorm=1.332, clip=70, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=17545
2023-01-09 18:46:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:16 - progress_bar.py[line:274] - INFO: epoch 001:   3423 / 100000 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0112, wps=98.8, ups=0.6, wpb=109.5, bsz=40, num_updates=3420, lr=4.275e-05, gnorm=0.932, clip=30, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=17562
2023-01-09 18:46:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:32 - progress_bar.py[line:274] - INFO: epoch 001:   3433 / 100000 loss=0.444, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0808, wps=100.4, ups=0.61, wpb=110.4, bsz=40, num_updates=3430, lr=4.2875e-05, gnorm=1.105, clip=60, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=17579
2023-01-09 18:46:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:49 - progress_bar.py[line:274] - INFO: epoch 001:   3443 / 100000 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0652, wps=101.7, ups=0.61, wpb=110.7, bsz=40, num_updates=3440, lr=4.3e-05, gnorm=1.065, clip=60, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=17595
2023-01-09 18:46:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:05 - progress_bar.py[line:274] - INFO: epoch 001:   3453 / 100000 loss=0.434, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.01, wps=101.2, ups=0.62, wpb=109.5, bsz=40, num_updates=3450, lr=4.3125e-05, gnorm=1.138, clip=60, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=17612
2023-01-09 18:47:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:22 - progress_bar.py[line:274] - INFO: epoch 001:   3463 / 100000 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0748, wps=103.6, ups=0.62, wpb=110.5, bsz=40, num_updates=3460, lr=4.325e-05, gnorm=1.45, clip=60, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=17628
2023-01-09 18:47:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:38 - progress_bar.py[line:274] - INFO: epoch 001:   3473 / 100000 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.044, wps=104.3, ups=0.63, wpb=110.1, bsz=40, num_updates=3470, lr=4.3375e-05, gnorm=1.312, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17644
2023-01-09 18:47:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:55 - progress_bar.py[line:274] - INFO: epoch 001:   3483 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0096, wps=97.2, ups=0.59, wpb=109.1, bsz=40, num_updates=3480, lr=4.35e-05, gnorm=1.283, clip=60, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=17661
2023-01-09 18:47:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:47:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:12 - progress_bar.py[line:274] - INFO: epoch 001:   3493 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0099, wps=99.9, ups=0.6, wpb=110.4, bsz=40, num_updates=3490, lr=4.3625e-05, gnorm=1.374, clip=90, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=17678
2023-01-09 18:48:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:28 - progress_bar.py[line:274] - INFO: epoch 001:   3503 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0208, wps=103.8, ups=0.64, wpb=108.5, bsz=40, num_updates=3500, lr=4.375e-05, gnorm=1.152, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17694
2023-01-09 18:48:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:44 - progress_bar.py[line:274] - INFO: epoch 001:   3513 / 100000 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0312, wps=101.9, ups=0.61, wpb=110.7, bsz=40, num_updates=3510, lr=4.3875e-05, gnorm=1.084, clip=60, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=17710
2023-01-09 18:48:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:48:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:00 - progress_bar.py[line:274] - INFO: epoch 001:   3523 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0481, wps=102.8, ups=0.63, wpb=108.7, bsz=40, num_updates=3520, lr=4.4e-05, gnorm=1.393, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17726
2023-01-09 18:49:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:17 - progress_bar.py[line:274] - INFO: epoch 001:   3533 / 100000 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0632, wps=101.6, ups=0.61, wpb=110.2, bsz=40, num_updates=3530, lr=4.4125e-05, gnorm=1.668, clip=90, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=17743
2023-01-09 18:49:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:33 - progress_bar.py[line:274] - INFO: epoch 001:   3543 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.028, wps=100.9, ups=0.62, wpb=108.7, bsz=40, num_updates=3540, lr=4.425e-05, gnorm=1.017, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=17759
2023-01-09 18:49:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:50 - progress_bar.py[line:274] - INFO: epoch 001:   3553 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0449, wps=102.6, ups=0.61, wpb=111.2, bsz=40, num_updates=3550, lr=4.4375e-05, gnorm=1.409, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17776
2023-01-09 18:49:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:49:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:06 - progress_bar.py[line:274] - INFO: epoch 001:   3563 / 100000 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0319, wps=102.7, ups=0.62, wpb=109.7, bsz=40, num_updates=3560, lr=4.45e-05, gnorm=1.38, clip=80, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=17792
2023-01-09 18:50:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:22 - progress_bar.py[line:274] - INFO: epoch 001:   3573 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0377, wps=102.5, ups=0.62, wpb=110.1, bsz=40, num_updates=3570, lr=4.4625e-05, gnorm=0.969, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17809
2023-01-09 18:50:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:39 - progress_bar.py[line:274] - INFO: epoch 001:   3583 / 100000 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0096, wps=100.7, ups=0.61, wpb=110.4, bsz=40, num_updates=3580, lr=4.475e-05, gnorm=1.163, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17825
2023-01-09 18:50:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:50:55 - progress_bar.py[line:274] - INFO: epoch 001:   3593 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0465, wps=101.7, ups=0.62, wpb=110.1, bsz=40, num_updates=3590, lr=4.4875e-05, gnorm=1.358, clip=60, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=17842
2023-01-09 18:50:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:12 - progress_bar.py[line:274] - INFO: epoch 001:   3603 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0532, wps=105.3, ups=0.63, wpb=111, bsz=40, num_updates=3600, lr=4.5e-05, gnorm=1.106, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=17858
2023-01-09 18:51:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:28 - progress_bar.py[line:274] - INFO: epoch 001:   3613 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0109, wps=103.2, ups=0.62, wpb=111.3, bsz=40, num_updates=3610, lr=4.5125e-05, gnorm=1.074, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=17874
2023-01-09 18:51:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:45 - progress_bar.py[line:274] - INFO: epoch 001:   3623 / 100000 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0288, wps=101.1, ups=0.61, wpb=110, bsz=40, num_updates=3620, lr=4.525e-05, gnorm=1.519, clip=90, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=17891
2023-01-09 18:51:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:51:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:01 - progress_bar.py[line:274] - INFO: epoch 001:   3633 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.037, wps=102.4, ups=0.62, wpb=109.8, bsz=40, num_updates=3630, lr=4.5375e-05, gnorm=1.059, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=17907
2023-01-09 18:52:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:17 - progress_bar.py[line:274] - INFO: epoch 001:   3643 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.019, wps=100.8, ups=0.62, wpb=108.9, bsz=40, num_updates=3640, lr=4.55e-05, gnorm=1.672, clip=80, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=17924
2023-01-09 18:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:34 - progress_bar.py[line:274] - INFO: epoch 001:   3653 / 100000 loss=0.447, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0189, wps=99.6, ups=0.61, wpb=108.7, bsz=40, num_updates=3650, lr=4.5625e-05, gnorm=1.103, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=17940
2023-01-09 18:52:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:51 - progress_bar.py[line:274] - INFO: epoch 001:   3663 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0275, wps=100, ups=0.61, wpb=109.3, bsz=40, num_updates=3660, lr=4.575e-05, gnorm=1.184, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=17957
2023-01-09 18:52:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:52:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:07 - progress_bar.py[line:274] - INFO: epoch 001:   3673 / 100000 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0202, wps=99.3, ups=0.6, wpb=109.5, bsz=40, num_updates=3670, lr=4.5875e-05, gnorm=1.372, clip=80, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=17974
2023-01-09 18:53:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:25 - progress_bar.py[line:274] - INFO: epoch 001:   3683 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0099, wps=98.1, ups=0.6, wpb=109.4, bsz=40, num_updates=3680, lr=4.6e-05, gnorm=1.49, clip=60, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=17991
2023-01-09 18:53:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:42 - progress_bar.py[line:274] - INFO: epoch 001:   3693 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0521, wps=98.1, ups=0.6, wpb=109.3, bsz=40, num_updates=3690, lr=4.6125e-05, gnorm=1.105, clip=50, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=18008
2023-01-09 18:53:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:53:58 - progress_bar.py[line:274] - INFO: epoch 001:   3703 / 100000 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0189, wps=101.2, ups=0.61, wpb=109.7, bsz=40, num_updates=3700, lr=4.625e-05, gnorm=1.247, clip=90, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=18024
2023-01-09 18:54:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:15 - progress_bar.py[line:274] - INFO: epoch 001:   3713 / 100000 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0741, wps=99.8, ups=0.62, wpb=107.9, bsz=40, num_updates=3710, lr=4.6375e-05, gnorm=1.33, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=18041
2023-01-09 18:54:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:31 - progress_bar.py[line:274] - INFO: epoch 001:   3723 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0333, wps=100.2, ups=0.62, wpb=108.5, bsz=40, num_updates=3720, lr=4.65e-05, gnorm=1.084, clip=70, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=18057
2023-01-09 18:54:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:47 - progress_bar.py[line:274] - INFO: epoch 001:   3733 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.02, wps=102.6, ups=0.63, wpb=109.1, bsz=40, num_updates=3730, lr=4.6625e-05, gnorm=1.348, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=18073
2023-01-09 18:54:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:54:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:04 - progress_bar.py[line:274] - INFO: epoch 001:   3743 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0505, wps=100.2, ups=0.61, wpb=109.1, bsz=40, num_updates=3740, lr=4.675e-05, gnorm=1.353, clip=60, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=18090
2023-01-09 18:55:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:20 - progress_bar.py[line:274] - INFO: epoch 001:   3753 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0377, wps=101.1, ups=0.61, wpb=109.6, bsz=40, num_updates=3750, lr=4.6875e-05, gnorm=1.11, clip=60, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=18107
2023-01-09 18:55:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:37 - progress_bar.py[line:274] - INFO: epoch 001:   3763 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0521, wps=98.9, ups=0.6, wpb=109.5, bsz=40, num_updates=3760, lr=4.7e-05, gnorm=1.173, clip=70, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=18123
2023-01-09 18:55:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:54 - progress_bar.py[line:274] - INFO: epoch 001:   3773 / 100000 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0288, wps=100.2, ups=0.61, wpb=109.1, bsz=40, num_updates=3770, lr=4.7125e-05, gnorm=1.06, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=18140
2023-01-09 18:55:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:55:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:10 - progress_bar.py[line:274] - INFO: epoch 001:   3783 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0211, wps=102.8, ups=0.62, wpb=110.2, bsz=40, num_updates=3780, lr=4.725e-05, gnorm=1.258, clip=70, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=18156
2023-01-09 18:56:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:27 - progress_bar.py[line:274] - INFO: epoch 001:   3793 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0538, wps=101, ups=0.61, wpb=110.6, bsz=40, num_updates=3790, lr=4.7375e-05, gnorm=1.551, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=18173
2023-01-09 18:56:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:44 - progress_bar.py[line:274] - INFO: epoch 001:   3803 / 100000 loss=0.435, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0396, wps=100.1, ups=0.61, wpb=110.2, bsz=40, num_updates=3800, lr=4.75e-05, gnorm=1.369, clip=80, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=18190
2023-01-09 18:56:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:56:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:00 - progress_bar.py[line:274] - INFO: epoch 001:   3813 / 100000 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0326, wps=101.4, ups=0.61, wpb=110.7, bsz=40, num_updates=3810, lr=4.7625e-05, gnorm=1.542, clip=90, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=18206
2023-01-09 18:57:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:17 - progress_bar.py[line:274] - INFO: epoch 001:   3823 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0104, wps=98, ups=0.59, wpb=110.2, bsz=40, num_updates=3820, lr=4.775e-05, gnorm=1.129, clip=30, loss_scale=1024, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=18224
2023-01-09 18:57:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:34 - progress_bar.py[line:274] - INFO: epoch 001:   3833 / 100000 loss=0.442, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0288, wps=100.9, ups=0.62, wpb=109.1, bsz=40, num_updates=3830, lr=4.7875e-05, gnorm=1.292, clip=70, loss_scale=1024, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=18240
2023-01-09 18:57:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:50 - progress_bar.py[line:274] - INFO: epoch 001:   3843 / 100000 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0481, wps=103.2, ups=0.62, wpb=110.5, bsz=40, num_updates=3840, lr=4.8e-05, gnorm=1.072, clip=50, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=18256
2023-01-09 18:57:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:57:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:06 - progress_bar.py[line:274] - INFO: epoch 001:   3853 / 100000 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0667, wps=103.8, ups=0.63, wpb=110.1, bsz=40, num_updates=3850, lr=4.8125e-05, gnorm=1.08, clip=30, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=18273
2023-01-09 18:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:23 - progress_bar.py[line:274] - INFO: epoch 001:   3863 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0521, wps=100.3, ups=0.61, wpb=110.1, bsz=40, num_updates=3860, lr=4.825e-05, gnorm=0.969, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=18289
2023-01-09 18:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:40 - progress_bar.py[line:274] - INFO: epoch 001:   3873 / 100000 loss=0.426, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0189, wps=99.8, ups=0.61, wpb=109.7, bsz=40, num_updates=3870, lr=4.8375e-05, gnorm=1.209, clip=60, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=18306
2023-01-09 18:58:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:58:56 - progress_bar.py[line:274] - INFO: epoch 001:   3883 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0485, wps=102.3, ups=0.62, wpb=109.6, bsz=40, num_updates=3880, lr=4.85e-05, gnorm=1.079, clip=40, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=18322
2023-01-09 18:58:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:13 - progress_bar.py[line:274] - INFO: epoch 001:   3893 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.037, wps=101.7, ups=0.61, wpb=110.3, bsz=40, num_updates=3890, lr=4.8625e-05, gnorm=1.004, clip=40, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=18339
2023-01-09 18:59:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:29 - progress_bar.py[line:274] - INFO: epoch 001:   3903 / 100000 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=107.733, nsentences=40, sample_size=107.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0583, wps=99.8, ups=0.62, wpb=107.7, bsz=40, num_updates=3900, lr=4.875e-05, gnorm=1.282, clip=40, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=18355
2023-01-09 18:59:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:46 - progress_bar.py[line:274] - INFO: epoch 001:   3913 / 100000 loss=0.445, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0261, wps=99.5, ups=0.61, wpb=108.8, bsz=40, num_updates=3910, lr=4.8875e-05, gnorm=1.275, clip=70, loss_scale=1024, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=18372
2023-01-09 18:59:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 18:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:03 - progress_bar.py[line:274] - INFO: epoch 001:   3923 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0891, wps=100.6, ups=0.61, wpb=110.5, bsz=40, num_updates=3920, lr=4.9e-05, gnorm=1.438, clip=80, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=18389
2023-01-09 19:00:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:21 - progress_bar.py[line:274] - INFO: epoch 001:   3933 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0568, wps=99.3, ups=0.6, wpb=110.4, bsz=40, num_updates=3930, lr=4.9125e-05, gnorm=1.119, clip=50, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=18406
2023-01-09 19:00:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:38 - progress_bar.py[line:274] - INFO: epoch 001:   3943 / 100000 loss=0.433, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0101, wps=103.1, ups=0.62, wpb=110.6, bsz=40, num_updates=3940, lr=4.925e-05, gnorm=1.184, clip=70, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=18423
2023-01-09 19:00:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:55 - progress_bar.py[line:274] - INFO: epoch 001:   3953 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0467, wps=97.5, ups=0.6, wpb=108.3, bsz=40, num_updates=3950, lr=4.9375e-05, gnorm=1.093, clip=50, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=18441
2023-01-09 19:00:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:00:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:12 - progress_bar.py[line:274] - INFO: epoch 001:   3963 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0192, wps=97.4, ups=0.6, wpb=108.2, bsz=40, num_updates=3960, lr=4.95e-05, gnorm=0.973, clip=40, loss_scale=1024, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=18458
2023-01-09 19:01:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:30 - progress_bar.py[line:274] - INFO: epoch 001:   3973 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0377, wps=99.2, ups=0.61, wpb=108.5, bsz=40, num_updates=3970, lr=4.9625e-05, gnorm=1.134, clip=60, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=18475
2023-01-09 19:01:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:47 - progress_bar.py[line:274] - INFO: epoch 001:   3983 / 100000 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0288, wps=97, ups=0.59, wpb=109, bsz=40, num_updates=3980, lr=4.975e-05, gnorm=1.069, clip=50, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=18493
2023-01-09 19:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:01:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:02:04 - progress_bar.py[line:274] - INFO: epoch 001:   3993 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.044, wps=102.6, ups=0.61, wpb=111.3, bsz=40, num_updates=3990, lr=4.9875e-05, gnorm=1.039, clip=40, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=18510
2023-01-09 19:02:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:02:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:02:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:02:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:02:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 19:02:21 - progress_bar.py[line:274] - INFO: epoch 001:   4003 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0392, wps=100.3, ups=0.61, wpb=108.8, bsz=40, num_updates=4000, lr=5e-05, gnorm=1.297, clip=60, loss_scale=1024, train_wall=16, gb_free=10.9, ema_decay=0.9999, wall=18527
2023-01-09 19:02:21 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-09 19:02:23 - train.py[line:549] - INFO: 0 / 4988
2023-01-09 19:02:23 - train.py[line:551] - INFO: load:1.44 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-09 19:04:55 - train.py[line:549] - INFO: 200 / 4988
2023-01-09 19:04:55 - train.py[line:551] - INFO: load:1.46 valid_run:151.57 task_valid:148.60 collect_output:1.87
2023-01-09 19:07:23 - train.py[line:549] - INFO: 400 / 4988
2023-01-09 19:07:23 - train.py[line:551] - INFO: load:1.49 valid_run:300.19 task_valid:292.47 collect_output:5.57
2023-01-09 19:09:55 - train.py[line:549] - INFO: 600 / 4988
2023-01-09 19:09:55 - train.py[line:551] - INFO: load:1.51 valid_run:452.01 task_valid:436.12 collect_output:12.69
2023-01-09 19:12:25 - train.py[line:549] - INFO: 800 / 4988
2023-01-09 19:12:25 - train.py[line:551] - INFO: load:1.54 valid_run:601.28 task_valid:581.71 collect_output:15.29
2023-01-09 19:14:57 - train.py[line:549] - INFO: 1000 / 4988
2023-01-09 19:14:57 - train.py[line:551] - INFO: load:1.56 valid_run:753.69 task_valid:729.78 collect_output:18.55
2023-01-09 19:17:29 - train.py[line:549] - INFO: 1200 / 4988
2023-01-09 19:17:29 - train.py[line:551] - INFO: load:1.59 valid_run:905.16 task_valid:875.71 collect_output:23.02
2023-01-09 19:20:01 - train.py[line:549] - INFO: 1400 / 4988
2023-01-09 19:20:01 - train.py[line:551] - INFO: load:1.61 valid_run:1057.70 task_valid:1022.00 collect_output:28.24
2023-01-09 19:22:32 - train.py[line:549] - INFO: 1600 / 4988
2023-01-09 19:22:32 - train.py[line:551] - INFO: load:1.64 valid_run:1208.14 task_valid:1163.35 collect_output:36.29
2023-01-09 19:25:01 - train.py[line:549] - INFO: 1800 / 4988
2023-01-09 19:25:01 - train.py[line:551] - INFO: load:1.67 valid_run:1357.56 task_valid:1308.74 collect_output:39.27
2023-01-09 19:27:30 - train.py[line:549] - INFO: 2000 / 4988
2023-01-09 19:27:30 - train.py[line:551] - INFO: load:1.69 valid_run:1505.74 task_valid:1452.17 collect_output:42.99
2023-01-09 19:29:59 - train.py[line:549] - INFO: 2200 / 4988
2023-01-09 19:29:59 - train.py[line:551] - INFO: load:1.72 valid_run:1654.97 task_valid:1597.23 collect_output:46.13
2023-01-09 19:32:29 - train.py[line:549] - INFO: 2400 / 4988
2023-01-09 19:32:29 - train.py[line:551] - INFO: load:1.75 valid_run:1804.52 task_valid:1742.41 collect_output:49.45
2023-01-09 19:34:58 - train.py[line:549] - INFO: 2600 / 4988
2023-01-09 19:34:58 - train.py[line:551] - INFO: load:1.78 valid_run:1953.40 task_valid:1884.50 collect_output:55.23
2023-01-09 19:37:27 - train.py[line:549] - INFO: 2800 / 4988
2023-01-09 19:37:27 - train.py[line:551] - INFO: load:1.80 valid_run:2103.16 task_valid:2030.19 collect_output:58.27
2023-01-09 19:39:57 - train.py[line:549] - INFO: 3000 / 4988
2023-01-09 19:39:57 - train.py[line:551] - INFO: load:1.83 valid_run:2252.77 task_valid:2176.67 collect_output:60.38
2023-01-09 19:42:27 - train.py[line:549] - INFO: 3200 / 4988
2023-01-09 19:42:27 - train.py[line:551] - INFO: load:1.86 valid_run:2402.20 task_valid:2320.84 collect_output:64.63
2023-01-09 19:44:57 - train.py[line:549] - INFO: 3400 / 4988
2023-01-09 19:44:57 - train.py[line:551] - INFO: load:1.88 valid_run:2552.86 task_valid:2466.40 collect_output:68.71
2023-01-09 19:47:28 - train.py[line:549] - INFO: 3600 / 4988
2023-01-09 19:47:28 - train.py[line:551] - INFO: load:1.91 valid_run:2702.98 task_valid:2613.33 collect_output:70.89
2023-01-09 19:49:55 - train.py[line:549] - INFO: 3800 / 4988
2023-01-09 19:49:55 - train.py[line:551] - INFO: load:1.94 valid_run:2850.39 task_valid:2754.89 collect_output:75.74
2023-01-09 19:52:25 - train.py[line:549] - INFO: 4000 / 4988
2023-01-09 19:52:25 - train.py[line:551] - INFO: load:1.96 valid_run:2999.78 task_valid:2899.95 collect_output:79.07
2023-01-09 19:54:56 - train.py[line:549] - INFO: 4200 / 4988
2023-01-09 19:54:56 - train.py[line:551] - INFO: load:1.99 valid_run:3150.67 task_valid:3044.60 collect_output:84.29
2023-01-09 19:57:24 - train.py[line:549] - INFO: 4400 / 4988
2023-01-09 19:57:24 - train.py[line:551] - INFO: load:2.02 valid_run:3299.46 task_valid:3189.16 collect_output:87.52
2023-01-09 19:59:55 - train.py[line:549] - INFO: 4600 / 4988
2023-01-09 19:59:55 - train.py[line:551] - INFO: load:2.04 valid_run:3449.80 task_valid:3335.35 collect_output:90.67
2023-01-09 20:02:26 - train.py[line:549] - INFO: 4800 / 4988
2023-01-09 20:02:26 - train.py[line:551] - INFO: load:2.07 valid_run:3600.43 task_valid:3481.86 collect_output:93.77

====================================================================================================
SGG eval:     R @ 50: 0.6002;     R @ 100: 0.6379;     R @ 500: 0.6883;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3454;    mR @ 100: 0.4147;    mR @ 500: 0.4635;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5732) (covered in:0.0625) (covering:0.7143) (eating:0.7059) (flying in:0.8636) (growing on:0.5000) (hanging from:0.3839) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7500) (playing:0.0000) (riding:0.9281) (says:0.0000) (sitting on:0.7268) (standing on:0.5010) (using:0.3500) (walking in:0.0000) (walking on:0.6486) (watching:0.3194) 
--------------------------------------------------------
====================================================================================================

2023-01-09 20:04:57 - train.py[line:487] - INFO: 0.6379027247262541

====================================================================================================
SGG eval:     R @ 50: 0.6002;     R @ 100: 0.6379;     R @ 500: 0.6883;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3454;    mR @ 100: 0.4147;    mR @ 500: 0.4635;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5732) (covered in:0.0625) (covering:0.7143) (eating:0.7059) (flying in:0.8636) (growing on:0.5000) (hanging from:0.3839) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7500) (playing:0.0000) (riding:0.9281) (says:0.0000) (sitting on:0.7268) (standing on:0.5010) (using:0.3500) (walking in:0.0000) (walking on:0.6486) (watching:0.3194) 
--------------------------------------------------------
====================================================================================================

2023-01-09 20:04:57 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-09 20:04:57 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.323 | loss_v1 0 | loss_v2 0 | nll_loss 0.171 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.637903 | ppl 1.13 | vqa_score 0.393 | wps 119.5 | wpb 89.9 | bsz 30 | num_updates 4000 | best_R@100 0.637903
2023-01-09 20:04:57 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 4000 updates
2023-01-09 20:04:57 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt
2023-01-09 20:05:51 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt
2023-01-09 20:09:04 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_4000.pt (epoch 1 @ 4000 updates, score 0.6379027247262541) (writing took 247.2283778525889 seconds)
2023-01-09 20:09:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:21 - progress_bar.py[line:274] - INFO: epoch 001:   4013 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1111, wps=0.4, ups=0, wpb=108.8, bsz=40, num_updates=4010, lr=4.99948e-05, gnorm=1.536, clip=80, loss_scale=1024, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=22547
2023-01-09 20:09:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:37 - progress_bar.py[line:274] - INFO: epoch 001:   4023 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0286, wps=103.9, ups=0.63, wpb=110.3, bsz=40, num_updates=4020, lr=4.99896e-05, gnorm=1.426, clip=70, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=22563
2023-01-09 20:09:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:53 - progress_bar.py[line:274] - INFO: epoch 001:   4033 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0273, wps=99.8, ups=0.61, wpb=109.5, bsz=40, num_updates=4030, lr=4.99844e-05, gnorm=1.039, clip=50, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=22580
2023-01-09 20:09:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:10 - progress_bar.py[line:274] - INFO: epoch 001:   4043 / 100000 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0326, wps=100.5, ups=0.61, wpb=110.7, bsz=40, num_updates=4040, lr=4.99792e-05, gnorm=1.013, clip=40, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=22596
2023-01-09 20:10:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:27 - progress_bar.py[line:274] - INFO: epoch 001:   4053 / 100000 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0842, wps=100.6, ups=0.61, wpb=109.7, bsz=40, num_updates=4050, lr=4.9974e-05, gnorm=0.95, clip=30, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=22613
2023-01-09 20:10:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:43 - progress_bar.py[line:274] - INFO: epoch 001:   4063 / 100000 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0367, wps=101.5, ups=0.62, wpb=109.7, bsz=40, num_updates=4060, lr=4.99688e-05, gnorm=1.179, clip=50, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=22630
2023-01-09 20:10:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:10:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:00 - progress_bar.py[line:274] - INFO: epoch 001:   4073 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0882, wps=100.4, ups=0.61, wpb=109.3, bsz=40, num_updates=4070, lr=4.99635e-05, gnorm=0.909, clip=40, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=22646
2023-01-09 20:11:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:08 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-09 20:11:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:17 - progress_bar.py[line:274] - INFO: epoch 001:   4084 / 100000 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.04, wps=98, ups=0.59, wpb=111.2, bsz=40, num_updates=4080, lr=4.99583e-05, gnorm=1.608, clip=90, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=22663
2023-01-09 20:11:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:34 - progress_bar.py[line:274] - INFO: epoch 001:   4094 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=112.533, nsentences=40, sample_size=112.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0602, wps=101.5, ups=0.6, wpb=112.5, bsz=40, num_updates=4090, lr=4.99531e-05, gnorm=1.187, clip=80, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=22680
2023-01-09 20:11:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:51 - progress_bar.py[line:274] - INFO: epoch 001:   4104 / 100000 loss=0.45, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0086, wps=100.4, ups=0.61, wpb=110.5, bsz=40, num_updates=4100, lr=4.99479e-05, gnorm=1.457, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=22697
2023-01-09 20:11:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:11:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:08 - progress_bar.py[line:274] - INFO: epoch 001:   4114 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0215, wps=99.4, ups=0.6, wpb=110, bsz=40, num_updates=4110, lr=4.99427e-05, gnorm=0.88, clip=30, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=22714
2023-01-09 20:12:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:24 - progress_bar.py[line:274] - INFO: epoch 001:   4124 / 100000 loss=0.457, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0093, wps=99.7, ups=0.61, wpb=108.7, bsz=40, num_updates=4120, lr=4.99375e-05, gnorm=1.431, clip=70, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=22730
2023-01-09 20:12:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:41 - progress_bar.py[line:274] - INFO: epoch 001:   4134 / 100000 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0561, wps=100.1, ups=0.62, wpb=107.9, bsz=40, num_updates=4130, lr=4.99323e-05, gnorm=1.211, clip=60, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=22747
2023-01-09 20:12:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:12:57 - progress_bar.py[line:274] - INFO: epoch 001:   4144 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0648, wps=102.5, ups=0.62, wpb=111.1, bsz=40, num_updates=4140, lr=4.99271e-05, gnorm=1.157, clip=60, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=22763
2023-01-09 20:12:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:14 - progress_bar.py[line:274] - INFO: epoch 001:   4154 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0421, wps=100.4, ups=0.61, wpb=110, bsz=40, num_updates=4150, lr=4.99219e-05, gnorm=0.773, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=22780
2023-01-09 20:13:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:31 - progress_bar.py[line:274] - INFO: epoch 001:   4164 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0632, wps=100.1, ups=0.6, wpb=110.6, bsz=40, num_updates=4160, lr=4.99167e-05, gnorm=1.331, clip=60, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=22797
2023-01-09 20:13:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:47 - progress_bar.py[line:274] - INFO: epoch 001:   4174 / 100000 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0385, wps=102.9, ups=0.62, wpb=109.9, bsz=40, num_updates=4170, lr=4.99115e-05, gnorm=1.041, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=22813
2023-01-09 20:13:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:13:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:04 - progress_bar.py[line:274] - INFO: epoch 001:   4184 / 100000 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0455, wps=100.6, ups=0.61, wpb=110.3, bsz=40, num_updates=4180, lr=4.99063e-05, gnorm=1.024, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=22830
2023-01-09 20:14:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:21 - progress_bar.py[line:274] - INFO: epoch 001:   4194 / 100000 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0495, wps=99.9, ups=0.6, wpb=110.7, bsz=40, num_updates=4190, lr=4.9901e-05, gnorm=1.056, clip=60, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=22847
2023-01-09 20:14:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:37 - progress_bar.py[line:274] - INFO: epoch 001:   4204 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0098, wps=98.1, ups=0.61, wpb=107.2, bsz=40, num_updates=4200, lr=4.98958e-05, gnorm=1.068, clip=60, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=22864
2023-01-09 20:14:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:54 - progress_bar.py[line:274] - INFO: epoch 001:   4214 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0217, wps=103.5, ups=0.63, wpb=109.9, bsz=40, num_updates=4210, lr=4.98906e-05, gnorm=1.059, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=22880
2023-01-09 20:14:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:14:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:10 - progress_bar.py[line:274] - INFO: epoch 001:   4224 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0638, wps=101.5, ups=0.62, wpb=109.7, bsz=40, num_updates=4220, lr=4.98854e-05, gnorm=0.965, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=22896
2023-01-09 20:15:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:27 - progress_bar.py[line:274] - INFO: epoch 001:   4234 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0204, wps=99.7, ups=0.61, wpb=109.5, bsz=40, num_updates=4230, lr=4.98802e-05, gnorm=1.037, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=22913
2023-01-09 20:15:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:43 - progress_bar.py[line:274] - INFO: epoch 001:   4244 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0412, wps=102.3, ups=0.61, wpb=111.1, bsz=40, num_updates=4240, lr=4.9875e-05, gnorm=1.188, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=22930
2023-01-09 20:15:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:15:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:00 - progress_bar.py[line:274] - INFO: epoch 001:   4254 / 100000 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.06, wps=100.1, ups=0.61, wpb=109.4, bsz=40, num_updates=4250, lr=4.98698e-05, gnorm=1.586, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=22946
2023-01-09 20:16:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:17 - progress_bar.py[line:274] - INFO: epoch 001:   4264 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0421, wps=99.9, ups=0.61, wpb=109.3, bsz=40, num_updates=4260, lr=4.98646e-05, gnorm=1.3, clip=60, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=22963
2023-01-09 20:16:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:34 - progress_bar.py[line:274] - INFO: epoch 001:   4274 / 100000 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0396, wps=99.9, ups=0.6, wpb=110.9, bsz=40, num_updates=4270, lr=4.98594e-05, gnorm=1.27, clip=80, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=22980
2023-01-09 20:16:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:51 - progress_bar.py[line:274] - INFO: epoch 001:   4284 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0213, wps=97.5, ups=0.6, wpb=108.9, bsz=40, num_updates=4280, lr=4.98542e-05, gnorm=1.011, clip=40, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=22997
2023-01-09 20:16:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:16:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:07 - progress_bar.py[line:274] - INFO: epoch 001:   4294 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0481, wps=101, ups=0.61, wpb=110.3, bsz=40, num_updates=4290, lr=4.9849e-05, gnorm=1.115, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=23013
2023-01-09 20:17:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:24 - progress_bar.py[line:274] - INFO: epoch 001:   4304 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0778, wps=104.3, ups=0.63, wpb=110.5, bsz=40, num_updates=4300, lr=4.98438e-05, gnorm=1.231, clip=70, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=23030
2023-01-09 20:17:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:40 - progress_bar.py[line:274] - INFO: epoch 001:   4314 / 100000 loss=0.425, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.01, wps=100.1, ups=0.61, wpb=109.6, bsz=40, num_updates=4310, lr=4.98385e-05, gnorm=1.596, clip=60, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=23046
2023-01-09 20:17:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:17:56 - progress_bar.py[line:274] - INFO: epoch 001:   4324 / 100000 loss=0.428, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.1078, wps=103.4, ups=0.63, wpb=108.9, bsz=40, num_updates=4320, lr=4.98333e-05, gnorm=1.256, clip=70, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=23063
2023-01-09 20:17:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:13 - progress_bar.py[line:274] - INFO: epoch 001:   4334 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0532, wps=103.7, ups=0.63, wpb=109.9, bsz=40, num_updates=4330, lr=4.98281e-05, gnorm=1.287, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23079
2023-01-09 20:18:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:29 - progress_bar.py[line:274] - INFO: epoch 001:   4344 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.037, wps=98.9, ups=0.6, wpb=109.2, bsz=40, num_updates=4340, lr=4.98229e-05, gnorm=2, clip=70, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=23096
2023-01-09 20:18:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:46 - progress_bar.py[line:274] - INFO: epoch 001:   4354 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0385, wps=99.6, ups=0.61, wpb=109.2, bsz=40, num_updates=4350, lr=4.98177e-05, gnorm=1.218, clip=70, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=23112
2023-01-09 20:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:18:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:03 - progress_bar.py[line:274] - INFO: epoch 001:   4364 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.018, wps=104.1, ups=0.62, wpb=111.3, bsz=40, num_updates=4360, lr=4.98125e-05, gnorm=1.013, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=23129
2023-01-09 20:19:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:19 - progress_bar.py[line:274] - INFO: epoch 001:   4374 / 100000 loss=0.437, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.0261, wps=102.2, ups=0.63, wpb=108.8, bsz=40, num_updates=4370, lr=4.98073e-05, gnorm=1.799, clip=60, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=23145
2023-01-09 20:19:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:35 - progress_bar.py[line:274] - INFO: epoch 001:   4384 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.033, wps=101.4, ups=0.61, wpb=110.6, bsz=40, num_updates=4380, lr=4.98021e-05, gnorm=1.404, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23162
2023-01-09 20:19:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:52 - progress_bar.py[line:274] - INFO: epoch 001:   4394 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0748, wps=101.5, ups=0.62, wpb=109.6, bsz=40, num_updates=4390, lr=4.97969e-05, gnorm=1.23, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23178
2023-01-09 20:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:19:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:09 - progress_bar.py[line:274] - INFO: epoch 001:   4404 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0561, wps=99.7, ups=0.6, wpb=111.2, bsz=40, num_updates=4400, lr=4.97917e-05, gnorm=1.302, clip=70, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=23195
2023-01-09 20:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:26 - progress_bar.py[line:274] - INFO: epoch 001:   4414 / 100000 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0472, wps=100.8, ups=0.61, wpb=110.4, bsz=40, num_updates=4410, lr=4.97865e-05, gnorm=1.163, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=23212
2023-01-09 20:20:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:42 - progress_bar.py[line:274] - INFO: epoch 001:   4424 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.069, wps=100.1, ups=0.6, wpb=110.7, bsz=40, num_updates=4420, lr=4.97813e-05, gnorm=1.006, clip=50, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=23229
2023-01-09 20:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:20:59 - progress_bar.py[line:274] - INFO: epoch 001:   4434 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0417, wps=102.3, ups=0.62, wpb=110.5, bsz=40, num_updates=4430, lr=4.9776e-05, gnorm=1.108, clip=70, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=23245
2023-01-09 20:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:16 - progress_bar.py[line:274] - INFO: epoch 001:   4444 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0283, wps=98.4, ups=0.61, wpb=108.1, bsz=40, num_updates=4440, lr=4.97708e-05, gnorm=1.754, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23262
2023-01-09 20:21:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:32 - progress_bar.py[line:274] - INFO: epoch 001:   4454 / 100000 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0364, wps=98.7, ups=0.6, wpb=108.9, bsz=40, num_updates=4450, lr=4.97656e-05, gnorm=1.986, clip=90, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=23279
2023-01-09 20:21:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:49 - progress_bar.py[line:274] - INFO: epoch 001:   4464 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.049, wps=100.6, ups=0.61, wpb=109.8, bsz=40, num_updates=4460, lr=4.97604e-05, gnorm=3.133, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23295
2023-01-09 20:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:21:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:06 - progress_bar.py[line:274] - INFO: epoch 001:   4474 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0323, wps=100, ups=0.6, wpb=110.3, bsz=40, num_updates=4470, lr=4.97552e-05, gnorm=1.198, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23312
2023-01-09 20:22:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:22 - progress_bar.py[line:274] - INFO: epoch 001:   4484 / 100000 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0857, wps=100.7, ups=0.61, wpb=110.5, bsz=40, num_updates=4480, lr=4.975e-05, gnorm=1.241, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23329
2023-01-09 20:22:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:39 - progress_bar.py[line:274] - INFO: epoch 001:   4494 / 100000 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0571, wps=100.9, ups=0.61, wpb=109.4, bsz=40, num_updates=4490, lr=4.97448e-05, gnorm=1.418, clip=80, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=23345
2023-01-09 20:22:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:56 - progress_bar.py[line:274] - INFO: epoch 001:   4504 / 100000 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0566, wps=97.1, ups=0.59, wpb=109.7, bsz=40, num_updates=4500, lr=4.97396e-05, gnorm=1.146, clip=40, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=23362
2023-01-09 20:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:13 - progress_bar.py[line:274] - INFO: epoch 001:   4514 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0784, wps=101.7, ups=0.62, wpb=110.1, bsz=40, num_updates=4510, lr=4.97344e-05, gnorm=1.263, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23379
2023-01-09 20:23:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:29 - progress_bar.py[line:274] - INFO: epoch 001:   4524 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1183, wps=102.3, ups=0.62, wpb=110, bsz=40, num_updates=4520, lr=4.97292e-05, gnorm=1.2, clip=70, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=23395
2023-01-09 20:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:46 - progress_bar.py[line:274] - INFO: epoch 001:   4534 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.086, wps=99.9, ups=0.61, wpb=109.3, bsz=40, num_updates=4530, lr=4.9724e-05, gnorm=1.084, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23412
2023-01-09 20:23:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:23:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:03 - progress_bar.py[line:274] - INFO: epoch 001:   4544 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0583, wps=99.1, ups=0.61, wpb=108.8, bsz=40, num_updates=4540, lr=4.97188e-05, gnorm=1.374, clip=70, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=23429
2023-01-09 20:24:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:19 - progress_bar.py[line:274] - INFO: epoch 001:   4554 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0444, wps=101.4, ups=0.61, wpb=111.1, bsz=40, num_updates=4550, lr=4.97135e-05, gnorm=1.245, clip=60, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=23445
2023-01-09 20:24:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:36 - progress_bar.py[line:274] - INFO: epoch 001:   4564 / 100000 loss=0.419, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0571, wps=102, ups=0.62, wpb=109.9, bsz=40, num_updates=4560, lr=4.97083e-05, gnorm=1.067, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=23462
2023-01-09 20:24:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:53 - progress_bar.py[line:274] - INFO: epoch 001:   4574 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0444, wps=100.3, ups=0.6, wpb=111.5, bsz=40, num_updates=4570, lr=4.97031e-05, gnorm=1.061, clip=40, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=23479
2023-01-09 20:24:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:24:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:09 - progress_bar.py[line:274] - INFO: epoch 001:   4584 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.051, wps=99.9, ups=0.61, wpb=108.7, bsz=40, num_updates=4580, lr=4.96979e-05, gnorm=1.121, clip=60, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=23495
2023-01-09 20:25:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:26 - progress_bar.py[line:274] - INFO: epoch 001:   4594 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0309, wps=102.4, ups=0.61, wpb=111.1, bsz=40, num_updates=4590, lr=4.96927e-05, gnorm=0.922, clip=40, loss_scale=1024, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=23512
2023-01-09 20:25:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:42 - progress_bar.py[line:274] - INFO: epoch 001:   4604 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0303, wps=99.5, ups=0.6, wpb=109.9, bsz=40, num_updates=4600, lr=4.96875e-05, gnorm=0.945, clip=40, loss_scale=1024, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=23529
2023-01-09 20:25:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:25:59 - progress_bar.py[line:274] - INFO: epoch 001:   4614 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0571, wps=100.2, ups=0.61, wpb=109.4, bsz=40, num_updates=4610, lr=4.96823e-05, gnorm=1.106, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23545
2023-01-09 20:26:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:16 - progress_bar.py[line:274] - INFO: epoch 001:   4624 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0957, wps=100.7, ups=0.62, wpb=108.7, bsz=40, num_updates=4620, lr=4.96771e-05, gnorm=1.254, clip=60, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23562
2023-01-09 20:26:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:32 - progress_bar.py[line:274] - INFO: epoch 001:   4634 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0918, wps=102.4, ups=0.62, wpb=109.5, bsz=40, num_updates=4630, lr=4.96719e-05, gnorm=1.053, clip=30, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23578
2023-01-09 20:26:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:35 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-09 20:26:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:50 - progress_bar.py[line:274] - INFO: epoch 001:   4645 / 100000 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1296, wps=94.1, ups=0.57, wpb=109.9, bsz=40, num_updates=4640, lr=4.96667e-05, gnorm=1.802, clip=60, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=23596
2023-01-09 20:26:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:26:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:06 - progress_bar.py[line:274] - INFO: epoch 001:   4655 / 100000 loss=0.436, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.24, vqa_score=0.0455, wps=102.7, ups=0.63, wpb=109.5, bsz=40, num_updates=4650, lr=4.96615e-05, gnorm=1.325, clip=60, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=23612
2023-01-09 20:27:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:22 - progress_bar.py[line:274] - INFO: epoch 001:   4665 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.028, wps=100.4, ups=0.62, wpb=108.3, bsz=40, num_updates=4660, lr=4.96563e-05, gnorm=1.245, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23628
2023-01-09 20:27:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:39 - progress_bar.py[line:274] - INFO: epoch 001:   4675 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0714, wps=101.8, ups=0.62, wpb=109.7, bsz=40, num_updates=4670, lr=4.9651e-05, gnorm=1.259, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23645
2023-01-09 20:27:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:55 - progress_bar.py[line:274] - INFO: epoch 001:   4685 / 100000 loss=0.46, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=106.867, nsentences=40, sample_size=106.867, sample_size_v1=0, sample_size_v2=0, ppl=1.25, vqa_score=0.0565, wps=100, ups=0.62, wpb=106.9, bsz=40, num_updates=4680, lr=4.96458e-05, gnorm=1.398, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=23661
2023-01-09 20:27:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:27:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:12 - progress_bar.py[line:274] - INFO: epoch 001:   4695 / 100000 loss=0.402, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0204, wps=99.5, ups=0.61, wpb=109.2, bsz=40, num_updates=4690, lr=4.96406e-05, gnorm=1.13, clip=70, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=23678
2023-01-09 20:28:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:28 - progress_bar.py[line:274] - INFO: epoch 001:   4705 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0556, wps=101, ups=0.61, wpb=111.2, bsz=40, num_updates=4700, lr=4.96354e-05, gnorm=1.35, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=23695
2023-01-09 20:28:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:45 - progress_bar.py[line:274] - INFO: epoch 001:   4715 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0326, wps=100.1, ups=0.61, wpb=108.8, bsz=40, num_updates=4710, lr=4.96302e-05, gnorm=1.141, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23711
2023-01-09 20:28:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:28:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:01 - progress_bar.py[line:274] - INFO: epoch 001:   4725 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0323, wps=102.7, ups=0.62, wpb=110.6, bsz=40, num_updates=4720, lr=4.9625e-05, gnorm=1.232, clip=60, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=23727
2023-01-09 20:29:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:18 - progress_bar.py[line:274] - INFO: epoch 001:   4735 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0755, wps=98.1, ups=0.61, wpb=107.9, bsz=40, num_updates=4730, lr=4.96198e-05, gnorm=1.07, clip=40, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=23744
2023-01-09 20:29:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:34 - progress_bar.py[line:274] - INFO: epoch 001:   4745 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0602, wps=105.8, ups=0.64, wpb=110.5, bsz=40, num_updates=4740, lr=4.96146e-05, gnorm=1.124, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23760
2023-01-09 20:29:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:51 - progress_bar.py[line:274] - INFO: epoch 001:   4755 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0583, wps=99, ups=0.6, wpb=109.8, bsz=40, num_updates=4750, lr=4.96094e-05, gnorm=1.124, clip=60, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=23777
2023-01-09 20:29:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:29:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:07 - progress_bar.py[line:274] - INFO: epoch 001:   4765 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0732, wps=103.9, ups=0.63, wpb=109.8, bsz=40, num_updates=4760, lr=4.96042e-05, gnorm=0.785, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=23793
2023-01-09 20:30:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:24 - progress_bar.py[line:274] - INFO: epoch 001:   4775 / 100000 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0294, wps=100.1, ups=0.61, wpb=109.4, bsz=40, num_updates=4770, lr=4.9599e-05, gnorm=1.101, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=23810
2023-01-09 20:30:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:40 - progress_bar.py[line:274] - INFO: epoch 001:   4785 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.05, wps=100.8, ups=0.61, wpb=110.4, bsz=40, num_updates=4780, lr=4.95938e-05, gnorm=0.9, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=23827
2023-01-09 20:30:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:57 - progress_bar.py[line:274] - INFO: epoch 001:   4795 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0568, wps=101.3, ups=0.61, wpb=111, bsz=40, num_updates=4790, lr=4.95885e-05, gnorm=0.98, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=23843
2023-01-09 20:30:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:30:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:14 - progress_bar.py[line:274] - INFO: epoch 001:   4805 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0648, wps=99.4, ups=0.61, wpb=109.3, bsz=40, num_updates=4800, lr=4.95833e-05, gnorm=1.149, clip=70, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=23860
2023-01-09 20:31:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:30 - progress_bar.py[line:274] - INFO: epoch 001:   4815 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0909, wps=104.2, ups=0.63, wpb=110, bsz=40, num_updates=4810, lr=4.95781e-05, gnorm=0.894, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23876
2023-01-09 20:31:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:47 - progress_bar.py[line:274] - INFO: epoch 001:   4825 / 100000 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0562, wps=100.8, ups=0.61, wpb=110, bsz=40, num_updates=4820, lr=4.95729e-05, gnorm=0.832, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=23893
2023-01-09 20:31:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:31:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:03 - progress_bar.py[line:274] - INFO: epoch 001:   4835 / 100000 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0532, wps=101.4, ups=0.61, wpb=111.5, bsz=40, num_updates=4830, lr=4.95677e-05, gnorm=0.999, clip=40, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=23909
2023-01-09 20:32:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:20 - progress_bar.py[line:274] - INFO: epoch 001:   4845 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0625, wps=100.6, ups=0.6, wpb=111.2, bsz=40, num_updates=4840, lr=4.95625e-05, gnorm=1.342, clip=50, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=23926
2023-01-09 20:32:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:36 - progress_bar.py[line:274] - INFO: epoch 001:   4855 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0571, wps=102.6, ups=0.63, wpb=108.4, bsz=40, num_updates=4850, lr=4.95573e-05, gnorm=1.033, clip=50, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=23942
2023-01-09 20:32:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:53 - progress_bar.py[line:274] - INFO: epoch 001:   4865 / 100000 loss=0.416, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.055, wps=99.2, ups=0.61, wpb=108.7, bsz=40, num_updates=4860, lr=4.95521e-05, gnorm=1.277, clip=60, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=23959
2023-01-09 20:32:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:32:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:09 - progress_bar.py[line:274] - INFO: epoch 001:   4875 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0549, wps=104.2, ups=0.63, wpb=111.1, bsz=40, num_updates=4870, lr=4.95469e-05, gnorm=0.925, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=23975
2023-01-09 20:33:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:25 - progress_bar.py[line:274] - INFO: epoch 001:   4885 / 100000 loss=0.42, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0283, wps=102.8, ups=0.62, wpb=109.7, bsz=40, num_updates=4880, lr=4.95417e-05, gnorm=1.396, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=23992
2023-01-09 20:33:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:42 - progress_bar.py[line:274] - INFO: epoch 001:   4895 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0515, wps=103.9, ups=0.63, wpb=109.9, bsz=40, num_updates=4890, lr=4.95365e-05, gnorm=1.049, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=24008
2023-01-09 20:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:33:58 - progress_bar.py[line:274] - INFO: epoch 001:   4905 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0957, wps=100.2, ups=0.6, wpb=110.9, bsz=40, num_updates=4900, lr=4.95313e-05, gnorm=0.942, clip=30, loss_scale=512, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=24025
2023-01-09 20:33:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:15 - progress_bar.py[line:274] - INFO: epoch 001:   4915 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0808, wps=98.4, ups=0.6, wpb=109.8, bsz=40, num_updates=4910, lr=4.9526e-05, gnorm=0.898, clip=30, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=24042
2023-01-09 20:34:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:32 - progress_bar.py[line:274] - INFO: epoch 001:   4925 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0532, wps=101.8, ups=0.62, wpb=110, bsz=40, num_updates=4920, lr=4.95208e-05, gnorm=1.674, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=24058
2023-01-09 20:34:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:48 - progress_bar.py[line:274] - INFO: epoch 001:   4935 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1068, wps=101.1, ups=0.62, wpb=109.3, bsz=40, num_updates=4930, lr=4.95156e-05, gnorm=1.061, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=24074
2023-01-09 20:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:34:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:05 - progress_bar.py[line:274] - INFO: epoch 001:   4945 / 100000 loss=0.418, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0571, wps=100.1, ups=0.61, wpb=109.7, bsz=40, num_updates=4940, lr=4.95104e-05, gnorm=1.071, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=24091
2023-01-09 20:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:21 - progress_bar.py[line:274] - INFO: epoch 001:   4955 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0303, wps=105.3, ups=0.63, wpb=111, bsz=40, num_updates=4950, lr=4.95052e-05, gnorm=1.044, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=24107
2023-01-09 20:35:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:38 - progress_bar.py[line:274] - INFO: epoch 001:   4965 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1183, wps=101.4, ups=0.61, wpb=110.7, bsz=40, num_updates=4960, lr=4.95e-05, gnorm=1.199, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=24124
2023-01-09 20:35:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:54 - progress_bar.py[line:274] - INFO: epoch 001:   4975 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0556, wps=100.2, ups=0.61, wpb=108.9, bsz=40, num_updates=4970, lr=4.94948e-05, gnorm=1.414, clip=70, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=24140
2023-01-09 20:35:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:35:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:11 - progress_bar.py[line:274] - INFO: epoch 001:   4985 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0588, wps=97, ups=0.6, wpb=108.3, bsz=40, num_updates=4980, lr=4.94896e-05, gnorm=1.144, clip=30, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=24157
2023-01-09 20:36:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:28 - progress_bar.py[line:274] - INFO: epoch 001:   4995 / 100000 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0686, wps=99, ups=0.6, wpb=110, bsz=40, num_updates=4990, lr=4.94844e-05, gnorm=1.258, clip=60, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=24174
2023-01-09 20:36:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 20:36:44 - progress_bar.py[line:274] - INFO: epoch 001:   5005 / 100000 loss=0.413, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0577, wps=102.3, ups=0.62, wpb=110.5, bsz=40, num_updates=5000, lr=4.94792e-05, gnorm=1.312, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=24191
2023-01-09 20:36:44 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-09 20:36:46 - train.py[line:549] - INFO: 0 / 4988
2023-01-09 20:36:46 - train.py[line:551] - INFO: load:1.21 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-09 20:37:02 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.21 GiB (GPU 0; 39.59 GiB total capacity; 9.30 GiB already allocated; 1.39 GiB free; 35.71 GiB reserved in total by PyTorch)
2023-01-09 20:37:02 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9525 MB |   14756 MB |    3383 TB |    3382 TB |
|       from large pool |    9351 MB |   14582 MB |    3381 TB |    3381 TB |
|       from small pool |     174 MB |     175 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9525 MB |   14756 MB |    3383 TB |    3382 TB |
|       from large pool |    9351 MB |   14582 MB |    3381 TB |    3381 TB |
|       from small pool |     174 MB |     175 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   36572 MB |   37968 MB |  189362 MB |  152790 MB |
|       from large pool |   36396 MB |   37786 MB |  189054 MB |  152658 MB |
|       from small pool |     176 MB |     182 MB |     308 MB |     132 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   27046 MB |   31618 MB |    3250 TB |    3250 TB |
|       from large pool |   27044 MB |   31615 MB |    3248 TB |    3248 TB |
|       from small pool |       1 MB |       2 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4634    |    4648    |  160133 K  |  160128 K  |
|       from large pool |     698    |     710    |   49910 K  |   49909 K  |
|       from small pool |    3936    |    3946    |  110222 K  |  110218 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4634    |    4648    |  160133 K  |  160128 K  |
|       from large pool |     698    |     710    |   49910 K  |   49909 K  |
|       from small pool |    3936    |    3946    |  110222 K  |  110218 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     190    |     196    |     668    |     478    |
|       from large pool |     102    |     105    |     514    |     412    |
|       from small pool |      88    |      91    |     154    |      66    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     120    |     132    |  116448 K  |  116448 K  |
|       from large pool |      67    |      72    |   20370 K  |   20370 K  |
|       from small pool |      53    |      64    |   96078 K  |   96078 K  |
|===========================================================================|

2023-01-09 20:37:02 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-09 20:37:02 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-01-09 20:39:19 - train.py[line:549] - INFO: 200 / 4988
2023-01-09 20:39:19 - train.py[line:551] - INFO: load:1.24 valid_run:153.03 task_valid:148.40 collect_output:2.32
2023-01-09 20:41:47 - train.py[line:549] - INFO: 400 / 4988
2023-01-09 20:41:47 - train.py[line:551] - INFO: load:1.27 valid_run:301.24 task_valid:291.83 collect_output:6.04
2023-01-09 20:44:20 - train.py[line:549] - INFO: 600 / 4988
2023-01-09 20:44:20 - train.py[line:551] - INFO: load:1.29 valid_run:453.57 task_valid:435.17 collect_output:13.97
2023-01-09 20:46:49 - train.py[line:549] - INFO: 800 / 4988
2023-01-09 20:46:49 - train.py[line:551] - INFO: load:1.32 valid_run:602.19 task_valid:580.10 collect_output:16.57
2023-01-09 20:49:20 - train.py[line:549] - INFO: 1000 / 4988
2023-01-09 20:49:20 - train.py[line:551] - INFO: load:1.35 valid_run:753.90 task_valid:727.54 collect_output:19.76
2023-01-09 20:51:51 - train.py[line:549] - INFO: 1200 / 4988
2023-01-09 20:51:51 - train.py[line:551] - INFO: load:1.38 valid_run:904.76 task_valid:872.74 collect_output:24.37
2023-01-09 20:54:24 - train.py[line:549] - INFO: 1400 / 4988
2023-01-09 20:54:24 - train.py[line:551] - INFO: load:1.41 valid_run:1056.94 task_valid:1018.76 collect_output:29.50
2023-01-09 20:56:54 - train.py[line:549] - INFO: 1600 / 4988
2023-01-09 20:56:54 - train.py[line:551] - INFO: load:1.44 valid_run:1207.06 task_valid:1159.56 collect_output:37.78
2023-01-09 20:59:23 - train.py[line:549] - INFO: 1800 / 4988
2023-01-09 20:59:23 - train.py[line:551] - INFO: load:1.47 valid_run:1356.00 task_valid:1304.18 collect_output:41.04
2023-01-09 21:01:51 - train.py[line:549] - INFO: 2000 / 4988
2023-01-09 21:01:51 - train.py[line:551] - INFO: load:1.50 valid_run:1503.75 task_valid:1447.11 collect_output:44.84
2023-01-09 21:04:20 - train.py[line:549] - INFO: 2200 / 4988
2023-01-09 21:04:20 - train.py[line:551] - INFO: load:1.52 valid_run:1653.01 task_valid:1591.97 collect_output:48.22
2023-01-09 21:06:49 - train.py[line:549] - INFO: 2400 / 4988
2023-01-09 21:06:49 - train.py[line:551] - INFO: load:1.55 valid_run:1802.12 task_valid:1736.77 collect_output:51.49
2023-01-09 21:09:18 - train.py[line:549] - INFO: 2600 / 4988
2023-01-09 21:09:18 - train.py[line:551] - INFO: load:1.58 valid_run:1950.95 task_valid:1878.39 collect_output:57.66
2023-01-09 21:11:48 - train.py[line:549] - INFO: 2800 / 4988
2023-01-09 21:11:48 - train.py[line:551] - INFO: load:1.61 valid_run:2100.76 task_valid:2023.78 collect_output:61.04
2023-01-09 21:14:18 - train.py[line:549] - INFO: 3000 / 4988
2023-01-09 21:14:18 - train.py[line:551] - INFO: load:1.64 valid_run:2250.68 task_valid:2170.46 collect_output:63.25
2023-01-09 21:16:47 - train.py[line:549] - INFO: 3200 / 4988
2023-01-09 21:16:47 - train.py[line:551] - INFO: load:1.67 valid_run:2400.06 task_valid:2314.75 collect_output:67.29
2023-01-09 21:19:18 - train.py[line:549] - INFO: 3400 / 4988
2023-01-09 21:19:18 - train.py[line:551] - INFO: load:1.70 valid_run:2550.55 task_valid:2460.11 collect_output:71.39
2023-01-09 21:21:48 - train.py[line:549] - INFO: 3600 / 4988
2023-01-09 21:21:48 - train.py[line:551] - INFO: load:1.73 valid_run:2700.39 task_valid:2606.79 collect_output:73.52
2023-01-09 21:24:16 - train.py[line:549] - INFO: 3800 / 4988
2023-01-09 21:24:16 - train.py[line:551] - INFO: load:1.76 valid_run:2848.01 task_valid:2748.20 collect_output:78.71
2023-01-09 21:26:45 - train.py[line:549] - INFO: 4000 / 4988
2023-01-09 21:26:45 - train.py[line:551] - INFO: load:1.79 valid_run:2997.36 task_valid:2893.12 collect_output:82.11
2023-01-09 21:29:16 - train.py[line:549] - INFO: 4200 / 4988
2023-01-09 21:29:16 - train.py[line:551] - INFO: load:1.82 valid_run:3148.02 task_valid:3037.49 collect_output:87.38
2023-01-09 21:31:44 - train.py[line:549] - INFO: 4400 / 4988
2023-01-09 21:31:44 - train.py[line:551] - INFO: load:1.84 valid_run:3296.55 task_valid:3181.82 collect_output:90.54
2023-01-09 21:34:15 - train.py[line:549] - INFO: 4600 / 4988
2023-01-09 21:34:15 - train.py[line:551] - INFO: load:1.87 valid_run:3447.01 task_valid:3327.98 collect_output:93.79
2023-01-09 21:36:46 - train.py[line:549] - INFO: 4800 / 4988
2023-01-09 21:36:46 - train.py[line:551] - INFO: load:1.90 valid_run:3597.58 task_valid:3474.31 collect_output:96.95

====================================================================================================
SGG eval:     R @ 50: 0.6453;     R @ 100: 0.6820;     R @ 500: 0.7156;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4105;    mR @ 100: 0.4860;    mR @ 500: 0.5279;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6951) (covered in:0.6250) (covering:0.7143) (eating:0.7647) (flying in:0.8636) (growing on:0.5000) (hanging from:0.4194) (lying on:0.2000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9428) (says:0.0000) (sitting on:0.7778) (standing on:0.4093) (using:0.4250) (walking in:0.0000) (walking on:0.7162) (watching:0.4583) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6453;     R @ 100: 0.6820;     R @ 500: 0.7156;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4105;    mR @ 100: 0.4860;    mR @ 500: 0.5279;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6951) (covered in:0.6250) (covering:0.7143) (eating:0.7647) (flying in:0.8636) (growing on:0.5000) (hanging from:0.4194) (lying on:0.2000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9428) (says:0.0000) (sitting on:0.7778) (standing on:0.4093) (using:0.4250) (walking in:0.0000) (walking on:0.7162) (watching:0.4583) 
--------------------------------------------------------
====================================================================================================

2023-01-09 21:39:17 - train.py[line:487] - INFO: 0.6820027247262542
2023-01-09 21:39:17 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-09 21:39:17 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.276 | loss_v1 0 | loss_v2 0 | nll_loss 0.114 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.682003 | ppl 1.08 | vqa_score 0.4786 | wps 119.6 | wpb 89.9 | bsz 30 | num_updates 5000 | best_R@100 0.682003
2023-01-09 21:39:17 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 5000 updates
2023-01-09 21:39:17 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_5000.pt
2023-01-09 21:40:02 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_5000.pt
2023-01-09 21:43:02 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_5000.pt (epoch 1 @ 5000 updates, score 0.6820027247262542) (writing took 225.1316861845553 seconds)
2023-01-09 21:43:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:19 - progress_bar.py[line:274] - INFO: epoch 001:   5015 / 100000 loss=0.43, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0667, wps=0.4, ups=0, wpb=109.9, bsz=40, num_updates=5010, lr=4.9474e-05, gnorm=1.258, clip=60, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=28185
2023-01-09 21:43:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:35 - progress_bar.py[line:274] - INFO: epoch 001:   5025 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0367, wps=103.9, ups=0.62, wpb=111.5, bsz=40, num_updates=5020, lr=4.94687e-05, gnorm=1.784, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28201
2023-01-09 21:43:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:52 - progress_bar.py[line:274] - INFO: epoch 001:   5035 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0323, wps=102.7, ups=0.62, wpb=111.1, bsz=40, num_updates=5030, lr=4.94635e-05, gnorm=1.588, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=28218
2023-01-09 21:43:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:43:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:08 - progress_bar.py[line:274] - INFO: epoch 001:   5045 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0381, wps=102.4, ups=0.62, wpb=110.7, bsz=40, num_updates=5040, lr=4.94583e-05, gnorm=1.34, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28234
2023-01-09 21:44:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:25 - progress_bar.py[line:274] - INFO: epoch 001:   5055 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.05, wps=101.2, ups=0.61, wpb=110.5, bsz=40, num_updates=5050, lr=4.94531e-05, gnorm=1.225, clip=50, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=28251
2023-01-09 21:44:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:42 - progress_bar.py[line:274] - INFO: epoch 001:   5065 / 100000 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0485, wps=99.7, ups=0.6, wpb=110.3, bsz=40, num_updates=5060, lr=4.94479e-05, gnorm=1.089, clip=50, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=28268
2023-01-09 21:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:44:58 - progress_bar.py[line:274] - INFO: epoch 001:   5075 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.06, wps=99.9, ups=0.61, wpb=110, bsz=40, num_updates=5070, lr=4.94427e-05, gnorm=1.401, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28285
2023-01-09 21:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:15 - progress_bar.py[line:274] - INFO: epoch 001:   5085 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0901, wps=101.4, ups=0.61, wpb=110.5, bsz=40, num_updates=5080, lr=4.94375e-05, gnorm=1.014, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=28301
2023-01-09 21:45:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:31 - progress_bar.py[line:274] - INFO: epoch 001:   5095 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.087, wps=103.2, ups=0.63, wpb=109.9, bsz=40, num_updates=5090, lr=4.94323e-05, gnorm=1.769, clip=70, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=28317
2023-01-09 21:45:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:47 - progress_bar.py[line:274] - INFO: epoch 001:   5105 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0682, wps=105.9, ups=0.63, wpb=111.2, bsz=40, num_updates=5100, lr=4.94271e-05, gnorm=1.129, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=28333
2023-01-09 21:45:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:04 - progress_bar.py[line:274] - INFO: epoch 001:   5115 / 100000 loss=0.424, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0659, wps=101.8, ups=0.62, wpb=110.1, bsz=40, num_updates=5110, lr=4.94219e-05, gnorm=1.402, clip=80, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=28350
2023-01-09 21:46:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:20 - progress_bar.py[line:274] - INFO: epoch 001:   5125 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0816, wps=100.2, ups=0.6, wpb=110.7, bsz=40, num_updates=5120, lr=4.94167e-05, gnorm=0.816, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=28367
2023-01-09 21:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:37 - progress_bar.py[line:274] - INFO: epoch 001:   5135 / 100000 loss=0.422, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0388, wps=102.5, ups=0.63, wpb=109.1, bsz=40, num_updates=5130, lr=4.94115e-05, gnorm=1.236, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28383
2023-01-09 21:46:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:53 - progress_bar.py[line:274] - INFO: epoch 001:   5145 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0734, wps=100, ups=0.61, wpb=109.2, bsz=40, num_updates=5140, lr=4.94063e-05, gnorm=1.054, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=28400
2023-01-09 21:46:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:10 - progress_bar.py[line:274] - INFO: epoch 001:   5155 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0495, wps=100.3, ups=0.6, wpb=110.7, bsz=40, num_updates=5150, lr=4.9401e-05, gnorm=1.163, clip=40, loss_scale=1024, train_wall=17, gb_free=10, ema_decay=0.9999, wall=28416
2023-01-09 21:47:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:27 - progress_bar.py[line:274] - INFO: epoch 001:   5165 / 100000 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0729, wps=101.5, ups=0.62, wpb=109.8, bsz=40, num_updates=5160, lr=4.93958e-05, gnorm=1.584, clip=90, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28433
2023-01-09 21:47:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:43 - progress_bar.py[line:274] - INFO: epoch 001:   5175 / 100000 loss=0.431, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=107.733, nsentences=40, sample_size=107.733, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.03, wps=99.3, ups=0.61, wpb=107.7, bsz=40, num_updates=5170, lr=4.93906e-05, gnorm=1.395, clip=50, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=28449
2023-01-09 21:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:47:57 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-09 21:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:02 - progress_bar.py[line:274] - INFO: epoch 001:   5186 / 100000 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=108.875, nsentences=40, sample_size=108.875, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.1, wps=92.5, ups=0.53, wpb=108.9, bsz=40, num_updates=5180, lr=4.93854e-05, gnorm=1.245, clip=70, loss_scale=512, train_wall=19, gb_free=10.4, ema_decay=0.9999, wall=28468
2023-01-09 21:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:18 - progress_bar.py[line:274] - INFO: epoch 001:   5196 / 100000 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0303, wps=104.4, ups=0.64, wpb=109.5, bsz=40, num_updates=5190, lr=4.93802e-05, gnorm=1.713, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=28484
2023-01-09 21:48:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:35 - progress_bar.py[line:274] - INFO: epoch 001:   5206 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0825, wps=99.2, ups=0.61, wpb=109.1, bsz=40, num_updates=5200, lr=4.9375e-05, gnorm=1.156, clip=40, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=28501
2023-01-09 21:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:51 - progress_bar.py[line:274] - INFO: epoch 001:   5216 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=112.267, nsentences=40, sample_size=112.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0612, wps=102.9, ups=0.61, wpb=112.3, bsz=40, num_updates=5210, lr=4.93698e-05, gnorm=2.042, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28518
2023-01-09 21:48:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:48:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:08 - progress_bar.py[line:274] - INFO: epoch 001:   5226 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.066, wps=101.5, ups=0.61, wpb=110.6, bsz=40, num_updates=5220, lr=4.93646e-05, gnorm=1.126, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=28534
2023-01-09 21:49:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:24 - progress_bar.py[line:274] - INFO: epoch 001:   5236 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.033, wps=101.4, ups=0.61, wpb=110.1, bsz=40, num_updates=5230, lr=4.93594e-05, gnorm=1.159, clip=50, loss_scale=512, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=28551
2023-01-09 21:49:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:41 - progress_bar.py[line:274] - INFO: epoch 001:   5246 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0467, wps=99, ups=0.6, wpb=109.5, bsz=40, num_updates=5240, lr=4.93542e-05, gnorm=1.488, clip=70, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=28567
2023-01-09 21:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:49:58 - progress_bar.py[line:274] - INFO: epoch 001:   5256 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.049, wps=99.9, ups=0.61, wpb=108.9, bsz=40, num_updates=5250, lr=4.9349e-05, gnorm=1.835, clip=70, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=28584
2023-01-09 21:49:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:15 - progress_bar.py[line:274] - INFO: epoch 001:   5266 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0345, wps=100.2, ups=0.61, wpb=109.9, bsz=40, num_updates=5260, lr=4.93437e-05, gnorm=1.014, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=28601
2023-01-09 21:50:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:31 - progress_bar.py[line:274] - INFO: epoch 001:   5276 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.051, wps=100.9, ups=0.61, wpb=109.7, bsz=40, num_updates=5270, lr=4.93385e-05, gnorm=1.089, clip=50, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=28617
2023-01-09 21:50:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:47 - progress_bar.py[line:274] - INFO: epoch 001:   5286 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0286, wps=100.9, ups=0.62, wpb=108.5, bsz=40, num_updates=5280, lr=4.93333e-05, gnorm=1.102, clip=50, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=28634
2023-01-09 21:50:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:50:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:04 - progress_bar.py[line:274] - INFO: epoch 001:   5296 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0632, wps=99.9, ups=0.6, wpb=110.3, bsz=40, num_updates=5290, lr=4.93281e-05, gnorm=1.331, clip=70, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=28650
2023-01-09 21:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:21 - progress_bar.py[line:274] - INFO: epoch 001:   5306 / 100000 loss=0.411, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0789, wps=100.1, ups=0.62, wpb=107.6, bsz=40, num_updates=5300, lr=4.93229e-05, gnorm=1.153, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28667
2023-01-09 21:51:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:38 - progress_bar.py[line:274] - INFO: epoch 001:   5316 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.033, wps=98.3, ups=0.6, wpb=109.7, bsz=40, num_updates=5310, lr=4.93177e-05, gnorm=1.011, clip=40, loss_scale=512, train_wall=17, gb_free=9.6, ema_decay=0.9999, wall=28684
2023-01-09 21:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:54 - progress_bar.py[line:274] - INFO: epoch 001:   5326 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.11, wps=102.6, ups=0.62, wpb=110.4, bsz=40, num_updates=5320, lr=4.93125e-05, gnorm=1.263, clip=50, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=28700
2023-01-09 21:51:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:51:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:10 - progress_bar.py[line:274] - INFO: epoch 001:   5336 / 100000 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0727, wps=101.8, ups=0.62, wpb=109.8, bsz=40, num_updates=5330, lr=4.93073e-05, gnorm=0.95, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28716
2023-01-09 21:52:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:27 - progress_bar.py[line:274] - INFO: epoch 001:   5346 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.087, wps=102.2, ups=0.61, wpb=111.7, bsz=40, num_updates=5340, lr=4.93021e-05, gnorm=1.523, clip=70, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28733
2023-01-09 21:52:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:44 - progress_bar.py[line:274] - INFO: epoch 001:   5356 / 100000 loss=0.421, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0991, wps=99.1, ups=0.61, wpb=108.4, bsz=40, num_updates=5350, lr=4.92969e-05, gnorm=1.532, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=28750
2023-01-09 21:52:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:52:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:00 - progress_bar.py[line:274] - INFO: epoch 001:   5366 / 100000 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0777, wps=102, ups=0.61, wpb=110.9, bsz=40, num_updates=5360, lr=4.92917e-05, gnorm=1.189, clip=70, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=28766
2023-01-09 21:53:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:17 - progress_bar.py[line:274] - INFO: epoch 001:   5376 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.044, wps=99.6, ups=0.6, wpb=110.3, bsz=40, num_updates=5370, lr=4.92865e-05, gnorm=0.737, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=28783
2023-01-09 21:53:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:34 - progress_bar.py[line:274] - INFO: epoch 001:   5386 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.087, wps=100.3, ups=0.61, wpb=110.5, bsz=40, num_updates=5380, lr=4.92813e-05, gnorm=1.284, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28800
2023-01-09 21:53:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:50 - progress_bar.py[line:274] - INFO: epoch 001:   5396 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0753, wps=100.9, ups=0.61, wpb=110.2, bsz=40, num_updates=5390, lr=4.9276e-05, gnorm=1.954, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=28817
2023-01-09 21:53:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:53:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:07 - progress_bar.py[line:274] - INFO: epoch 001:   5406 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0673, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=5400, lr=4.92708e-05, gnorm=1.64, clip=50, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=28833
2023-01-09 21:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:24 - progress_bar.py[line:274] - INFO: epoch 001:   5416 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0693, wps=98.9, ups=0.6, wpb=110.1, bsz=40, num_updates=5410, lr=4.92656e-05, gnorm=1.03, clip=50, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=28850
2023-01-09 21:54:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:40 - progress_bar.py[line:274] - INFO: epoch 001:   5426 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0891, wps=100.6, ups=0.62, wpb=108.6, bsz=40, num_updates=5420, lr=4.92604e-05, gnorm=1.189, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28867
2023-01-09 21:54:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:57 - progress_bar.py[line:274] - INFO: epoch 001:   5436 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0778, wps=101.3, ups=0.61, wpb=110.5, bsz=40, num_updates=5430, lr=4.92552e-05, gnorm=0.745, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28883
2023-01-09 21:54:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:54:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:13 - progress_bar.py[line:274] - INFO: epoch 001:   5446 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0777, wps=100.5, ups=0.62, wpb=108.5, bsz=40, num_updates=5440, lr=4.925e-05, gnorm=0.964, clip=40, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=28900
2023-01-09 21:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:30 - progress_bar.py[line:274] - INFO: epoch 001:   5456 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.086, wps=102.9, ups=0.62, wpb=110.1, bsz=40, num_updates=5450, lr=4.92448e-05, gnorm=1.168, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28916
2023-01-09 21:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:46 - progress_bar.py[line:274] - INFO: epoch 001:   5466 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0288, wps=101.1, ups=0.61, wpb=110.1, bsz=40, num_updates=5460, lr=4.92396e-05, gnorm=1.681, clip=50, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=28932
2023-01-09 21:55:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:55:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:03 - progress_bar.py[line:274] - INFO: epoch 001:   5476 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0217, wps=100.9, ups=0.6, wpb=111.4, bsz=40, num_updates=5470, lr=4.92344e-05, gnorm=1.782, clip=50, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=28949
2023-01-09 21:56:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:19 - progress_bar.py[line:274] - INFO: epoch 001:   5486 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0521, wps=102.1, ups=0.62, wpb=109.1, bsz=40, num_updates=5480, lr=4.92292e-05, gnorm=1.151, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28965
2023-01-09 21:56:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:36 - progress_bar.py[line:274] - INFO: epoch 001:   5496 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0889, wps=101.3, ups=0.61, wpb=110.5, bsz=40, num_updates=5490, lr=4.9224e-05, gnorm=1.136, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28982
2023-01-09 21:56:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:53 - progress_bar.py[line:274] - INFO: epoch 001:   5506 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0381, wps=99, ups=0.61, wpb=108.3, bsz=40, num_updates=5500, lr=4.92188e-05, gnorm=1.446, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=28999
2023-01-09 21:56:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:56:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:09 - progress_bar.py[line:274] - INFO: epoch 001:   5516 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0353, wps=102.3, ups=0.61, wpb=111.3, bsz=40, num_updates=5510, lr=4.92135e-05, gnorm=1.851, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29015
2023-01-09 21:57:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:25 - progress_bar.py[line:274] - INFO: epoch 001:   5526 / 100000 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1215, wps=103.9, ups=0.63, wpb=109.5, bsz=40, num_updates=5520, lr=4.92083e-05, gnorm=1.343, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29031
2023-01-09 21:57:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:42 - progress_bar.py[line:274] - INFO: epoch 001:   5536 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0632, wps=100.3, ups=0.6, wpb=110.7, bsz=40, num_updates=5530, lr=4.92031e-05, gnorm=0.877, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29048
2023-01-09 21:57:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:57:59 - progress_bar.py[line:274] - INFO: epoch 001:   5546 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0396, wps=98.7, ups=0.6, wpb=109, bsz=40, num_updates=5540, lr=4.91979e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=17, gb_free=9.6, ema_decay=0.9999, wall=29065
2023-01-09 21:57:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:15 - progress_bar.py[line:274] - INFO: epoch 001:   5556 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1443, wps=102.5, ups=0.62, wpb=109.9, bsz=40, num_updates=5550, lr=4.91927e-05, gnorm=1.094, clip=30, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=29081
2023-01-09 21:58:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:32 - progress_bar.py[line:274] - INFO: epoch 001:   5566 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0714, wps=100.3, ups=0.61, wpb=109.1, bsz=40, num_updates=5560, lr=4.91875e-05, gnorm=1.26, clip=70, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=29098
2023-01-09 21:58:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:48 - progress_bar.py[line:274] - INFO: epoch 001:   5576 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1237, wps=101.3, ups=0.61, wpb=109.9, bsz=40, num_updates=5570, lr=4.91823e-05, gnorm=1.114, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29114
2023-01-09 21:58:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:58:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:04 - progress_bar.py[line:274] - INFO: epoch 001:   5586 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0467, wps=102.8, ups=0.62, wpb=110.4, bsz=40, num_updates=5580, lr=4.91771e-05, gnorm=0.85, clip=10, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=29131
2023-01-09 21:59:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:22 - progress_bar.py[line:274] - INFO: epoch 001:   5596 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.11, wps=94.1, ups=0.57, wpb=109.6, bsz=40, num_updates=5590, lr=4.91719e-05, gnorm=0.72, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=29148
2023-01-09 21:59:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:39 - progress_bar.py[line:274] - INFO: epoch 001:   5606 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0488, wps=101.3, ups=0.61, wpb=111, bsz=40, num_updates=5600, lr=4.91667e-05, gnorm=0.935, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29165
2023-01-09 21:59:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:55 - progress_bar.py[line:274] - INFO: epoch 001:   5616 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0762, wps=102.9, ups=0.63, wpb=109.7, bsz=40, num_updates=5610, lr=4.91615e-05, gnorm=0.837, clip=20, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=29181
2023-01-09 21:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 21:59:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:12 - progress_bar.py[line:274] - INFO: epoch 001:   5626 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1383, wps=100.1, ups=0.6, wpb=110.7, bsz=40, num_updates=5620, lr=4.91563e-05, gnorm=1.381, clip=40, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=29198
2023-01-09 22:00:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:29 - progress_bar.py[line:274] - INFO: epoch 001:   5636 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0753, wps=100.7, ups=0.6, wpb=111.8, bsz=40, num_updates=5630, lr=4.9151e-05, gnorm=0.773, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=29215
2023-01-09 22:00:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:46 - progress_bar.py[line:274] - INFO: epoch 001:   5646 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0412, wps=98.5, ups=0.6, wpb=109.6, bsz=40, num_updates=5640, lr=4.91458e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=29232
2023-01-09 22:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:00:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:02 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-09 22:01:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:04 - progress_bar.py[line:274] - INFO: epoch 001:   5657 / 100000 loss=0.41, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=109.688, nsentences=40, sample_size=109.688, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0579, wps=95.7, ups=0.55, wpb=109.7, bsz=40, num_updates=5650, lr=4.91406e-05, gnorm=1.189, clip=50, loss_scale=256, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=29250
2023-01-09 22:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:21 - progress_bar.py[line:274] - INFO: epoch 001:   5667 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1, wps=99.3, ups=0.61, wpb=109.3, bsz=40, num_updates=5660, lr=4.91354e-05, gnorm=1.027, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29267
2023-01-09 22:01:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:38 - progress_bar.py[line:274] - INFO: epoch 001:   5677 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0833, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=5670, lr=4.91302e-05, gnorm=0.902, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29284
2023-01-09 22:01:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:54 - progress_bar.py[line:274] - INFO: epoch 001:   5687 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0808, wps=100.5, ups=0.61, wpb=109.2, bsz=40, num_updates=5680, lr=4.9125e-05, gnorm=0.927, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29300
2023-01-09 22:01:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:01:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:11 - progress_bar.py[line:274] - INFO: epoch 001:   5697 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0294, wps=100.7, ups=0.61, wpb=109.9, bsz=40, num_updates=5690, lr=4.91198e-05, gnorm=1.617, clip=70, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29317
2023-01-09 22:02:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:27 - progress_bar.py[line:274] - INFO: epoch 001:   5707 / 100000 loss=0.405, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0808, wps=100.8, ups=0.61, wpb=109.7, bsz=40, num_updates=5700, lr=4.91146e-05, gnorm=1.691, clip=70, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29333
2023-01-09 22:02:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:44 - progress_bar.py[line:274] - INFO: epoch 001:   5717 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1058, wps=98.5, ups=0.6, wpb=109.1, bsz=40, num_updates=5710, lr=4.91094e-05, gnorm=1.25, clip=60, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=29350
2023-01-09 22:02:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:02:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:01 - progress_bar.py[line:274] - INFO: epoch 001:   5727 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0769, wps=99.7, ups=0.61, wpb=109, bsz=40, num_updates=5720, lr=4.91042e-05, gnorm=1.368, clip=80, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29367
2023-01-09 22:03:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:17 - progress_bar.py[line:274] - INFO: epoch 001:   5737 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0761, wps=101.1, ups=0.61, wpb=110.3, bsz=40, num_updates=5730, lr=4.9099e-05, gnorm=1.153, clip=50, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29383
2023-01-09 22:03:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:34 - progress_bar.py[line:274] - INFO: epoch 001:   5747 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1648, wps=100.8, ups=0.61, wpb=110.1, bsz=40, num_updates=5740, lr=4.90938e-05, gnorm=0.868, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=29400
2023-01-09 22:03:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:50 - progress_bar.py[line:274] - INFO: epoch 001:   5757 / 100000 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0297, wps=99.8, ups=0.61, wpb=108.8, bsz=40, num_updates=5750, lr=4.90885e-05, gnorm=0.977, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=29417
2023-01-09 22:03:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:03:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:07 - progress_bar.py[line:274] - INFO: epoch 001:   5767 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1064, wps=101.5, ups=0.61, wpb=111, bsz=40, num_updates=5760, lr=4.90833e-05, gnorm=1.058, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29433
2023-01-09 22:04:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:23 - progress_bar.py[line:274] - INFO: epoch 001:   5777 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1165, wps=103.1, ups=0.63, wpb=109.9, bsz=40, num_updates=5770, lr=4.90781e-05, gnorm=0.807, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=29449
2023-01-09 22:04:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:40 - progress_bar.py[line:274] - INFO: epoch 001:   5787 / 100000 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.1062, wps=101.7, ups=0.62, wpb=108.7, bsz=40, num_updates=5780, lr=4.90729e-05, gnorm=1.859, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29466
2023-01-09 22:04:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:56 - progress_bar.py[line:274] - INFO: epoch 001:   5797 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.09, wps=101.8, ups=0.61, wpb=111.1, bsz=40, num_updates=5790, lr=4.90677e-05, gnorm=1.206, clip=50, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=29482
2023-01-09 22:04:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:04:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:13 - progress_bar.py[line:274] - INFO: epoch 001:   5807 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0404, wps=100.2, ups=0.61, wpb=109.6, bsz=40, num_updates=5800, lr=4.90625e-05, gnorm=1.069, clip=50, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29499
2023-01-09 22:05:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:29 - progress_bar.py[line:274] - INFO: epoch 001:   5817 / 100000 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0702, wps=101.4, ups=0.62, wpb=108.4, bsz=40, num_updates=5810, lr=4.90573e-05, gnorm=1.241, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29515
2023-01-09 22:05:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:46 - progress_bar.py[line:274] - INFO: epoch 001:   5827 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0808, wps=99.9, ups=0.61, wpb=109.5, bsz=40, num_updates=5820, lr=4.90521e-05, gnorm=1.045, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29532
2023-01-09 22:05:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:05:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:02 - progress_bar.py[line:274] - INFO: epoch 001:   5837 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0686, wps=102, ups=0.63, wpb=108.3, bsz=40, num_updates=5830, lr=4.90469e-05, gnorm=1.015, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29548
2023-01-09 22:06:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:18 - progress_bar.py[line:274] - INFO: epoch 001:   5847 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0625, wps=103.6, ups=0.62, wpb=110.8, bsz=40, num_updates=5840, lr=4.90417e-05, gnorm=1.242, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29564
2023-01-09 22:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:34 - progress_bar.py[line:274] - INFO: epoch 001:   5857 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.134, wps=101.8, ups=0.62, wpb=109.7, bsz=40, num_updates=5850, lr=4.90365e-05, gnorm=1.605, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29581
2023-01-09 22:06:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:51 - progress_bar.py[line:274] - INFO: epoch 001:   5867 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0625, wps=101.4, ups=0.61, wpb=109.9, bsz=40, num_updates=5860, lr=4.90313e-05, gnorm=1.193, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29597
2023-01-09 22:06:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:06:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:08 - progress_bar.py[line:274] - INFO: epoch 001:   5877 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=107.333, nsentences=40, sample_size=107.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0769, wps=98.7, ups=0.61, wpb=107.3, bsz=40, num_updates=5870, lr=4.9026e-05, gnorm=0.85, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29614
2023-01-09 22:07:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:24 - progress_bar.py[line:274] - INFO: epoch 001:   5887 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0583, wps=100.2, ups=0.61, wpb=109.6, bsz=40, num_updates=5880, lr=4.90208e-05, gnorm=0.951, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29630
2023-01-09 22:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:41 - progress_bar.py[line:274] - INFO: epoch 001:   5897 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0968, wps=99.5, ups=0.6, wpb=109.9, bsz=40, num_updates=5890, lr=4.90156e-05, gnorm=0.994, clip=40, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=29647
2023-01-09 22:07:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:07:58 - progress_bar.py[line:274] - INFO: epoch 001:   5907 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0421, wps=100.5, ups=0.61, wpb=110.2, bsz=40, num_updates=5900, lr=4.90104e-05, gnorm=0.787, clip=40, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=29664
2023-01-09 22:07:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:14 - progress_bar.py[line:274] - INFO: epoch 001:   5917 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1053, wps=100.1, ups=0.61, wpb=110.3, bsz=40, num_updates=5910, lr=4.90052e-05, gnorm=1.115, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29681
2023-01-09 22:08:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:31 - progress_bar.py[line:274] - INFO: epoch 001:   5927 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.09, wps=102.1, ups=0.62, wpb=110.1, bsz=40, num_updates=5920, lr=4.9e-05, gnorm=2.297, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=29697
2023-01-09 22:08:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:47 - progress_bar.py[line:274] - INFO: epoch 001:   5937 / 100000 loss=0.412, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0515, wps=102.8, ups=0.62, wpb=110.1, bsz=40, num_updates=5930, lr=4.89948e-05, gnorm=1.492, clip=80, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=29713
2023-01-09 22:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:08:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:03 - progress_bar.py[line:274] - INFO: epoch 001:   5947 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0784, wps=102.6, ups=0.62, wpb=109.8, bsz=40, num_updates=5940, lr=4.89896e-05, gnorm=1.125, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29730
2023-01-09 22:09:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:20 - progress_bar.py[line:274] - INFO: epoch 001:   5957 / 100000 loss=0.407, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0654, wps=98.7, ups=0.61, wpb=108, bsz=40, num_updates=5950, lr=4.89844e-05, gnorm=1.321, clip=70, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29746
2023-01-09 22:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:37 - progress_bar.py[line:274] - INFO: epoch 001:   5967 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1212, wps=101.3, ups=0.62, wpb=109.7, bsz=40, num_updates=5960, lr=4.89792e-05, gnorm=1.429, clip=50, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=29763
2023-01-09 22:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:53 - progress_bar.py[line:274] - INFO: epoch 001:   5977 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0323, wps=101.4, ups=0.61, wpb=110.5, bsz=40, num_updates=5970, lr=4.8974e-05, gnorm=0.888, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=29779
2023-01-09 22:09:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:09:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:09 - progress_bar.py[line:274] - INFO: epoch 001:   5987 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.066, wps=101.7, ups=0.62, wpb=109.3, bsz=40, num_updates=5980, lr=4.89688e-05, gnorm=0.772, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=29796
2023-01-09 22:10:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:26 - progress_bar.py[line:274] - INFO: epoch 001:   5997 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1196, wps=102.1, ups=0.62, wpb=110.6, bsz=40, num_updates=5990, lr=4.89635e-05, gnorm=1.223, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=29812
2023-01-09 22:10:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 22:10:43 - progress_bar.py[line:274] - INFO: epoch 001:   6007 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0808, wps=101.1, ups=0.61, wpb=110.2, bsz=40, num_updates=6000, lr=4.89583e-05, gnorm=1.555, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=29829
2023-01-09 22:10:43 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-09 22:10:44 - train.py[line:549] - INFO: 0 / 4988
2023-01-09 22:10:44 - train.py[line:551] - INFO: load:1.28 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-09 22:10:45 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.14 GiB (GPU 1; 39.59 GiB total capacity; 9.27 GiB already allocated; 5.21 GiB free; 31.89 GiB reserved in total by PyTorch)
2023-01-09 22:10:45 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-09 22:10:45 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9490 MB |   10715 MB |    4173 TB |    4173 TB |
|       from large pool |    9316 MB |   10540 MB |    4171 TB |    4171 TB |
|       from small pool |     174 MB |     175 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9490 MB |   10715 MB |    4173 TB |    4173 TB |
|       from large pool |    9316 MB |   10540 MB |    4171 TB |    4171 TB |
|       from small pool |     174 MB |     175 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   32658 MB |   33138 MB |  273182 MB |  240524 MB |
|       from large pool |   32482 MB |   32960 MB |  272820 MB |  240338 MB |
|       from small pool |     176 MB |     178 MB |     362 MB |     186 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   23167 MB |   27659 MB |    4050 TB |    4050 TB |
|       from large pool |   23165 MB |   27656 MB |    4049 TB |    4049 TB |
|       from small pool |       1 MB |       2 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4623    |    4637    |  197607 K  |  197603 K  |
|       from large pool |     698    |     710    |   61430 K  |   61430 K  |
|       from small pool |    3925    |    3943    |  136176 K  |  136173 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4623    |    4637    |  197607 K  |  197603 K  |
|       from large pool |     698    |     710    |   61430 K  |   61430 K  |
|       from small pool |    3925    |    3943    |  136176 K  |  136173 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     207    |     217    |     825    |     618    |
|       from large pool |     119    |     128    |     644    |     525    |
|       from small pool |      88    |      89    |     181    |      93    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     127    |     135    |  143891 K  |  143891 K  |
|       from large pool |      72    |      78    |   24790 K  |   24790 K  |
|       from small pool |      55    |      64    |  119100 K  |  119100 K  |
|===========================================================================|

2023-01-09 22:10:45 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-01-09 22:13:17 - train.py[line:549] - INFO: 200 / 4988
2023-01-09 22:13:17 - train.py[line:551] - INFO: load:1.30 valid_run:153.12 task_valid:148.95 collect_output:3.11
2023-01-09 22:15:46 - train.py[line:549] - INFO: 400 / 4988
2023-01-09 22:15:46 - train.py[line:551] - INFO: load:1.33 valid_run:301.20 task_valid:292.55 collect_output:6.50
2023-01-09 22:18:17 - train.py[line:549] - INFO: 600 / 4988
2023-01-09 22:18:17 - train.py[line:551] - INFO: load:1.36 valid_run:452.55 task_valid:435.68 collect_output:13.71
2023-01-09 22:20:46 - train.py[line:549] - INFO: 800 / 4988
2023-01-09 22:20:46 - train.py[line:551] - INFO: load:1.38 valid_run:601.28 task_valid:580.71 collect_output:16.37
2023-01-09 22:23:18 - train.py[line:549] - INFO: 1000 / 4988
2023-01-09 22:23:19 - train.py[line:551] - INFO: load:1.41 valid_run:753.19 task_valid:728.27 collect_output:19.71
2023-01-09 22:25:50 - train.py[line:549] - INFO: 1200 / 4988
2023-01-09 22:25:50 - train.py[line:551] - INFO: load:1.44 valid_run:904.18 task_valid:873.69 collect_output:24.27
2023-01-09 22:28:22 - train.py[line:549] - INFO: 1400 / 4988
2023-01-09 22:28:22 - train.py[line:551] - INFO: load:1.46 valid_run:1056.69 task_valid:1019.78 collect_output:29.67
2023-01-09 22:30:53 - train.py[line:549] - INFO: 1600 / 4988
2023-01-09 22:30:53 - train.py[line:551] - INFO: load:1.49 valid_run:1207.02 task_valid:1160.78 collect_output:37.99
2023-01-09 22:33:22 - train.py[line:549] - INFO: 1800 / 4988
2023-01-09 22:33:22 - train.py[line:551] - INFO: load:1.51 valid_run:1356.13 task_valid:1305.76 collect_output:41.08
2023-01-09 22:35:50 - train.py[line:549] - INFO: 2000 / 4988
2023-01-09 22:35:50 - train.py[line:551] - INFO: load:1.54 valid_run:1504.09 task_valid:1449.24 collect_output:44.51
2023-01-09 22:38:19 - train.py[line:549] - INFO: 2200 / 4988
2023-01-09 22:38:19 - train.py[line:551] - INFO: load:1.57 valid_run:1653.44 task_valid:1594.42 collect_output:47.63
2023-01-09 22:40:49 - train.py[line:549] - INFO: 2400 / 4988
2023-01-09 22:40:49 - train.py[line:551] - INFO: load:1.59 valid_run:1802.97 task_valid:1739.55 collect_output:50.99
2023-01-09 22:43:18 - train.py[line:549] - INFO: 2600 / 4988
2023-01-09 22:43:18 - train.py[line:551] - INFO: load:1.62 valid_run:1952.24 task_valid:1881.80 collect_output:56.98
2023-01-09 22:45:49 - train.py[line:549] - INFO: 2800 / 4988
2023-01-09 22:45:49 - train.py[line:551] - INFO: load:1.64 valid_run:2102.49 task_valid:2027.54 collect_output:60.45
2023-01-09 22:48:19 - train.py[line:549] - INFO: 3000 / 4988
2023-01-09 22:48:19 - train.py[line:551] - INFO: load:1.67 valid_run:2252.26 task_valid:2174.14 collect_output:62.61
2023-01-09 22:50:48 - train.py[line:549] - INFO: 3200 / 4988
2023-01-09 22:50:48 - train.py[line:551] - INFO: load:1.70 valid_run:2401.77 task_valid:2318.28 collect_output:66.94
2023-01-09 22:53:19 - train.py[line:549] - INFO: 3400 / 4988
2023-01-09 22:53:19 - train.py[line:551] - INFO: load:1.72 valid_run:2552.44 task_valid:2463.95 collect_output:70.92
2023-01-09 22:55:49 - train.py[line:549] - INFO: 3600 / 4988
2023-01-09 22:55:49 - train.py[line:551] - INFO: load:1.75 valid_run:2702.53 task_valid:2610.98 collect_output:72.96
2023-01-09 22:58:17 - train.py[line:549] - INFO: 3800 / 4988
2023-01-09 22:58:17 - train.py[line:551] - INFO: load:1.78 valid_run:2850.03 task_valid:2752.55 collect_output:77.87
2023-01-09 23:00:46 - train.py[line:549] - INFO: 4000 / 4988
2023-01-09 23:00:46 - train.py[line:551] - INFO: load:1.80 valid_run:2999.68 task_valid:2897.63 collect_output:81.40
2023-01-09 23:03:17 - train.py[line:549] - INFO: 4200 / 4988
2023-01-09 23:03:17 - train.py[line:551] - INFO: load:1.83 valid_run:3150.58 task_valid:3042.30 collect_output:86.62
2023-01-09 23:05:46 - train.py[line:549] - INFO: 4400 / 4988
2023-01-09 23:05:46 - train.py[line:551] - INFO: load:1.86 valid_run:3299.35 task_valid:3186.87 collect_output:89.78
2023-01-09 23:08:17 - train.py[line:549] - INFO: 4600 / 4988
2023-01-09 23:08:17 - train.py[line:551] - INFO: load:1.88 valid_run:3449.76 task_valid:3333.15 collect_output:92.89
2023-01-09 23:10:48 - train.py[line:549] - INFO: 4800 / 4988
2023-01-09 23:10:48 - train.py[line:551] - INFO: load:1.91 valid_run:3600.65 task_valid:3479.74 collect_output:96.17

====================================================================================================
SGG eval:     R @ 50: 0.6504;     R @ 100: 0.6901;     R @ 500: 0.7168;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4415;    mR @ 100: 0.4980;    mR @ 500: 0.5388;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7317) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.8636) (growing on:0.5000) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9477) (says:0.0000) (sitting on:0.8061) (standing on:0.3393) (using:0.5500) (walking in:0.0000) (walking on:0.7568) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6504;     R @ 100: 0.6901;     R @ 500: 0.7168;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4415;    mR @ 100: 0.4980;    mR @ 500: 0.5388;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7317) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.8636) (growing on:0.5000) (hanging from:0.4516) (lying on:0.3000) (mounted on:0.0000) (painted on:0.3333) (parked on:0.9583) (playing:0.0000) (riding:0.9477) (says:0.0000) (sitting on:0.8061) (standing on:0.3393) (using:0.5500) (walking in:0.0000) (walking on:0.7568) (watching:0.4722) 
--------------------------------------------------------
====================================================================================================

2023-01-09 23:13:19 - train.py[line:487] - INFO: 0.6900503437738732
2023-01-09 23:13:19 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-09 23:13:20 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.297 | loss_v1 0 | loss_v2 0 | nll_loss 0.14 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.69005 | ppl 1.1 | vqa_score 0.5484 | wps 119.5 | wpb 89.9 | bsz 30 | num_updates 6000 | best_R@100 0.69005
2023-01-09 23:13:20 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6000 updates
2023-01-09 23:13:20 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt
2023-01-09 23:14:13 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt
2023-01-09 23:17:30 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_6000.pt (epoch 1 @ 6000 updates, score 0.6900503437738732) (writing took 250.0688121598214 seconds)
2023-01-09 23:17:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:17:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:17:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:17:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:17:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:17:47 - progress_bar.py[line:274] - INFO: epoch 001:   6017 / 100000 loss=0.427, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.0727, wps=0.4, ups=0, wpb=108.8, bsz=40, num_updates=6010, lr=4.89531e-05, gnorm=1.035, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=33853
2023-01-09 23:17:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:17:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:17:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:17:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:03 - progress_bar.py[line:274] - INFO: epoch 001:   6027 / 100000 loss=0.404, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0901, wps=100.7, ups=0.62, wpb=108.3, bsz=40, num_updates=6020, lr=4.89479e-05, gnorm=1.334, clip=60, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=33869
2023-01-09 23:18:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:20 - progress_bar.py[line:274] - INFO: epoch 001:   6037 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0909, wps=99.5, ups=0.6, wpb=110.3, bsz=40, num_updates=6030, lr=4.89427e-05, gnorm=0.903, clip=30, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=33886
2023-01-09 23:18:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:37 - progress_bar.py[line:274] - INFO: epoch 001:   6047 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0645, wps=100.4, ups=0.6, wpb=111.4, bsz=40, num_updates=6040, lr=4.89375e-05, gnorm=0.767, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=33903
2023-01-09 23:18:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:54 - progress_bar.py[line:274] - INFO: epoch 001:   6057 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0909, wps=99.3, ups=0.6, wpb=110.1, bsz=40, num_updates=6050, lr=4.89323e-05, gnorm=2.111, clip=70, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=33920
2023-01-09 23:18:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:18:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:10 - progress_bar.py[line:274] - INFO: epoch 001:   6067 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0648, wps=99.5, ups=0.61, wpb=109, bsz=40, num_updates=6060, lr=4.89271e-05, gnorm=1.069, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=33936
2023-01-09 23:19:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:27 - progress_bar.py[line:274] - INFO: epoch 001:   6077 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0714, wps=100.1, ups=0.6, wpb=110.7, bsz=40, num_updates=6070, lr=4.89219e-05, gnorm=2.044, clip=70, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=33953
2023-01-09 23:19:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:44 - progress_bar.py[line:274] - INFO: epoch 001:   6087 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0561, wps=102.1, ups=0.62, wpb=110.4, bsz=40, num_updates=6080, lr=4.89167e-05, gnorm=1.339, clip=50, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=33970
2023-01-09 23:19:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:19:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:00 - progress_bar.py[line:274] - INFO: epoch 001:   6097 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1188, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=6090, lr=4.89115e-05, gnorm=0.671, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=33986
2023-01-09 23:20:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:16 - progress_bar.py[line:274] - INFO: epoch 001:   6107 / 100000 loss=0.409, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0667, wps=101.9, ups=0.62, wpb=108.7, bsz=40, num_updates=6100, lr=4.89063e-05, gnorm=1.482, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34003
2023-01-09 23:20:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:33 - progress_bar.py[line:274] - INFO: epoch 001:   6117 / 100000 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0693, wps=102.9, ups=0.63, wpb=109.5, bsz=40, num_updates=6110, lr=4.8901e-05, gnorm=1.038, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34019
2023-01-09 23:20:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:49 - progress_bar.py[line:274] - INFO: epoch 001:   6127 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0686, wps=101.7, ups=0.62, wpb=108.7, bsz=40, num_updates=6120, lr=4.88958e-05, gnorm=1.252, clip=60, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34035
2023-01-09 23:20:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:20:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:05 - progress_bar.py[line:274] - INFO: epoch 001:   6137 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0421, wps=101.2, ups=0.61, wpb=110, bsz=40, num_updates=6130, lr=4.88906e-05, gnorm=1.692, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=34052
2023-01-09 23:21:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:22 - progress_bar.py[line:274] - INFO: epoch 001:   6147 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1304, wps=102.8, ups=0.62, wpb=111.3, bsz=40, num_updates=6140, lr=4.88854e-05, gnorm=0.892, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34068
2023-01-09 23:21:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:39 - progress_bar.py[line:274] - INFO: epoch 001:   6157 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0636, wps=98, ups=0.6, wpb=108.5, bsz=40, num_updates=6150, lr=4.88802e-05, gnorm=1.96, clip=60, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=34085
2023-01-09 23:21:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:55 - progress_bar.py[line:274] - INFO: epoch 001:   6167 / 100000 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0583, wps=99.2, ups=0.6, wpb=109.7, bsz=40, num_updates=6160, lr=4.8875e-05, gnorm=1.753, clip=60, loss_scale=256, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=34102
2023-01-09 23:21:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:21:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:12 - progress_bar.py[line:274] - INFO: epoch 001:   6177 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.101, wps=100, ups=0.61, wpb=110.1, bsz=40, num_updates=6170, lr=4.88698e-05, gnorm=0.96, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34118
2023-01-09 23:22:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:29 - progress_bar.py[line:274] - INFO: epoch 001:   6187 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.04, wps=100.2, ups=0.62, wpb=108.3, bsz=40, num_updates=6180, lr=4.88646e-05, gnorm=1.177, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=34135
2023-01-09 23:22:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:45 - progress_bar.py[line:274] - INFO: epoch 001:   6197 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0642, wps=101.2, ups=0.61, wpb=110.5, bsz=40, num_updates=6190, lr=4.88594e-05, gnorm=0.681, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34152
2023-01-09 23:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:02 - progress_bar.py[line:274] - INFO: epoch 001:   6207 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0385, wps=102.9, ups=0.62, wpb=110, bsz=40, num_updates=6200, lr=4.88542e-05, gnorm=0.657, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34168
2023-01-09 23:23:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:18 - progress_bar.py[line:274] - INFO: epoch 001:   6217 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1226, wps=101.4, ups=0.62, wpb=109.2, bsz=40, num_updates=6210, lr=4.8849e-05, gnorm=1.069, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34184
2023-01-09 23:23:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:35 - progress_bar.py[line:274] - INFO: epoch 001:   6227 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.0741, wps=101.4, ups=0.61, wpb=110.1, bsz=40, num_updates=6220, lr=4.88438e-05, gnorm=0.668, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=34201
2023-01-09 23:23:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:51 - progress_bar.py[line:274] - INFO: epoch 001:   6237 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1333, wps=100.4, ups=0.61, wpb=110.3, bsz=40, num_updates=6230, lr=4.88385e-05, gnorm=1.618, clip=60, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=34217
2023-01-09 23:23:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:23:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:08 - progress_bar.py[line:274] - INFO: epoch 001:   6247 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1, wps=100.4, ups=0.61, wpb=110.5, bsz=40, num_updates=6240, lr=4.88333e-05, gnorm=0.926, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34234
2023-01-09 23:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:24 - progress_bar.py[line:274] - INFO: epoch 001:   6257 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1111, wps=102.6, ups=0.62, wpb=110.9, bsz=40, num_updates=6250, lr=4.88281e-05, gnorm=1.245, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34251
2023-01-09 23:24:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:41 - progress_bar.py[line:274] - INFO: epoch 001:   6267 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0566, wps=102.1, ups=0.62, wpb=109.5, bsz=40, num_updates=6260, lr=4.88229e-05, gnorm=1.138, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34267
2023-01-09 23:24:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:57 - progress_bar.py[line:274] - INFO: epoch 001:   6277 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0926, wps=100.9, ups=0.62, wpb=108.3, bsz=40, num_updates=6270, lr=4.88177e-05, gnorm=1.258, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34283
2023-01-09 23:24:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:24:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:14 - progress_bar.py[line:274] - INFO: epoch 001:   6287 / 100000 loss=0.408, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0536, wps=101.5, ups=0.62, wpb=109.6, bsz=40, num_updates=6280, lr=4.88125e-05, gnorm=0.902, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34300
2023-01-09 23:25:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:31 - progress_bar.py[line:274] - INFO: epoch 001:   6297 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1136, wps=101.3, ups=0.61, wpb=111, bsz=40, num_updates=6290, lr=4.88073e-05, gnorm=0.868, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34317
2023-01-09 23:25:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:47 - progress_bar.py[line:274] - INFO: epoch 001:   6307 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0215, wps=99.9, ups=0.6, wpb=110.4, bsz=40, num_updates=6300, lr=4.88021e-05, gnorm=1.357, clip=40, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=34334
2023-01-09 23:25:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:25:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:04 - progress_bar.py[line:274] - INFO: epoch 001:   6317 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0471, wps=104.5, ups=0.63, wpb=111.5, bsz=40, num_updates=6310, lr=4.87969e-05, gnorm=0.899, clip=40, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=34350
2023-01-09 23:26:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:20 - progress_bar.py[line:274] - INFO: epoch 001:   6327 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.1034, wps=100.5, ups=0.6, wpb=110.8, bsz=40, num_updates=6320, lr=4.87917e-05, gnorm=1.146, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34367
2023-01-09 23:26:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:37 - progress_bar.py[line:274] - INFO: epoch 001:   6337 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0737, wps=104.2, ups=0.62, wpb=111.3, bsz=40, num_updates=6330, lr=4.87865e-05, gnorm=1.237, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=34383
2023-01-09 23:26:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:53 - progress_bar.py[line:274] - INFO: epoch 001:   6347 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.114, wps=100.7, ups=0.61, wpb=109.3, bsz=40, num_updates=6340, lr=4.87813e-05, gnorm=0.725, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34399
2023-01-09 23:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:26:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:09 - progress_bar.py[line:274] - INFO: epoch 001:   6357 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0734, wps=101.8, ups=0.62, wpb=108.9, bsz=40, num_updates=6350, lr=4.8776e-05, gnorm=1.334, clip=70, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34416
2023-01-09 23:27:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:26 - progress_bar.py[line:274] - INFO: epoch 001:   6367 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.087, wps=99.7, ups=0.6, wpb=110, bsz=40, num_updates=6360, lr=4.87708e-05, gnorm=1.687, clip=50, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=34432
2023-01-09 23:27:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:43 - progress_bar.py[line:274] - INFO: epoch 001:   6377 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0566, wps=100.2, ups=0.61, wpb=109.3, bsz=40, num_updates=6370, lr=4.87656e-05, gnorm=1.466, clip=80, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34449
2023-01-09 23:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:27:59 - progress_bar.py[line:274] - INFO: epoch 001:   6387 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0588, wps=102, ups=0.62, wpb=108.9, bsz=40, num_updates=6380, lr=4.87604e-05, gnorm=2.191, clip=70, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=34465
2023-01-09 23:27:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:15 - progress_bar.py[line:274] - INFO: epoch 001:   6397 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0825, wps=105.6, ups=0.64, wpb=109.7, bsz=40, num_updates=6390, lr=4.87552e-05, gnorm=1.09, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34481
2023-01-09 23:28:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:31 - progress_bar.py[line:274] - INFO: epoch 001:   6407 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.122, wps=101.5, ups=0.61, wpb=111.1, bsz=40, num_updates=6400, lr=4.875e-05, gnorm=1.14, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34498
2023-01-09 23:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:48 - progress_bar.py[line:274] - INFO: epoch 001:   6417 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0874, wps=99.9, ups=0.61, wpb=109.9, bsz=40, num_updates=6410, lr=4.87448e-05, gnorm=1.219, clip=50, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=34514
2023-01-09 23:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:28:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:05 - progress_bar.py[line:274] - INFO: epoch 001:   6427 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.11, wps=98.8, ups=0.61, wpb=107.9, bsz=40, num_updates=6420, lr=4.87396e-05, gnorm=1.018, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34531
2023-01-09 23:29:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:21 - progress_bar.py[line:274] - INFO: epoch 001:   6437 / 100000 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1121, wps=100.1, ups=0.61, wpb=110.1, bsz=40, num_updates=6430, lr=4.87344e-05, gnorm=0.99, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34548
2023-01-09 23:29:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:38 - progress_bar.py[line:274] - INFO: epoch 001:   6447 / 100000 loss=0.415, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.0882, wps=97.9, ups=0.6, wpb=109.1, bsz=40, num_updates=6440, lr=4.87292e-05, gnorm=0.957, clip=50, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=34565
2023-01-09 23:29:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:55 - progress_bar.py[line:274] - INFO: epoch 001:   6457 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0825, wps=102.7, ups=0.62, wpb=110.1, bsz=40, num_updates=6450, lr=4.8724e-05, gnorm=0.846, clip=30, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=34581
2023-01-09 23:29:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:29:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:12 - progress_bar.py[line:274] - INFO: epoch 001:   6467 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1287, wps=98.6, ups=0.6, wpb=109.1, bsz=40, num_updates=6460, lr=4.87188e-05, gnorm=0.989, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=34598
2023-01-09 23:30:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:28 - progress_bar.py[line:274] - INFO: epoch 001:   6477 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1196, wps=102.3, ups=0.62, wpb=110.3, bsz=40, num_updates=6470, lr=4.87135e-05, gnorm=1.261, clip=60, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=34614
2023-01-09 23:30:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:45 - progress_bar.py[line:274] - INFO: epoch 001:   6487 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1081, wps=97.6, ups=0.6, wpb=108.8, bsz=40, num_updates=6480, lr=4.87083e-05, gnorm=0.874, clip=30, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=34631
2023-01-09 23:30:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:30:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:02 - progress_bar.py[line:274] - INFO: epoch 001:   6497 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1154, wps=99.7, ups=0.61, wpb=109.6, bsz=40, num_updates=6490, lr=4.87031e-05, gnorm=1.178, clip=60, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34648
2023-01-09 23:31:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:19 - progress_bar.py[line:274] - INFO: epoch 001:   6507 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0882, wps=99.8, ups=0.61, wpb=109.1, bsz=40, num_updates=6500, lr=4.86979e-05, gnorm=1.029, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34665
2023-01-09 23:31:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:35 - progress_bar.py[line:274] - INFO: epoch 001:   6517 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0392, wps=102.8, ups=0.62, wpb=109.8, bsz=40, num_updates=6510, lr=4.86927e-05, gnorm=1.479, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34681
2023-01-09 23:31:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:51 - progress_bar.py[line:274] - INFO: epoch 001:   6527 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1081, wps=99.8, ups=0.61, wpb=108.9, bsz=40, num_updates=6520, lr=4.86875e-05, gnorm=1.247, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34698
2023-01-09 23:31:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:08 - progress_bar.py[line:274] - INFO: epoch 001:   6537 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1062, wps=100.6, ups=0.61, wpb=109.9, bsz=40, num_updates=6530, lr=4.86823e-05, gnorm=0.976, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34714
2023-01-09 23:32:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:24 - progress_bar.py[line:274] - INFO: epoch 001:   6547 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.0778, wps=104, ups=0.63, wpb=110.7, bsz=40, num_updates=6540, lr=4.86771e-05, gnorm=0.831, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34730
2023-01-09 23:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:41 - progress_bar.py[line:274] - INFO: epoch 001:   6557 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1111, wps=103.4, ups=0.63, wpb=109.9, bsz=40, num_updates=6550, lr=4.86719e-05, gnorm=1.115, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34747
2023-01-09 23:32:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:32:58 - progress_bar.py[line:274] - INFO: epoch 001:   6567 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1442, wps=97.7, ups=0.6, wpb=109.1, bsz=40, num_updates=6560, lr=4.86667e-05, gnorm=1.002, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=34764
2023-01-09 23:32:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:15 - progress_bar.py[line:274] - INFO: epoch 001:   6577 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0947, wps=98.6, ups=0.59, wpb=111.1, bsz=40, num_updates=6570, lr=4.86615e-05, gnorm=0.822, clip=20, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=34781
2023-01-09 23:33:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:31 - progress_bar.py[line:274] - INFO: epoch 001:   6587 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.1264, wps=103.1, ups=0.62, wpb=111.5, bsz=40, num_updates=6580, lr=4.86563e-05, gnorm=0.978, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34797
2023-01-09 23:33:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:47 - progress_bar.py[line:274] - INFO: epoch 001:   6597 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.11, wps=105.4, ups=0.64, wpb=109.9, bsz=40, num_updates=6590, lr=4.8651e-05, gnorm=1.191, clip=40, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=34813
2023-01-09 23:33:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:33:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:04 - progress_bar.py[line:274] - INFO: epoch 001:   6607 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1279, wps=102.3, ups=0.62, wpb=110.7, bsz=40, num_updates=6600, lr=4.86458e-05, gnorm=0.944, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34830
2023-01-09 23:34:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:21 - progress_bar.py[line:274] - INFO: epoch 001:   6617 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.101, wps=94.5, ups=0.57, wpb=109.7, bsz=40, num_updates=6610, lr=4.86406e-05, gnorm=0.961, clip=10, loss_scale=512, train_wall=17, gb_free=9.2, ema_decay=0.9999, wall=34847
2023-01-09 23:34:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:38 - progress_bar.py[line:274] - INFO: epoch 001:   6627 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0686, wps=101.1, ups=0.61, wpb=110.9, bsz=40, num_updates=6620, lr=4.86354e-05, gnorm=1.07, clip=40, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=34864
2023-01-09 23:34:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:55 - progress_bar.py[line:274] - INFO: epoch 001:   6637 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0505, wps=99.9, ups=0.6, wpb=110.5, bsz=40, num_updates=6630, lr=4.86302e-05, gnorm=1.125, clip=50, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=34881
2023-01-09 23:34:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:34:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:11 - progress_bar.py[line:274] - INFO: epoch 001:   6647 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1481, wps=101.2, ups=0.62, wpb=108.3, bsz=40, num_updates=6640, lr=4.8625e-05, gnorm=0.86, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=34897
2023-01-09 23:35:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:27 - progress_bar.py[line:274] - INFO: epoch 001:   6657 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1212, wps=100.5, ups=0.62, wpb=108.9, bsz=40, num_updates=6650, lr=4.86198e-05, gnorm=1.036, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34914
2023-01-09 23:35:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:44 - progress_bar.py[line:274] - INFO: epoch 001:   6667 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0917, wps=98.8, ups=0.6, wpb=109.7, bsz=40, num_updates=6660, lr=4.86146e-05, gnorm=1.048, clip=60, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=34931
2023-01-09 23:35:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:35:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:01 - progress_bar.py[line:274] - INFO: epoch 001:   6677 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.15, wps=102.3, ups=0.62, wpb=110.4, bsz=40, num_updates=6670, lr=4.86094e-05, gnorm=0.655, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=34947
2023-01-09 23:36:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:17 - progress_bar.py[line:274] - INFO: epoch 001:   6687 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.134, wps=103.2, ups=0.63, wpb=109.1, bsz=40, num_updates=6680, lr=4.86042e-05, gnorm=1.255, clip=50, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34963
2023-01-09 23:36:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:33 - progress_bar.py[line:274] - INFO: epoch 001:   6697 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0769, wps=102.2, ups=0.61, wpb=111.1, bsz=40, num_updates=6690, lr=4.8599e-05, gnorm=1.256, clip=40, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34980
2023-01-09 23:36:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:50 - progress_bar.py[line:274] - INFO: epoch 001:   6707 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1468, wps=101, ups=0.62, wpb=109.2, bsz=40, num_updates=6700, lr=4.85938e-05, gnorm=0.924, clip=30, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=34996
2023-01-09 23:36:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:06 - progress_bar.py[line:274] - INFO: epoch 001:   6717 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1327, wps=101.9, ups=0.62, wpb=109.5, bsz=40, num_updates=6710, lr=4.85885e-05, gnorm=0.593, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35012
2023-01-09 23:37:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:22 - progress_bar.py[line:274] - INFO: epoch 001:   6727 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0808, wps=104.3, ups=0.63, wpb=110.5, bsz=40, num_updates=6720, lr=4.85833e-05, gnorm=1.316, clip=70, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35029
2023-01-09 23:37:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:39 - progress_bar.py[line:274] - INFO: epoch 001:   6737 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0446, wps=100.2, ups=0.61, wpb=109.1, bsz=40, num_updates=6730, lr=4.85781e-05, gnorm=1.029, clip=30, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35045
2023-01-09 23:37:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:56 - progress_bar.py[line:274] - INFO: epoch 001:   6747 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1136, wps=101.3, ups=0.61, wpb=110.3, bsz=40, num_updates=6740, lr=4.85729e-05, gnorm=1.023, clip=60, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=35062
2023-01-09 23:37:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:37:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:12 - progress_bar.py[line:274] - INFO: epoch 001:   6757 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1031, wps=100.8, ups=0.61, wpb=109.5, bsz=40, num_updates=6750, lr=4.85677e-05, gnorm=0.675, clip=20, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=35078
2023-01-09 23:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:29 - progress_bar.py[line:274] - INFO: epoch 001:   6767 / 100000 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.068, wps=100.3, ups=0.61, wpb=110.4, bsz=40, num_updates=6760, lr=4.85625e-05, gnorm=0.701, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35095
2023-01-09 23:38:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:45 - progress_bar.py[line:274] - INFO: epoch 001:   6777 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1477, wps=102.1, ups=0.61, wpb=111.1, bsz=40, num_updates=6770, lr=4.85573e-05, gnorm=0.85, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35112
2023-01-09 23:38:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:38:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:02 - progress_bar.py[line:274] - INFO: epoch 001:   6787 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1087, wps=103.5, ups=0.62, wpb=111.3, bsz=40, num_updates=6780, lr=4.85521e-05, gnorm=2.057, clip=50, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35128
2023-01-09 23:39:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:19 - progress_bar.py[line:274] - INFO: epoch 001:   6797 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1346, wps=99.3, ups=0.6, wpb=109.7, bsz=40, num_updates=6790, lr=4.85469e-05, gnorm=1.036, clip=40, loss_scale=1024, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=35145
2023-01-09 23:39:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:27 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-09 23:39:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:37 - progress_bar.py[line:274] - INFO: epoch 001:   6808 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.75, nsentences=40, sample_size=110.75, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1261, wps=96.1, ups=0.54, wpb=110.8, bsz=40, num_updates=6800, lr=4.85417e-05, gnorm=1.122, clip=30, loss_scale=512, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=35163
2023-01-09 23:39:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:53 - progress_bar.py[line:274] - INFO: epoch 001:   6818 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1122, wps=103.6, ups=0.63, wpb=110.1, bsz=40, num_updates=6810, lr=4.85365e-05, gnorm=0.826, clip=30, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=35180
2023-01-09 23:39:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:39:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:10 - progress_bar.py[line:274] - INFO: epoch 001:   6828 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1667, wps=99.9, ups=0.61, wpb=109.6, bsz=40, num_updates=6820, lr=4.85312e-05, gnorm=0.74, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35196
2023-01-09 23:40:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:26 - progress_bar.py[line:274] - INFO: epoch 001:   6838 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.1228, wps=103.2, ups=0.63, wpb=109.8, bsz=40, num_updates=6830, lr=4.8526e-05, gnorm=1.383, clip=70, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35213
2023-01-09 23:40:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:43 - progress_bar.py[line:274] - INFO: epoch 001:   6848 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1456, wps=100.8, ups=0.61, wpb=109.6, bsz=40, num_updates=6840, lr=4.85208e-05, gnorm=1.052, clip=40, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=35229
2023-01-09 23:40:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:40:59 - progress_bar.py[line:274] - INFO: epoch 001:   6858 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1373, wps=100.9, ups=0.61, wpb=109.5, bsz=40, num_updates=6850, lr=4.85156e-05, gnorm=0.973, clip=40, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=35246
2023-01-09 23:40:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:16 - progress_bar.py[line:274] - INFO: epoch 001:   6868 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0707, wps=99.8, ups=0.61, wpb=108.8, bsz=40, num_updates=6860, lr=4.85104e-05, gnorm=1.544, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=35262
2023-01-09 23:41:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:33 - progress_bar.py[line:274] - INFO: epoch 001:   6878 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1687, wps=101.2, ups=0.61, wpb=111.3, bsz=40, num_updates=6870, lr=4.85052e-05, gnorm=0.702, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=35279
2023-01-09 23:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:49 - progress_bar.py[line:274] - INFO: epoch 001:   6888 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1584, wps=102.1, ups=0.62, wpb=109.5, bsz=40, num_updates=6880, lr=4.85e-05, gnorm=0.71, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35295
2023-01-09 23:41:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:41:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:06 - progress_bar.py[line:274] - INFO: epoch 001:   6898 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0645, wps=100.1, ups=0.6, wpb=111.3, bsz=40, num_updates=6890, lr=4.84948e-05, gnorm=0.834, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=35312
2023-01-09 23:42:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:22 - progress_bar.py[line:274] - INFO: epoch 001:   6908 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1176, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=6900, lr=4.84896e-05, gnorm=0.882, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=35329
2023-01-09 23:42:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:39 - progress_bar.py[line:274] - INFO: epoch 001:   6918 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.125, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=6910, lr=4.84844e-05, gnorm=0.933, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=35345
2023-01-09 23:42:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:55 - progress_bar.py[line:274] - INFO: epoch 001:   6928 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1058, wps=102.5, ups=0.62, wpb=109.5, bsz=40, num_updates=6920, lr=4.84792e-05, gnorm=1.003, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35361
2023-01-09 23:42:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:42:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:12 - progress_bar.py[line:274] - INFO: epoch 001:   6938 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.14, wps=101.8, ups=0.62, wpb=109.8, bsz=40, num_updates=6930, lr=4.8474e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35378
2023-01-09 23:43:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:28 - progress_bar.py[line:274] - INFO: epoch 001:   6948 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0909, wps=102.2, ups=0.61, wpb=111.5, bsz=40, num_updates=6940, lr=4.84688e-05, gnorm=0.758, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=35394
2023-01-09 23:43:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:45 - progress_bar.py[line:274] - INFO: epoch 001:   6958 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1224, wps=101.1, ups=0.61, wpb=110.2, bsz=40, num_updates=6950, lr=4.84635e-05, gnorm=0.991, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=35411
2023-01-09 23:43:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:43:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:01 - progress_bar.py[line:274] - INFO: epoch 001:   6968 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1515, wps=103.6, ups=0.62, wpb=110.9, bsz=40, num_updates=6960, lr=4.84583e-05, gnorm=0.817, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35427
2023-01-09 23:44:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:18 - progress_bar.py[line:274] - INFO: epoch 001:   6978 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1724, wps=99.3, ups=0.6, wpb=110.1, bsz=40, num_updates=6970, lr=4.84531e-05, gnorm=1.101, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=35444
2023-01-09 23:44:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:35 - progress_bar.py[line:274] - INFO: epoch 001:   6988 / 100000 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1771, wps=101.6, ups=0.61, wpb=110.7, bsz=40, num_updates=6980, lr=4.84479e-05, gnorm=1.005, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35461
2023-01-09 23:44:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:51 - progress_bar.py[line:274] - INFO: epoch 001:   6998 / 100000 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1376, wps=102.6, ups=0.62, wpb=110.1, bsz=40, num_updates=6990, lr=4.84427e-05, gnorm=0.92, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35477
2023-01-09 23:44:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:44:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-09 23:45:07 - progress_bar.py[line:274] - INFO: epoch 001:   7008 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0909, wps=102.6, ups=0.62, wpb=109.7, bsz=40, num_updates=7000, lr=4.84375e-05, gnorm=0.873, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=35493
2023-01-09 23:45:07 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-09 23:45:09 - train.py[line:549] - INFO: 0 / 4988
2023-01-09 23:45:09 - train.py[line:551] - INFO: load:1.18 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-09 23:47:41 - train.py[line:549] - INFO: 200 / 4988
2023-01-09 23:47:41 - train.py[line:551] - INFO: load:1.20 valid_run:152.74 task_valid:148.73 collect_output:2.96
2023-01-09 23:50:10 - train.py[line:549] - INFO: 400 / 4988
2023-01-09 23:50:10 - train.py[line:551] - INFO: load:1.23 valid_run:300.87 task_valid:291.83 collect_output:6.96
2023-01-09 23:52:42 - train.py[line:549] - INFO: 600 / 4988
2023-01-09 23:52:42 - train.py[line:551] - INFO: load:1.25 valid_run:453.59 task_valid:435.37 collect_output:15.09
2023-01-09 23:55:11 - train.py[line:549] - INFO: 800 / 4988
2023-01-09 23:55:11 - train.py[line:551] - INFO: load:1.28 valid_run:602.35 task_valid:580.50 collect_output:17.70
2023-01-09 23:57:43 - train.py[line:549] - INFO: 1000 / 4988
2023-01-09 23:57:43 - train.py[line:551] - INFO: load:1.31 valid_run:754.35 task_valid:728.18 collect_output:21.00
2023-01-10 00:00:15 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 00:00:15 - train.py[line:551] - INFO: load:1.33 valid_run:905.54 task_valid:874.12 collect_output:25.19
2023-01-10 00:02:47 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 00:02:47 - train.py[line:551] - INFO: load:1.36 valid_run:1057.84 task_valid:1020.35 collect_output:30.24
2023-01-10 00:05:17 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 00:05:17 - train.py[line:551] - INFO: load:1.38 valid_run:1208.05 task_valid:1161.48 collect_output:38.30
2023-01-10 00:07:46 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 00:07:46 - train.py[line:551] - INFO: load:1.41 valid_run:1357.08 task_valid:1306.23 collect_output:41.58
2023-01-10 00:10:14 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 00:10:14 - train.py[line:551] - INFO: load:1.44 valid_run:1504.90 task_valid:1449.46 collect_output:45.14
2023-01-10 00:12:44 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 00:12:44 - train.py[line:551] - INFO: load:1.46 valid_run:1654.20 task_valid:1594.51 collect_output:48.39
2023-01-10 00:15:13 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 00:15:13 - train.py[line:551] - INFO: load:1.49 valid_run:1803.71 task_valid:1739.53 collect_output:51.85
2023-01-10 00:17:42 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 00:17:42 - train.py[line:551] - INFO: load:1.52 valid_run:1952.51 task_valid:1881.32 collect_output:57.84
2023-01-10 00:20:12 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 00:20:12 - train.py[line:551] - INFO: load:1.55 valid_run:2102.40 task_valid:2026.83 collect_output:61.19
2023-01-10 00:22:42 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 00:22:42 - train.py[line:551] - INFO: load:1.57 valid_run:2252.24 task_valid:2173.56 collect_output:63.27
2023-01-10 00:25:12 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 00:25:12 - train.py[line:551] - INFO: load:1.60 valid_run:2401.88 task_valid:2317.83 collect_output:67.60
2023-01-10 00:27:42 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 00:27:42 - train.py[line:551] - INFO: load:1.63 valid_run:2552.39 task_valid:2463.33 collect_output:71.56
2023-01-10 00:30:13 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 00:30:13 - train.py[line:551] - INFO: load:1.66 valid_run:2702.56 task_valid:2610.53 collect_output:73.47
2023-01-10 00:32:40 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 00:32:40 - train.py[line:551] - INFO: load:1.69 valid_run:2850.18 task_valid:2752.52 collect_output:78.06
2023-01-10 00:35:10 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 00:35:10 - train.py[line:551] - INFO: load:1.71 valid_run:2999.77 task_valid:2897.73 collect_output:81.43
2023-01-10 00:37:41 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 00:37:41 - train.py[line:551] - INFO: load:1.74 valid_run:3150.31 task_valid:3042.28 collect_output:86.41
2023-01-10 00:40:10 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 00:40:10 - train.py[line:551] - INFO: load:1.77 valid_run:3299.11 task_valid:3187.03 collect_output:89.43
2023-01-10 00:42:40 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 00:42:40 - train.py[line:551] - INFO: load:1.80 valid_run:3449.50 task_valid:3333.48 collect_output:92.35
2023-01-10 00:45:11 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 00:45:11 - train.py[line:551] - INFO: load:1.83 valid_run:3600.17 task_valid:3480.09 collect_output:95.38

====================================================================================================
SGG eval:     R @ 50: 0.6404;     R @ 100: 0.6747;     R @ 500: 0.7055;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4104;    mR @ 100: 0.4568;    mR @ 500: 0.5004;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8049) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.3636) (growing on:0.3750) (hanging from:0.4194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9379) (says:0.0000) (sitting on:0.8129) (standing on:0.3243) (using:0.5500) (walking in:0.0000) (walking on:0.7568) (watching:0.3333) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6404;     R @ 100: 0.6747;     R @ 500: 0.7055;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4104;    mR @ 100: 0.4568;    mR @ 500: 0.5004;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8049) (covered in:0.8125) (covering:0.3714) (eating:0.7647) (flying in:0.3636) (growing on:0.3750) (hanging from:0.4194) (lying on:0.3000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9379) (says:0.0000) (sitting on:0.8129) (standing on:0.3243) (using:0.5500) (walking in:0.0000) (walking on:0.7568) (watching:0.3333) 
--------------------------------------------------------
====================================================================================================

2023-01-10 00:47:42 - train.py[line:487] - INFO: 0.67471701044054
2023-01-10 00:47:42 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 00:47:42 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.315 | loss_v1 0 | loss_v2 0 | nll_loss 0.161 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.674717 | ppl 1.12 | vqa_score 0.5698 | wps 119.5 | wpb 89.9 | bsz 30 | num_updates 7000 | best_R@100 0.69005
2023-01-10 00:47:42 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 7000 updates
2023-01-10 00:47:42 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_7000.pt
2023-01-10 00:48:24 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_7000.pt
2023-01-10 00:49:52 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_7000.pt (epoch 1 @ 7000 updates, score 0.67471701044054) (writing took 130.18213466368616 seconds)
2023-01-10 00:49:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:49:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:09 - progress_bar.py[line:274] - INFO: epoch 001:   7018 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1239, wps=0.4, ups=0, wpb=110.1, bsz=40, num_updates=7010, lr=4.84323e-05, gnorm=1.14, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=39395
2023-01-10 00:50:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:25 - progress_bar.py[line:274] - INFO: epoch 001:   7028 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.1075, wps=101.5, ups=0.62, wpb=109.5, bsz=40, num_updates=7020, lr=4.84271e-05, gnorm=1.104, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=39412
2023-01-10 00:50:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:42 - progress_bar.py[line:274] - INFO: epoch 001:   7038 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1089, wps=102.2, ups=0.62, wpb=110.6, bsz=40, num_updates=7030, lr=4.84219e-05, gnorm=0.919, clip=30, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=39428
2023-01-10 00:50:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:50:58 - progress_bar.py[line:274] - INFO: epoch 001:   7048 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0532, wps=99.5, ups=0.61, wpb=108.7, bsz=40, num_updates=7040, lr=4.84167e-05, gnorm=0.589, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=39445
2023-01-10 00:50:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:15 - progress_bar.py[line:274] - INFO: epoch 001:   7058 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1485, wps=99.7, ups=0.6, wpb=110.1, bsz=40, num_updates=7050, lr=4.84115e-05, gnorm=0.835, clip=30, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=39461
2023-01-10 00:51:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:32 - progress_bar.py[line:274] - INFO: epoch 001:   7068 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1495, wps=99.9, ups=0.61, wpb=110, bsz=40, num_updates=7060, lr=4.84063e-05, gnorm=1.033, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=39478
2023-01-10 00:51:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:48 - progress_bar.py[line:274] - INFO: epoch 001:   7078 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0693, wps=102.3, ups=0.62, wpb=109.6, bsz=40, num_updates=7070, lr=4.8401e-05, gnorm=1.134, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=39495
2023-01-10 00:51:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:51:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:05 - progress_bar.py[line:274] - INFO: epoch 001:   7088 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1239, wps=99.6, ups=0.61, wpb=109, bsz=40, num_updates=7080, lr=4.83958e-05, gnorm=1.001, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=39511
2023-01-10 00:52:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:22 - progress_bar.py[line:274] - INFO: epoch 001:   7098 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1398, wps=100.9, ups=0.61, wpb=110.2, bsz=40, num_updates=7090, lr=4.83906e-05, gnorm=0.828, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=39528
2023-01-10 00:52:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:38 - progress_bar.py[line:274] - INFO: epoch 001:   7108 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1075, wps=102.1, ups=0.61, wpb=110.7, bsz=40, num_updates=7100, lr=4.83854e-05, gnorm=0.975, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=39544
2023-01-10 00:52:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:54 - progress_bar.py[line:274] - INFO: epoch 001:   7118 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1279, wps=101.6, ups=0.62, wpb=109.7, bsz=40, num_updates=7110, lr=4.83802e-05, gnorm=0.892, clip=40, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=39561
2023-01-10 00:52:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:52:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:11 - progress_bar.py[line:274] - INFO: epoch 001:   7128 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1579, wps=102.3, ups=0.61, wpb=111.2, bsz=40, num_updates=7120, lr=4.8375e-05, gnorm=1.17, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=39577
2023-01-10 00:53:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:28 - progress_bar.py[line:274] - INFO: epoch 001:   7138 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1651, wps=100.5, ups=0.61, wpb=110.5, bsz=40, num_updates=7130, lr=4.83698e-05, gnorm=0.908, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=39594
2023-01-10 00:53:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:44 - progress_bar.py[line:274] - INFO: epoch 001:   7148 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.125, wps=101, ups=0.61, wpb=109.5, bsz=40, num_updates=7140, lr=4.83646e-05, gnorm=1.01, clip=30, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=39610
2023-01-10 00:53:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:53:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:00 - progress_bar.py[line:274] - INFO: epoch 001:   7158 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1553, wps=105.3, ups=0.64, wpb=109.7, bsz=40, num_updates=7150, lr=4.83594e-05, gnorm=1.181, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=39626
2023-01-10 00:54:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:16 - progress_bar.py[line:274] - INFO: epoch 001:   7168 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0909, wps=99.9, ups=0.62, wpb=108.3, bsz=40, num_updates=7160, lr=4.83542e-05, gnorm=0.707, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=39643
2023-01-10 00:54:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:32 - progress_bar.py[line:274] - INFO: epoch 001:   7178 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1429, wps=105.6, ups=0.63, wpb=111.9, bsz=40, num_updates=7170, lr=4.8349e-05, gnorm=1.049, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=39659
2023-01-10 00:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:49 - progress_bar.py[line:274] - INFO: epoch 001:   7188 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.1429, wps=104.4, ups=0.63, wpb=111.2, bsz=40, num_updates=7180, lr=4.83438e-05, gnorm=0.976, clip=20, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=39675
2023-01-10 00:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:54:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:05 - progress_bar.py[line:274] - INFO: epoch 001:   7198 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1346, wps=100.4, ups=0.61, wpb=109.5, bsz=40, num_updates=7190, lr=4.83385e-05, gnorm=0.907, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=39691
2023-01-10 00:55:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:22 - progress_bar.py[line:274] - INFO: epoch 001:   7208 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1569, wps=101.1, ups=0.61, wpb=109.9, bsz=40, num_updates=7200, lr=4.83333e-05, gnorm=1.391, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=39708
2023-01-10 00:55:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:38 - progress_bar.py[line:274] - INFO: epoch 001:   7218 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1383, wps=101.5, ups=0.61, wpb=110.7, bsz=40, num_updates=7210, lr=4.83281e-05, gnorm=0.892, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=39724
2023-01-10 00:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:55 - progress_bar.py[line:274] - INFO: epoch 001:   7228 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1165, wps=101.9, ups=0.62, wpb=109.7, bsz=40, num_updates=7220, lr=4.83229e-05, gnorm=0.91, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=39741
2023-01-10 00:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:55:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:11 - progress_bar.py[line:274] - INFO: epoch 001:   7238 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.117, wps=100.7, ups=0.61, wpb=110.3, bsz=40, num_updates=7230, lr=4.83177e-05, gnorm=0.903, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=39758
2023-01-10 00:56:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:28 - progress_bar.py[line:274] - INFO: epoch 001:   7248 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1048, wps=100.2, ups=0.61, wpb=109.5, bsz=40, num_updates=7240, lr=4.83125e-05, gnorm=0.946, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=39774
2023-01-10 00:56:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:45 - progress_bar.py[line:274] - INFO: epoch 001:   7258 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1313, wps=100, ups=0.6, wpb=110.5, bsz=40, num_updates=7250, lr=4.83073e-05, gnorm=1, clip=30, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=39791
2023-01-10 00:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:56:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:01 - progress_bar.py[line:274] - INFO: epoch 001:   7268 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1524, wps=100, ups=0.61, wpb=109.3, bsz=40, num_updates=7260, lr=4.83021e-05, gnorm=0.89, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=39808
2023-01-10 00:57:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:18 - progress_bar.py[line:274] - INFO: epoch 001:   7278 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1321, wps=99.8, ups=0.6, wpb=110.2, bsz=40, num_updates=7270, lr=4.82969e-05, gnorm=1.229, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=39824
2023-01-10 00:57:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:35 - progress_bar.py[line:274] - INFO: epoch 001:   7288 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1413, wps=102.2, ups=0.62, wpb=110.4, bsz=40, num_updates=7280, lr=4.82917e-05, gnorm=0.663, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=39841
2023-01-10 00:57:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:51 - progress_bar.py[line:274] - INFO: epoch 001:   7298 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.1848, wps=98.5, ups=0.6, wpb=108.9, bsz=40, num_updates=7290, lr=4.82865e-05, gnorm=1.006, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=39858
2023-01-10 00:57:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:57:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:08 - progress_bar.py[line:274] - INFO: epoch 001:   7308 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1165, wps=99.6, ups=0.6, wpb=110.7, bsz=40, num_updates=7300, lr=4.82812e-05, gnorm=1.064, clip=40, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=39875
2023-01-10 00:58:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:25 - progress_bar.py[line:274] - INFO: epoch 001:   7318 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.0769, wps=102.2, ups=0.62, wpb=110, bsz=40, num_updates=7310, lr=4.8276e-05, gnorm=0.847, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=39891
2023-01-10 00:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:42 - progress_bar.py[line:274] - INFO: epoch 001:   7328 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1262, wps=98, ups=0.59, wpb=109.9, bsz=40, num_updates=7320, lr=4.82708e-05, gnorm=1.088, clip=50, loss_scale=1024, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=39908
2023-01-10 00:58:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:46 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-10 00:58:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:58:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:00 - progress_bar.py[line:274] - INFO: epoch 001:   7339 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1241, wps=94.8, ups=0.54, wpb=109, bsz=40, num_updates=7330, lr=4.82656e-05, gnorm=0.967, clip=30, loss_scale=512, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=39927
2023-01-10 00:59:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:17 - progress_bar.py[line:274] - INFO: epoch 001:   7349 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0652, wps=103.7, ups=0.63, wpb=109.4, bsz=40, num_updates=7340, lr=4.82604e-05, gnorm=1.183, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=39943
2023-01-10 00:59:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:33 - progress_bar.py[line:274] - INFO: epoch 001:   7359 / 100000 loss=0.401, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.0917, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=7350, lr=4.82552e-05, gnorm=0.884, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=39959
2023-01-10 00:59:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:50 - progress_bar.py[line:274] - INFO: epoch 001:   7369 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.17, wps=98.2, ups=0.6, wpb=109.1, bsz=40, num_updates=7360, lr=4.825e-05, gnorm=0.647, clip=10, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=39976
2023-01-10 00:59:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 00:59:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:07 - progress_bar.py[line:274] - INFO: epoch 001:   7379 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0808, wps=100, ups=0.61, wpb=108.9, bsz=40, num_updates=7370, lr=4.82448e-05, gnorm=0.886, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=39993
2023-01-10 01:00:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:23 - progress_bar.py[line:274] - INFO: epoch 001:   7389 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1111, wps=106.7, ups=0.64, wpb=111.9, bsz=40, num_updates=7380, lr=4.82396e-05, gnorm=0.672, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40009
2023-01-10 01:00:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:39 - progress_bar.py[line:274] - INFO: epoch 001:   7399 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2035, wps=101.6, ups=0.62, wpb=109.4, bsz=40, num_updates=7390, lr=4.82344e-05, gnorm=0.77, clip=20, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=40025
2023-01-10 01:00:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:00:55 - progress_bar.py[line:274] - INFO: epoch 001:   7409 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2323, wps=102.1, ups=0.62, wpb=110.4, bsz=40, num_updates=7400, lr=4.82292e-05, gnorm=0.649, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=40042
2023-01-10 01:00:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:12 - progress_bar.py[line:274] - INFO: epoch 001:   7419 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1222, wps=101.9, ups=0.61, wpb=111, bsz=40, num_updates=7410, lr=4.8224e-05, gnorm=1.044, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=40058
2023-01-10 01:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:28 - progress_bar.py[line:274] - INFO: epoch 001:   7429 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1477, wps=103.1, ups=0.62, wpb=111, bsz=40, num_updates=7420, lr=4.82188e-05, gnorm=1.276, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40074
2023-01-10 01:01:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:45 - progress_bar.py[line:274] - INFO: epoch 001:   7439 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1212, wps=99.1, ups=0.6, wpb=109.9, bsz=40, num_updates=7430, lr=4.82135e-05, gnorm=0.787, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=40091
2023-01-10 01:01:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:01:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:02 - progress_bar.py[line:274] - INFO: epoch 001:   7449 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1413, wps=101, ups=0.61, wpb=110.6, bsz=40, num_updates=7440, lr=4.82083e-05, gnorm=0.865, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=40108
2023-01-10 01:02:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:18 - progress_bar.py[line:274] - INFO: epoch 001:   7459 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1538, wps=102, ups=0.61, wpb=111.5, bsz=40, num_updates=7450, lr=4.82031e-05, gnorm=0.619, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40125
2023-01-10 01:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:35 - progress_bar.py[line:274] - INFO: epoch 001:   7469 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.172, wps=101.3, ups=0.61, wpb=111.5, bsz=40, num_updates=7460, lr=4.81979e-05, gnorm=1.012, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40141
2023-01-10 01:02:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:51 - progress_bar.py[line:274] - INFO: epoch 001:   7479 / 100000 loss=0.414, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.21, vqa_score=0.1308, wps=102.5, ups=0.62, wpb=110.3, bsz=40, num_updates=7470, lr=4.81927e-05, gnorm=1.171, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=40158
2023-01-10 01:02:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:08 - progress_bar.py[line:274] - INFO: epoch 001:   7489 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1635, wps=99.3, ups=0.6, wpb=109.7, bsz=40, num_updates=7480, lr=4.81875e-05, gnorm=0.576, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=40174
2023-01-10 01:03:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:25 - progress_bar.py[line:274] - INFO: epoch 001:   7499 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1461, wps=100.8, ups=0.61, wpb=110.4, bsz=40, num_updates=7490, lr=4.81823e-05, gnorm=0.9, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=40191
2023-01-10 01:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:42 - progress_bar.py[line:274] - INFO: epoch 001:   7509 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1176, wps=97.9, ups=0.6, wpb=109.1, bsz=40, num_updates=7500, lr=4.81771e-05, gnorm=0.948, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=40208
2023-01-10 01:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:03:58 - progress_bar.py[line:274] - INFO: epoch 001:   7519 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1277, wps=102.3, ups=0.62, wpb=110.3, bsz=40, num_updates=7510, lr=4.81719e-05, gnorm=0.769, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40224
2023-01-10 01:03:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:15 - progress_bar.py[line:274] - INFO: epoch 001:   7529 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.13, wps=99.1, ups=0.6, wpb=109.4, bsz=40, num_updates=7520, lr=4.81667e-05, gnorm=0.925, clip=40, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=40241
2023-01-10 01:04:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:31 - progress_bar.py[line:274] - INFO: epoch 001:   7539 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.1515, wps=102, ups=0.62, wpb=109.1, bsz=40, num_updates=7530, lr=4.81615e-05, gnorm=0.635, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40257
2023-01-10 01:04:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:48 - progress_bar.py[line:274] - INFO: epoch 001:   7549 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1212, wps=98.9, ups=0.6, wpb=110.3, bsz=40, num_updates=7540, lr=4.81563e-05, gnorm=1.219, clip=60, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=40274
2023-01-10 01:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:04:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:05 - progress_bar.py[line:274] - INFO: epoch 001:   7559 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1529, wps=100, ups=0.6, wpb=110.6, bsz=40, num_updates=7550, lr=4.8151e-05, gnorm=0.829, clip=40, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=40291
2023-01-10 01:05:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:21 - progress_bar.py[line:274] - INFO: epoch 001:   7569 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1209, wps=103.4, ups=0.62, wpb=111.5, bsz=40, num_updates=7560, lr=4.81458e-05, gnorm=1.345, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40308
2023-01-10 01:05:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:38 - progress_bar.py[line:274] - INFO: epoch 001:   7579 / 100000 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1495, wps=97.7, ups=0.6, wpb=108.5, bsz=40, num_updates=7570, lr=4.81406e-05, gnorm=1.4, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=40324
2023-01-10 01:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:05:55 - progress_bar.py[line:274] - INFO: epoch 001:   7589 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.156, wps=98.7, ups=0.6, wpb=108.9, bsz=40, num_updates=7580, lr=4.81354e-05, gnorm=0.875, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=40341
2023-01-10 01:05:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:12 - progress_bar.py[line:274] - INFO: epoch 001:   7599 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1495, wps=99.5, ups=0.6, wpb=109.7, bsz=40, num_updates=7590, lr=4.81302e-05, gnorm=1.302, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=40358
2023-01-10 01:06:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:29 - progress_bar.py[line:274] - INFO: epoch 001:   7609 / 100000 loss=0.403, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1, wps=98.2, ups=0.6, wpb=108.7, bsz=40, num_updates=7600, lr=4.8125e-05, gnorm=0.936, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=40375
2023-01-10 01:06:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:45 - progress_bar.py[line:274] - INFO: epoch 001:   7619 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.217, wps=100.6, ups=0.62, wpb=108.9, bsz=40, num_updates=7610, lr=4.81198e-05, gnorm=1.326, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40391
2023-01-10 01:06:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:06:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:01 - progress_bar.py[line:274] - INFO: epoch 001:   7629 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.0947, wps=103.4, ups=0.62, wpb=110.7, bsz=40, num_updates=7620, lr=4.81146e-05, gnorm=1.422, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40408
2023-01-10 01:07:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:18 - progress_bar.py[line:274] - INFO: epoch 001:   7639 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.18, wps=101.7, ups=0.62, wpb=109.5, bsz=40, num_updates=7630, lr=4.81094e-05, gnorm=1.154, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40424
2023-01-10 01:07:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:34 - progress_bar.py[line:274] - INFO: epoch 001:   7649 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1101, wps=98.8, ups=0.61, wpb=107.9, bsz=40, num_updates=7640, lr=4.81042e-05, gnorm=0.757, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40440
2023-01-10 01:07:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:51 - progress_bar.py[line:274] - INFO: epoch 001:   7659 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.0583, wps=100.7, ups=0.6, wpb=111, bsz=40, num_updates=7650, lr=4.8099e-05, gnorm=0.874, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40457
2023-01-10 01:07:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:07:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:07 - progress_bar.py[line:274] - INFO: epoch 001:   7669 / 100000 loss=0.406, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1228, wps=101.5, ups=0.62, wpb=109.1, bsz=40, num_updates=7660, lr=4.80938e-05, gnorm=0.761, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40474
2023-01-10 01:08:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:24 - progress_bar.py[line:274] - INFO: epoch 001:   7679 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1075, wps=102.3, ups=0.62, wpb=109.9, bsz=40, num_updates=7670, lr=4.80885e-05, gnorm=0.903, clip=30, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=40490
2023-01-10 01:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:40 - progress_bar.py[line:274] - INFO: epoch 001:   7689 / 100000 loss=0.396, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.125, wps=98.4, ups=0.61, wpb=108.2, bsz=40, num_updates=7680, lr=4.80833e-05, gnorm=0.869, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40507
2023-01-10 01:08:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:08:57 - progress_bar.py[line:274] - INFO: epoch 001:   7699 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0971, wps=101.5, ups=0.61, wpb=110.7, bsz=40, num_updates=7690, lr=4.80781e-05, gnorm=0.982, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40523
2023-01-10 01:08:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:13 - progress_bar.py[line:274] - INFO: epoch 001:   7709 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.17, wps=102.9, ups=0.62, wpb=111.2, bsz=40, num_updates=7700, lr=4.80729e-05, gnorm=0.571, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=40540
2023-01-10 01:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:30 - progress_bar.py[line:274] - INFO: epoch 001:   7719 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1442, wps=103.6, ups=0.63, wpb=110.3, bsz=40, num_updates=7710, lr=4.80677e-05, gnorm=1.261, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40556
2023-01-10 01:09:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:46 - progress_bar.py[line:274] - INFO: epoch 001:   7729 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1183, wps=101.1, ups=0.61, wpb=110.5, bsz=40, num_updates=7720, lr=4.80625e-05, gnorm=0.759, clip=30, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=40572
2023-01-10 01:09:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:03 - progress_bar.py[line:274] - INFO: epoch 001:   7739 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.25, wps=101.1, ups=0.62, wpb=109.5, bsz=40, num_updates=7730, lr=4.80573e-05, gnorm=1.123, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40589
2023-01-10 01:10:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:19 - progress_bar.py[line:274] - INFO: epoch 001:   7749 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1782, wps=104.2, ups=0.63, wpb=110.3, bsz=40, num_updates=7740, lr=4.80521e-05, gnorm=0.607, clip=10, loss_scale=512, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=40605
2023-01-10 01:10:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:36 - progress_bar.py[line:274] - INFO: epoch 001:   7759 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2136, wps=100.7, ups=0.62, wpb=109, bsz=40, num_updates=7750, lr=4.80469e-05, gnorm=1.311, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40622
2023-01-10 01:10:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:52 - progress_bar.py[line:274] - INFO: epoch 001:   7769 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1134, wps=100.5, ups=0.62, wpb=108.6, bsz=40, num_updates=7760, lr=4.80417e-05, gnorm=1.014, clip=30, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=40638
2023-01-10 01:10:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:10:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:09 - progress_bar.py[line:274] - INFO: epoch 001:   7779 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1217, wps=100.2, ups=0.61, wpb=108.7, bsz=40, num_updates=7770, lr=4.80365e-05, gnorm=0.862, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=40655
2023-01-10 01:11:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:25 - progress_bar.py[line:274] - INFO: epoch 001:   7789 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.172, wps=101.3, ups=0.61, wpb=109.9, bsz=40, num_updates=7780, lr=4.80312e-05, gnorm=0.864, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40671
2023-01-10 01:11:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:42 - progress_bar.py[line:274] - INFO: epoch 001:   7799 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.08, wps=100.1, ups=0.61, wpb=109.1, bsz=40, num_updates=7790, lr=4.8026e-05, gnorm=0.784, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40688
2023-01-10 01:11:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:11:59 - progress_bar.py[line:274] - INFO: epoch 001:   7809 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1441, wps=99, ups=0.6, wpb=109.3, bsz=40, num_updates=7800, lr=4.80208e-05, gnorm=1.146, clip=40, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=40705
2023-01-10 01:11:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:15 - progress_bar.py[line:274] - INFO: epoch 001:   7819 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1224, wps=104.4, ups=0.63, wpb=110.1, bsz=40, num_updates=7810, lr=4.80156e-05, gnorm=0.757, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40721
2023-01-10 01:12:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:31 - progress_bar.py[line:274] - INFO: epoch 001:   7829 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1613, wps=103.2, ups=0.62, wpb=111.9, bsz=40, num_updates=7820, lr=4.80104e-05, gnorm=0.596, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40737
2023-01-10 01:12:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:48 - progress_bar.py[line:274] - INFO: epoch 001:   7839 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1731, wps=98.7, ups=0.6, wpb=109.6, bsz=40, num_updates=7830, lr=4.80052e-05, gnorm=0.867, clip=30, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=40754
2023-01-10 01:12:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:12:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:05 - progress_bar.py[line:274] - INFO: epoch 001:   7849 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.181, wps=99.9, ups=0.61, wpb=108.7, bsz=40, num_updates=7840, lr=4.8e-05, gnorm=0.79, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40771
2023-01-10 01:13:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:21 - progress_bar.py[line:274] - INFO: epoch 001:   7859 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2277, wps=101.4, ups=0.61, wpb=110.3, bsz=40, num_updates=7850, lr=4.79948e-05, gnorm=0.85, clip=30, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40787
2023-01-10 01:13:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:37 - progress_bar.py[line:274] - INFO: epoch 001:   7869 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1959, wps=103.4, ups=0.63, wpb=110, bsz=40, num_updates=7860, lr=4.79896e-05, gnorm=0.825, clip=30, loss_scale=1024, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=40803
2023-01-10 01:13:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:13:53 - progress_bar.py[line:274] - INFO: epoch 001:   7879 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2442, wps=105.7, ups=0.64, wpb=110.7, bsz=40, num_updates=7870, lr=4.79844e-05, gnorm=0.622, clip=10, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40819
2023-01-10 01:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:10 - progress_bar.py[line:274] - INFO: epoch 001:   7889 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.0816, wps=100.7, ups=0.61, wpb=109.7, bsz=40, num_updates=7880, lr=4.79792e-05, gnorm=1.155, clip=30, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=40836
2023-01-10 01:14:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:27 - progress_bar.py[line:274] - INFO: epoch 001:   7899 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1957, wps=99.9, ups=0.6, wpb=110.4, bsz=40, num_updates=7890, lr=4.7974e-05, gnorm=0.846, clip=20, loss_scale=1024, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=40853
2023-01-10 01:14:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:43 - progress_bar.py[line:274] - INFO: epoch 001:   7909 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1441, wps=98.2, ups=0.6, wpb=108.5, bsz=40, num_updates=7900, lr=4.79688e-05, gnorm=0.77, clip=10, loss_scale=1024, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=40870
2023-01-10 01:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:14:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:00 - progress_bar.py[line:274] - INFO: epoch 001:   7919 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.1923, wps=99.7, ups=0.6, wpb=110.1, bsz=40, num_updates=7910, lr=4.79635e-05, gnorm=0.898, clip=40, loss_scale=1024, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=40886
2023-01-10 01:15:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:17 - progress_bar.py[line:274] - INFO: epoch 001:   7929 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=112.333, nsentences=40, sample_size=112.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1728, wps=103.7, ups=0.62, wpb=112.3, bsz=40, num_updates=7920, lr=4.79583e-05, gnorm=0.816, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40903
2023-01-10 01:15:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:33 - progress_bar.py[line:274] - INFO: epoch 001:   7939 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1319, wps=101.1, ups=0.61, wpb=110.1, bsz=40, num_updates=7930, lr=4.79531e-05, gnorm=0.845, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40919
2023-01-10 01:15:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:37 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-10 01:15:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:15:52 - progress_bar.py[line:274] - INFO: epoch 001:   7950 / 100000 loss=0.417, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=108.438, nsentences=40, sample_size=108.438, sample_size_v1=0, sample_size_v2=0, ppl=1.22, vqa_score=0.1805, wps=95.1, ups=0.55, wpb=108.4, bsz=40, num_updates=7940, lr=4.79479e-05, gnorm=1.466, clip=50, loss_scale=512, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=40938
2023-01-10 01:15:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:08 - progress_bar.py[line:274] - INFO: epoch 001:   7960 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.172, wps=100.4, ups=0.61, wpb=110.4, bsz=40, num_updates=7950, lr=4.79427e-05, gnorm=0.866, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40955
2023-01-10 01:16:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:25 - progress_bar.py[line:274] - INFO: epoch 001:   7970 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.25, wps=100.6, ups=0.61, wpb=109.9, bsz=40, num_updates=7960, lr=4.79375e-05, gnorm=1.473, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=40971
2023-01-10 01:16:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:42 - progress_bar.py[line:274] - INFO: epoch 001:   7980 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1368, wps=100.9, ups=0.61, wpb=110.1, bsz=40, num_updates=7970, lr=4.79323e-05, gnorm=1.914, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=40988
2023-01-10 01:16:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:16:58 - progress_bar.py[line:274] - INFO: epoch 001:   7990 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1089, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=7980, lr=4.79271e-05, gnorm=5.049, clip=80, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=41004
2023-01-10 01:17:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:17:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:17:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:17:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:17:15 - progress_bar.py[line:274] - INFO: epoch 001:   8000 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1759, wps=100.9, ups=0.62, wpb=108.8, bsz=40, num_updates=7990, lr=4.79219e-05, gnorm=1.126, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=41021
2023-01-10 01:17:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:17:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:17:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:17:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 01:17:31 - progress_bar.py[line:274] - INFO: epoch 001:   8010 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.16, wps=103.6, ups=0.62, wpb=110.7, bsz=40, num_updates=8000, lr=4.79167e-05, gnorm=0.756, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=41037
2023-01-10 01:17:31 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 01:17:32 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 01:17:32 - train.py[line:551] - INFO: load:1.21 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 01:17:33 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.14 GiB (GPU 1; 39.59 GiB total capacity; 9.28 GiB already allocated; 4.04 GiB free; 33.06 GiB reserved in total by PyTorch)
2023-01-10 01:17:33 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-10 01:17:33 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9501 MB |   10726 MB |    5744 TB |    5744 TB |
|       from large pool |    9327 MB |   10551 MB |    5742 TB |    5742 TB |
|       from small pool |     174 MB |     175 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9501 MB |   10726 MB |    5744 TB |    5744 TB |
|       from large pool |    9327 MB |   10551 MB |    5742 TB |    5742 TB |
|       from small pool |     174 MB |     175 MB |       1 TB |       1 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   33852 MB |   33940 MB |  327688 MB |  293836 MB |
|       from large pool |   33676 MB |   33758 MB |  327298 MB |  293622 MB |
|       from small pool |     176 MB |     182 MB |     390 MB |     214 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24350 MB |   28762 MB |    5820 TB |    5820 TB |
|       from large pool |   24348 MB |   28760 MB |    5818 TB |    5818 TB |
|       from small pool |       1 MB |       3 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4623    |    4637    |  272785 K  |  272780 K  |
|       from large pool |     698    |     710    |   84535 K  |   84534 K  |
|       from small pool |    3925    |    3943    |  188250 K  |  188246 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4623    |    4637    |  272785 K  |  272780 K  |
|       from large pool |     698    |     710    |   84535 K  |   84534 K  |
|       from small pool |    3925    |    3943    |  188250 K  |  188246 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     203    |     209    |     894    |     691    |
|       from large pool |     115    |     118    |     699    |     584    |
|       from small pool |      88    |      91    |     195    |     107    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     129    |     133    |  200566 K  |  200565 K  |
|       from large pool |      67    |      70    |   36255 K  |   36255 K  |
|       from small pool |      62    |      69    |  164310 K  |  164310 K  |
|===========================================================================|

2023-01-10 01:17:33 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-01-10 01:20:05 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 01:20:05 - train.py[line:551] - INFO: load:1.24 valid_run:152.70 task_valid:148.75 collect_output:2.84
2023-01-10 01:22:33 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 01:22:33 - train.py[line:551] - INFO: load:1.26 valid_run:300.67 task_valid:292.07 collect_output:6.48
2023-01-10 01:25:05 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 01:25:05 - train.py[line:551] - INFO: load:1.29 valid_run:452.16 task_valid:435.60 collect_output:13.39
2023-01-10 01:27:34 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 01:27:34 - train.py[line:551] - INFO: load:1.31 valid_run:601.01 task_valid:580.84 collect_output:15.92
2023-01-10 01:30:06 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 01:30:06 - train.py[line:551] - INFO: load:1.34 valid_run:752.93 task_valid:728.47 collect_output:19.23
2023-01-10 01:32:37 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 01:32:37 - train.py[line:551] - INFO: load:1.37 valid_run:904.07 task_valid:874.23 collect_output:23.56
2023-01-10 01:35:09 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 01:35:09 - train.py[line:551] - INFO: load:1.39 valid_run:1056.32 task_valid:1020.38 collect_output:28.65
2023-01-10 01:37:40 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 01:37:40 - train.py[line:551] - INFO: load:1.42 valid_run:1206.59 task_valid:1161.61 collect_output:36.67
2023-01-10 01:40:09 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 01:40:09 - train.py[line:551] - INFO: load:1.44 valid_run:1355.70 task_valid:1306.48 collect_output:39.89
2023-01-10 01:42:37 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 01:42:37 - train.py[line:551] - INFO: load:1.47 valid_run:1503.55 task_valid:1449.64 collect_output:43.59
2023-01-10 01:45:06 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 01:45:06 - train.py[line:551] - INFO: load:1.50 valid_run:1652.56 task_valid:1594.49 collect_output:46.73
2023-01-10 01:47:35 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 01:47:35 - train.py[line:551] - INFO: load:1.53 valid_run:1801.75 task_valid:1739.37 collect_output:50.02
2023-01-10 01:50:04 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 01:50:04 - train.py[line:551] - INFO: load:1.56 valid_run:1951.06 task_valid:1881.56 collect_output:56.10
2023-01-10 01:52:35 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 01:52:35 - train.py[line:551] - INFO: load:1.58 valid_run:2101.23 task_valid:2027.08 collect_output:59.74
2023-01-10 01:55:05 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 01:55:05 - train.py[line:551] - INFO: load:1.61 valid_run:2251.08 task_valid:2173.80 collect_output:61.80
2023-01-10 01:57:34 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 01:57:34 - train.py[line:551] - INFO: load:1.64 valid_run:2400.62 task_valid:2318.15 collect_output:65.96
2023-01-10 02:00:05 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 02:00:05 - train.py[line:551] - INFO: load:1.67 valid_run:2551.48 task_valid:2463.94 collect_output:70.00
2023-01-10 02:02:35 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 02:02:35 - train.py[line:551] - INFO: load:1.69 valid_run:2701.52 task_valid:2610.87 collect_output:72.10
2023-01-10 02:05:03 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 02:05:03 - train.py[line:551] - INFO: load:1.72 valid_run:2848.78 task_valid:2752.33 collect_output:76.90
2023-01-10 02:07:32 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 02:07:32 - train.py[line:551] - INFO: load:1.75 valid_run:2998.13 task_valid:2897.36 collect_output:80.20
2023-01-10 02:10:03 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 02:10:03 - train.py[line:551] - INFO: load:1.77 valid_run:3148.80 task_valid:3042.10 collect_output:85.12
2023-01-10 02:12:32 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 02:12:32 - train.py[line:551] - INFO: load:1.80 valid_run:3297.58 task_valid:3186.68 collect_output:88.30
2023-01-10 02:15:02 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 02:15:02 - train.py[line:551] - INFO: load:1.83 valid_run:3447.81 task_valid:3332.75 collect_output:91.43
2023-01-10 02:17:33 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 02:17:33 - train.py[line:551] - INFO: load:1.86 valid_run:3598.36 task_valid:3479.12 collect_output:94.60

====================================================================================================
SGG eval:     R @ 50: 0.6276;     R @ 100: 0.6703;     R @ 500: 0.7008;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4012;    mR @ 100: 0.4354;    mR @ 500: 0.4766;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8049) (covered in:0.8125) (covering:0.3714) (eating:0.7353) (flying in:0.0000) (growing on:0.3750) (hanging from:0.4516) (lying on:0.2000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9379) (says:0.0000) (sitting on:0.7971) (standing on:0.3243) (using:0.6000) (walking in:0.0000) (walking on:0.7568) (watching:0.3333) 
--------------------------------------------------------
====================================================================================================

2023-01-10 02:20:04 - train.py[line:487] - INFO: 0.6702624649859945
2023-01-10 02:20:04 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])

====================================================================================================
SGG eval:     R @ 50: 0.6276;     R @ 100: 0.6703;     R @ 500: 0.7008;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.4012;    mR @ 100: 0.4354;    mR @ 500: 0.4766;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8049) (covered in:0.8125) (covering:0.3714) (eating:0.7353) (flying in:0.0000) (growing on:0.3750) (hanging from:0.4516) (lying on:0.2000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9379) (says:0.0000) (sitting on:0.7971) (standing on:0.3243) (using:0.6000) (walking in:0.0000) (walking on:0.7568) (watching:0.3333) 
--------------------------------------------------------
====================================================================================================

2023-01-10 02:20:04 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.332 | loss_v1 0 | loss_v2 0 | nll_loss 0.184 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.670262 | ppl 1.14 | vqa_score 0.5833 | wps 119.6 | wpb 89.9 | bsz 30 | num_updates 8000 | best_R@100 0.69005
2023-01-10 02:20:04 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 8000 updates
2023-01-10 02:20:04 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt
2023-01-10 02:20:44 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt
2023-01-10 02:22:10 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_8000.pt (epoch 1 @ 8000 updates, score 0.6702624649859945) (writing took 125.81164534389973 seconds)
2023-01-10 02:22:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:26 - progress_bar.py[line:274] - INFO: epoch 001:   8020 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1596, wps=0.4, ups=0, wpb=109.7, bsz=40, num_updates=8010, lr=4.79115e-05, gnorm=0.944, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=44932
2023-01-10 02:22:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:43 - progress_bar.py[line:274] - INFO: epoch 001:   8030 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1515, wps=103.5, ups=0.62, wpb=111.3, bsz=40, num_updates=8020, lr=4.79063e-05, gnorm=1.104, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=44949
2023-01-10 02:22:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:22:59 - progress_bar.py[line:274] - INFO: epoch 001:   8040 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.099, wps=99.2, ups=0.61, wpb=108.2, bsz=40, num_updates=8030, lr=4.7901e-05, gnorm=0.85, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=44965
2023-01-10 02:23:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:16 - progress_bar.py[line:274] - INFO: epoch 001:   8050 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1304, wps=103.1, ups=0.62, wpb=110.4, bsz=40, num_updates=8040, lr=4.78958e-05, gnorm=0.791, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=44982
2023-01-10 02:23:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:32 - progress_bar.py[line:274] - INFO: epoch 001:   8060 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1607, wps=101.2, ups=0.62, wpb=108.9, bsz=40, num_updates=8050, lr=4.78906e-05, gnorm=1.119, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=44998
2023-01-10 02:23:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:48 - progress_bar.py[line:274] - INFO: epoch 001:   8070 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1565, wps=101, ups=0.62, wpb=108.6, bsz=40, num_updates=8060, lr=4.78854e-05, gnorm=1.599, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45015
2023-01-10 02:23:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:23:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:05 - progress_bar.py[line:274] - INFO: epoch 001:   8080 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1346, wps=99.9, ups=0.61, wpb=109.7, bsz=40, num_updates=8070, lr=4.78802e-05, gnorm=1.299, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45031
2023-01-10 02:24:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:22 - progress_bar.py[line:274] - INFO: epoch 001:   8090 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1596, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=8080, lr=4.7875e-05, gnorm=1.701, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45048
2023-01-10 02:24:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:38 - progress_bar.py[line:274] - INFO: epoch 001:   8100 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1505, wps=102.3, ups=0.62, wpb=110.5, bsz=40, num_updates=8090, lr=4.78698e-05, gnorm=0.59, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45064
2023-01-10 02:24:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:24:55 - progress_bar.py[line:274] - INFO: epoch 001:   8110 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.0952, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=8100, lr=4.78646e-05, gnorm=1.87, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45081
2023-01-10 02:25:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:11 - progress_bar.py[line:274] - INFO: epoch 001:   8120 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.181, wps=98.7, ups=0.6, wpb=109.5, bsz=40, num_updates=8110, lr=4.78594e-05, gnorm=0.854, clip=30, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=45098
2023-01-10 02:25:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:28 - progress_bar.py[line:274] - INFO: epoch 001:   8130 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1346, wps=101.6, ups=0.61, wpb=110.4, bsz=40, num_updates=8120, lr=4.78542e-05, gnorm=0.876, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45114
2023-01-10 02:25:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:45 - progress_bar.py[line:274] - INFO: epoch 001:   8140 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1932, wps=99.1, ups=0.6, wpb=110.7, bsz=40, num_updates=8130, lr=4.7849e-05, gnorm=1.1, clip=40, loss_scale=512, train_wall=17, gb_free=9.4, ema_decay=0.9999, wall=45131
2023-01-10 02:25:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:25:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:01 - progress_bar.py[line:274] - INFO: epoch 001:   8150 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1134, wps=102.7, ups=0.62, wpb=110.6, bsz=40, num_updates=8140, lr=4.78438e-05, gnorm=0.675, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45148
2023-01-10 02:26:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:18 - progress_bar.py[line:274] - INFO: epoch 001:   8160 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1744, wps=103.6, ups=0.62, wpb=111.2, bsz=40, num_updates=8150, lr=4.78385e-05, gnorm=1.373, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45164
2023-01-10 02:26:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:34 - progress_bar.py[line:274] - INFO: epoch 001:   8170 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.198, wps=100.2, ups=0.61, wpb=109.5, bsz=40, num_updates=8160, lr=4.78333e-05, gnorm=0.782, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45180
2023-01-10 02:26:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:51 - progress_bar.py[line:274] - INFO: epoch 001:   8180 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.23, wps=101, ups=0.62, wpb=109.3, bsz=40, num_updates=8170, lr=4.78281e-05, gnorm=1.184, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45197
2023-01-10 02:26:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:26:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:07 - progress_bar.py[line:274] - INFO: epoch 001:   8190 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1702, wps=105.1, ups=0.64, wpb=109.8, bsz=40, num_updates=8180, lr=4.78229e-05, gnorm=0.936, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=45213
2023-01-10 02:27:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:23 - progress_bar.py[line:274] - INFO: epoch 001:   8200 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1905, wps=99.8, ups=0.61, wpb=108.3, bsz=40, num_updates=8190, lr=4.78177e-05, gnorm=0.668, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45229
2023-01-10 02:27:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:40 - progress_bar.py[line:274] - INFO: epoch 001:   8210 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1538, wps=98.5, ups=0.6, wpb=108.9, bsz=40, num_updates=8200, lr=4.78125e-05, gnorm=1.082, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=45246
2023-01-10 02:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:27:56 - progress_bar.py[line:274] - INFO: epoch 001:   8220 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1818, wps=102.6, ups=0.63, wpb=108.5, bsz=40, num_updates=8210, lr=4.78073e-05, gnorm=0.997, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=45262
2023-01-10 02:28:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:12 - progress_bar.py[line:274] - INFO: epoch 001:   8230 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1633, wps=102.4, ups=0.62, wpb=111, bsz=40, num_updates=8220, lr=4.78021e-05, gnorm=0.809, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45279
2023-01-10 02:28:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:29 - progress_bar.py[line:274] - INFO: epoch 001:   8240 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1376, wps=100.9, ups=0.62, wpb=109.1, bsz=40, num_updates=8230, lr=4.77969e-05, gnorm=0.686, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45295
2023-01-10 02:28:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:46 - progress_bar.py[line:274] - INFO: epoch 001:   8250 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.219, wps=100.8, ups=0.61, wpb=110.2, bsz=40, num_updates=8240, lr=4.77917e-05, gnorm=1.824, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45312
2023-01-10 02:28:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:28:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:02 - progress_bar.py[line:274] - INFO: epoch 001:   8260 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2211, wps=102, ups=0.61, wpb=110.9, bsz=40, num_updates=8250, lr=4.77865e-05, gnorm=0.779, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=45328
2023-01-10 02:29:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:18 - progress_bar.py[line:274] - INFO: epoch 001:   8270 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.125, wps=101.8, ups=0.62, wpb=109.7, bsz=40, num_updates=8260, lr=4.77812e-05, gnorm=0.564, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45345
2023-01-10 02:29:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:35 - progress_bar.py[line:274] - INFO: epoch 001:   8280 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1633, wps=98.7, ups=0.6, wpb=110.2, bsz=40, num_updates=8270, lr=4.7776e-05, gnorm=1.232, clip=30, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=45362
2023-01-10 02:29:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:29:52 - progress_bar.py[line:274] - INFO: epoch 001:   8290 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1651, wps=99.3, ups=0.61, wpb=108.7, bsz=40, num_updates=8280, lr=4.77708e-05, gnorm=0.943, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45378
2023-01-10 02:29:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:09 - progress_bar.py[line:274] - INFO: epoch 001:   8300 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.186, wps=101.2, ups=0.61, wpb=110.7, bsz=40, num_updates=8290, lr=4.77656e-05, gnorm=0.702, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=45395
2023-01-10 02:30:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:26 - progress_bar.py[line:274] - INFO: epoch 001:   8310 / 100000 loss=0.394, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2277, wps=97, ups=0.6, wpb=107.9, bsz=40, num_updates=8300, lr=4.77604e-05, gnorm=1.243, clip=50, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=45412
2023-01-10 02:30:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:42 - progress_bar.py[line:274] - INFO: epoch 001:   8320 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1835, wps=99.7, ups=0.61, wpb=108.8, bsz=40, num_updates=8310, lr=4.77552e-05, gnorm=0.849, clip=30, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=45428
2023-01-10 02:30:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:30:59 - progress_bar.py[line:274] - INFO: epoch 001:   8330 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1087, wps=101, ups=0.6, wpb=111.3, bsz=40, num_updates=8320, lr=4.775e-05, gnorm=0.98, clip=50, loss_scale=512, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=45445
2023-01-10 02:31:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:15 - progress_bar.py[line:274] - INFO: epoch 001:   8340 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1277, wps=104.1, ups=0.63, wpb=109.9, bsz=40, num_updates=8330, lr=4.77448e-05, gnorm=0.881, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45461
2023-01-10 02:31:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:32 - progress_bar.py[line:274] - INFO: epoch 001:   8350 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.13, wps=98.4, ups=0.6, wpb=109.9, bsz=40, num_updates=8340, lr=4.77396e-05, gnorm=1.426, clip=30, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=45478
2023-01-10 02:31:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:49 - progress_bar.py[line:274] - INFO: epoch 001:   8360 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1758, wps=100.9, ups=0.61, wpb=110.9, bsz=40, num_updates=8350, lr=4.77344e-05, gnorm=0.773, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=45495
2023-01-10 02:31:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:31:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:05 - progress_bar.py[line:274] - INFO: epoch 001:   8370 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1522, wps=100.7, ups=0.62, wpb=109.1, bsz=40, num_updates=8360, lr=4.77292e-05, gnorm=0.983, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=45511
2023-01-10 02:32:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:21 - progress_bar.py[line:274] - INFO: epoch 001:   8380 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2396, wps=104.8, ups=0.63, wpb=110.9, bsz=40, num_updates=8370, lr=4.7724e-05, gnorm=1.144, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45527
2023-01-10 02:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:38 - progress_bar.py[line:274] - INFO: epoch 001:   8390 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1553, wps=98.9, ups=0.6, wpb=109.2, bsz=40, num_updates=8380, lr=4.77187e-05, gnorm=0.604, clip=0, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=45544
2023-01-10 02:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:32:54 - progress_bar.py[line:274] - INFO: epoch 001:   8400 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1546, wps=103.6, ups=0.62, wpb=110.5, bsz=40, num_updates=8390, lr=4.77135e-05, gnorm=0.949, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45560
2023-01-10 02:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:11 - progress_bar.py[line:274] - INFO: epoch 001:   8410 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1531, wps=98.9, ups=0.6, wpb=109.7, bsz=40, num_updates=8400, lr=4.77083e-05, gnorm=0.992, clip=40, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=45577
2023-01-10 02:33:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:28 - progress_bar.py[line:274] - INFO: epoch 001:   8420 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2143, wps=100.9, ups=0.61, wpb=109.7, bsz=40, num_updates=8410, lr=4.77031e-05, gnorm=0.771, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45594
2023-01-10 02:33:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:44 - progress_bar.py[line:274] - INFO: epoch 001:   8430 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1458, wps=101, ups=0.61, wpb=110.9, bsz=40, num_updates=8420, lr=4.76979e-05, gnorm=0.967, clip=40, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=45611
2023-01-10 02:33:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:33:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:01 - progress_bar.py[line:274] - INFO: epoch 001:   8440 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1827, wps=98.9, ups=0.61, wpb=107.9, bsz=40, num_updates=8430, lr=4.76927e-05, gnorm=0.851, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45627
2023-01-10 02:34:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:17 - progress_bar.py[line:274] - INFO: epoch 001:   8450 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.198, wps=102.4, ups=0.62, wpb=109.5, bsz=40, num_updates=8440, lr=4.76875e-05, gnorm=1.135, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45643
2023-01-10 02:34:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:34 - progress_bar.py[line:274] - INFO: epoch 001:   8460 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2043, wps=100.2, ups=0.6, wpb=110.5, bsz=40, num_updates=8450, lr=4.76823e-05, gnorm=0.658, clip=10, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45660
2023-01-10 02:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:51 - progress_bar.py[line:274] - INFO: epoch 001:   8470 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1913, wps=99.3, ups=0.61, wpb=107.8, bsz=40, num_updates=8460, lr=4.76771e-05, gnorm=1.641, clip=50, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45677
2023-01-10 02:34:54 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-10 02:34:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:34:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:08 - progress_bar.py[line:274] - INFO: epoch 001:   8481 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2268, wps=95.1, ups=0.58, wpb=109, bsz=40, num_updates=8470, lr=4.76719e-05, gnorm=0.936, clip=20, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=45694
2023-01-10 02:35:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:25 - progress_bar.py[line:274] - INFO: epoch 001:   8491 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2636, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=8480, lr=4.76667e-05, gnorm=1.32, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=45711
2023-01-10 02:35:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:41 - progress_bar.py[line:274] - INFO: epoch 001:   8501 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1789, wps=101.9, ups=0.62, wpb=109.9, bsz=40, num_updates=8490, lr=4.76615e-05, gnorm=1.002, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45727
2023-01-10 02:35:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:35:57 - progress_bar.py[line:274] - INFO: epoch 001:   8511 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.243, wps=100.7, ups=0.62, wpb=108.9, bsz=40, num_updates=8500, lr=4.76563e-05, gnorm=1.053, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45744
2023-01-10 02:36:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:14 - progress_bar.py[line:274] - INFO: epoch 001:   8521 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2037, wps=100.8, ups=0.61, wpb=109.6, bsz=40, num_updates=8510, lr=4.7651e-05, gnorm=0.833, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45760
2023-01-10 02:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:31 - progress_bar.py[line:274] - INFO: epoch 001:   8531 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1154, wps=98.8, ups=0.6, wpb=108.9, bsz=40, num_updates=8520, lr=4.76458e-05, gnorm=1.114, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=45777
2023-01-10 02:36:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:47 - progress_bar.py[line:274] - INFO: epoch 001:   8541 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1698, wps=103, ups=0.63, wpb=108.7, bsz=40, num_updates=8530, lr=4.76406e-05, gnorm=1.597, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45793
2023-01-10 02:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:36:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:04 - progress_bar.py[line:274] - INFO: epoch 001:   8551 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2088, wps=99.5, ups=0.61, wpb=109.5, bsz=40, num_updates=8540, lr=4.76354e-05, gnorm=1.392, clip=30, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=45810
2023-01-10 02:37:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:20 - progress_bar.py[line:274] - INFO: epoch 001:   8561 / 100000 loss=0.393, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1842, wps=104.1, ups=0.63, wpb=110.1, bsz=40, num_updates=8550, lr=4.76302e-05, gnorm=0.959, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=45826
2023-01-10 02:37:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:36 - progress_bar.py[line:274] - INFO: epoch 001:   8571 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2353, wps=101.3, ups=0.61, wpb=110.7, bsz=40, num_updates=8560, lr=4.7625e-05, gnorm=0.657, clip=20, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=45843
2023-01-10 02:37:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:37:53 - progress_bar.py[line:274] - INFO: epoch 001:   8581 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1262, wps=99.4, ups=0.61, wpb=109.4, bsz=40, num_updates=8570, lr=4.76198e-05, gnorm=0.694, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45859
2023-01-10 02:37:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:10 - progress_bar.py[line:274] - INFO: epoch 001:   8591 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1188, wps=99.2, ups=0.61, wpb=108.3, bsz=40, num_updates=8580, lr=4.76146e-05, gnorm=0.855, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45876
2023-01-10 02:38:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:26 - progress_bar.py[line:274] - INFO: epoch 001:   8601 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2347, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=8590, lr=4.76094e-05, gnorm=0.845, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45892
2023-01-10 02:38:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:42 - progress_bar.py[line:274] - INFO: epoch 001:   8611 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1868, wps=105.5, ups=0.63, wpb=111.5, bsz=40, num_updates=8600, lr=4.76042e-05, gnorm=0.663, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=45908
2023-01-10 02:38:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:38:59 - progress_bar.py[line:274] - INFO: epoch 001:   8621 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1696, wps=101.8, ups=0.62, wpb=109.5, bsz=40, num_updates=8610, lr=4.7599e-05, gnorm=1.408, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45925
2023-01-10 02:39:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:15 - progress_bar.py[line:274] - INFO: epoch 001:   8631 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.233, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1667, wps=102.8, ups=0.62, wpb=110.8, bsz=40, num_updates=8620, lr=4.75938e-05, gnorm=0.967, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=45941
2023-01-10 02:39:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:31 - progress_bar.py[line:274] - INFO: epoch 001:   8641 / 100000 loss=0.39, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1944, wps=101.6, ups=0.62, wpb=108.3, bsz=40, num_updates=8630, lr=4.75885e-05, gnorm=1.185, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=45957
2023-01-10 02:39:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:48 - progress_bar.py[line:274] - INFO: epoch 001:   8651 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2, wps=98.6, ups=0.6, wpb=109.8, bsz=40, num_updates=8640, lr=4.75833e-05, gnorm=0.436, clip=0, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=45974
2023-01-10 02:39:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:39:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:05 - progress_bar.py[line:274] - INFO: epoch 001:   8661 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2222, wps=102.5, ups=0.62, wpb=110.3, bsz=40, num_updates=8650, lr=4.75781e-05, gnorm=0.69, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=45991
2023-01-10 02:40:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:21 - progress_bar.py[line:274] - INFO: epoch 001:   8671 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2524, wps=100.4, ups=0.61, wpb=110.5, bsz=40, num_updates=8660, lr=4.75729e-05, gnorm=0.999, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46008
2023-01-10 02:40:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:38 - progress_bar.py[line:274] - INFO: epoch 001:   8681 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1714, wps=98.7, ups=0.61, wpb=108.7, bsz=40, num_updates=8670, lr=4.75677e-05, gnorm=0.772, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46024
2023-01-10 02:40:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:40:54 - progress_bar.py[line:274] - INFO: epoch 001:   8691 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1224, wps=102.2, ups=0.62, wpb=110.5, bsz=40, num_updates=8680, lr=4.75625e-05, gnorm=0.852, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46041
2023-01-10 02:40:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:11 - progress_bar.py[line:274] - INFO: epoch 001:   8701 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2407, wps=99.6, ups=0.61, wpb=109.3, bsz=40, num_updates=8690, lr=4.75573e-05, gnorm=1.355, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=46057
2023-01-10 02:41:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:27 - progress_bar.py[line:274] - INFO: epoch 001:   8711 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.198, wps=103.6, ups=0.63, wpb=110.1, bsz=40, num_updates=8700, lr=4.75521e-05, gnorm=1.556, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46074
2023-01-10 02:41:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:44 - progress_bar.py[line:274] - INFO: epoch 001:   8721 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.191, wps=101.3, ups=0.61, wpb=111.1, bsz=40, num_updates=8710, lr=4.75469e-05, gnorm=1.427, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=46090
2023-01-10 02:41:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:41:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:01 - progress_bar.py[line:274] - INFO: epoch 001:   8731 / 100000 loss=0.398, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1712, wps=99.2, ups=0.61, wpb=108.5, bsz=40, num_updates=8720, lr=4.75417e-05, gnorm=0.781, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46107
2023-01-10 02:42:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:17 - progress_bar.py[line:274] - INFO: epoch 001:   8741 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2421, wps=101.6, ups=0.62, wpb=109.6, bsz=40, num_updates=8730, lr=4.75365e-05, gnorm=1.075, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46123
2023-01-10 02:42:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:33 - progress_bar.py[line:274] - INFO: epoch 001:   8751 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2234, wps=105.4, ups=0.63, wpb=112.2, bsz=40, num_updates=8740, lr=4.75313e-05, gnorm=1.351, clip=30, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=46140
2023-01-10 02:42:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:50 - progress_bar.py[line:274] - INFO: epoch 001:   8761 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2277, wps=102.2, ups=0.62, wpb=110.1, bsz=40, num_updates=8750, lr=4.7526e-05, gnorm=1.209, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=46156
2023-01-10 02:42:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:42:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:06 - progress_bar.py[line:274] - INFO: epoch 001:   8771 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.28, wps=100.1, ups=0.61, wpb=109.7, bsz=40, num_updates=8760, lr=4.75208e-05, gnorm=1.264, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46173
2023-01-10 02:43:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:23 - progress_bar.py[line:274] - INFO: epoch 001:   8781 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2222, wps=101, ups=0.61, wpb=110.1, bsz=40, num_updates=8770, lr=4.75156e-05, gnorm=0.679, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46189
2023-01-10 02:43:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:39 - progress_bar.py[line:274] - INFO: epoch 001:   8791 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.177, wps=99.8, ups=0.61, wpb=108.4, bsz=40, num_updates=8780, lr=4.75104e-05, gnorm=0.603, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46206
2023-01-10 02:43:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:43:56 - progress_bar.py[line:274] - INFO: epoch 001:   8801 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.19, wps=103.2, ups=0.62, wpb=111.6, bsz=40, num_updates=8790, lr=4.75052e-05, gnorm=0.913, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46222
2023-01-10 02:44:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:12 - progress_bar.py[line:274] - INFO: epoch 001:   8811 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1982, wps=101.6, ups=0.62, wpb=109.4, bsz=40, num_updates=8800, lr=4.75e-05, gnorm=1.066, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46238
2023-01-10 02:44:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:29 - progress_bar.py[line:274] - INFO: epoch 001:   8821 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2451, wps=99.4, ups=0.6, wpb=109.7, bsz=40, num_updates=8810, lr=4.74948e-05, gnorm=0.731, clip=10, loss_scale=512, train_wall=17, gb_free=9.5, ema_decay=0.9999, wall=46255
2023-01-10 02:44:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:46 - progress_bar.py[line:274] - INFO: epoch 001:   8831 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1909, wps=100.2, ups=0.61, wpb=108.9, bsz=40, num_updates=8820, lr=4.74896e-05, gnorm=1.085, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=46272
2023-01-10 02:44:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:02 - progress_bar.py[line:274] - INFO: epoch 001:   8841 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1753, wps=99.3, ups=0.61, wpb=109.1, bsz=40, num_updates=8830, lr=4.74844e-05, gnorm=0.796, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=46288
2023-01-10 02:45:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:19 - progress_bar.py[line:274] - INFO: epoch 001:   8851 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2604, wps=101.7, ups=0.62, wpb=109.5, bsz=40, num_updates=8840, lr=4.74792e-05, gnorm=0.566, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46305
2023-01-10 02:45:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:35 - progress_bar.py[line:274] - INFO: epoch 001:   8861 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2385, wps=98.6, ups=0.6, wpb=108.7, bsz=40, num_updates=8850, lr=4.7474e-05, gnorm=0.653, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46322
2023-01-10 02:45:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:52 - progress_bar.py[line:274] - INFO: epoch 001:   8871 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1927, wps=100.2, ups=0.61, wpb=109.8, bsz=40, num_updates=8860, lr=4.74687e-05, gnorm=0.832, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46338
2023-01-10 02:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:45:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:09 - progress_bar.py[line:274] - INFO: epoch 001:   8881 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.25, wps=101.8, ups=0.62, wpb=109.6, bsz=40, num_updates=8870, lr=4.74635e-05, gnorm=1.018, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46355
2023-01-10 02:46:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:25 - progress_bar.py[line:274] - INFO: epoch 001:   8891 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2174, wps=101.6, ups=0.62, wpb=109.7, bsz=40, num_updates=8880, lr=4.74583e-05, gnorm=0.58, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=46371
2023-01-10 02:46:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:42 - progress_bar.py[line:274] - INFO: epoch 001:   8901 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2752, wps=97.6, ups=0.6, wpb=108.1, bsz=40, num_updates=8890, lr=4.74531e-05, gnorm=1.179, clip=30, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=46388
2023-01-10 02:46:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:46:58 - progress_bar.py[line:274] - INFO: epoch 001:   8911 / 100000 loss=0.392, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2233, wps=101.5, ups=0.62, wpb=109.7, bsz=40, num_updates=8900, lr=4.74479e-05, gnorm=0.829, clip=30, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=46404
2023-01-10 02:47:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:15 - progress_bar.py[line:274] - INFO: epoch 001:   8921 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1947, wps=101.8, ups=0.62, wpb=109.2, bsz=40, num_updates=8910, lr=4.74427e-05, gnorm=0.712, clip=20, loss_scale=512, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=46421
2023-01-10 02:47:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:31 - progress_bar.py[line:274] - INFO: epoch 001:   8931 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2451, wps=102.2, ups=0.62, wpb=110.3, bsz=40, num_updates=8920, lr=4.74375e-05, gnorm=0.619, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=46437
2023-01-10 02:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:48 - progress_bar.py[line:274] - INFO: epoch 001:   8941 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1782, wps=101.4, ups=0.62, wpb=109.5, bsz=40, num_updates=8930, lr=4.74323e-05, gnorm=0.825, clip=30, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=46454
2023-01-10 02:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:47:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:04 - progress_bar.py[line:274] - INFO: epoch 001:   8951 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.181, wps=101.5, ups=0.62, wpb=109.1, bsz=40, num_updates=8940, lr=4.74271e-05, gnorm=0.762, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46470
2023-01-10 02:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:21 - progress_bar.py[line:274] - INFO: epoch 001:   8961 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2083, wps=100.4, ups=0.61, wpb=109.6, bsz=40, num_updates=8950, lr=4.74219e-05, gnorm=0.569, clip=10, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=46487
2023-01-10 02:48:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:37 - progress_bar.py[line:274] - INFO: epoch 001:   8971 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1538, wps=103.1, ups=0.62, wpb=110.3, bsz=40, num_updates=8960, lr=4.74167e-05, gnorm=0.604, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=46503
2023-01-10 02:48:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:53 - progress_bar.py[line:274] - INFO: epoch 001:   8981 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2967, wps=105.7, ups=0.63, wpb=111.2, bsz=40, num_updates=8970, lr=4.74115e-05, gnorm=1.026, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=46519
2023-01-10 02:48:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:48:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:03 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-10 02:49:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:10 - progress_bar.py[line:274] - INFO: epoch 001:   8992 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.1875, wps=94.1, ups=0.58, wpb=108.7, bsz=40, num_updates=8980, lr=4.74063e-05, gnorm=0.56, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=46537
2023-01-10 02:49:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:27 - progress_bar.py[line:274] - INFO: epoch 001:   9002 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1456, wps=103.1, ups=0.62, wpb=110.2, bsz=40, num_updates=8990, lr=4.7401e-05, gnorm=1.376, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=46553
2023-01-10 02:49:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 02:49:43 - progress_bar.py[line:274] - INFO: epoch 001:   9012 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2793, wps=100.8, ups=0.61, wpb=109.4, bsz=40, num_updates=9000, lr=4.73958e-05, gnorm=1.018, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=46569
2023-01-10 02:49:43 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 02:49:45 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 02:49:45 - train.py[line:551] - INFO: load:1.26 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 02:52:16 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 02:52:16 - train.py[line:551] - INFO: load:1.28 valid_run:151.61 task_valid:148.26 collect_output:2.27
2023-01-10 02:54:45 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 02:54:45 - train.py[line:551] - INFO: load:1.31 valid_run:300.05 task_valid:291.72 collect_output:6.22
2023-01-10 02:57:17 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 02:57:17 - train.py[line:551] - INFO: load:1.33 valid_run:452.31 task_valid:435.19 collect_output:13.97
2023-01-10 02:59:46 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 02:59:46 - train.py[line:551] - INFO: load:1.36 valid_run:601.10 task_valid:580.50 collect_output:16.38
2023-01-10 03:02:18 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 03:02:18 - train.py[line:551] - INFO: load:1.39 valid_run:752.68 task_valid:728.13 collect_output:19.27
2023-01-10 03:04:50 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 03:04:50 - train.py[line:551] - INFO: load:1.43 valid_run:904.56 task_valid:874.74 collect_output:23.29
2023-01-10 03:07:22 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 03:07:22 - train.py[line:551] - INFO: load:1.46 valid_run:1056.65 task_valid:1021.09 collect_output:27.98
2023-01-10 03:09:52 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 03:09:52 - train.py[line:551] - INFO: load:1.49 valid_run:1207.05 task_valid:1162.86 collect_output:35.51
2023-01-10 03:12:21 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 03:12:21 - train.py[line:551] - INFO: load:1.52 valid_run:1356.02 task_valid:1307.77 collect_output:38.51
2023-01-10 03:14:50 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 03:14:50 - train.py[line:551] - INFO: load:1.54 valid_run:1504.11 task_valid:1451.26 collect_output:42.08
2023-01-10 03:17:19 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 03:17:19 - train.py[line:551] - INFO: load:1.57 valid_run:1653.69 task_valid:1596.72 collect_output:45.10
2023-01-10 03:19:49 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 03:19:49 - train.py[line:551] - INFO: load:1.60 valid_run:1803.09 task_valid:1741.91 collect_output:48.28
2023-01-10 03:22:18 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 03:22:18 - train.py[line:551] - INFO: load:1.63 valid_run:1951.95 task_valid:1884.14 collect_output:53.82
2023-01-10 03:24:48 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 03:24:48 - train.py[line:551] - INFO: load:1.66 valid_run:2102.18 task_valid:2030.06 collect_output:57.12
2023-01-10 03:27:18 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 03:27:18 - train.py[line:551] - INFO: load:1.69 valid_run:2251.80 task_valid:2176.61 collect_output:59.15
2023-01-10 03:29:47 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 03:29:47 - train.py[line:551] - INFO: load:1.72 valid_run:2401.31 task_valid:2321.17 collect_output:63.04
2023-01-10 03:32:18 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 03:32:18 - train.py[line:551] - INFO: load:1.75 valid_run:2551.84 task_valid:2466.76 collect_output:66.96
2023-01-10 03:34:48 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 03:34:48 - train.py[line:551] - INFO: load:1.78 valid_run:2702.15 task_valid:2613.91 collect_output:69.09
2023-01-10 03:37:16 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 03:37:16 - train.py[line:551] - INFO: load:1.81 valid_run:2849.65 task_valid:2755.55 collect_output:73.90
2023-01-10 03:39:45 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 03:39:45 - train.py[line:551] - INFO: load:1.83 valid_run:2999.02 task_valid:2900.67 collect_output:77.11
2023-01-10 03:42:16 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 03:42:16 - train.py[line:551] - INFO: load:1.86 valid_run:3149.54 task_valid:3045.43 collect_output:81.84
2023-01-10 03:44:45 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 03:44:45 - train.py[line:551] - INFO: load:1.89 valid_run:3298.46 task_valid:3190.35 collect_output:84.81
2023-01-10 03:47:15 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 03:47:15 - train.py[line:551] - INFO: load:1.92 valid_run:3448.70 task_valid:3336.70 collect_output:87.68
2023-01-10 03:49:46 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 03:49:46 - train.py[line:551] - INFO: load:1.95 valid_run:3599.39 task_valid:3483.42 collect_output:90.62

====================================================================================================
SGG eval:     R @ 50: 0.6236;     R @ 100: 0.6660;     R @ 500: 0.7006;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3978;    mR @ 100: 0.4338;    mR @ 500: 0.4741;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8049) (covered in:0.8750) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4839) (lying on:0.2000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9239) (says:0.0000) (sitting on:0.7834) (standing on:0.3143) (using:0.6000) (walking in:0.0000) (walking on:0.7658) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6236;     R @ 100: 0.6660;     R @ 500: 0.7006;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3978;    mR @ 100: 0.4338;    mR @ 500: 0.4741;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8049) (covered in:0.8750) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4839) (lying on:0.2000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.9583) (playing:0.0000) (riding:0.9239) (says:0.0000) (sitting on:0.7834) (standing on:0.3143) (using:0.6000) (walking in:0.0000) (walking on:0.7658) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-10 03:52:17 - train.py[line:487] - INFO: 0.6660148459383752
2023-01-10 03:52:17 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 03:52:18 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.327 | loss_v1 0 | loss_v2 0 | nll_loss 0.177 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.666015 | ppl 1.13 | vqa_score 0.5867 | wps 119.5 | wpb 89.9 | bsz 30 | num_updates 9000 | best_R@100 0.69005
2023-01-10 03:52:18 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 9000 updates
2023-01-10 03:52:18 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_9000.pt
2023-01-10 03:53:04 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_9000.pt
2023-01-10 03:54:34 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_9000.pt (epoch 1 @ 9000 updates, score 0.6660148459383752) (writing took 135.95625871419907 seconds)
2023-01-10 03:54:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:54:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:54:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:54:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:54:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:54:51 - progress_bar.py[line:274] - INFO: epoch 001:   9022 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2021, wps=0.4, ups=0, wpb=109.9, bsz=40, num_updates=9010, lr=4.73906e-05, gnorm=1.736, clip=60, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=50476
2023-01-10 03:54:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:54:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:54:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:09 - progress_bar.py[line:274] - INFO: epoch 001:   9032 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2655, wps=99, ups=0.6, wpb=109.5, bsz=40, num_updates=9020, lr=4.73854e-05, gnorm=0.942, clip=50, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=50494
2023-01-10 03:55:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:26 - progress_bar.py[line:274] - INFO: epoch 001:   9042 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2584, wps=100.3, ups=0.61, wpb=109.2, bsz=40, num_updates=9030, lr=4.73802e-05, gnorm=1.04, clip=60, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=50511
2023-01-10 03:55:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:43 - progress_bar.py[line:274] - INFO: epoch 001:   9052 / 100000 loss=0.4, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.1712, wps=97.9, ups=0.6, wpb=108.8, bsz=40, num_updates=9040, lr=4.7375e-05, gnorm=0.701, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=50529
2023-01-10 03:55:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:00 - progress_bar.py[line:274] - INFO: epoch 001:   9062 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2828, wps=100.4, ups=0.61, wpb=109.5, bsz=40, num_updates=9050, lr=4.73698e-05, gnorm=0.906, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=50546
2023-01-10 03:56:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:18 - progress_bar.py[line:274] - INFO: epoch 001:   9072 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2929, wps=98.8, ups=0.61, wpb=108.3, bsz=40, num_updates=9060, lr=4.73646e-05, gnorm=1.029, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=50563
2023-01-10 03:56:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:35 - progress_bar.py[line:274] - INFO: epoch 001:   9082 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1875, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=9070, lr=4.73594e-05, gnorm=1.345, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=50581
2023-01-10 03:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:52 - progress_bar.py[line:274] - INFO: epoch 001:   9092 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1982, wps=98.1, ups=0.6, wpb=108.3, bsz=40, num_updates=9080, lr=4.73542e-05, gnorm=0.886, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=50598
2023-01-10 03:56:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:56:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:10 - progress_bar.py[line:274] - INFO: epoch 001:   9102 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2553, wps=100.4, ups=0.61, wpb=110.5, bsz=40, num_updates=9090, lr=4.7349e-05, gnorm=1.186, clip=50, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=50615
2023-01-10 03:57:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:27 - progress_bar.py[line:274] - INFO: epoch 001:   9112 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2162, wps=98.4, ups=0.6, wpb=108.8, bsz=40, num_updates=9100, lr=4.73438e-05, gnorm=0.612, clip=10, loss_scale=512, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=50633
2023-01-10 03:57:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:44 - progress_bar.py[line:274] - INFO: epoch 001:   9122 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2243, wps=100.5, ups=0.61, wpb=109.3, bsz=40, num_updates=9110, lr=4.73385e-05, gnorm=0.611, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=50650
2023-01-10 03:57:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:57:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:01 - progress_bar.py[line:274] - INFO: epoch 001:   9132 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2308, wps=99.1, ups=0.6, wpb=109.7, bsz=40, num_updates=9120, lr=4.73333e-05, gnorm=0.976, clip=40, loss_scale=512, train_wall=17, gb_free=9.6, ema_decay=0.9999, wall=50667
2023-01-10 03:58:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:18 - progress_bar.py[line:274] - INFO: epoch 001:   9142 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.234, wps=99, ups=0.59, wpb=111.3, bsz=40, num_updates=9130, lr=4.73281e-05, gnorm=0.652, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=50684
2023-01-10 03:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:35 - progress_bar.py[line:274] - INFO: epoch 001:   9152 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2621, wps=100.9, ups=0.61, wpb=110.4, bsz=40, num_updates=9140, lr=4.73229e-05, gnorm=0.855, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=50701
2023-01-10 03:58:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:52 - progress_bar.py[line:274] - INFO: epoch 001:   9162 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.1919, wps=101.1, ups=0.6, wpb=111.9, bsz=40, num_updates=9150, lr=4.73177e-05, gnorm=1.301, clip=60, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=50718
2023-01-10 03:58:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:58:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:08 - progress_bar.py[line:274] - INFO: epoch 001:   9172 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2203, wps=104.1, ups=0.63, wpb=109.4, bsz=40, num_updates=9160, lr=4.73125e-05, gnorm=0.814, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=50734
2023-01-10 03:59:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:25 - progress_bar.py[line:274] - INFO: epoch 001:   9182 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2717, wps=101.1, ups=0.62, wpb=108.9, bsz=40, num_updates=9170, lr=4.73073e-05, gnorm=0.923, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=50751
2023-01-10 03:59:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:41 - progress_bar.py[line:274] - INFO: epoch 001:   9192 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1714, wps=100.5, ups=0.62, wpb=108.1, bsz=40, num_updates=9180, lr=4.73021e-05, gnorm=0.88, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=50767
2023-01-10 03:59:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 03:59:58 - progress_bar.py[line:274] - INFO: epoch 001:   9202 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1414, wps=102.4, ups=0.62, wpb=109.9, bsz=40, num_updates=9190, lr=4.72969e-05, gnorm=0.77, clip=30, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=50784
2023-01-10 04:00:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:14 - progress_bar.py[line:274] - INFO: epoch 001:   9212 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2336, wps=102.9, ups=0.63, wpb=109, bsz=40, num_updates=9200, lr=4.72917e-05, gnorm=0.703, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=50800
2023-01-10 04:00:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:31 - progress_bar.py[line:274] - INFO: epoch 001:   9222 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1982, wps=98.7, ups=0.61, wpb=107.7, bsz=40, num_updates=9210, lr=4.72865e-05, gnorm=0.732, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=50817
2023-01-10 04:00:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:47 - progress_bar.py[line:274] - INFO: epoch 001:   9232 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2897, wps=99.7, ups=0.61, wpb=108.9, bsz=40, num_updates=9220, lr=4.72813e-05, gnorm=1.123, clip=40, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=50833
2023-01-10 04:00:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:00:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:04 - progress_bar.py[line:274] - INFO: epoch 001:   9242 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2812, wps=100.5, ups=0.61, wpb=109.6, bsz=40, num_updates=9230, lr=4.7276e-05, gnorm=1.239, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=50850
2023-01-10 04:01:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:20 - progress_bar.py[line:274] - INFO: epoch 001:   9252 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2136, wps=101.6, ups=0.61, wpb=110.6, bsz=40, num_updates=9240, lr=4.72708e-05, gnorm=0.694, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=50867
2023-01-10 04:01:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:37 - progress_bar.py[line:274] - INFO: epoch 001:   9262 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2165, wps=101.7, ups=0.62, wpb=109.8, bsz=40, num_updates=9250, lr=4.72656e-05, gnorm=0.846, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=50883
2023-01-10 04:01:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:53 - progress_bar.py[line:274] - INFO: epoch 001:   9272 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2255, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=9260, lr=4.72604e-05, gnorm=0.879, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=50899
2023-01-10 04:01:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:01:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:10 - progress_bar.py[line:274] - INFO: epoch 001:   9282 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.16, wps=100.1, ups=0.6, wpb=110.5, bsz=40, num_updates=9270, lr=4.72552e-05, gnorm=1.946, clip=60, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=50916
2023-01-10 04:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:27 - progress_bar.py[line:274] - INFO: epoch 001:   9292 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2577, wps=97.4, ups=0.59, wpb=109.1, bsz=40, num_updates=9280, lr=4.725e-05, gnorm=0.895, clip=20, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=50933
2023-01-10 04:02:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:43 - progress_bar.py[line:274] - INFO: epoch 001:   9302 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2455, wps=100.5, ups=0.62, wpb=108.1, bsz=40, num_updates=9290, lr=4.72448e-05, gnorm=0.704, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=50950
2023-01-10 04:02:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:02:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:00 - progress_bar.py[line:274] - INFO: epoch 001:   9312 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2577, wps=103.1, ups=0.62, wpb=110.5, bsz=40, num_updates=9300, lr=4.72396e-05, gnorm=0.885, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=50966
2023-01-10 04:03:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:16 - progress_bar.py[line:274] - INFO: epoch 001:   9322 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2473, wps=103.6, ups=0.63, wpb=110.5, bsz=40, num_updates=9310, lr=4.72344e-05, gnorm=0.69, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=50982
2023-01-10 04:03:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:32 - progress_bar.py[line:274] - INFO: epoch 001:   9332 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3, wps=103.2, ups=0.63, wpb=109.7, bsz=40, num_updates=9320, lr=4.72292e-05, gnorm=0.573, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=50998
2023-01-10 04:03:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:49 - progress_bar.py[line:274] - INFO: epoch 001:   9342 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.268, wps=101.2, ups=0.62, wpb=109.7, bsz=40, num_updates=9330, lr=4.7224e-05, gnorm=0.751, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=51015
2023-01-10 04:03:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:03:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:05 - progress_bar.py[line:274] - INFO: epoch 001:   9352 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2091, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=9340, lr=4.72187e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=51031
2023-01-10 04:04:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:22 - progress_bar.py[line:274] - INFO: epoch 001:   9362 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2143, wps=103.1, ups=0.62, wpb=111.1, bsz=40, num_updates=9350, lr=4.72135e-05, gnorm=0.833, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51048
2023-01-10 04:04:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:38 - progress_bar.py[line:274] - INFO: epoch 001:   9372 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2574, wps=99.3, ups=0.6, wpb=109.5, bsz=40, num_updates=9360, lr=4.72083e-05, gnorm=0.729, clip=30, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=51065
2023-01-10 04:04:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:04:55 - progress_bar.py[line:274] - INFO: epoch 001:   9382 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.21, wps=103.7, ups=0.63, wpb=110.5, bsz=40, num_updates=9370, lr=4.72031e-05, gnorm=0.739, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51081
2023-01-10 04:04:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:11 - progress_bar.py[line:274] - INFO: epoch 001:   9392 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2, wps=101.6, ups=0.61, wpb=111.1, bsz=40, num_updates=9380, lr=4.71979e-05, gnorm=0.923, clip=20, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=51098
2023-01-10 04:05:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:28 - progress_bar.py[line:274] - INFO: epoch 001:   9402 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1553, wps=103, ups=0.63, wpb=109.6, bsz=40, num_updates=9390, lr=4.71927e-05, gnorm=0.642, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51114
2023-01-10 04:05:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:42 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-10 04:05:45 - progress_bar.py[line:274] - INFO: epoch 001:   9413 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1616, wps=93.4, ups=0.57, wpb=109.1, bsz=40, num_updates=9400, lr=4.71875e-05, gnorm=0.987, clip=40, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=51132
2023-01-10 04:05:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:05:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:02 - progress_bar.py[line:274] - INFO: epoch 001:   9423 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.25, wps=100.2, ups=0.6, wpb=110.9, bsz=40, num_updates=9410, lr=4.71823e-05, gnorm=0.776, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=51148
2023-01-10 04:06:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:19 - progress_bar.py[line:274] - INFO: epoch 001:   9433 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2636, wps=99.2, ups=0.61, wpb=108.4, bsz=40, num_updates=9420, lr=4.71771e-05, gnorm=0.972, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=51165
2023-01-10 04:06:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:35 - progress_bar.py[line:274] - INFO: epoch 001:   9443 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2551, wps=102.9, ups=0.62, wpb=110, bsz=40, num_updates=9430, lr=4.71719e-05, gnorm=1.11, clip=20, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=51181
2023-01-10 04:06:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:52 - progress_bar.py[line:274] - INFO: epoch 001:   9453 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.25, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=9440, lr=4.71667e-05, gnorm=0.726, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=51198
2023-01-10 04:06:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:06:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:08 - progress_bar.py[line:274] - INFO: epoch 001:   9463 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2593, wps=103.1, ups=0.62, wpb=110, bsz=40, num_updates=9450, lr=4.71615e-05, gnorm=1.335, clip=30, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=51214
2023-01-10 04:07:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:24 - progress_bar.py[line:274] - INFO: epoch 001:   9473 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2232, wps=102.4, ups=0.63, wpb=108.7, bsz=40, num_updates=9460, lr=4.71563e-05, gnorm=0.985, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=51230
2023-01-10 04:07:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:41 - progress_bar.py[line:274] - INFO: epoch 001:   9483 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2424, wps=102, ups=0.61, wpb=111.5, bsz=40, num_updates=9470, lr=4.7151e-05, gnorm=1.435, clip=50, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=51247
2023-01-10 04:07:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:07:57 - progress_bar.py[line:274] - INFO: epoch 001:   9493 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2959, wps=100.7, ups=0.61, wpb=109.7, bsz=40, num_updates=9480, lr=4.71458e-05, gnorm=0.999, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=51264
2023-01-10 04:07:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:14 - progress_bar.py[line:274] - INFO: epoch 001:   9503 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2604, wps=104.2, ups=0.63, wpb=111.1, bsz=40, num_updates=9490, lr=4.71406e-05, gnorm=0.96, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=51280
2023-01-10 04:08:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:30 - progress_bar.py[line:274] - INFO: epoch 001:   9513 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2353, wps=104.1, ups=0.63, wpb=109.8, bsz=40, num_updates=9500, lr=4.71354e-05, gnorm=0.925, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51296
2023-01-10 04:08:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:46 - progress_bar.py[line:274] - INFO: epoch 001:   9523 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3529, wps=102, ups=0.62, wpb=109.2, bsz=40, num_updates=9510, lr=4.71302e-05, gnorm=0.802, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51312
2023-01-10 04:08:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:08:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:03 - progress_bar.py[line:274] - INFO: epoch 001:   9533 / 100000 loss=0.381, loss_v1=0, loss_v2=0, nll_loss=0.237, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.21, wps=103.3, ups=0.62, wpb=111.5, bsz=40, num_updates=9520, lr=4.7125e-05, gnorm=1.074, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51329
2023-01-10 04:09:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:19 - progress_bar.py[line:274] - INFO: epoch 001:   9543 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2842, wps=101.1, ups=0.61, wpb=110.5, bsz=40, num_updates=9530, lr=4.71198e-05, gnorm=0.835, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51346
2023-01-10 04:09:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:36 - progress_bar.py[line:274] - INFO: epoch 001:   9553 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2477, wps=98.4, ups=0.61, wpb=108.2, bsz=40, num_updates=9540, lr=4.71146e-05, gnorm=0.788, clip=20, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=51362
2023-01-10 04:09:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:52 - progress_bar.py[line:274] - INFO: epoch 001:   9563 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2768, wps=102, ups=0.63, wpb=108.5, bsz=40, num_updates=9550, lr=4.71094e-05, gnorm=0.926, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51378
2023-01-10 04:09:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:09:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:08 - progress_bar.py[line:274] - INFO: epoch 001:   9573 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.25, wps=101.4, ups=0.62, wpb=108.7, bsz=40, num_updates=9560, lr=4.71042e-05, gnorm=1.33, clip=30, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=51395
2023-01-10 04:10:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:25 - progress_bar.py[line:274] - INFO: epoch 001:   9583 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2165, wps=100.6, ups=0.6, wpb=111.1, bsz=40, num_updates=9570, lr=4.7099e-05, gnorm=0.78, clip=20, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=51411
2023-01-10 04:10:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:42 - progress_bar.py[line:274] - INFO: epoch 001:   9593 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.198, wps=98.8, ups=0.6, wpb=109.7, bsz=40, num_updates=9580, lr=4.70938e-05, gnorm=0.807, clip=20, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=51428
2023-01-10 04:10:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:10:59 - progress_bar.py[line:274] - INFO: epoch 001:   9603 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2755, wps=98.6, ups=0.6, wpb=110.4, bsz=40, num_updates=9590, lr=4.70885e-05, gnorm=1.48, clip=60, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=51445
2023-01-10 04:11:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:15 - progress_bar.py[line:274] - INFO: epoch 001:   9613 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.18, wps=102.9, ups=0.62, wpb=109.8, bsz=40, num_updates=9600, lr=4.70833e-05, gnorm=0.724, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51462
2023-01-10 04:11:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:32 - progress_bar.py[line:274] - INFO: epoch 001:   9623 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2188, wps=99.7, ups=0.6, wpb=110.5, bsz=40, num_updates=9610, lr=4.70781e-05, gnorm=1.067, clip=40, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=51478
2023-01-10 04:11:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:49 - progress_bar.py[line:274] - INFO: epoch 001:   9633 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3107, wps=102.9, ups=0.62, wpb=110.6, bsz=40, num_updates=9620, lr=4.70729e-05, gnorm=0.467, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51495
2023-01-10 04:11:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:11:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:05 - progress_bar.py[line:274] - INFO: epoch 001:   9643 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3191, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=9630, lr=4.70677e-05, gnorm=0.669, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51511
2023-01-10 04:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:22 - progress_bar.py[line:274] - INFO: epoch 001:   9653 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2796, wps=103.5, ups=0.62, wpb=110.7, bsz=40, num_updates=9640, lr=4.70625e-05, gnorm=0.989, clip=50, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51528
2023-01-10 04:12:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:38 - progress_bar.py[line:274] - INFO: epoch 001:   9663 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2796, wps=105.1, ups=0.63, wpb=110.9, bsz=40, num_updates=9650, lr=4.70573e-05, gnorm=0.826, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51544
2023-01-10 04:12:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:54 - progress_bar.py[line:274] - INFO: epoch 001:   9673 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.27, wps=102.8, ups=0.62, wpb=109.9, bsz=40, num_updates=9660, lr=4.70521e-05, gnorm=0.852, clip=40, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=51560
2023-01-10 04:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:12:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:10 - progress_bar.py[line:274] - INFO: epoch 001:   9683 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2165, wps=101.3, ups=0.62, wpb=109.3, bsz=40, num_updates=9670, lr=4.70469e-05, gnorm=1.251, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51576
2023-01-10 04:13:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:27 - progress_bar.py[line:274] - INFO: epoch 001:   9693 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.1939, wps=98.6, ups=0.6, wpb=109.1, bsz=40, num_updates=9680, lr=4.70417e-05, gnorm=0.949, clip=30, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=51593
2023-01-10 04:13:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:44 - progress_bar.py[line:274] - INFO: epoch 001:   9703 / 100000 loss=0.388, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2308, wps=101.2, ups=0.62, wpb=109.5, bsz=40, num_updates=9690, lr=4.70365e-05, gnorm=1.936, clip=80, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=51610
2023-01-10 04:13:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:13:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:00 - progress_bar.py[line:274] - INFO: epoch 001:   9713 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2427, wps=101.8, ups=0.62, wpb=109.7, bsz=40, num_updates=9700, lr=4.70313e-05, gnorm=0.682, clip=20, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=51626
2023-01-10 04:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:17 - progress_bar.py[line:274] - INFO: epoch 001:   9723 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2, wps=101.3, ups=0.61, wpb=109.9, bsz=40, num_updates=9710, lr=4.7026e-05, gnorm=0.932, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=51643
2023-01-10 04:14:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:33 - progress_bar.py[line:274] - INFO: epoch 001:   9733 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2727, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=9720, lr=4.70208e-05, gnorm=1.027, clip=10, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=51660
2023-01-10 04:14:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:50 - progress_bar.py[line:274] - INFO: epoch 001:   9743 / 100000 loss=0.387, loss_v1=0, loss_v2=0, nll_loss=0.244, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2243, wps=100.2, ups=0.61, wpb=109.1, bsz=40, num_updates=9730, lr=4.70156e-05, gnorm=0.685, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51676
2023-01-10 04:14:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:14:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:07 - progress_bar.py[line:274] - INFO: epoch 001:   9753 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2883, wps=97.5, ups=0.6, wpb=108.5, bsz=40, num_updates=9740, lr=4.70104e-05, gnorm=0.994, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=51693
2023-01-10 04:15:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:24 - progress_bar.py[line:274] - INFO: epoch 001:   9763 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2294, wps=101.4, ups=0.62, wpb=109.6, bsz=40, num_updates=9750, lr=4.70052e-05, gnorm=0.81, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51710
2023-01-10 04:15:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:40 - progress_bar.py[line:274] - INFO: epoch 001:   9773 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2336, wps=99.5, ups=0.61, wpb=108.7, bsz=40, num_updates=9760, lr=4.7e-05, gnorm=1.764, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51726
2023-01-10 04:15:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:15:57 - progress_bar.py[line:274] - INFO: epoch 001:   9783 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2981, wps=98.7, ups=0.6, wpb=110, bsz=40, num_updates=9770, lr=4.69948e-05, gnorm=0.766, clip=30, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=51743
2023-01-10 04:15:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:13 - progress_bar.py[line:274] - INFO: epoch 001:   9793 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2784, wps=103.1, ups=0.62, wpb=111.1, bsz=40, num_updates=9780, lr=4.69896e-05, gnorm=1.052, clip=40, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=51760
2023-01-10 04:16:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:30 - progress_bar.py[line:274] - INFO: epoch 001:   9803 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.1863, wps=102.8, ups=0.62, wpb=111.3, bsz=40, num_updates=9790, lr=4.69844e-05, gnorm=1.694, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51776
2023-01-10 04:16:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:46 - progress_bar.py[line:274] - INFO: epoch 001:   9813 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2083, wps=102.4, ups=0.62, wpb=110.6, bsz=40, num_updates=9800, lr=4.69792e-05, gnorm=1.605, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51793
2023-01-10 04:16:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:16:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:03 - progress_bar.py[line:274] - INFO: epoch 001:   9823 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.283, wps=101.4, ups=0.61, wpb=110.3, bsz=40, num_updates=9810, lr=4.6974e-05, gnorm=1.262, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51809
2023-01-10 04:17:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:19 - progress_bar.py[line:274] - INFO: epoch 001:   9833 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.21, wps=102.4, ups=0.62, wpb=110.1, bsz=40, num_updates=9820, lr=4.69687e-05, gnorm=0.757, clip=30, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=51826
2023-01-10 04:17:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:36 - progress_bar.py[line:274] - INFO: epoch 001:   9843 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2752, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=9830, lr=4.69635e-05, gnorm=0.839, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51842
2023-01-10 04:17:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:53 - progress_bar.py[line:274] - INFO: epoch 001:   9853 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2222, wps=99.7, ups=0.6, wpb=110.2, bsz=40, num_updates=9840, lr=4.69583e-05, gnorm=0.584, clip=20, loss_scale=256, train_wall=17, gb_free=10.7, ema_decay=0.9999, wall=51859
2023-01-10 04:17:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:17:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:09 - progress_bar.py[line:274] - INFO: epoch 001:   9863 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.266, wps=101.6, ups=0.62, wpb=109.5, bsz=40, num_updates=9850, lr=4.69531e-05, gnorm=0.781, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51875
2023-01-10 04:18:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:26 - progress_bar.py[line:274] - INFO: epoch 001:   9873 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2222, wps=99.1, ups=0.61, wpb=109.1, bsz=40, num_updates=9860, lr=4.69479e-05, gnorm=1.458, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51892
2023-01-10 04:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:42 - progress_bar.py[line:274] - INFO: epoch 001:   9883 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2234, wps=102.7, ups=0.62, wpb=110.1, bsz=40, num_updates=9870, lr=4.69427e-05, gnorm=0.847, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=51908
2023-01-10 04:18:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:18:59 - progress_bar.py[line:274] - INFO: epoch 001:   9893 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2308, wps=101.9, ups=0.61, wpb=111.1, bsz=40, num_updates=9880, lr=4.69375e-05, gnorm=0.552, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51925
2023-01-10 04:19:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:15 - progress_bar.py[line:274] - INFO: epoch 001:   9903 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2059, wps=103.5, ups=0.63, wpb=109.7, bsz=40, num_updates=9890, lr=4.69323e-05, gnorm=1.034, clip=50, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=51941
2023-01-10 04:19:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:31 - progress_bar.py[line:274] - INFO: epoch 001:   9913 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2727, wps=102.5, ups=0.62, wpb=110.8, bsz=40, num_updates=9900, lr=4.69271e-05, gnorm=1.574, clip=40, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=51958
2023-01-10 04:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:48 - progress_bar.py[line:274] - INFO: epoch 001:   9923 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2474, wps=98.4, ups=0.6, wpb=109.5, bsz=40, num_updates=9910, lr=4.69219e-05, gnorm=1.12, clip=40, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=51975
2023-01-10 04:19:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:19:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:05 - progress_bar.py[line:274] - INFO: epoch 001:   9933 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2655, wps=99.4, ups=0.61, wpb=108.9, bsz=40, num_updates=9920, lr=4.69167e-05, gnorm=0.983, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=51991
2023-01-10 04:20:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:21 - progress_bar.py[line:274] - INFO: epoch 001:   9943 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2233, wps=100.4, ups=0.62, wpb=108.7, bsz=40, num_updates=9930, lr=4.69115e-05, gnorm=1.039, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=52008
2023-01-10 04:20:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:38 - progress_bar.py[line:274] - INFO: epoch 001:   9953 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2596, wps=99.2, ups=0.6, wpb=110.6, bsz=40, num_updates=9940, lr=4.69062e-05, gnorm=0.779, clip=20, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=52025
2023-01-10 04:20:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:20:55 - progress_bar.py[line:274] - INFO: epoch 001:   9963 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2571, wps=99.5, ups=0.61, wpb=108.3, bsz=40, num_updates=9950, lr=4.6901e-05, gnorm=0.737, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=52041
2023-01-10 04:20:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:13 - progress_bar.py[line:274] - INFO: epoch 001:   9973 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.181, wps=99.9, ups=0.61, wpb=109.3, bsz=40, num_updates=9960, lr=4.68958e-05, gnorm=0.652, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52058
2023-01-10 04:21:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:29 - progress_bar.py[line:274] - INFO: epoch 001:   9983 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.245, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2913, wps=101.4, ups=0.62, wpb=109.1, bsz=40, num_updates=9970, lr=4.68906e-05, gnorm=1.22, clip=40, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=52075
2023-01-10 04:21:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:46 - progress_bar.py[line:274] - INFO: epoch 001:   9993 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2551, wps=99.9, ups=0.61, wpb=109.1, bsz=40, num_updates=9980, lr=4.68854e-05, gnorm=0.826, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=52092
2023-01-10 04:21:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:21:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:22:03 - progress_bar.py[line:274] - INFO: epoch 001:  10003 / 100000 loss=0.397, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2037, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=9990, lr=4.68802e-05, gnorm=0.933, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52109
2023-01-10 04:22:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:22:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:22:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:22:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:22:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 04:22:20 - progress_bar.py[line:274] - INFO: epoch 001:  10013 / 100000 loss=0.383, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2371, wps=100.3, ups=0.61, wpb=109, bsz=40, num_updates=10000, lr=4.6875e-05, gnorm=0.631, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=52126
2023-01-10 04:22:20 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 04:22:22 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 04:22:22 - train.py[line:551] - INFO: load:1.30 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 04:22:22 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.14 GiB (GPU 1; 39.59 GiB total capacity; 9.26 GiB already allocated; 898.19 MiB free; 36.22 GiB reserved in total by PyTorch)
2023-01-10 04:22:22 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-10 04:22:23 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9483 MB |   10708 MB |    7315 TB |    7315 TB |
|       from large pool |    9309 MB |   10533 MB |    7313 TB |    7313 TB |
|       from small pool |     174 MB |     175 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9483 MB |   10708 MB |    7315 TB |    7315 TB |
|       from large pool |    9309 MB |   10533 MB |    7313 TB |    7313 TB |
|       from small pool |     174 MB |     175 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   37090 MB |   37984 MB |  381952 MB |  344862 MB |
|       from large pool |   36914 MB |   37802 MB |  381536 MB |  344622 MB |
|       from small pool |     176 MB |     182 MB |     416 MB |     240 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   27606 MB |   32136 MB |    7856 TB |    7856 TB |
|       from large pool |   27604 MB |   32133 MB |    7853 TB |    7853 TB |
|       from small pool |       1 MB |       3 MB |       2 TB |       2 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4623    |    4637    |  347936 K  |  347931 K  |
|       from large pool |     698    |     710    |  107633 K  |  107632 K  |
|       from small pool |    3925    |    3944    |  240302 K  |  240298 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4623    |    4637    |  347936 K  |  347931 K  |
|       from large pool |     698    |     710    |  107633 K  |  107632 K  |
|       from small pool |    3925    |    3944    |  240302 K  |  240298 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     193    |     197    |     940    |     747    |
|       from large pool |     105    |     106    |     732    |     627    |
|       from small pool |      88    |      91    |     208    |     120    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     145    |  257240 K  |  257240 K  |
|       from large pool |      71    |      75    |   46858 K  |   46858 K  |
|       from small pool |      67    |      74    |  210381 K  |  210381 K  |
|===========================================================================|

2023-01-10 04:22:23 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-01-10 04:24:54 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 04:24:54 - train.py[line:551] - INFO: load:1.33 valid_run:151.79 task_valid:148.11 collect_output:2.65
2023-01-10 04:27:22 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 04:27:22 - train.py[line:551] - INFO: load:1.35 valid_run:300.07 task_valid:291.77 collect_output:6.22
2023-01-10 04:29:54 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 04:29:54 - train.py[line:551] - INFO: load:1.38 valid_run:451.30 task_valid:434.89 collect_output:13.34
2023-01-10 04:32:22 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 04:32:22 - train.py[line:551] - INFO: load:1.40 valid_run:599.80 task_valid:579.97 collect_output:15.75
2023-01-10 04:34:54 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 04:34:54 - train.py[line:551] - INFO: load:1.43 valid_run:751.38 task_valid:727.42 collect_output:18.87
2023-01-10 04:37:26 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 04:37:26 - train.py[line:551] - INFO: load:1.45 valid_run:902.75 task_valid:873.38 collect_output:23.25
2023-01-10 04:39:58 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 04:39:58 - train.py[line:551] - INFO: load:1.48 valid_run:1054.85 task_valid:1019.36 collect_output:28.36
2023-01-10 04:42:28 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 04:42:28 - train.py[line:551] - INFO: load:1.51 valid_run:1204.69 task_valid:1160.39 collect_output:36.16
2023-01-10 04:44:57 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 04:44:57 - train.py[line:551] - INFO: load:1.53 valid_run:1353.56 task_valid:1305.08 collect_output:39.33
2023-01-10 04:47:25 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 04:47:25 - train.py[line:551] - INFO: load:1.56 valid_run:1501.94 task_valid:1448.76 collect_output:42.99
2023-01-10 04:49:55 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 04:49:55 - train.py[line:551] - INFO: load:1.59 valid_run:1651.30 task_valid:1594.01 collect_output:46.04
2023-01-10 04:52:24 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 04:52:24 - train.py[line:551] - INFO: load:1.62 valid_run:1800.51 task_valid:1738.88 collect_output:49.36
2023-01-10 04:54:53 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 04:54:53 - train.py[line:551] - INFO: load:1.64 valid_run:1949.39 task_valid:1880.76 collect_output:55.32
2023-01-10 04:57:24 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 04:57:24 - train.py[line:551] - INFO: load:1.67 valid_run:2099.93 task_valid:2026.89 collect_output:58.67
2023-01-10 04:59:54 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 04:59:54 - train.py[line:551] - INFO: load:1.71 valid_run:2249.93 task_valid:2173.86 collect_output:60.62
2023-01-10 05:02:23 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 05:02:23 - train.py[line:551] - INFO: load:1.74 valid_run:2399.62 task_valid:2318.64 collect_output:64.44
2023-01-10 05:04:54 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 05:04:54 - train.py[line:551] - INFO: load:1.77 valid_run:2550.60 task_valid:2464.80 collect_output:68.17
2023-01-10 05:07:25 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 05:07:25 - train.py[line:551] - INFO: load:1.80 valid_run:2701.40 task_valid:2612.54 collect_output:70.16
2023-01-10 05:09:53 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 05:09:53 - train.py[line:551] - INFO: load:1.83 valid_run:2849.04 task_valid:2754.62 collect_output:74.65
2023-01-10 05:12:23 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 05:12:23 - train.py[line:551] - INFO: load:1.86 valid_run:2998.69 task_valid:2900.28 collect_output:77.57
2023-01-10 05:14:54 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 05:14:54 - train.py[line:551] - INFO: load:1.89 valid_run:3149.44 task_valid:3045.27 collect_output:82.27
2023-01-10 05:17:23 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 05:17:23 - train.py[line:551] - INFO: load:1.92 valid_run:3298.77 task_valid:3190.56 collect_output:85.25
2023-01-10 05:19:54 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 05:19:54 - train.py[line:551] - INFO: load:1.95 valid_run:3449.46 task_valid:3337.40 collect_output:88.02
2023-01-10 05:22:25 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 05:22:25 - train.py[line:551] - INFO: load:1.98 valid_run:3600.51 task_valid:3484.62 collect_output:90.78

====================================================================================================
SGG eval:     R @ 50: 0.6100;     R @ 100: 0.6578;     R @ 500: 0.6983;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3871;    mR @ 100: 0.4258;    mR @ 500: 0.4655;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8000) (covered in:0.8750) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4516) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9141) (says:0.0000) (sitting on:0.7806) (standing on:0.3243) (using:0.6000) (walking in:0.0000) (walking on:0.7297) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6100;     R @ 100: 0.6578;     R @ 500: 0.6983;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3871;    mR @ 100: 0.4258;    mR @ 500: 0.4655;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.8000) (covered in:0.8750) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4516) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.9141) (says:0.0000) (sitting on:0.7806) (standing on:0.3243) (using:0.6000) (walking in:0.0000) (walking on:0.7297) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-10 05:24:57 - train.py[line:487] - INFO: 0.657781512605042
2023-01-10 05:24:57 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 05:24:57 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.283 | loss_v1 0 | loss_v2 0 | nll_loss 0.133 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.657782 | ppl 1.1 | vqa_score 0.5845 | wps 119.5 | wpb 89.9 | bsz 30 | num_updates 10000 | best_R@100 0.69005
2023-01-10 05:24:57 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 10000 updates
2023-01-10 05:24:57 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt
2023-01-10 05:25:36 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt
2023-01-10 05:26:58 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_10000.pt (epoch 1 @ 10000 updates, score 0.657781512605042) (writing took 120.99265217408538 seconds)
2023-01-10 05:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:14 - progress_bar.py[line:274] - INFO: epoch 001:  10023 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.234, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2626, wps=0.4, ups=0, wpb=110, bsz=40, num_updates=10010, lr=4.68698e-05, gnorm=1.008, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56021
2023-01-10 05:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:31 - progress_bar.py[line:274] - INFO: epoch 001:  10033 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2255, wps=102.8, ups=0.62, wpb=110.2, bsz=40, num_updates=10020, lr=4.68646e-05, gnorm=1.012, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56037
2023-01-10 05:27:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:47 - progress_bar.py[line:274] - INFO: epoch 001:  10043 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2121, wps=100.8, ups=0.61, wpb=109.5, bsz=40, num_updates=10030, lr=4.68594e-05, gnorm=0.781, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=56053
2023-01-10 05:27:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:27:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:04 - progress_bar.py[line:274] - INFO: epoch 001:  10053 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1875, wps=98.4, ups=0.6, wpb=108.9, bsz=40, num_updates=10040, lr=4.68542e-05, gnorm=0.613, clip=0, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=56070
2023-01-10 05:28:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:21 - progress_bar.py[line:274] - INFO: epoch 001:  10063 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.25, wps=99.3, ups=0.61, wpb=109.2, bsz=40, num_updates=10050, lr=4.6849e-05, gnorm=1.028, clip=20, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=56087
2023-01-10 05:28:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:38 - progress_bar.py[line:274] - INFO: epoch 001:  10073 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2527, wps=101.4, ups=0.61, wpb=110.5, bsz=40, num_updates=10060, lr=4.68438e-05, gnorm=0.768, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=56104
2023-01-10 05:28:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:54 - progress_bar.py[line:274] - INFO: epoch 001:  10083 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2637, wps=100.7, ups=0.61, wpb=110, bsz=40, num_updates=10070, lr=4.68385e-05, gnorm=1.686, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=56120
2023-01-10 05:28:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:28:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:10 - progress_bar.py[line:274] - INFO: epoch 001:  10093 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2549, wps=106.2, ups=0.64, wpb=111.1, bsz=40, num_updates=10080, lr=4.68333e-05, gnorm=0.596, clip=0, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=56136
2023-01-10 05:29:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:27 - progress_bar.py[line:274] - INFO: epoch 001:  10103 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3407, wps=102.2, ups=0.62, wpb=110.2, bsz=40, num_updates=10090, lr=4.68281e-05, gnorm=0.582, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=56153
2023-01-10 05:29:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:43 - progress_bar.py[line:274] - INFO: epoch 001:  10113 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3295, wps=103, ups=0.62, wpb=111, bsz=40, num_updates=10100, lr=4.68229e-05, gnorm=0.596, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56169
2023-01-10 05:29:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:29:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:00 - progress_bar.py[line:274] - INFO: epoch 001:  10123 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3191, wps=100.1, ups=0.61, wpb=109.5, bsz=40, num_updates=10110, lr=4.68177e-05, gnorm=0.546, clip=10, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=56186
2023-01-10 05:30:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:16 - progress_bar.py[line:274] - INFO: epoch 001:  10133 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2021, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=10120, lr=4.68125e-05, gnorm=1.091, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=56202
2023-01-10 05:30:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:33 - progress_bar.py[line:274] - INFO: epoch 001:  10143 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.213, wps=98.7, ups=0.6, wpb=109.4, bsz=40, num_updates=10130, lr=4.68073e-05, gnorm=1.813, clip=40, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=56219
2023-01-10 05:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:50 - progress_bar.py[line:274] - INFO: epoch 001:  10153 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3152, wps=101.2, ups=0.62, wpb=109.5, bsz=40, num_updates=10140, lr=4.68021e-05, gnorm=0.87, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56236
2023-01-10 05:30:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:30:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:06 - progress_bar.py[line:274] - INFO: epoch 001:  10163 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2472, wps=100.6, ups=0.62, wpb=108.9, bsz=40, num_updates=10150, lr=4.67969e-05, gnorm=0.953, clip=20, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=56252
2023-01-10 05:31:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:22 - progress_bar.py[line:274] - INFO: epoch 001:  10173 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3302, wps=102, ups=0.63, wpb=108, bsz=40, num_updates=10160, lr=4.67917e-05, gnorm=1.094, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=56269
2023-01-10 05:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:39 - progress_bar.py[line:274] - INFO: epoch 001:  10183 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3084, wps=100.8, ups=0.61, wpb=109.3, bsz=40, num_updates=10170, lr=4.67865e-05, gnorm=0.913, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56285
2023-01-10 05:31:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:31:56 - progress_bar.py[line:274] - INFO: epoch 001:  10193 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2959, wps=99.9, ups=0.6, wpb=110.3, bsz=40, num_updates=10180, lr=4.67813e-05, gnorm=0.731, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=56302
2023-01-10 05:31:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:12 - progress_bar.py[line:274] - INFO: epoch 001:  10203 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2604, wps=101.2, ups=0.62, wpb=109.5, bsz=40, num_updates=10190, lr=4.6776e-05, gnorm=0.624, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56318
2023-01-10 05:32:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:29 - progress_bar.py[line:274] - INFO: epoch 001:  10213 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2692, wps=99.3, ups=0.61, wpb=108.4, bsz=40, num_updates=10200, lr=4.67708e-05, gnorm=1.472, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=56335
2023-01-10 05:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:45 - progress_bar.py[line:274] - INFO: epoch 001:  10223 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=112.267, nsentences=40, sample_size=112.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2812, wps=102.5, ups=0.61, wpb=112.3, bsz=40, num_updates=10210, lr=4.67656e-05, gnorm=1.305, clip=40, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=56352
2023-01-10 05:32:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:32:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:02 - progress_bar.py[line:274] - INFO: epoch 001:  10233 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2451, wps=104, ups=0.63, wpb=110.3, bsz=40, num_updates=10220, lr=4.67604e-05, gnorm=0.731, clip=10, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=56368
2023-01-10 05:33:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:18 - progress_bar.py[line:274] - INFO: epoch 001:  10243 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2936, wps=100.9, ups=0.61, wpb=109.5, bsz=40, num_updates=10230, lr=4.67552e-05, gnorm=1.101, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=56384
2023-01-10 05:33:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:35 - progress_bar.py[line:274] - INFO: epoch 001:  10253 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.228, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2571, wps=99.4, ups=0.61, wpb=109.3, bsz=40, num_updates=10240, lr=4.675e-05, gnorm=0.586, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56401
2023-01-10 05:33:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:52 - progress_bar.py[line:274] - INFO: epoch 001:  10263 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2981, wps=99.7, ups=0.6, wpb=110, bsz=40, num_updates=10250, lr=4.67448e-05, gnorm=0.519, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=56418
2023-01-10 05:33:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:33:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:08 - progress_bar.py[line:274] - INFO: epoch 001:  10273 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2551, wps=102.9, ups=0.62, wpb=110.1, bsz=40, num_updates=10260, lr=4.67396e-05, gnorm=0.442, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=56434
2023-01-10 05:34:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:24 - progress_bar.py[line:274] - INFO: epoch 001:  10283 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3056, wps=101.7, ups=0.62, wpb=109.8, bsz=40, num_updates=10270, lr=4.67344e-05, gnorm=0.9, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56451
2023-01-10 05:34:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:41 - progress_bar.py[line:274] - INFO: epoch 001:  10293 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.301, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=10280, lr=4.67292e-05, gnorm=1.236, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=56467
2023-01-10 05:34:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:34:57 - progress_bar.py[line:274] - INFO: epoch 001:  10303 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2872, wps=101.6, ups=0.61, wpb=110.5, bsz=40, num_updates=10290, lr=4.6724e-05, gnorm=0.724, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=56484
2023-01-10 05:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:13 - progress_bar.py[line:274] - INFO: epoch 001:  10313 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3039, wps=105, ups=0.64, wpb=108.9, bsz=40, num_updates=10300, lr=4.67188e-05, gnorm=0.859, clip=20, loss_scale=512, train_wall=15, gb_free=10.2, ema_decay=0.9999, wall=56499
2023-01-10 05:35:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:30 - progress_bar.py[line:274] - INFO: epoch 001:  10323 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1546, wps=98.4, ups=0.6, wpb=109, bsz=40, num_updates=10310, lr=4.67135e-05, gnorm=2.022, clip=40, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=56516
2023-01-10 05:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:47 - progress_bar.py[line:274] - INFO: epoch 001:  10333 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2857, wps=99.5, ups=0.61, wpb=109.5, bsz=40, num_updates=10320, lr=4.67083e-05, gnorm=1.136, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=56533
2023-01-10 05:35:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:35:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:04 - progress_bar.py[line:274] - INFO: epoch 001:  10343 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.236, ntokens=112.267, nsentences=40, sample_size=112.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2121, wps=101.9, ups=0.6, wpb=112.3, bsz=40, num_updates=10330, lr=4.67031e-05, gnorm=1.087, clip=60, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=56550
2023-01-10 05:36:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:21 - progress_bar.py[line:274] - INFO: epoch 001:  10353 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.219, wps=101, ups=0.61, wpb=110.3, bsz=40, num_updates=10340, lr=4.66979e-05, gnorm=0.743, clip=30, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=56567
2023-01-10 05:36:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:37 - progress_bar.py[line:274] - INFO: epoch 001:  10363 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3204, wps=99.9, ups=0.61, wpb=109.2, bsz=40, num_updates=10350, lr=4.66927e-05, gnorm=0.736, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=56583
2023-01-10 05:36:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:54 - progress_bar.py[line:274] - INFO: epoch 001:  10373 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3431, wps=100, ups=0.61, wpb=109.3, bsz=40, num_updates=10360, lr=4.66875e-05, gnorm=0.582, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=56600
2023-01-10 05:36:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:36:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:11 - progress_bar.py[line:274] - INFO: epoch 001:  10383 / 100000 loss=0.38, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1889, wps=101.5, ups=0.61, wpb=110.6, bsz=40, num_updates=10370, lr=4.66823e-05, gnorm=0.644, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=56617
2023-01-10 05:37:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:27 - progress_bar.py[line:274] - INFO: epoch 001:  10393 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2524, wps=101.5, ups=0.62, wpb=109.8, bsz=40, num_updates=10380, lr=4.66771e-05, gnorm=0.752, clip=20, loss_scale=512, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=56633
2023-01-10 05:37:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:43 - progress_bar.py[line:274] - INFO: epoch 001:  10403 / 100000 loss=0.399, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.2, vqa_score=0.2315, wps=103.1, ups=0.63, wpb=108.9, bsz=40, num_updates=10390, lr=4.66719e-05, gnorm=1.082, clip=30, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=56649
2023-01-10 05:37:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:37:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:00 - progress_bar.py[line:274] - INFO: epoch 001:  10413 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3011, wps=101.2, ups=0.61, wpb=111.3, bsz=40, num_updates=10400, lr=4.66667e-05, gnorm=0.633, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=56666
2023-01-10 05:38:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:17 - progress_bar.py[line:274] - INFO: epoch 001:  10423 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2895, wps=98.4, ups=0.61, wpb=108.1, bsz=40, num_updates=10410, lr=4.66615e-05, gnorm=0.753, clip=30, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=56683
2023-01-10 05:38:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:33 - progress_bar.py[line:274] - INFO: epoch 001:  10433 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2264, wps=101.8, ups=0.62, wpb=110.2, bsz=40, num_updates=10420, lr=4.66562e-05, gnorm=0.541, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=56699
2023-01-10 05:38:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:50 - progress_bar.py[line:274] - INFO: epoch 001:  10443 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2019, wps=100, ups=0.61, wpb=109.7, bsz=40, num_updates=10430, lr=4.6651e-05, gnorm=0.871, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56716
2023-01-10 05:38:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:38:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:03 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-10 05:39:07 - progress_bar.py[line:274] - INFO: epoch 001:  10454 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2545, wps=94.3, ups=0.58, wpb=108.7, bsz=40, num_updates=10440, lr=4.66458e-05, gnorm=1.284, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=56733
2023-01-10 05:39:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:24 - progress_bar.py[line:274] - INFO: epoch 001:  10464 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2453, wps=100.6, ups=0.61, wpb=109.4, bsz=40, num_updates=10450, lr=4.66406e-05, gnorm=0.959, clip=30, loss_scale=512, train_wall=16, gb_free=10.9, ema_decay=0.9999, wall=56750
2023-01-10 05:39:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:31 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-10 05:39:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:41 - progress_bar.py[line:274] - INFO: epoch 001:  10475 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.322, wps=93.9, ups=0.58, wpb=108.4, bsz=40, num_updates=10460, lr=4.66354e-05, gnorm=0.91, clip=30, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=56767
2023-01-10 05:39:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:39:58 - progress_bar.py[line:274] - INFO: epoch 001:  10485 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3218, wps=101.4, ups=0.61, wpb=110, bsz=40, num_updates=10470, lr=4.66302e-05, gnorm=0.563, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=56784
2023-01-10 05:39:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:14 - progress_bar.py[line:274] - INFO: epoch 001:  10495 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.28, wps=99.9, ups=0.61, wpb=108.8, bsz=40, num_updates=10480, lr=4.6625e-05, gnorm=0.586, clip=20, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=56801
2023-01-10 05:40:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:31 - progress_bar.py[line:274] - INFO: epoch 001:  10505 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3069, wps=102, ups=0.62, wpb=110.2, bsz=40, num_updates=10490, lr=4.66198e-05, gnorm=0.815, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56817
2023-01-10 05:40:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:47 - progress_bar.py[line:274] - INFO: epoch 001:  10515 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.297, wps=100.8, ups=0.61, wpb=110, bsz=40, num_updates=10500, lr=4.66146e-05, gnorm=0.701, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=56834
2023-01-10 05:40:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:40:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:04 - progress_bar.py[line:274] - INFO: epoch 001:  10525 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3021, wps=100.3, ups=0.61, wpb=109.7, bsz=40, num_updates=10510, lr=4.66094e-05, gnorm=0.946, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=56850
2023-01-10 05:41:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:20 - progress_bar.py[line:274] - INFO: epoch 001:  10535 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3125, wps=102.9, ups=0.62, wpb=109.8, bsz=40, num_updates=10520, lr=4.66042e-05, gnorm=0.697, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=56866
2023-01-10 05:41:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:37 - progress_bar.py[line:274] - INFO: epoch 001:  10545 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.266, wps=100.1, ups=0.6, wpb=110.7, bsz=40, num_updates=10530, lr=4.6599e-05, gnorm=0.453, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=56883
2023-01-10 05:41:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:54 - progress_bar.py[line:274] - INFO: epoch 001:  10555 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2959, wps=100.4, ups=0.61, wpb=109.1, bsz=40, num_updates=10540, lr=4.65938e-05, gnorm=0.569, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=56900
2023-01-10 05:41:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:41:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:11 - progress_bar.py[line:274] - INFO: epoch 001:  10565 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.31, wps=97, ups=0.6, wpb=108.7, bsz=40, num_updates=10550, lr=4.65885e-05, gnorm=0.826, clip=40, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=56917
2023-01-10 05:42:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:27 - progress_bar.py[line:274] - INFO: epoch 001:  10575 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3053, wps=100, ups=0.6, wpb=110.4, bsz=40, num_updates=10560, lr=4.65833e-05, gnorm=0.926, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=56934
2023-01-10 05:42:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:44 - progress_bar.py[line:274] - INFO: epoch 001:  10585 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1758, wps=100.1, ups=0.61, wpb=109.7, bsz=40, num_updates=10570, lr=4.65781e-05, gnorm=0.716, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=56950
2023-01-10 05:42:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:42:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:01 - progress_bar.py[line:274] - INFO: epoch 001:  10595 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2703, wps=100.5, ups=0.61, wpb=109.7, bsz=40, num_updates=10580, lr=4.65729e-05, gnorm=1.043, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=56967
2023-01-10 05:43:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:17 - progress_bar.py[line:274] - INFO: epoch 001:  10605 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3168, wps=99.9, ups=0.61, wpb=109.3, bsz=40, num_updates=10590, lr=4.65677e-05, gnorm=0.669, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=56984
2023-01-10 05:43:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:34 - progress_bar.py[line:274] - INFO: epoch 001:  10615 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3587, wps=102.9, ups=0.63, wpb=109.7, bsz=40, num_updates=10600, lr=4.65625e-05, gnorm=0.742, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57000
2023-01-10 05:43:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:50 - progress_bar.py[line:274] - INFO: epoch 001:  10625 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2527, wps=102.1, ups=0.62, wpb=109.8, bsz=40, num_updates=10610, lr=4.65573e-05, gnorm=0.669, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=57016
2023-01-10 05:43:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:43:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:06 - progress_bar.py[line:274] - INFO: epoch 001:  10635 / 100000 loss=0.391, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.3119, wps=102.5, ups=0.63, wpb=108.8, bsz=40, num_updates=10620, lr=4.65521e-05, gnorm=0.642, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57032
2023-01-10 05:44:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:22 - progress_bar.py[line:274] - INFO: epoch 001:  10645 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.24, wps=104.4, ups=0.64, wpb=109.3, bsz=40, num_updates=10630, lr=4.65469e-05, gnorm=0.914, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57048
2023-01-10 05:44:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:38 - progress_bar.py[line:274] - INFO: epoch 001:  10655 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3125, wps=102.1, ups=0.62, wpb=109.9, bsz=40, num_updates=10640, lr=4.65417e-05, gnorm=0.673, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57065
2023-01-10 05:44:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:55 - progress_bar.py[line:274] - INFO: epoch 001:  10665 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.28, wps=102, ups=0.61, wpb=110.9, bsz=40, num_updates=10650, lr=4.65365e-05, gnorm=0.643, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57081
2023-01-10 05:44:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:11 - progress_bar.py[line:274] - INFO: epoch 001:  10675 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3, wps=100.2, ups=0.61, wpb=109.1, bsz=40, num_updates=10660, lr=4.65313e-05, gnorm=0.748, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=57098
2023-01-10 05:45:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:28 - progress_bar.py[line:274] - INFO: epoch 001:  10685 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2364, wps=101, ups=0.62, wpb=109.4, bsz=40, num_updates=10670, lr=4.6526e-05, gnorm=1.484, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57114
2023-01-10 05:45:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:44 - progress_bar.py[line:274] - INFO: epoch 001:  10695 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2292, wps=102.8, ups=0.62, wpb=110.1, bsz=40, num_updates=10680, lr=4.65208e-05, gnorm=0.707, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57130
2023-01-10 05:45:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:45:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:01 - progress_bar.py[line:274] - INFO: epoch 001:  10705 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2947, wps=100.9, ups=0.62, wpb=109.2, bsz=40, num_updates=10690, lr=4.65156e-05, gnorm=0.757, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57147
2023-01-10 05:46:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:17 - progress_bar.py[line:274] - INFO: epoch 001:  10715 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3789, wps=101.9, ups=0.62, wpb=110.1, bsz=40, num_updates=10700, lr=4.65104e-05, gnorm=0.473, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57163
2023-01-10 05:46:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:34 - progress_bar.py[line:274] - INFO: epoch 001:  10725 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2547, wps=100.4, ups=0.61, wpb=109.2, bsz=40, num_updates=10710, lr=4.65052e-05, gnorm=0.458, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=57180
2023-01-10 05:46:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:51 - progress_bar.py[line:274] - INFO: epoch 001:  10735 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2647, wps=98.2, ups=0.6, wpb=109.9, bsz=40, num_updates=10720, lr=4.65e-05, gnorm=0.778, clip=20, loss_scale=256, train_wall=17, gb_free=10.9, ema_decay=0.9999, wall=57197
2023-01-10 05:46:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:08 - progress_bar.py[line:274] - INFO: epoch 001:  10745 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.1919, wps=99.7, ups=0.6, wpb=110.3, bsz=40, num_updates=10730, lr=4.64948e-05, gnorm=0.416, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=57214
2023-01-10 05:47:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:24 - progress_bar.py[line:274] - INFO: epoch 001:  10755 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2424, wps=102.1, ups=0.61, wpb=111.3, bsz=40, num_updates=10740, lr=4.64896e-05, gnorm=0.98, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57231
2023-01-10 05:47:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:41 - progress_bar.py[line:274] - INFO: epoch 001:  10765 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2793, wps=101, ups=0.62, wpb=109, bsz=40, num_updates=10750, lr=4.64844e-05, gnorm=0.569, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57247
2023-01-10 05:47:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:57 - progress_bar.py[line:274] - INFO: epoch 001:  10775 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3673, wps=102, ups=0.62, wpb=110.4, bsz=40, num_updates=10760, lr=4.64792e-05, gnorm=0.439, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57263
2023-01-10 05:47:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:47:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:14 - progress_bar.py[line:274] - INFO: epoch 001:  10785 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3604, wps=97.7, ups=0.6, wpb=108.7, bsz=40, num_updates=10770, lr=4.6474e-05, gnorm=0.685, clip=20, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=57280
2023-01-10 05:48:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:31 - progress_bar.py[line:274] - INFO: epoch 001:  10795 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3131, wps=102.5, ups=0.61, wpb=111.6, bsz=40, num_updates=10780, lr=4.64688e-05, gnorm=0.709, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57297
2023-01-10 05:48:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:48 - progress_bar.py[line:274] - INFO: epoch 001:  10805 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2778, wps=97.6, ups=0.59, wpb=109.6, bsz=40, num_updates=10790, lr=4.64635e-05, gnorm=0.756, clip=30, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=57314
2023-01-10 05:48:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:48:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:05 - progress_bar.py[line:274] - INFO: epoch 001:  10815 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.23, wps=99.5, ups=0.6, wpb=109.9, bsz=40, num_updates=10800, lr=4.64583e-05, gnorm=0.638, clip=20, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=57331
2023-01-10 05:49:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:21 - progress_bar.py[line:274] - INFO: epoch 001:  10825 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2571, wps=99.6, ups=0.61, wpb=108.4, bsz=40, num_updates=10810, lr=4.64531e-05, gnorm=0.721, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57347
2023-01-10 05:49:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:38 - progress_bar.py[line:274] - INFO: epoch 001:  10835 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2826, wps=101.4, ups=0.61, wpb=110.3, bsz=40, num_updates=10820, lr=4.64479e-05, gnorm=0.683, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57364
2023-01-10 05:49:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:54 - progress_bar.py[line:274] - INFO: epoch 001:  10845 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3226, wps=103.4, ups=0.62, wpb=110.9, bsz=40, num_updates=10830, lr=4.64427e-05, gnorm=0.648, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57380
2023-01-10 05:49:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:49:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:10 - progress_bar.py[line:274] - INFO: epoch 001:  10855 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2816, wps=102.2, ups=0.62, wpb=109.7, bsz=40, num_updates=10840, lr=4.64375e-05, gnorm=0.855, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57397
2023-01-10 05:50:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:27 - progress_bar.py[line:274] - INFO: epoch 001:  10865 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3125, wps=100.4, ups=0.61, wpb=109.6, bsz=40, num_updates=10850, lr=4.64323e-05, gnorm=0.744, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57413
2023-01-10 05:50:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:43 - progress_bar.py[line:274] - INFO: epoch 001:  10875 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2941, wps=102.2, ups=0.62, wpb=109.1, bsz=40, num_updates=10860, lr=4.64271e-05, gnorm=1.631, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57429
2023-01-10 05:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:00 - progress_bar.py[line:274] - INFO: epoch 001:  10885 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2472, wps=101.8, ups=0.61, wpb=110.7, bsz=40, num_updates=10870, lr=4.64219e-05, gnorm=0.723, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57446
2023-01-10 05:51:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:16 - progress_bar.py[line:274] - INFO: epoch 001:  10895 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3241, wps=101, ups=0.62, wpb=109.1, bsz=40, num_updates=10880, lr=4.64167e-05, gnorm=0.515, clip=0, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=57462
2023-01-10 05:51:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:33 - progress_bar.py[line:274] - INFO: epoch 001:  10905 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2277, wps=99, ups=0.61, wpb=108.5, bsz=40, num_updates=10890, lr=4.64115e-05, gnorm=1.061, clip=40, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=57479
2023-01-10 05:51:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:49 - progress_bar.py[line:274] - INFO: epoch 001:  10915 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.28, wps=100.7, ups=0.61, wpb=109.7, bsz=40, num_updates=10900, lr=4.64062e-05, gnorm=1.241, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57496
2023-01-10 05:51:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:51:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:05 - progress_bar.py[line:274] - INFO: epoch 001:  10925 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2747, wps=104.4, ups=0.63, wpb=110.7, bsz=40, num_updates=10910, lr=4.6401e-05, gnorm=0.934, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57512
2023-01-10 05:52:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:22 - progress_bar.py[line:274] - INFO: epoch 001:  10935 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3168, wps=102.4, ups=0.62, wpb=109.5, bsz=40, num_updates=10920, lr=4.63958e-05, gnorm=0.688, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57528
2023-01-10 05:52:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:39 - progress_bar.py[line:274] - INFO: epoch 001:  10945 / 100000 loss=0.367, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3551, wps=98.6, ups=0.6, wpb=109.2, bsz=40, num_updates=10930, lr=4.63906e-05, gnorm=1.297, clip=30, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=57545
2023-01-10 05:52:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:55 - progress_bar.py[line:274] - INFO: epoch 001:  10955 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.301, wps=98.7, ups=0.61, wpb=108.1, bsz=40, num_updates=10940, lr=4.63854e-05, gnorm=0.702, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=57561
2023-01-10 05:52:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:52:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:12 - progress_bar.py[line:274] - INFO: epoch 001:  10965 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3043, wps=101.8, ups=0.61, wpb=111.6, bsz=40, num_updates=10950, lr=4.63802e-05, gnorm=0.739, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57578
2023-01-10 05:53:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:28 - progress_bar.py[line:274] - INFO: epoch 001:  10975 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2796, wps=102.2, ups=0.61, wpb=110.9, bsz=40, num_updates=10960, lr=4.6375e-05, gnorm=1.648, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57595
2023-01-10 05:53:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:44 - progress_bar.py[line:274] - INFO: epoch 001:  10985 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.1562, wps=102.9, ups=0.63, wpb=109.1, bsz=40, num_updates=10970, lr=4.63698e-05, gnorm=0.787, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=57611
2023-01-10 05:53:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:53:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:54:01 - progress_bar.py[line:274] - INFO: epoch 001:  10995 / 100000 loss=0.37, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3333, wps=98.5, ups=0.6, wpb=108.5, bsz=40, num_updates=10980, lr=4.63646e-05, gnorm=1.319, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57627
2023-01-10 05:54:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:54:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:54:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:54:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:54:17 - progress_bar.py[line:274] - INFO: epoch 001:  11005 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3704, wps=106.3, ups=0.64, wpb=110.6, bsz=40, num_updates=10990, lr=4.63594e-05, gnorm=0.678, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=57643
2023-01-10 05:54:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:54:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:54:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:54:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:54:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 05:54:33 - progress_bar.py[line:274] - INFO: epoch 001:  11015 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.307, wps=105.8, ups=0.65, wpb=109.1, bsz=40, num_updates=11000, lr=4.63542e-05, gnorm=0.429, clip=0, loss_scale=512, train_wall=15, gb_free=10.2, ema_decay=0.9999, wall=57659
2023-01-10 05:54:33 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 05:54:34 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 05:54:34 - train.py[line:551] - INFO: load:1.24 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 05:57:06 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 05:57:06 - train.py[line:551] - INFO: load:1.27 valid_run:152.01 task_valid:148.87 collect_output:2.03
2023-01-10 05:59:35 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 05:59:35 - train.py[line:551] - INFO: load:1.30 valid_run:300.46 task_valid:292.79 collect_output:5.47
2023-01-10 06:02:07 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 06:02:07 - train.py[line:551] - INFO: load:1.33 valid_run:452.72 task_valid:436.51 collect_output:12.90
2023-01-10 06:04:36 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 06:04:36 - train.py[line:551] - INFO: load:1.36 valid_run:601.49 task_valid:581.89 collect_output:15.19
2023-01-10 06:07:08 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 06:07:08 - train.py[line:551] - INFO: load:1.39 valid_run:753.29 task_valid:729.71 collect_output:18.08
2023-01-10 06:09:39 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 06:09:40 - train.py[line:551] - INFO: load:1.42 valid_run:904.46 task_valid:875.78 collect_output:22.11
2023-01-10 06:12:12 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 06:12:12 - train.py[line:551] - INFO: load:1.45 valid_run:1056.86 task_valid:1022.35 collect_output:26.86
2023-01-10 06:14:42 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 06:14:42 - train.py[line:551] - INFO: load:1.48 valid_run:1206.95 task_valid:1163.73 collect_output:34.51
2023-01-10 06:17:11 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 06:17:11 - train.py[line:551] - INFO: load:1.51 valid_run:1355.96 task_valid:1308.77 collect_output:37.41
2023-01-10 06:19:40 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 06:19:40 - train.py[line:551] - INFO: load:1.54 valid_run:1504.11 task_valid:1452.58 collect_output:40.68
2023-01-10 06:22:09 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 06:22:09 - train.py[line:551] - INFO: load:1.57 valid_run:1653.55 task_valid:1597.81 collect_output:43.83
2023-01-10 06:24:38 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 06:24:38 - train.py[line:551] - INFO: load:1.60 valid_run:1802.79 task_valid:1743.06 collect_output:46.76
2023-01-10 06:27:07 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 06:27:07 - train.py[line:551] - INFO: load:1.63 valid_run:1951.64 task_valid:1885.19 collect_output:52.42
2023-01-10 06:29:37 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 06:29:37 - train.py[line:551] - INFO: load:1.66 valid_run:2101.74 task_valid:2031.13 collect_output:55.52
2023-01-10 06:32:08 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 06:32:08 - train.py[line:551] - INFO: load:1.69 valid_run:2251.85 task_valid:2178.11 collect_output:57.59
2023-01-10 06:34:37 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 06:34:37 - train.py[line:551] - INFO: load:1.72 valid_run:2401.20 task_valid:2322.53 collect_output:61.43
2023-01-10 06:37:08 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 06:37:08 - train.py[line:551] - INFO: load:1.75 valid_run:2552.04 task_valid:2468.50 collect_output:65.21
2023-01-10 06:39:38 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 06:39:38 - train.py[line:551] - INFO: load:1.78 valid_run:2702.30 task_valid:2615.82 collect_output:67.09
2023-01-10 06:42:06 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 06:42:06 - train.py[line:551] - INFO: load:1.81 valid_run:2849.91 task_valid:2757.81 collect_output:71.65
2023-01-10 06:44:36 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 06:44:36 - train.py[line:551] - INFO: load:1.84 valid_run:2999.56 task_valid:2903.30 collect_output:74.75
2023-01-10 06:47:07 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 06:47:07 - train.py[line:551] - INFO: load:1.87 valid_run:3150.32 task_valid:3048.46 collect_output:79.30
2023-01-10 06:49:36 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 06:49:36 - train.py[line:551] - INFO: load:1.90 valid_run:3299.25 task_valid:3193.62 collect_output:82.00
2023-01-10 06:52:06 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 06:52:06 - train.py[line:551] - INFO: load:1.93 valid_run:3449.89 task_valid:3340.17 collect_output:85.02
2023-01-10 06:54:37 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 06:54:37 - train.py[line:551] - INFO: load:1.96 valid_run:3600.76 task_valid:3487.11 collect_output:87.85

====================================================================================================
SGG eval:     R @ 50: 0.5954;     R @ 100: 0.6545;     R @ 500: 0.6975;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3781;    mR @ 100: 0.4236;    mR @ 500: 0.4630;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7951) (covered in:0.8750) (covering:0.3714) (eating:0.6765) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4516) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.8984) (says:0.0000) (sitting on:0.7727) (standing on:0.3493) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5954;     R @ 100: 0.6545;     R @ 500: 0.6975;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3781;    mR @ 100: 0.4236;    mR @ 500: 0.4630;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7951) (covered in:0.8750) (covering:0.3714) (eating:0.6765) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4516) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:1.0000) (playing:0.0000) (riding:0.8984) (says:0.0000) (sitting on:0.7727) (standing on:0.3493) (using:0.6000) (walking in:0.0000) (walking on:0.6757) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-10 06:57:09 - train.py[line:487] - INFO: 0.6545148459383753
2023-01-10 06:57:09 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 06:57:09 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.34 | loss_v1 0 | loss_v2 0 | nll_loss 0.187 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.654515 | ppl 1.14 | vqa_score 0.5822 | wps 119.5 | wpb 89.9 | bsz 30 | num_updates 11000 | best_R@100 0.69005
2023-01-10 06:57:09 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 11000 updates
2023-01-10 06:57:09 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_11000.pt
2023-01-10 06:57:46 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_11000.pt
2023-01-10 06:59:05 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_11000.pt (epoch 1 @ 11000 updates, score 0.6545148459383753) (writing took 115.43460787646472 seconds)
2023-01-10 06:59:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:21 - progress_bar.py[line:274] - INFO: epoch 001:  11025 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2917, wps=0.4, ups=0, wpb=108.5, bsz=40, num_updates=11010, lr=4.6349e-05, gnorm=0.851, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=61547
2023-01-10 06:59:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:38 - progress_bar.py[line:274] - INFO: epoch 001:  11035 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2784, wps=100.6, ups=0.6, wpb=111.2, bsz=40, num_updates=11020, lr=4.63438e-05, gnorm=0.664, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=61564
2023-01-10 06:59:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:54 - progress_bar.py[line:274] - INFO: epoch 001:  11045 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2653, wps=103.6, ups=0.62, wpb=111.2, bsz=40, num_updates=11030, lr=4.63385e-05, gnorm=0.596, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=61581
2023-01-10 06:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 06:59:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:11 - progress_bar.py[line:274] - INFO: epoch 001:  11055 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2581, wps=102.5, ups=0.61, wpb=111.3, bsz=40, num_updates=11040, lr=4.63333e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=61597
2023-01-10 07:00:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:28 - progress_bar.py[line:274] - INFO: epoch 001:  11065 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2685, wps=98.3, ups=0.59, wpb=110.2, bsz=40, num_updates=11050, lr=4.63281e-05, gnorm=0.957, clip=20, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=61614
2023-01-10 07:00:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:45 - progress_bar.py[line:274] - INFO: epoch 001:  11075 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2062, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=11060, lr=4.63229e-05, gnorm=0.681, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=61631
2023-01-10 07:00:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:00:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:01 - progress_bar.py[line:274] - INFO: epoch 001:  11085 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2366, wps=100.1, ups=0.61, wpb=110.2, bsz=40, num_updates=11070, lr=4.63177e-05, gnorm=0.409, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=61648
2023-01-10 07:01:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:18 - progress_bar.py[line:274] - INFO: epoch 001:  11095 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2947, wps=98.5, ups=0.6, wpb=109.3, bsz=40, num_updates=11080, lr=4.63125e-05, gnorm=0.707, clip=10, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=61665
2023-01-10 07:01:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:35 - progress_bar.py[line:274] - INFO: epoch 001:  11105 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3514, wps=99.3, ups=0.61, wpb=108.7, bsz=40, num_updates=11090, lr=4.63073e-05, gnorm=0.578, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=61681
2023-01-10 07:01:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:51 - progress_bar.py[line:274] - INFO: epoch 001:  11115 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.233, wps=105, ups=0.63, wpb=110.7, bsz=40, num_updates=11100, lr=4.63021e-05, gnorm=1.521, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=61697
2023-01-10 07:01:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:01:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:08 - progress_bar.py[line:274] - INFO: epoch 001:  11125 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2692, wps=100.3, ups=0.61, wpb=110.5, bsz=40, num_updates=11110, lr=4.62969e-05, gnorm=1.105, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=61714
2023-01-10 07:02:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:25 - progress_bar.py[line:274] - INFO: epoch 001:  11135 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2596, wps=101.5, ups=0.62, wpb=109.9, bsz=40, num_updates=11120, lr=4.62917e-05, gnorm=0.873, clip=30, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=61731
2023-01-10 07:02:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:41 - progress_bar.py[line:274] - INFO: epoch 001:  11145 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3611, wps=101.4, ups=0.62, wpb=108.9, bsz=40, num_updates=11130, lr=4.62865e-05, gnorm=0.917, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=61747
2023-01-10 07:02:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:02:57 - progress_bar.py[line:274] - INFO: epoch 001:  11155 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4327, wps=103.9, ups=0.62, wpb=111.4, bsz=40, num_updates=11140, lr=4.62813e-05, gnorm=1.505, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=61764
2023-01-10 07:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:14 - progress_bar.py[line:274] - INFO: epoch 001:  11165 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2936, wps=98.8, ups=0.6, wpb=109.4, bsz=40, num_updates=11150, lr=4.6276e-05, gnorm=1.813, clip=50, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=61780
2023-01-10 07:03:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:31 - progress_bar.py[line:274] - INFO: epoch 001:  11175 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3143, wps=102.2, ups=0.61, wpb=111.3, bsz=40, num_updates=11160, lr=4.62708e-05, gnorm=0.71, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=61797
2023-01-10 07:03:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:48 - progress_bar.py[line:274] - INFO: epoch 001:  11185 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.268, wps=99.7, ups=0.6, wpb=110.3, bsz=40, num_updates=11170, lr=4.62656e-05, gnorm=0.527, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=61814
2023-01-10 07:03:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:03:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:05 - progress_bar.py[line:274] - INFO: epoch 001:  11195 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.231, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2692, wps=98.9, ups=0.6, wpb=109.9, bsz=40, num_updates=11180, lr=4.62604e-05, gnorm=1.256, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=61831
2023-01-10 07:04:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:21 - progress_bar.py[line:274] - INFO: epoch 001:  11205 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2667, wps=101.4, ups=0.62, wpb=109.5, bsz=40, num_updates=11190, lr=4.62552e-05, gnorm=0.771, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=61847
2023-01-10 07:04:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:38 - progress_bar.py[line:274] - INFO: epoch 001:  11215 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2708, wps=101.4, ups=0.61, wpb=110.9, bsz=40, num_updates=11200, lr=4.625e-05, gnorm=1.609, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=61864
2023-01-10 07:04:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:55 - progress_bar.py[line:274] - INFO: epoch 001:  11225 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2947, wps=100.8, ups=0.61, wpb=110.1, bsz=40, num_updates=11210, lr=4.62448e-05, gnorm=0.884, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=61881
2023-01-10 07:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:04:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:11 - progress_bar.py[line:274] - INFO: epoch 001:  11235 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.26, wps=100.7, ups=0.61, wpb=109.9, bsz=40, num_updates=11220, lr=4.62396e-05, gnorm=0.558, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=61897
2023-01-10 07:05:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:28 - progress_bar.py[line:274] - INFO: epoch 001:  11245 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2432, wps=99, ups=0.61, wpb=108.9, bsz=40, num_updates=11230, lr=4.62344e-05, gnorm=1.4, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=61914
2023-01-10 07:05:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:44 - progress_bar.py[line:274] - INFO: epoch 001:  11255 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2198, wps=101.7, ups=0.61, wpb=110.4, bsz=40, num_updates=11240, lr=4.62292e-05, gnorm=1.012, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=61931
2023-01-10 07:05:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:05:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:01 - progress_bar.py[line:274] - INFO: epoch 001:  11265 / 100000 loss=0.429, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.23, vqa_score=0.2294, wps=98.7, ups=0.61, wpb=108.3, bsz=40, num_updates=11250, lr=4.6224e-05, gnorm=1.45, clip=60, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=61947
2023-01-10 07:06:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:18 - progress_bar.py[line:274] - INFO: epoch 001:  11275 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3011, wps=99, ups=0.6, wpb=110.7, bsz=40, num_updates=11260, lr=4.62188e-05, gnorm=1.045, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=61964
2023-01-10 07:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:35 - progress_bar.py[line:274] - INFO: epoch 001:  11285 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2883, wps=102, ups=0.62, wpb=109.2, bsz=40, num_updates=11270, lr=4.62135e-05, gnorm=0.656, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=61981
2023-01-10 07:06:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:51 - progress_bar.py[line:274] - INFO: epoch 001:  11295 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2617, wps=100.1, ups=0.62, wpb=108.3, bsz=40, num_updates=11280, lr=4.62083e-05, gnorm=0.63, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=61997
2023-01-10 07:06:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:06:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:08 - progress_bar.py[line:274] - INFO: epoch 001:  11305 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2449, wps=101, ups=0.62, wpb=109.3, bsz=40, num_updates=11290, lr=4.62031e-05, gnorm=1.142, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=62014
2023-01-10 07:07:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:24 - progress_bar.py[line:274] - INFO: epoch 001:  11315 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2414, wps=104.8, ups=0.63, wpb=110.9, bsz=40, num_updates=11300, lr=4.61979e-05, gnorm=0.807, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=62030
2023-01-10 07:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:40 - progress_bar.py[line:274] - INFO: epoch 001:  11325 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3762, wps=106.5, ups=0.64, wpb=110.4, bsz=40, num_updates=11310, lr=4.61927e-05, gnorm=0.703, clip=30, loss_scale=512, train_wall=15, gb_free=10.2, ema_decay=0.9999, wall=62046
2023-01-10 07:07:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:56 - progress_bar.py[line:274] - INFO: epoch 001:  11335 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.297, wps=99.9, ups=0.61, wpb=109, bsz=40, num_updates=11320, lr=4.61875e-05, gnorm=0.7, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=62062
2023-01-10 07:07:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:07:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:13 - progress_bar.py[line:274] - INFO: epoch 001:  11345 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4082, wps=98.8, ups=0.6, wpb=110.3, bsz=40, num_updates=11330, lr=4.61823e-05, gnorm=0.451, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=62079
2023-01-10 07:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:29 - progress_bar.py[line:274] - INFO: epoch 001:  11355 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2609, wps=105.1, ups=0.63, wpb=111.3, bsz=40, num_updates=11340, lr=4.61771e-05, gnorm=0.779, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62095
2023-01-10 07:08:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:46 - progress_bar.py[line:274] - INFO: epoch 001:  11365 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3721, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=11350, lr=4.61719e-05, gnorm=1.478, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62112
2023-01-10 07:08:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:08:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:02 - progress_bar.py[line:274] - INFO: epoch 001:  11375 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3366, wps=100.7, ups=0.61, wpb=109.4, bsz=40, num_updates=11360, lr=4.61667e-05, gnorm=1.313, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62129
2023-01-10 07:09:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:19 - progress_bar.py[line:274] - INFO: epoch 001:  11385 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.186, wps=100.5, ups=0.61, wpb=110.3, bsz=40, num_updates=11370, lr=4.61615e-05, gnorm=0.656, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62145
2023-01-10 07:09:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:36 - progress_bar.py[line:274] - INFO: epoch 001:  11395 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.297, wps=100.9, ups=0.62, wpb=109, bsz=40, num_updates=11380, lr=4.61563e-05, gnorm=0.738, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62162
2023-01-10 07:09:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:53 - progress_bar.py[line:274] - INFO: epoch 001:  11405 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2386, wps=99.5, ups=0.6, wpb=109.7, bsz=40, num_updates=11390, lr=4.6151e-05, gnorm=0.43, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62179
2023-01-10 07:09:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:09:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:09 - progress_bar.py[line:274] - INFO: epoch 001:  11415 / 100000 loss=0.386, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3186, wps=98.4, ups=0.6, wpb=108.5, bsz=40, num_updates=11400, lr=4.61458e-05, gnorm=1.177, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62196
2023-01-10 07:10:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:26 - progress_bar.py[line:274] - INFO: epoch 001:  11425 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3238, wps=103, ups=0.62, wpb=111.1, bsz=40, num_updates=11410, lr=4.61406e-05, gnorm=0.541, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62212
2023-01-10 07:10:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:43 - progress_bar.py[line:274] - INFO: epoch 001:  11435 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2857, wps=99.1, ups=0.61, wpb=108.5, bsz=40, num_updates=11420, lr=4.61354e-05, gnorm=0.638, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62229
2023-01-10 07:10:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:10:59 - progress_bar.py[line:274] - INFO: epoch 001:  11445 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3398, wps=102.9, ups=0.62, wpb=111, bsz=40, num_updates=11430, lr=4.61302e-05, gnorm=0.526, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=62245
2023-01-10 07:10:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:15 - progress_bar.py[line:274] - INFO: epoch 001:  11455 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3111, wps=103.5, ups=0.62, wpb=110.9, bsz=40, num_updates=11440, lr=4.6125e-05, gnorm=0.814, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=62262
2023-01-10 07:11:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:32 - progress_bar.py[line:274] - INFO: epoch 001:  11465 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3478, wps=101.6, ups=0.61, wpb=111.4, bsz=40, num_updates=11450, lr=4.61198e-05, gnorm=0.88, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62278
2023-01-10 07:11:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:49 - progress_bar.py[line:274] - INFO: epoch 001:  11475 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2551, wps=101.4, ups=0.61, wpb=110.1, bsz=40, num_updates=11460, lr=4.61146e-05, gnorm=0.518, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=62295
2023-01-10 07:11:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:11:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:06 - progress_bar.py[line:274] - INFO: epoch 001:  11485 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.27, wps=99.5, ups=0.6, wpb=110.7, bsz=40, num_updates=11470, lr=4.61094e-05, gnorm=0.691, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=62312
2023-01-10 07:12:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:22 - progress_bar.py[line:274] - INFO: epoch 001:  11495 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2059, wps=100.6, ups=0.62, wpb=108.9, bsz=40, num_updates=11480, lr=4.61042e-05, gnorm=0.568, clip=20, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=62328
2023-01-10 07:12:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:39 - progress_bar.py[line:274] - INFO: epoch 001:  11505 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2772, wps=101.4, ups=0.62, wpb=109.5, bsz=40, num_updates=11490, lr=4.6099e-05, gnorm=0.75, clip=30, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=62345
2023-01-10 07:12:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:55 - progress_bar.py[line:274] - INFO: epoch 001:  11515 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2804, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=11500, lr=4.60937e-05, gnorm=0.645, clip=20, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62361
2023-01-10 07:12:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:12:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:12 - progress_bar.py[line:274] - INFO: epoch 001:  11525 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3667, wps=102.3, ups=0.62, wpb=110.5, bsz=40, num_updates=11510, lr=4.60885e-05, gnorm=0.644, clip=20, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62378
2023-01-10 07:13:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:28 - progress_bar.py[line:274] - INFO: epoch 001:  11535 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3868, wps=101.3, ups=0.62, wpb=108.5, bsz=40, num_updates=11520, lr=4.60833e-05, gnorm=0.561, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62394
2023-01-10 07:13:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:45 - progress_bar.py[line:274] - INFO: epoch 001:  11545 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2065, wps=101.6, ups=0.61, wpb=110.2, bsz=40, num_updates=11530, lr=4.60781e-05, gnorm=0.544, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62411
2023-01-10 07:13:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:01 - progress_bar.py[line:274] - INFO: epoch 001:  11555 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2844, wps=100.1, ups=0.61, wpb=109.5, bsz=40, num_updates=11540, lr=4.60729e-05, gnorm=0.469, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62428
2023-01-10 07:14:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:18 - progress_bar.py[line:274] - INFO: epoch 001:  11565 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3511, wps=102.2, ups=0.61, wpb=110.9, bsz=40, num_updates=11550, lr=4.60677e-05, gnorm=0.527, clip=10, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=62444
2023-01-10 07:14:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:35 - progress_bar.py[line:274] - INFO: epoch 001:  11575 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3036, wps=99.5, ups=0.61, wpb=108.9, bsz=40, num_updates=11560, lr=4.60625e-05, gnorm=0.563, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62461
2023-01-10 07:14:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:49 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-10 07:14:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:53 - progress_bar.py[line:274] - INFO: epoch 001:  11586 / 100000 loss=0.389, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=108.562, nsentences=40, sample_size=108.562, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2131, wps=94.7, ups=0.54, wpb=108.6, bsz=40, num_updates=11570, lr=4.60573e-05, gnorm=1.193, clip=30, loss_scale=512, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=62479
2023-01-10 07:14:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:14:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:10 - progress_bar.py[line:274] - INFO: epoch 001:  11596 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3542, wps=100.5, ups=0.61, wpb=110.2, bsz=40, num_updates=11580, lr=4.60521e-05, gnorm=0.539, clip=10, loss_scale=512, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=62496
2023-01-10 07:15:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:26 - progress_bar.py[line:274] - INFO: epoch 001:  11606 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3474, wps=102, ups=0.61, wpb=110.9, bsz=40, num_updates=11590, lr=4.60469e-05, gnorm=0.746, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=62513
2023-01-10 07:15:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:43 - progress_bar.py[line:274] - INFO: epoch 001:  11616 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2857, wps=101.7, ups=0.62, wpb=109.1, bsz=40, num_updates=11600, lr=4.60417e-05, gnorm=0.908, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62529
2023-01-10 07:15:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:15:59 - progress_bar.py[line:274] - INFO: epoch 001:  11626 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.25, wps=101.2, ups=0.61, wpb=110.3, bsz=40, num_updates=11610, lr=4.60365e-05, gnorm=0.748, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62546
2023-01-10 07:15:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:16 - progress_bar.py[line:274] - INFO: epoch 001:  11636 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.383, wps=99.4, ups=0.6, wpb=110.1, bsz=40, num_updates=11620, lr=4.60313e-05, gnorm=1.007, clip=20, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=62562
2023-01-10 07:16:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:33 - progress_bar.py[line:274] - INFO: epoch 001:  11646 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3204, wps=101, ups=0.62, wpb=109.1, bsz=40, num_updates=11630, lr=4.6026e-05, gnorm=1.084, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62579
2023-01-10 07:16:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:49 - progress_bar.py[line:274] - INFO: epoch 001:  11656 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3053, wps=101.5, ups=0.61, wpb=110.1, bsz=40, num_updates=11640, lr=4.60208e-05, gnorm=1.033, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62596
2023-01-10 07:16:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:06 - progress_bar.py[line:274] - INFO: epoch 001:  11666 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3714, wps=100.4, ups=0.62, wpb=108.3, bsz=40, num_updates=11650, lr=4.60156e-05, gnorm=0.615, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62612
2023-01-10 07:17:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:22 - progress_bar.py[line:274] - INFO: epoch 001:  11676 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.23, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.404, wps=99.6, ups=0.61, wpb=108.7, bsz=40, num_updates=11660, lr=4.60104e-05, gnorm=0.998, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=62629
2023-01-10 07:17:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:39 - progress_bar.py[line:274] - INFO: epoch 001:  11686 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2474, wps=100.4, ups=0.61, wpb=110.3, bsz=40, num_updates=11670, lr=4.60052e-05, gnorm=0.596, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62645
2023-01-10 07:17:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:55 - progress_bar.py[line:274] - INFO: epoch 001:  11696 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3333, wps=103.8, ups=0.63, wpb=110.7, bsz=40, num_updates=11680, lr=4.6e-05, gnorm=0.679, clip=20, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=62662
2023-01-10 07:17:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:17:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:12 - progress_bar.py[line:274] - INFO: epoch 001:  11706 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3056, wps=99.6, ups=0.61, wpb=108.5, bsz=40, num_updates=11690, lr=4.59948e-05, gnorm=1.983, clip=50, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=62678
2023-01-10 07:18:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:28 - progress_bar.py[line:274] - INFO: epoch 001:  11716 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2326, wps=102.5, ups=0.62, wpb=110.5, bsz=40, num_updates=11700, lr=4.59896e-05, gnorm=0.495, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62695
2023-01-10 07:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:45 - progress_bar.py[line:274] - INFO: epoch 001:  11726 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.221, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.29, wps=101.4, ups=0.62, wpb=109, bsz=40, num_updates=11710, lr=4.59844e-05, gnorm=1.803, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62711
2023-01-10 07:18:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:18:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:01 - progress_bar.py[line:274] - INFO: epoch 001:  11736 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.287, wps=100.4, ups=0.61, wpb=108.9, bsz=40, num_updates=11720, lr=4.59792e-05, gnorm=0.857, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62728
2023-01-10 07:19:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:18 - progress_bar.py[line:274] - INFO: epoch 001:  11746 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2692, wps=102, ups=0.62, wpb=110.1, bsz=40, num_updates=11730, lr=4.5974e-05, gnorm=0.537, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=62744
2023-01-10 07:19:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:34 - progress_bar.py[line:274] - INFO: epoch 001:  11756 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3019, wps=102.4, ups=0.63, wpb=109.1, bsz=40, num_updates=11740, lr=4.59688e-05, gnorm=0.794, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62760
2023-01-10 07:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:51 - progress_bar.py[line:274] - INFO: epoch 001:  11766 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2762, wps=101.5, ups=0.62, wpb=108.6, bsz=40, num_updates=11750, lr=4.59635e-05, gnorm=0.685, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=62777
2023-01-10 07:19:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:19:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:07 - progress_bar.py[line:274] - INFO: epoch 001:  11776 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2921, wps=102, ups=0.62, wpb=110.1, bsz=40, num_updates=11760, lr=4.59583e-05, gnorm=0.757, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62793
2023-01-10 07:20:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:24 - progress_bar.py[line:274] - INFO: epoch 001:  11786 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3053, wps=100.4, ups=0.61, wpb=110.3, bsz=40, num_updates=11770, lr=4.59531e-05, gnorm=1.037, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=62810
2023-01-10 07:20:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:41 - progress_bar.py[line:274] - INFO: epoch 001:  11796 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.314, wps=99.3, ups=0.6, wpb=110.1, bsz=40, num_updates=11780, lr=4.59479e-05, gnorm=0.73, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=62827
2023-01-10 07:20:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:57 - progress_bar.py[line:274] - INFO: epoch 001:  11806 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2842, wps=100.7, ups=0.61, wpb=109.9, bsz=40, num_updates=11790, lr=4.59427e-05, gnorm=0.89, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=62843
2023-01-10 07:20:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:20:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:14 - progress_bar.py[line:274] - INFO: epoch 001:  11816 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3483, wps=98.3, ups=0.6, wpb=109.5, bsz=40, num_updates=11800, lr=4.59375e-05, gnorm=0.915, clip=40, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=62860
2023-01-10 07:21:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:31 - progress_bar.py[line:274] - INFO: epoch 001:  11826 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3645, wps=101.9, ups=0.61, wpb=110.9, bsz=40, num_updates=11810, lr=4.59323e-05, gnorm=0.52, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=62877
2023-01-10 07:21:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:48 - progress_bar.py[line:274] - INFO: epoch 001:  11836 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2692, wps=97.8, ups=0.6, wpb=108.4, bsz=40, num_updates=11820, lr=4.59271e-05, gnorm=0.906, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=62894
2023-01-10 07:21:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:21:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:04 - progress_bar.py[line:274] - INFO: epoch 001:  11846 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2553, wps=101.4, ups=0.61, wpb=110, bsz=40, num_updates=11830, lr=4.59219e-05, gnorm=0.912, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=62910
2023-01-10 07:22:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:21 - progress_bar.py[line:274] - INFO: epoch 001:  11856 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3258, wps=100.7, ups=0.6, wpb=111.5, bsz=40, num_updates=11840, lr=4.59167e-05, gnorm=0.962, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=62927
2023-01-10 07:22:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:38 - progress_bar.py[line:274] - INFO: epoch 001:  11866 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3962, wps=100.5, ups=0.61, wpb=109.4, bsz=40, num_updates=11850, lr=4.59115e-05, gnorm=0.458, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=62944
2023-01-10 07:22:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:54 - progress_bar.py[line:274] - INFO: epoch 001:  11876 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3608, wps=101, ups=0.62, wpb=108.9, bsz=40, num_updates=11860, lr=4.59063e-05, gnorm=0.494, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=62960
2023-01-10 07:22:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:11 - progress_bar.py[line:274] - INFO: epoch 001:  11886 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2449, wps=100, ups=0.6, wpb=110.2, bsz=40, num_updates=11870, lr=4.5901e-05, gnorm=0.774, clip=20, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=62977
2023-01-10 07:23:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:27 - progress_bar.py[line:274] - INFO: epoch 001:  11896 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2762, wps=100.7, ups=0.62, wpb=108.4, bsz=40, num_updates=11880, lr=4.58958e-05, gnorm=0.996, clip=50, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=62993
2023-01-10 07:23:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:44 - progress_bar.py[line:274] - INFO: epoch 001:  11906 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3398, wps=99.6, ups=0.61, wpb=108.3, bsz=40, num_updates=11890, lr=4.58906e-05, gnorm=0.733, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=63010
2023-01-10 07:23:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:23:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:00 - progress_bar.py[line:274] - INFO: epoch 001:  11916 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3077, wps=102.6, ups=0.62, wpb=109.9, bsz=40, num_updates=11900, lr=4.58854e-05, gnorm=0.436, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=63026
2023-01-10 07:24:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:16 - progress_bar.py[line:274] - INFO: epoch 001:  11926 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3125, wps=103, ups=0.63, wpb=109, bsz=40, num_updates=11910, lr=4.58802e-05, gnorm=0.615, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=63042
2023-01-10 07:24:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:33 - progress_bar.py[line:274] - INFO: epoch 001:  11936 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2903, wps=99.4, ups=0.6, wpb=110.1, bsz=40, num_updates=11920, lr=4.5875e-05, gnorm=0.614, clip=20, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=63059
2023-01-10 07:24:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:50 - progress_bar.py[line:274] - INFO: epoch 001:  11946 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2745, wps=98.2, ups=0.6, wpb=109.5, bsz=40, num_updates=11930, lr=4.58698e-05, gnorm=0.693, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=63076
2023-01-10 07:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:24:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:07 - progress_bar.py[line:274] - INFO: epoch 001:  11956 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3333, wps=102.3, ups=0.62, wpb=110.5, bsz=40, num_updates=11940, lr=4.58646e-05, gnorm=1.145, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=63093
2023-01-10 07:25:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:23 - progress_bar.py[line:274] - INFO: epoch 001:  11966 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3646, wps=103.5, ups=0.62, wpb=111.5, bsz=40, num_updates=11950, lr=4.58594e-05, gnorm=0.68, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63109
2023-01-10 07:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:40 - progress_bar.py[line:274] - INFO: epoch 001:  11976 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2842, wps=101.9, ups=0.61, wpb=111.2, bsz=40, num_updates=11960, lr=4.58542e-05, gnorm=0.603, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63126
2023-01-10 07:25:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:57 - progress_bar.py[line:274] - INFO: epoch 001:  11986 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3434, wps=100.5, ups=0.6, wpb=111, bsz=40, num_updates=11970, lr=4.5849e-05, gnorm=0.676, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=63143
2023-01-10 07:25:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:25:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:13 - progress_bar.py[line:274] - INFO: epoch 001:  11996 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2991, wps=102, ups=0.62, wpb=108.9, bsz=40, num_updates=11980, lr=4.58437e-05, gnorm=0.838, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=63159
2023-01-10 07:26:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:30 - progress_bar.py[line:274] - INFO: epoch 001:  12006 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3711, wps=98.7, ups=0.6, wpb=109.5, bsz=40, num_updates=11990, lr=4.58385e-05, gnorm=1.254, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=63176
2023-01-10 07:26:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 07:26:46 - progress_bar.py[line:274] - INFO: epoch 001:  12016 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3778, wps=103, ups=0.62, wpb=110.6, bsz=40, num_updates=12000, lr=4.58333e-05, gnorm=1.301, clip=40, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=63192
2023-01-10 07:26:46 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 07:26:48 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 07:26:48 - train.py[line:551] - INFO: load:1.33 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 07:29:20 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 07:29:20 - train.py[line:551] - INFO: load:1.36 valid_run:151.71 task_valid:148.79 collect_output:1.60
2023-01-10 07:31:48 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 07:31:48 - train.py[line:551] - INFO: load:1.39 valid_run:299.74 task_valid:292.31 collect_output:5.03
2023-01-10 07:34:19 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 07:34:19 - train.py[line:551] - INFO: load:1.42 valid_run:451.18 task_valid:435.69 collect_output:12.01
2023-01-10 07:36:48 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 07:36:48 - train.py[line:551] - INFO: load:1.45 valid_run:599.97 task_valid:580.94 collect_output:14.47
2023-01-10 07:39:20 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 07:39:20 - train.py[line:551] - INFO: load:1.48 valid_run:751.70 task_valid:728.43 collect_output:17.65
2023-01-10 07:41:51 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 07:41:51 - train.py[line:551] - INFO: load:1.51 valid_run:902.83 task_valid:874.09 collect_output:22.04
2023-01-10 07:44:23 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 07:44:23 - train.py[line:551] - INFO: load:1.54 valid_run:1054.91 task_valid:1020.23 collect_output:26.91
2023-01-10 07:46:53 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 07:46:53 - train.py[line:551] - INFO: load:1.57 valid_run:1204.93 task_valid:1161.88 collect_output:34.21
2023-01-10 07:49:22 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 07:49:23 - train.py[line:551] - INFO: load:1.61 valid_run:1353.87 task_valid:1306.79 collect_output:37.17
2023-01-10 07:51:50 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 07:51:50 - train.py[line:551] - INFO: load:1.64 valid_run:1501.79 task_valid:1450.14 collect_output:40.67
2023-01-10 07:54:20 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 07:54:20 - train.py[line:551] - INFO: load:1.67 valid_run:1650.95 task_valid:1595.22 collect_output:43.68
2023-01-10 07:56:49 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 07:56:49 - train.py[line:551] - INFO: load:1.70 valid_run:1800.48 task_valid:1740.71 collect_output:46.65
2023-01-10 07:59:18 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 07:59:18 - train.py[line:551] - INFO: load:1.73 valid_run:1949.38 task_valid:1882.76 collect_output:52.42
2023-01-10 08:01:49 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 08:01:49 - train.py[line:551] - INFO: load:1.76 valid_run:2099.69 task_valid:2028.68 collect_output:55.73
2023-01-10 08:04:18 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 08:04:18 - train.py[line:551] - INFO: load:1.79 valid_run:2249.34 task_valid:2175.24 collect_output:57.74
2023-01-10 08:06:48 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 08:06:48 - train.py[line:551] - INFO: load:1.83 valid_run:2398.79 task_valid:2319.95 collect_output:61.40
2023-01-10 08:09:18 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 08:09:18 - train.py[line:551] - INFO: load:1.86 valid_run:2549.13 task_valid:2465.52 collect_output:65.13
2023-01-10 08:11:49 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 08:11:49 - train.py[line:551] - INFO: load:1.89 valid_run:2699.35 task_valid:2612.67 collect_output:67.14
2023-01-10 08:14:16 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 08:14:16 - train.py[line:551] - INFO: load:1.92 valid_run:2846.76 task_valid:2754.64 collect_output:71.50
2023-01-10 08:16:46 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 08:16:46 - train.py[line:551] - INFO: load:1.95 valid_run:2996.62 task_valid:2900.48 collect_output:74.48
2023-01-10 08:19:17 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 08:19:17 - train.py[line:551] - INFO: load:1.98 valid_run:3147.15 task_valid:3045.48 collect_output:78.93
2023-01-10 08:21:46 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 08:21:46 - train.py[line:551] - INFO: load:2.01 valid_run:3296.12 task_valid:3190.40 collect_output:81.90
2023-01-10 08:24:17 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 08:24:17 - train.py[line:551] - INFO: load:2.04 valid_run:3446.78 task_valid:3337.22 collect_output:84.67
2023-01-10 08:26:47 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 08:26:48 - train.py[line:551] - INFO: load:2.08 valid_run:3597.54 task_valid:3484.10 collect_output:87.50

====================================================================================================
SGG eval:     R @ 50: 0.5809;     R @ 100: 0.6458;     R @ 500: 0.6928;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3707;    mR @ 100: 0.4174;    mR @ 500: 0.4605;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7854) (covered in:0.8750) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4032) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.8859) (says:0.0000) (sitting on:0.7829) (standing on:0.3343) (using:0.6500) (walking in:0.0000) (walking on:0.6486) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5809;     R @ 100: 0.6458;     R @ 500: 0.6928;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3707;    mR @ 100: 0.4174;    mR @ 500: 0.4605;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7854) (covered in:0.8750) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4032) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.8859) (says:0.0000) (sitting on:0.7829) (standing on:0.3343) (using:0.6500) (walking in:0.0000) (walking on:0.6486) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-10 08:29:19 - train.py[line:487] - INFO: 0.6458481792717087
2023-01-10 08:29:20 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 08:29:20 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.308 | loss_v1 0 | loss_v2 0 | nll_loss 0.15 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.645848 | ppl 1.11 | vqa_score 0.5856 | wps 119.6 | wpb 89.9 | bsz 30 | num_updates 12000 | best_R@100 0.69005
2023-01-10 08:29:20 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 12000 updates
2023-01-10 08:29:20 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt
2023-01-10 08:30:00 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt
2023-01-10 08:31:20 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_12000.pt (epoch 1 @ 12000 updates, score 0.6458481792717087) (writing took 120.57217068225145 seconds)
2023-01-10 08:31:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:27 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-10 08:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:39 - progress_bar.py[line:274] - INFO: epoch 001:  12027 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.75, nsentences=40, sample_size=110.75, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3306, wps=0.5, ups=0, wpb=110.8, bsz=40, num_updates=12010, lr=4.58281e-05, gnorm=1.406, clip=30, loss_scale=256, train_wall=18, gb_free=10.4, ema_decay=0.9999, wall=67085
2023-01-10 08:31:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:56 - progress_bar.py[line:274] - INFO: epoch 001:  12037 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.34, wps=100, ups=0.61, wpb=109.4, bsz=40, num_updates=12020, lr=4.58229e-05, gnorm=1.607, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=67102
2023-01-10 08:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:31:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:13 - progress_bar.py[line:274] - INFO: epoch 001:  12047 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2642, wps=99.9, ups=0.61, wpb=109.4, bsz=40, num_updates=12030, lr=4.58177e-05, gnorm=1.981, clip=40, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=67119
2023-01-10 08:32:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:29 - progress_bar.py[line:274] - INFO: epoch 001:  12057 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2979, wps=102.1, ups=0.62, wpb=109.7, bsz=40, num_updates=12040, lr=4.58125e-05, gnorm=0.721, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=67135
2023-01-10 08:32:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:46 - progress_bar.py[line:274] - INFO: epoch 001:  12067 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2697, wps=100.8, ups=0.61, wpb=110.3, bsz=40, num_updates=12050, lr=4.58073e-05, gnorm=0.696, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=67152
2023-01-10 08:32:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:32:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:02 - progress_bar.py[line:274] - INFO: epoch 001:  12077 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2913, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=12060, lr=4.58021e-05, gnorm=1.381, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67169
2023-01-10 08:33:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:19 - progress_bar.py[line:274] - INFO: epoch 001:  12087 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3119, wps=101.2, ups=0.62, wpb=108.7, bsz=40, num_updates=12070, lr=4.57969e-05, gnorm=0.624, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=67185
2023-01-10 08:33:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:36 - progress_bar.py[line:274] - INFO: epoch 001:  12097 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.314, wps=101.6, ups=0.61, wpb=111.7, bsz=40, num_updates=12080, lr=4.57917e-05, gnorm=1.1, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=67202
2023-01-10 08:33:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:53 - progress_bar.py[line:274] - INFO: epoch 001:  12107 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3663, wps=98.3, ups=0.6, wpb=109.9, bsz=40, num_updates=12090, lr=4.57865e-05, gnorm=0.485, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=67219
2023-01-10 08:33:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:33:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:09 - progress_bar.py[line:274] - INFO: epoch 001:  12117 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3333, wps=100.8, ups=0.62, wpb=108, bsz=40, num_updates=12100, lr=4.57813e-05, gnorm=1.907, clip=70, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=67235
2023-01-10 08:34:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:26 - progress_bar.py[line:274] - INFO: epoch 001:  12127 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.266, wps=100.4, ups=0.61, wpb=109.3, bsz=40, num_updates=12110, lr=4.5776e-05, gnorm=0.972, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67252
2023-01-10 08:34:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:42 - progress_bar.py[line:274] - INFO: epoch 001:  12137 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3333, wps=103.3, ups=0.62, wpb=110.4, bsz=40, num_updates=12120, lr=4.57708e-05, gnorm=1.276, clip=50, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67268
2023-01-10 08:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:34:59 - progress_bar.py[line:274] - INFO: epoch 001:  12147 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3291, wps=102.3, ups=0.61, wpb=111.6, bsz=40, num_updates=12130, lr=4.57656e-05, gnorm=0.516, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67285
2023-01-10 08:34:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:15 - progress_bar.py[line:274] - INFO: epoch 001:  12157 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3929, wps=100, ups=0.62, wpb=108, bsz=40, num_updates=12140, lr=4.57604e-05, gnorm=0.588, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67301
2023-01-10 08:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:32 - progress_bar.py[line:274] - INFO: epoch 001:  12167 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2444, wps=102.1, ups=0.62, wpb=109.5, bsz=40, num_updates=12150, lr=4.57552e-05, gnorm=0.795, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67318
2023-01-10 08:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:48 - progress_bar.py[line:274] - INFO: epoch 001:  12177 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3646, wps=101.8, ups=0.62, wpb=109.2, bsz=40, num_updates=12160, lr=4.575e-05, gnorm=0.876, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67334
2023-01-10 08:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:35:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:05 - progress_bar.py[line:274] - INFO: epoch 001:  12187 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3365, wps=101.1, ups=0.61, wpb=110, bsz=40, num_updates=12170, lr=4.57448e-05, gnorm=0.895, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67351
2023-01-10 08:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:22 - progress_bar.py[line:274] - INFO: epoch 001:  12197 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2788, wps=98.8, ups=0.6, wpb=109.5, bsz=40, num_updates=12180, lr=4.57396e-05, gnorm=0.752, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=67368
2023-01-10 08:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:26 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-10 08:36:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:40 - progress_bar.py[line:274] - INFO: epoch 001:  12208 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.812, nsentences=40, sample_size=109.812, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3308, wps=95, ups=0.54, wpb=109.8, bsz=40, num_updates=12190, lr=4.57344e-05, gnorm=0.523, clip=0, loss_scale=128, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=67387
2023-01-10 08:36:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:57 - progress_bar.py[line:274] - INFO: epoch 001:  12218 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3226, wps=100.7, ups=0.61, wpb=110.9, bsz=40, num_updates=12200, lr=4.57292e-05, gnorm=1.127, clip=40, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=67403
2023-01-10 08:36:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:36:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:14 - progress_bar.py[line:274] - INFO: epoch 001:  12228 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3434, wps=102.1, ups=0.62, wpb=109.9, bsz=40, num_updates=12210, lr=4.5724e-05, gnorm=0.442, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=67420
2023-01-10 08:37:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:30 - progress_bar.py[line:274] - INFO: epoch 001:  12238 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2202, wps=101.5, ups=0.62, wpb=109, bsz=40, num_updates=12220, lr=4.57187e-05, gnorm=1.71, clip=50, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67436
2023-01-10 08:37:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:47 - progress_bar.py[line:274] - INFO: epoch 001:  12248 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2857, wps=100.5, ups=0.61, wpb=109.4, bsz=40, num_updates=12230, lr=4.57135e-05, gnorm=1.588, clip=60, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=67453
2023-01-10 08:37:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:37:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:03 - progress_bar.py[line:274] - INFO: epoch 001:  12258 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3182, wps=98.3, ups=0.61, wpb=108.2, bsz=40, num_updates=12240, lr=4.57083e-05, gnorm=0.694, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=67470
2023-01-10 08:38:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:20 - progress_bar.py[line:274] - INFO: epoch 001:  12268 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.35, wps=104.1, ups=0.63, wpb=110.9, bsz=40, num_updates=12250, lr=4.57031e-05, gnorm=0.59, clip=10, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=67486
2023-01-10 08:38:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:36 - progress_bar.py[line:274] - INFO: epoch 001:  12278 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3804, wps=102.4, ups=0.62, wpb=110.9, bsz=40, num_updates=12260, lr=4.56979e-05, gnorm=0.672, clip=20, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=67502
2023-01-10 08:38:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:53 - progress_bar.py[line:274] - INFO: epoch 001:  12288 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3, wps=99.9, ups=0.61, wpb=109.7, bsz=40, num_updates=12270, lr=4.56927e-05, gnorm=0.616, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=67519
2023-01-10 08:38:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:38:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:09 - progress_bar.py[line:274] - INFO: epoch 001:  12298 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3333, wps=100.3, ups=0.61, wpb=109, bsz=40, num_updates=12280, lr=4.56875e-05, gnorm=0.809, clip=30, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=67536
2023-01-10 08:39:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:26 - progress_bar.py[line:274] - INFO: epoch 001:  12308 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2929, wps=99.7, ups=0.61, wpb=109.8, bsz=40, num_updates=12290, lr=4.56823e-05, gnorm=0.513, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=67552
2023-01-10 08:39:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:43 - progress_bar.py[line:274] - INFO: epoch 001:  12318 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.419, wps=101.2, ups=0.62, wpb=109.6, bsz=40, num_updates=12300, lr=4.56771e-05, gnorm=0.712, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=67569
2023-01-10 08:39:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:39:59 - progress_bar.py[line:274] - INFO: epoch 001:  12328 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2967, wps=103.2, ups=0.62, wpb=110.9, bsz=40, num_updates=12310, lr=4.56719e-05, gnorm=0.708, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=67585
2023-01-10 08:39:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:16 - progress_bar.py[line:274] - INFO: epoch 001:  12338 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.223, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2524, wps=102.1, ups=0.62, wpb=109.5, bsz=40, num_updates=12320, lr=4.56667e-05, gnorm=0.825, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=67602
2023-01-10 08:40:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:32 - progress_bar.py[line:274] - INFO: epoch 001:  12348 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3694, wps=99.9, ups=0.61, wpb=108.3, bsz=40, num_updates=12330, lr=4.56615e-05, gnorm=0.844, clip=30, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67618
2023-01-10 08:40:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:48 - progress_bar.py[line:274] - INFO: epoch 001:  12358 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2526, wps=104.1, ups=0.63, wpb=110.7, bsz=40, num_updates=12340, lr=4.56563e-05, gnorm=1.094, clip=30, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67635
2023-01-10 08:40:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:40:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:05 - progress_bar.py[line:274] - INFO: epoch 001:  12368 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3684, wps=98.4, ups=0.6, wpb=109.7, bsz=40, num_updates=12350, lr=4.5651e-05, gnorm=0.701, clip=20, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=67652
2023-01-10 08:41:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:22 - progress_bar.py[line:274] - INFO: epoch 001:  12378 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3229, wps=104.4, ups=0.62, wpb=111.7, bsz=40, num_updates=12360, lr=4.56458e-05, gnorm=0.332, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=67668
2023-01-10 08:41:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:38 - progress_bar.py[line:274] - INFO: epoch 001:  12388 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3511, wps=103, ups=0.62, wpb=111.1, bsz=40, num_updates=12370, lr=4.56406e-05, gnorm=0.941, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=67684
2023-01-10 08:41:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:55 - progress_bar.py[line:274] - INFO: epoch 001:  12398 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3131, wps=101.5, ups=0.61, wpb=110.7, bsz=40, num_updates=12380, lr=4.56354e-05, gnorm=1.06, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=67701
2023-01-10 08:41:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:41:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:11 - progress_bar.py[line:274] - INFO: epoch 001:  12408 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3039, wps=102.2, ups=0.62, wpb=109.9, bsz=40, num_updates=12390, lr=4.56302e-05, gnorm=1.285, clip=40, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67718
2023-01-10 08:42:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:28 - progress_bar.py[line:274] - INFO: epoch 001:  12418 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3048, wps=99.3, ups=0.6, wpb=109.8, bsz=40, num_updates=12400, lr=4.5625e-05, gnorm=0.432, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=67734
2023-01-10 08:42:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:45 - progress_bar.py[line:274] - INFO: epoch 001:  12428 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3304, wps=100.5, ups=0.62, wpb=108.9, bsz=40, num_updates=12410, lr=4.56198e-05, gnorm=2.042, clip=20, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=67751
2023-01-10 08:42:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:42:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:01 - progress_bar.py[line:274] - INFO: epoch 001:  12438 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.24, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3303, wps=99.9, ups=0.61, wpb=109.4, bsz=40, num_updates=12420, lr=4.56146e-05, gnorm=0.768, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=67768
2023-01-10 08:43:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:18 - progress_bar.py[line:274] - INFO: epoch 001:  12448 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.37, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=12430, lr=4.56094e-05, gnorm=0.843, clip=10, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=67784
2023-01-10 08:43:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:34 - progress_bar.py[line:274] - INFO: epoch 001:  12458 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2973, wps=103.6, ups=0.63, wpb=109.4, bsz=40, num_updates=12440, lr=4.56042e-05, gnorm=1.355, clip=30, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67800
2023-01-10 08:43:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:51 - progress_bar.py[line:274] - INFO: epoch 001:  12468 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3774, wps=100.5, ups=0.62, wpb=108.8, bsz=40, num_updates=12450, lr=4.5599e-05, gnorm=0.803, clip=30, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=67817
2023-01-10 08:43:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:43:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:07 - progress_bar.py[line:274] - INFO: epoch 001:  12478 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3902, wps=102.1, ups=0.61, wpb=111.5, bsz=40, num_updates=12460, lr=4.55938e-05, gnorm=1.064, clip=40, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67834
2023-01-10 08:44:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:23 - progress_bar.py[line:274] - INFO: epoch 001:  12488 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3936, wps=105.1, ups=0.64, wpb=110.1, bsz=40, num_updates=12470, lr=4.55885e-05, gnorm=1.521, clip=30, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67850
2023-01-10 08:44:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:40 - progress_bar.py[line:274] - INFO: epoch 001:  12498 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2708, wps=102.2, ups=0.62, wpb=110.5, bsz=40, num_updates=12480, lr=4.55833e-05, gnorm=0.43, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=67866
2023-01-10 08:44:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:56 - progress_bar.py[line:274] - INFO: epoch 001:  12508 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3263, wps=104.1, ups=0.63, wpb=109.9, bsz=40, num_updates=12490, lr=4.55781e-05, gnorm=1.095, clip=40, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=67882
2023-01-10 08:44:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:44:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:12 - progress_bar.py[line:274] - INFO: epoch 001:  12518 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.21, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.398, wps=103.8, ups=0.63, wpb=110.3, bsz=40, num_updates=12500, lr=4.55729e-05, gnorm=0.777, clip=30, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=67898
2023-01-10 08:45:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:29 - progress_bar.py[line:274] - INFO: epoch 001:  12528 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3474, wps=100.2, ups=0.6, wpb=111.1, bsz=40, num_updates=12510, lr=4.55677e-05, gnorm=1.006, clip=30, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=67915
2023-01-10 08:45:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:46 - progress_bar.py[line:274] - INFO: epoch 001:  12538 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2571, wps=98.2, ups=0.61, wpb=107.7, bsz=40, num_updates=12520, lr=4.55625e-05, gnorm=1.125, clip=40, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67932
2023-01-10 08:45:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:45:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:02 - progress_bar.py[line:274] - INFO: epoch 001:  12548 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3585, wps=102, ups=0.62, wpb=110, bsz=40, num_updates=12530, lr=4.55573e-05, gnorm=0.521, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=67948
2023-01-10 08:46:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:18 - progress_bar.py[line:274] - INFO: epoch 001:  12558 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3182, wps=103, ups=0.63, wpb=109.2, bsz=40, num_updates=12540, lr=4.55521e-05, gnorm=0.506, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=67965
2023-01-10 08:46:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:35 - progress_bar.py[line:274] - INFO: epoch 001:  12568 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3364, wps=98.4, ups=0.61, wpb=108.1, bsz=40, num_updates=12550, lr=4.55469e-05, gnorm=0.96, clip=40, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=67981
2023-01-10 08:46:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:52 - progress_bar.py[line:274] - INFO: epoch 001:  12578 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2688, wps=100.8, ups=0.61, wpb=110.3, bsz=40, num_updates=12560, lr=4.55417e-05, gnorm=1.251, clip=40, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=67998
2023-01-10 08:46:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:46:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:08 - progress_bar.py[line:274] - INFO: epoch 001:  12588 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3474, wps=101.6, ups=0.61, wpb=110.5, bsz=40, num_updates=12570, lr=4.55365e-05, gnorm=2.031, clip=40, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68015
2023-01-10 08:47:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:25 - progress_bar.py[line:274] - INFO: epoch 001:  12598 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3402, wps=101.3, ups=0.6, wpb=111.7, bsz=40, num_updates=12580, lr=4.55313e-05, gnorm=0.746, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68031
2023-01-10 08:47:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:41 - progress_bar.py[line:274] - INFO: epoch 001:  12608 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3608, wps=103.5, ups=0.63, wpb=109.6, bsz=40, num_updates=12590, lr=4.5526e-05, gnorm=0.987, clip=10, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=68047
2023-01-10 08:47:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:47:58 - progress_bar.py[line:274] - INFO: epoch 001:  12618 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3407, wps=99.4, ups=0.6, wpb=110, bsz=40, num_updates=12600, lr=4.55208e-05, gnorm=1.325, clip=30, loss_scale=128, train_wall=17, gb_free=10, ema_decay=0.9999, wall=68064
2023-01-10 08:47:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:15 - progress_bar.py[line:274] - INFO: epoch 001:  12628 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3804, wps=101.7, ups=0.61, wpb=111.3, bsz=40, num_updates=12610, lr=4.55156e-05, gnorm=0.489, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68081
2023-01-10 08:48:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:31 - progress_bar.py[line:274] - INFO: epoch 001:  12638 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.36, wps=106.3, ups=0.64, wpb=111.2, bsz=40, num_updates=12620, lr=4.55104e-05, gnorm=0.568, clip=10, loss_scale=128, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=68097
2023-01-10 08:48:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:47 - progress_bar.py[line:274] - INFO: epoch 001:  12648 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.34, wps=103.1, ups=0.62, wpb=110.3, bsz=40, num_updates=12630, lr=4.55052e-05, gnorm=0.801, clip=20, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=68113
2023-01-10 08:48:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:48:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:04 - progress_bar.py[line:274] - INFO: epoch 001:  12658 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2887, wps=98.7, ups=0.6, wpb=110, bsz=40, num_updates=12640, lr=4.55e-05, gnorm=0.795, clip=30, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=68130
2023-01-10 08:49:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:21 - progress_bar.py[line:274] - INFO: epoch 001:  12668 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3261, wps=100.4, ups=0.61, wpb=110.5, bsz=40, num_updates=12650, lr=4.54948e-05, gnorm=0.876, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68147
2023-01-10 08:49:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:37 - progress_bar.py[line:274] - INFO: epoch 001:  12678 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3684, wps=103.1, ups=0.63, wpb=109.7, bsz=40, num_updates=12660, lr=4.54896e-05, gnorm=0.618, clip=10, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=68163
2023-01-10 08:49:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:53 - progress_bar.py[line:274] - INFO: epoch 001:  12688 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3163, wps=102.9, ups=0.63, wpb=109.5, bsz=40, num_updates=12670, lr=4.54844e-05, gnorm=1.257, clip=40, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68179
2023-01-10 08:49:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:49:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:09 - progress_bar.py[line:274] - INFO: epoch 001:  12698 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2979, wps=103.7, ups=0.62, wpb=111.5, bsz=40, num_updates=12680, lr=4.54792e-05, gnorm=0.76, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68196
2023-01-10 08:50:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:26 - progress_bar.py[line:274] - INFO: epoch 001:  12708 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3619, wps=100.2, ups=0.62, wpb=108.3, bsz=40, num_updates=12690, lr=4.5474e-05, gnorm=0.822, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68212
2023-01-10 08:50:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:42 - progress_bar.py[line:274] - INFO: epoch 001:  12718 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.36, wps=100.1, ups=0.61, wpb=108.8, bsz=40, num_updates=12700, lr=4.54688e-05, gnorm=0.837, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68229
2023-01-10 08:50:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:50:59 - progress_bar.py[line:274] - INFO: epoch 001:  12728 / 100000 loss=0.374, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3235, wps=101, ups=0.62, wpb=109.5, bsz=40, num_updates=12710, lr=4.54635e-05, gnorm=1.062, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=68245
2023-01-10 08:50:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:15 - progress_bar.py[line:274] - INFO: epoch 001:  12738 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3095, wps=103.8, ups=0.63, wpb=110.5, bsz=40, num_updates=12720, lr=4.54583e-05, gnorm=0.539, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68261
2023-01-10 08:51:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:33 - progress_bar.py[line:274] - INFO: epoch 001:  12748 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3404, wps=96.2, ups=0.58, wpb=110.9, bsz=40, num_updates=12730, lr=4.54531e-05, gnorm=0.777, clip=20, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=68279
2023-01-10 08:51:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:50 - progress_bar.py[line:274] - INFO: epoch 001:  12758 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.29, wps=98.6, ups=0.6, wpb=109.5, bsz=40, num_updates=12740, lr=4.54479e-05, gnorm=0.473, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=68296
2023-01-10 08:51:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:51:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:06 - progress_bar.py[line:274] - INFO: epoch 001:  12768 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3111, wps=101.5, ups=0.61, wpb=110.9, bsz=40, num_updates=12750, lr=4.54427e-05, gnorm=1.39, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68312
2023-01-10 08:52:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:23 - progress_bar.py[line:274] - INFO: epoch 001:  12778 / 100000 loss=0.377, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.301, wps=100.8, ups=0.62, wpb=109.2, bsz=40, num_updates=12760, lr=4.54375e-05, gnorm=0.93, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68329
2023-01-10 08:52:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:40 - progress_bar.py[line:274] - INFO: epoch 001:  12788 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3933, wps=100.4, ups=0.6, wpb=111, bsz=40, num_updates=12770, lr=4.54323e-05, gnorm=1.013, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=68346
2023-01-10 08:52:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:57 - progress_bar.py[line:274] - INFO: epoch 001:  12798 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2688, wps=99.5, ups=0.6, wpb=110.8, bsz=40, num_updates=12780, lr=4.54271e-05, gnorm=0.602, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=68363
2023-01-10 08:52:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:52:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:13 - progress_bar.py[line:274] - INFO: epoch 001:  12808 / 100000 loss=0.385, loss_v1=0, loss_v2=0, nll_loss=0.239, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2925, wps=98.4, ups=0.6, wpb=109.3, bsz=40, num_updates=12790, lr=4.54219e-05, gnorm=0.606, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=68380
2023-01-10 08:53:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:30 - progress_bar.py[line:274] - INFO: epoch 001:  12818 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.37, wps=100.1, ups=0.61, wpb=110.3, bsz=40, num_updates=12800, lr=4.54167e-05, gnorm=0.47, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=68396
2023-01-10 08:53:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:47 - progress_bar.py[line:274] - INFO: epoch 001:  12828 / 100000 loss=0.376, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3085, wps=103.3, ups=0.62, wpb=110.3, bsz=40, num_updates=12810, lr=4.54115e-05, gnorm=0.726, clip=50, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68413
2023-01-10 08:53:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:53:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:03 - progress_bar.py[line:274] - INFO: epoch 001:  12838 / 100000 loss=0.395, loss_v1=0, loss_v2=0, nll_loss=0.25, ntokens=106.8, nsentences=40, sample_size=106.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, vqa_score=0.2273, wps=97, ups=0.61, wpb=106.8, bsz=40, num_updates=12820, lr=4.54063e-05, gnorm=0.627, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68430
2023-01-10 08:54:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:20 - progress_bar.py[line:274] - INFO: epoch 001:  12848 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3398, wps=101.2, ups=0.61, wpb=110.7, bsz=40, num_updates=12830, lr=4.5401e-05, gnorm=0.498, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=68446
2023-01-10 08:54:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:36 - progress_bar.py[line:274] - INFO: epoch 001:  12858 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2805, wps=102.7, ups=0.62, wpb=110.8, bsz=40, num_updates=12840, lr=4.53958e-05, gnorm=0.648, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=68463
2023-01-10 08:54:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:53 - progress_bar.py[line:274] - INFO: epoch 001:  12868 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3462, wps=101.3, ups=0.61, wpb=110, bsz=40, num_updates=12850, lr=4.53906e-05, gnorm=0.663, clip=20, loss_scale=256, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=68479
2023-01-10 08:54:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:54:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:09 - progress_bar.py[line:274] - INFO: epoch 001:  12878 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3095, wps=102, ups=0.61, wpb=110.6, bsz=40, num_updates=12860, lr=4.53854e-05, gnorm=0.853, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68496
2023-01-10 08:55:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:26 - progress_bar.py[line:274] - INFO: epoch 001:  12888 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3023, wps=102.8, ups=0.62, wpb=111.2, bsz=40, num_updates=12870, lr=4.53802e-05, gnorm=0.816, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68512
2023-01-10 08:55:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:42 - progress_bar.py[line:274] - INFO: epoch 001:  12898 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.28, wps=100.6, ups=0.62, wpb=108.4, bsz=40, num_updates=12880, lr=4.5375e-05, gnorm=1.169, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68529
2023-01-10 08:55:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:55:59 - progress_bar.py[line:274] - INFO: epoch 001:  12908 / 100000 loss=0.369, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2768, wps=100.5, ups=0.61, wpb=109.1, bsz=40, num_updates=12890, lr=4.53698e-05, gnorm=0.922, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68545
2023-01-10 08:55:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:16 - progress_bar.py[line:274] - INFO: epoch 001:  12918 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3168, wps=101.5, ups=0.61, wpb=110.9, bsz=40, num_updates=12900, lr=4.53646e-05, gnorm=0.521, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=68562
2023-01-10 08:56:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:32 - progress_bar.py[line:274] - INFO: epoch 001:  12928 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3558, wps=101.2, ups=0.62, wpb=108.5, bsz=40, num_updates=12910, lr=4.53594e-05, gnorm=0.706, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=68578
2023-01-10 08:56:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:49 - progress_bar.py[line:274] - INFO: epoch 001:  12938 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2885, wps=100.5, ups=0.6, wpb=110.9, bsz=40, num_updates=12920, lr=4.53542e-05, gnorm=0.587, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=68595
2023-01-10 08:56:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:56:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:05 - progress_bar.py[line:274] - INFO: epoch 001:  12948 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.22, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3529, wps=100.3, ups=0.61, wpb=110.2, bsz=40, num_updates=12930, lr=4.5349e-05, gnorm=0.708, clip=20, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=68612
2023-01-10 08:57:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:22 - progress_bar.py[line:274] - INFO: epoch 001:  12958 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2708, wps=102, ups=0.61, wpb=111.7, bsz=40, num_updates=12940, lr=4.53438e-05, gnorm=0.794, clip=20, loss_scale=256, train_wall=16, gb_free=9.2, ema_decay=0.9999, wall=68628
2023-01-10 08:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:39 - progress_bar.py[line:274] - INFO: epoch 001:  12968 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2772, wps=102.2, ups=0.63, wpb=108.9, bsz=40, num_updates=12950, lr=4.53385e-05, gnorm=0.659, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68645
2023-01-10 08:57:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:55 - progress_bar.py[line:274] - INFO: epoch 001:  12978 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.219, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3725, wps=101.9, ups=0.62, wpb=109.9, bsz=40, num_updates=12960, lr=4.53333e-05, gnorm=0.629, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=68661
2023-01-10 08:57:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:57:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:12 - progress_bar.py[line:274] - INFO: epoch 001:  12988 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2871, wps=100.5, ups=0.61, wpb=109.4, bsz=40, num_updates=12970, lr=4.53281e-05, gnorm=0.484, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=68678
2023-01-10 08:58:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:29 - progress_bar.py[line:274] - INFO: epoch 001:  12998 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3474, wps=98.6, ups=0.6, wpb=108.8, bsz=40, num_updates=12980, lr=4.53229e-05, gnorm=0.479, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=68695
2023-01-10 08:58:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:45 - progress_bar.py[line:274] - INFO: epoch 001:  13008 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3774, wps=101.4, ups=0.61, wpb=110.1, bsz=40, num_updates=12990, lr=4.53177e-05, gnorm=0.722, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=68711
2023-01-10 08:58:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:58:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 08:59:01 - progress_bar.py[line:274] - INFO: epoch 001:  13018 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2647, wps=103.2, ups=0.63, wpb=109.3, bsz=40, num_updates=13000, lr=4.53125e-05, gnorm=0.513, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=68728
2023-01-10 08:59:01 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 08:59:03 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 08:59:03 - train.py[line:551] - INFO: load:1.19 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 09:01:35 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 09:01:35 - train.py[line:551] - INFO: load:1.22 valid_run:151.88 task_valid:148.30 collect_output:2.51
2023-01-10 09:04:03 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 09:04:03 - train.py[line:551] - INFO: load:1.24 valid_run:299.97 task_valid:291.60 collect_output:6.32
2023-01-10 09:06:35 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 09:06:35 - train.py[line:551] - INFO: load:1.27 valid_run:452.12 task_valid:434.87 collect_output:14.19
2023-01-10 09:09:04 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 09:09:04 - train.py[line:551] - INFO: load:1.29 valid_run:600.61 task_valid:579.76 collect_output:16.78
2023-01-10 09:11:36 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 09:11:36 - train.py[line:551] - INFO: load:1.32 valid_run:752.52 task_valid:727.40 collect_output:20.03
2023-01-10 09:14:07 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 09:14:07 - train.py[line:551] - INFO: load:1.35 valid_run:903.77 task_valid:873.21 collect_output:24.42
2023-01-10 09:16:39 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 09:16:39 - train.py[line:551] - INFO: load:1.37 valid_run:1055.73 task_valid:1019.14 collect_output:29.43
2023-01-10 09:19:09 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 09:19:09 - train.py[line:551] - INFO: load:1.40 valid_run:1205.68 task_valid:1160.21 collect_output:37.32
2023-01-10 09:21:38 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 09:21:38 - train.py[line:551] - INFO: load:1.42 valid_run:1354.87 task_valid:1305.28 collect_output:40.41
2023-01-10 09:24:06 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 09:24:06 - train.py[line:551] - INFO: load:1.45 valid_run:1502.69 task_valid:1448.49 collect_output:44.03
2023-01-10 09:26:36 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 09:26:36 - train.py[line:551] - INFO: load:1.47 valid_run:1651.90 task_valid:1593.60 collect_output:47.10
2023-01-10 09:29:05 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 09:29:05 - train.py[line:551] - INFO: load:1.50 valid_run:1801.35 task_valid:1738.71 collect_output:50.44
2023-01-10 09:31:35 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 09:31:35 - train.py[line:551] - INFO: load:1.53 valid_run:1950.70 task_valid:1880.85 collect_output:56.63
2023-01-10 09:34:05 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 09:34:05 - train.py[line:551] - INFO: load:1.55 valid_run:2100.67 task_valid:2026.44 collect_output:60.00
2023-01-10 09:36:35 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 09:36:35 - train.py[line:551] - INFO: load:1.58 valid_run:2250.40 task_valid:2173.10 collect_output:62.04
2023-01-10 09:39:04 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 09:39:04 - train.py[line:551] - INFO: load:1.61 valid_run:2399.80 task_valid:2317.45 collect_output:66.07
2023-01-10 09:41:35 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 09:41:35 - train.py[line:551] - INFO: load:1.63 valid_run:2550.57 task_valid:2463.15 collect_output:70.15
2023-01-10 09:44:05 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 09:44:05 - train.py[line:551] - INFO: load:1.66 valid_run:2700.37 task_valid:2610.05 collect_output:72.06
2023-01-10 09:46:32 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 09:46:32 - train.py[line:551] - INFO: load:1.68 valid_run:2847.55 task_valid:2751.61 collect_output:76.66
2023-01-10 09:49:01 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 09:49:01 - train.py[line:551] - INFO: load:1.71 valid_run:2996.73 task_valid:2896.69 collect_output:79.77
2023-01-10 09:51:32 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 09:51:32 - train.py[line:551] - INFO: load:1.73 valid_run:3147.33 task_valid:3041.42 collect_output:84.65
2023-01-10 09:54:01 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 09:54:01 - train.py[line:551] - INFO: load:1.76 valid_run:3295.88 task_valid:3185.96 collect_output:87.67
2023-01-10 09:56:31 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 09:56:31 - train.py[line:551] - INFO: load:1.79 valid_run:3445.99 task_valid:3332.16 collect_output:90.56
2023-01-10 09:59:01 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 09:59:01 - train.py[line:551] - INFO: load:1.81 valid_run:3596.43 task_valid:3478.69 collect_output:93.47

====================================================================================================
SGG eval:     R @ 50: 0.5643;     R @ 100: 0.6364;     R @ 500: 0.6894;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3545;    mR @ 100: 0.4109;    mR @ 500: 0.4590;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8750) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4032) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.8840) (says:0.0000) (sitting on:0.7659) (standing on:0.3393) (using:0.6000) (walking in:0.0000) (walking on:0.6126) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5643;     R @ 100: 0.6364;     R @ 500: 0.6894;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3545;    mR @ 100: 0.4109;    mR @ 500: 0.4590;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8750) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4032) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.8840) (says:0.0000) (sitting on:0.7659) (standing on:0.3393) (using:0.6000) (walking in:0.0000) (walking on:0.6126) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-10 10:01:32 - train.py[line:487] - INFO: 0.6364148459383754
2023-01-10 10:01:32 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 10:01:32 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.349 | loss_v1 0 | loss_v2 0 | nll_loss 0.197 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.636415 | ppl 1.15 | vqa_score 0.5755 | wps 119.6 | wpb 89.9 | bsz 30 | num_updates 13000 | best_R@100 0.69005
2023-01-10 10:01:33 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 13000 updates
2023-01-10 10:01:33 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_13000.pt
2023-01-10 10:02:18 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_13000.pt
2023-01-10 10:03:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_13000.pt (epoch 1 @ 13000 updates, score 0.6364148459383754) (writing took 131.8986832164228 seconds)
2023-01-10 10:03:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:03:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:03:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:03:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:03:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:01 - progress_bar.py[line:274] - INFO: epoch 001:  13028 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3009, wps=0.4, ups=0, wpb=107.8, bsz=40, num_updates=13010, lr=4.53073e-05, gnorm=0.433, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=72627
2023-01-10 10:04:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:18 - progress_bar.py[line:274] - INFO: epoch 001:  13038 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3679, wps=96.8, ups=0.6, wpb=108.3, bsz=40, num_updates=13020, lr=4.53021e-05, gnorm=0.595, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=72644
2023-01-10 10:04:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:34 - progress_bar.py[line:274] - INFO: epoch 001:  13048 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2857, wps=101.1, ups=0.62, wpb=108.9, bsz=40, num_updates=13030, lr=4.52969e-05, gnorm=0.877, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=72660
2023-01-10 10:04:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:51 - progress_bar.py[line:274] - INFO: epoch 001:  13058 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3627, wps=100.4, ups=0.61, wpb=109.5, bsz=40, num_updates=13040, lr=4.52917e-05, gnorm=0.798, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=72677
2023-01-10 10:04:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:04:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:08 - progress_bar.py[line:274] - INFO: epoch 001:  13068 / 100000 loss=0.382, loss_v1=0, loss_v2=0, nll_loss=0.238, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2755, wps=99.5, ups=0.6, wpb=110, bsz=40, num_updates=13050, lr=4.52865e-05, gnorm=2.076, clip=50, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=72694
2023-01-10 10:05:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:24 - progress_bar.py[line:274] - INFO: epoch 001:  13078 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3789, wps=99.9, ups=0.6, wpb=110.9, bsz=40, num_updates=13060, lr=4.52812e-05, gnorm=0.961, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=72711
2023-01-10 10:05:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:41 - progress_bar.py[line:274] - INFO: epoch 001:  13088 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3854, wps=98, ups=0.6, wpb=109.3, bsz=40, num_updates=13070, lr=4.5276e-05, gnorm=0.59, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=72728
2023-01-10 10:05:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:05:58 - progress_bar.py[line:274] - INFO: epoch 001:  13098 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2632, wps=96.8, ups=0.6, wpb=107.9, bsz=40, num_updates=13080, lr=4.52708e-05, gnorm=0.37, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=72745
2023-01-10 10:05:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:16 - progress_bar.py[line:274] - INFO: epoch 001:  13108 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3093, wps=99.7, ups=0.61, wpb=109.6, bsz=40, num_updates=13090, lr=4.52656e-05, gnorm=0.828, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=72761
2023-01-10 10:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:33 - progress_bar.py[line:274] - INFO: epoch 001:  13118 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3302, wps=103.4, ups=0.62, wpb=110.6, bsz=40, num_updates=13100, lr=4.52604e-05, gnorm=0.738, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=72778
2023-01-10 10:06:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:50 - progress_bar.py[line:274] - INFO: epoch 001:  13128 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3529, wps=100.8, ups=0.61, wpb=110.5, bsz=40, num_updates=13110, lr=4.52552e-05, gnorm=0.569, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=72796
2023-01-10 10:06:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:06:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:07 - progress_bar.py[line:274] - INFO: epoch 001:  13138 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3036, wps=99.4, ups=0.61, wpb=109, bsz=40, num_updates=13120, lr=4.525e-05, gnorm=0.765, clip=20, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=72813
2023-01-10 10:07:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:25 - progress_bar.py[line:274] - INFO: epoch 001:  13148 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2376, wps=98.7, ups=0.6, wpb=109.5, bsz=40, num_updates=13130, lr=4.52448e-05, gnorm=1.162, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=72830
2023-01-10 10:07:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:42 - progress_bar.py[line:274] - INFO: epoch 001:  13158 / 100000 loss=0.379, loss_v1=0, loss_v2=0, nll_loss=0.232, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3211, wps=100.1, ups=0.62, wpb=107.6, bsz=40, num_updates=13140, lr=4.52396e-05, gnorm=1.879, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=72847
2023-01-10 10:07:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:07:59 - progress_bar.py[line:274] - INFO: epoch 001:  13168 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3626, wps=101.4, ups=0.62, wpb=109.7, bsz=40, num_updates=13150, lr=4.52344e-05, gnorm=0.584, clip=20, loss_scale=256, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=72864
2023-01-10 10:07:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:16 - progress_bar.py[line:274] - INFO: epoch 001:  13178 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4206, wps=100.9, ups=0.61, wpb=110.3, bsz=40, num_updates=13160, lr=4.52292e-05, gnorm=0.606, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=72881
2023-01-10 10:08:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:33 - progress_bar.py[line:274] - INFO: epoch 001:  13188 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.433, wps=100.7, ups=0.61, wpb=110.3, bsz=40, num_updates=13170, lr=4.5224e-05, gnorm=0.948, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=72899
2023-01-10 10:08:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:50 - progress_bar.py[line:274] - INFO: epoch 001:  13198 / 100000 loss=0.358, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3714, wps=100.1, ups=0.61, wpb=110, bsz=40, num_updates=13180, lr=4.52188e-05, gnorm=0.47, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=72916
2023-01-10 10:08:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:08:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:07 - progress_bar.py[line:274] - INFO: epoch 001:  13208 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.299, wps=101.3, ups=0.61, wpb=110.2, bsz=40, num_updates=13190, lr=4.52135e-05, gnorm=0.435, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=72933
2023-01-10 10:09:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:25 - progress_bar.py[line:274] - INFO: epoch 001:  13218 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4021, wps=97.2, ups=0.59, wpb=109.5, bsz=40, num_updates=13200, lr=4.52083e-05, gnorm=0.77, clip=20, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=72950
2023-01-10 10:09:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:42 - progress_bar.py[line:274] - INFO: epoch 001:  13228 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2872, wps=98.3, ups=0.6, wpb=109.5, bsz=40, num_updates=13210, lr=4.52031e-05, gnorm=0.521, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=72968
2023-01-10 10:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:09:59 - progress_bar.py[line:274] - INFO: epoch 001:  13238 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.299, wps=101.9, ups=0.62, wpb=110.3, bsz=40, num_updates=13220, lr=4.51979e-05, gnorm=0.836, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=72985
2023-01-10 10:09:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:16 - progress_bar.py[line:274] - INFO: epoch 001:  13248 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.222, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2574, wps=101, ups=0.61, wpb=110.6, bsz=40, num_updates=13230, lr=4.51927e-05, gnorm=0.623, clip=20, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=73002
2023-01-10 10:10:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:34 - progress_bar.py[line:274] - INFO: epoch 001:  13258 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4444, wps=100.1, ups=0.61, wpb=109.1, bsz=40, num_updates=13240, lr=4.51875e-05, gnorm=0.562, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73019
2023-01-10 10:10:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:50 - progress_bar.py[line:274] - INFO: epoch 001:  13268 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3091, wps=101.5, ups=0.62, wpb=108.6, bsz=40, num_updates=13250, lr=4.51823e-05, gnorm=0.596, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73036
2023-01-10 10:10:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:10:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:08 - progress_bar.py[line:274] - INFO: epoch 001:  13278 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3678, wps=100.6, ups=0.6, wpb=111.4, bsz=40, num_updates=13260, lr=4.51771e-05, gnorm=0.503, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=73053
2023-01-10 10:11:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:25 - progress_bar.py[line:274] - INFO: epoch 001:  13288 / 100000 loss=0.384, loss_v1=0, loss_v2=0, nll_loss=0.241, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.3273, wps=99.4, ups=0.6, wpb=110.7, bsz=40, num_updates=13270, lr=4.51719e-05, gnorm=0.934, clip=40, loss_scale=512, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=73071
2023-01-10 10:11:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:42 - progress_bar.py[line:274] - INFO: epoch 001:  13298 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.404, wps=101.7, ups=0.61, wpb=110.3, bsz=40, num_updates=13280, lr=4.51667e-05, gnorm=1.477, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=73088
2023-01-10 10:11:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:11:59 - progress_bar.py[line:274] - INFO: epoch 001:  13308 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2527, wps=101.4, ups=0.61, wpb=110.2, bsz=40, num_updates=13290, lr=4.51615e-05, gnorm=0.847, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73105
2023-01-10 10:12:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:16 - progress_bar.py[line:274] - INFO: epoch 001:  13318 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.28, wps=102.4, ups=0.61, wpb=111, bsz=40, num_updates=13300, lr=4.51563e-05, gnorm=1.01, clip=40, loss_scale=512, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=73122
2023-01-10 10:12:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:34 - progress_bar.py[line:274] - INFO: epoch 001:  13328 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3853, wps=97.5, ups=0.6, wpb=108.7, bsz=40, num_updates=13310, lr=4.5151e-05, gnorm=1.429, clip=30, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=73140
2023-01-10 10:12:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:51 - progress_bar.py[line:274] - INFO: epoch 001:  13338 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4242, wps=99.5, ups=0.61, wpb=108.9, bsz=40, num_updates=13320, lr=4.51458e-05, gnorm=0.546, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73157
2023-01-10 10:12:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:12:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:08 - progress_bar.py[line:274] - INFO: epoch 001:  13348 / 100000 loss=0.363, loss_v1=0, loss_v2=0, nll_loss=0.212, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3838, wps=99.9, ups=0.61, wpb=109.9, bsz=40, num_updates=13330, lr=4.51406e-05, gnorm=0.494, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73174
2023-01-10 10:13:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:10 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-10 10:13:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:28 - progress_bar.py[line:274] - INFO: epoch 001:  13359 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.125, nsentences=40, sample_size=108.125, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3206, wps=94, ups=0.54, wpb=108.1, bsz=40, num_updates=13340, lr=4.51354e-05, gnorm=0.712, clip=10, loss_scale=256, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=73193
2023-01-10 10:13:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:45 - progress_bar.py[line:274] - INFO: epoch 001:  13369 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.224, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3301, wps=101.3, ups=0.61, wpb=109.9, bsz=40, num_updates=13350, lr=4.51302e-05, gnorm=1.151, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73210
2023-01-10 10:13:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:13:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:02 - progress_bar.py[line:274] - INFO: epoch 001:  13379 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3864, wps=101.2, ups=0.61, wpb=109.9, bsz=40, num_updates=13360, lr=4.5125e-05, gnorm=0.652, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73227
2023-01-10 10:14:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:19 - progress_bar.py[line:274] - INFO: epoch 001:  13389 / 100000 loss=0.372, loss_v1=0, loss_v2=0, nll_loss=0.226, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.2157, wps=99.3, ups=0.61, wpb=108.9, bsz=40, num_updates=13370, lr=4.51198e-05, gnorm=1.014, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73245
2023-01-10 10:14:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:36 - progress_bar.py[line:274] - INFO: epoch 001:  13399 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.44, wps=100.3, ups=0.6, wpb=110.6, bsz=40, num_updates=13380, lr=4.51146e-05, gnorm=0.638, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73262
2023-01-10 10:14:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:14:54 - progress_bar.py[line:274] - INFO: epoch 001:  13409 / 100000 loss=0.371, loss_v1=0, loss_v2=0, nll_loss=0.227, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3611, wps=101.9, ups=0.62, wpb=110.2, bsz=40, num_updates=13390, lr=4.51094e-05, gnorm=0.741, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73279
2023-01-10 10:14:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:11 - progress_bar.py[line:274] - INFO: epoch 001:  13419 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4272, wps=99.3, ups=0.61, wpb=109.2, bsz=40, num_updates=13400, lr=4.51042e-05, gnorm=0.394, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73297
2023-01-10 10:15:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:28 - progress_bar.py[line:274] - INFO: epoch 001:  13429 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3, wps=101.5, ups=0.62, wpb=108.5, bsz=40, num_updates=13410, lr=4.5099e-05, gnorm=0.818, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73314
2023-01-10 10:15:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:45 - progress_bar.py[line:274] - INFO: epoch 001:  13439 / 100000 loss=0.364, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3396, wps=100.1, ups=0.61, wpb=109.5, bsz=40, num_updates=13420, lr=4.50938e-05, gnorm=1.042, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73331
2023-01-10 10:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:15:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:02 - progress_bar.py[line:274] - INFO: epoch 001:  13449 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3604, wps=100.6, ups=0.62, wpb=108.3, bsz=40, num_updates=13430, lr=4.50885e-05, gnorm=1.389, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73348
2023-01-10 10:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:19 - progress_bar.py[line:274] - INFO: epoch 001:  13459 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3918, wps=102.2, ups=0.62, wpb=109.5, bsz=40, num_updates=13440, lr=4.50833e-05, gnorm=1.203, clip=60, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=73364
2023-01-10 10:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:36 - progress_bar.py[line:274] - INFO: epoch 001:  13469 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4151, wps=99.5, ups=0.61, wpb=108.9, bsz=40, num_updates=13450, lr=4.50781e-05, gnorm=0.482, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73382
2023-01-10 10:16:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:16:53 - progress_bar.py[line:274] - INFO: epoch 001:  13479 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=113.067, nsentences=40, sample_size=113.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3295, wps=104.2, ups=0.61, wpb=113.1, bsz=40, num_updates=13460, lr=4.50729e-05, gnorm=0.589, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=73398
2023-01-10 10:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:09 - progress_bar.py[line:274] - INFO: epoch 001:  13489 / 100000 loss=0.373, loss_v1=0, loss_v2=0, nll_loss=0.225, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3153, wps=102, ups=0.62, wpb=109.4, bsz=40, num_updates=13470, lr=4.50677e-05, gnorm=0.518, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=73415
2023-01-10 10:17:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:26 - progress_bar.py[line:274] - INFO: epoch 001:  13499 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3232, wps=103, ups=0.63, wpb=109.1, bsz=40, num_updates=13480, lr=4.50625e-05, gnorm=0.692, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73432
2023-01-10 10:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:43 - progress_bar.py[line:274] - INFO: epoch 001:  13509 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.34, wps=102.1, ups=0.61, wpb=111.3, bsz=40, num_updates=13490, lr=4.50573e-05, gnorm=0.667, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73449
2023-01-10 10:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:00 - progress_bar.py[line:274] - INFO: epoch 001:  13519 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2816, wps=101.7, ups=0.61, wpb=110.5, bsz=40, num_updates=13500, lr=4.50521e-05, gnorm=0.766, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=73466
2023-01-10 10:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:18 - progress_bar.py[line:274] - INFO: epoch 001:  13529 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3889, wps=100.2, ups=0.6, wpb=110.8, bsz=40, num_updates=13510, lr=4.50469e-05, gnorm=0.807, clip=20, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=73483
2023-01-10 10:18:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:35 - progress_bar.py[line:274] - INFO: epoch 001:  13539 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3663, wps=100.2, ups=0.61, wpb=109.3, bsz=40, num_updates=13520, lr=4.50417e-05, gnorm=0.948, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73500
2023-01-10 10:18:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:18:52 - progress_bar.py[line:274] - INFO: epoch 001:  13549 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.266, wps=101.7, ups=0.61, wpb=111, bsz=40, num_updates=13530, lr=4.50365e-05, gnorm=0.634, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=73517
2023-01-10 10:18:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:09 - progress_bar.py[line:274] - INFO: epoch 001:  13559 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3068, wps=101.3, ups=0.61, wpb=111.1, bsz=40, num_updates=13540, lr=4.50313e-05, gnorm=1.534, clip=30, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=73535
2023-01-10 10:19:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:27 - progress_bar.py[line:274] - INFO: epoch 001:  13569 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3878, wps=97.7, ups=0.59, wpb=109.7, bsz=40, num_updates=13550, lr=4.5026e-05, gnorm=0.763, clip=30, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=73552
2023-01-10 10:19:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:44 - progress_bar.py[line:274] - INFO: epoch 001:  13579 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4412, wps=99.8, ups=0.61, wpb=109.1, bsz=40, num_updates=13560, lr=4.50208e-05, gnorm=0.955, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73569
2023-01-10 10:19:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:19:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:01 - progress_bar.py[line:274] - INFO: epoch 001:  13589 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3333, wps=100.6, ups=0.61, wpb=109.9, bsz=40, num_updates=13570, lr=4.50156e-05, gnorm=0.616, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73587
2023-01-10 10:20:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:18 - progress_bar.py[line:274] - INFO: epoch 001:  13599 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3737, wps=99.4, ups=0.6, wpb=109.5, bsz=40, num_updates=13580, lr=4.50104e-05, gnorm=0.757, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73604
2023-01-10 10:20:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:35 - progress_bar.py[line:274] - INFO: epoch 001:  13609 / 100000 loss=0.362, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3077, wps=102.7, ups=0.63, wpb=109.5, bsz=40, num_updates=13590, lr=4.50052e-05, gnorm=1.063, clip=40, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=73621
2023-01-10 10:20:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:20:52 - progress_bar.py[line:274] - INFO: epoch 001:  13619 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2529, wps=100.5, ups=0.6, wpb=111.1, bsz=40, num_updates=13600, lr=4.5e-05, gnorm=0.938, clip=40, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=73638
2023-01-10 10:20:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:09 - progress_bar.py[line:274] - INFO: epoch 001:  13629 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2909, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=13610, lr=4.49948e-05, gnorm=0.653, clip=10, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=73655
2023-01-10 10:21:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:26 - progress_bar.py[line:274] - INFO: epoch 001:  13639 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3524, wps=102.9, ups=0.63, wpb=109.7, bsz=40, num_updates=13620, lr=4.49896e-05, gnorm=0.847, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73672
2023-01-10 10:21:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:43 - progress_bar.py[line:274] - INFO: epoch 001:  13649 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3267, wps=99.9, ups=0.61, wpb=109.7, bsz=40, num_updates=13630, lr=4.49844e-05, gnorm=0.509, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=73689
2023-01-10 10:21:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:21:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:00 - progress_bar.py[line:274] - INFO: epoch 001:  13659 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3267, wps=101.5, ups=0.61, wpb=110.4, bsz=40, num_updates=13640, lr=4.49792e-05, gnorm=1.201, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=73706
2023-01-10 10:22:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:18 - progress_bar.py[line:274] - INFO: epoch 001:  13669 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3689, wps=98.7, ups=0.6, wpb=109.5, bsz=40, num_updates=13650, lr=4.4974e-05, gnorm=0.6, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=73723
2023-01-10 10:22:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:35 - progress_bar.py[line:274] - INFO: epoch 001:  13679 / 100000 loss=0.375, loss_v1=0, loss_v2=0, nll_loss=0.229, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.17, vqa_score=0.3417, wps=99.1, ups=0.61, wpb=107.7, bsz=40, num_updates=13660, lr=4.49688e-05, gnorm=1.043, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73740
2023-01-10 10:22:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:22:52 - progress_bar.py[line:274] - INFO: epoch 001:  13689 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4286, wps=99.2, ups=0.6, wpb=109.4, bsz=40, num_updates=13670, lr=4.49635e-05, gnorm=1.102, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73758
2023-01-10 10:22:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:09 - progress_bar.py[line:274] - INFO: epoch 001:  13699 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3918, wps=101, ups=0.61, wpb=109.5, bsz=40, num_updates=13680, lr=4.49583e-05, gnorm=0.867, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73775
2023-01-10 10:23:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:26 - progress_bar.py[line:274] - INFO: epoch 001:  13709 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3053, wps=102.1, ups=0.62, wpb=110.2, bsz=40, num_updates=13690, lr=4.49531e-05, gnorm=0.87, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=73792
2023-01-10 10:23:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:43 - progress_bar.py[line:274] - INFO: epoch 001:  13719 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3048, wps=102.5, ups=0.62, wpb=110.1, bsz=40, num_updates=13700, lr=4.49479e-05, gnorm=0.585, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=73809
2023-01-10 10:23:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:23:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:00 - progress_bar.py[line:274] - INFO: epoch 001:  13729 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3394, wps=98.9, ups=0.61, wpb=107.6, bsz=40, num_updates=13710, lr=4.49427e-05, gnorm=0.504, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=73826
2023-01-10 10:24:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:17 - progress_bar.py[line:274] - INFO: epoch 001:  13739 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3663, wps=100.4, ups=0.61, wpb=109.5, bsz=40, num_updates=13720, lr=4.49375e-05, gnorm=0.582, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73843
2023-01-10 10:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:34 - progress_bar.py[line:274] - INFO: epoch 001:  13749 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4, wps=101.1, ups=0.62, wpb=109.5, bsz=40, num_updates=13730, lr=4.49323e-05, gnorm=0.743, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73860
2023-01-10 10:24:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:52 - progress_bar.py[line:274] - INFO: epoch 001:  13759 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3299, wps=99, ups=0.6, wpb=109.2, bsz=40, num_updates=13740, lr=4.49271e-05, gnorm=0.621, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=73877
2023-01-10 10:24:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:24:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:08 - progress_bar.py[line:274] - INFO: epoch 001:  13769 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4184, wps=103.4, ups=0.63, wpb=109.1, bsz=40, num_updates=13750, lr=4.49219e-05, gnorm=0.789, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73894
2023-01-10 10:25:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:26 - progress_bar.py[line:274] - INFO: epoch 001:  13779 / 100000 loss=0.378, loss_v1=0, loss_v2=0, nll_loss=0.235, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.18, vqa_score=0.2885, wps=99, ups=0.61, wpb=108.4, bsz=40, num_updates=13760, lr=4.49167e-05, gnorm=1.132, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73911
2023-01-10 10:25:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:43 - progress_bar.py[line:274] - INFO: epoch 001:  13789 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3367, wps=99, ups=0.6, wpb=109.1, bsz=40, num_updates=13770, lr=4.49115e-05, gnorm=0.926, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73929
2023-01-10 10:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:25:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:00 - progress_bar.py[line:274] - INFO: epoch 001:  13799 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.36, wps=104.1, ups=0.62, wpb=111.5, bsz=40, num_updates=13780, lr=4.49063e-05, gnorm=0.457, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=73946
2023-01-10 10:26:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:17 - progress_bar.py[line:274] - INFO: epoch 001:  13809 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3061, wps=99.7, ups=0.6, wpb=110.1, bsz=40, num_updates=13790, lr=4.4901e-05, gnorm=0.682, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=73963
2023-01-10 10:26:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:35 - progress_bar.py[line:274] - INFO: epoch 001:  13819 / 100000 loss=0.368, loss_v1=0, loss_v2=0, nll_loss=0.217, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3366, wps=100.3, ups=0.61, wpb=110.3, bsz=40, num_updates=13800, lr=4.48958e-05, gnorm=0.532, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=73980
2023-01-10 10:26:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:26:52 - progress_bar.py[line:274] - INFO: epoch 001:  13829 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3962, wps=99.4, ups=0.6, wpb=110.1, bsz=40, num_updates=13810, lr=4.48906e-05, gnorm=0.459, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=73998
2023-01-10 10:26:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:09 - progress_bar.py[line:274] - INFO: epoch 001:  13839 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3056, wps=100.5, ups=0.61, wpb=109.6, bsz=40, num_updates=13820, lr=4.48854e-05, gnorm=0.653, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=74015
2023-01-10 10:27:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:26 - progress_bar.py[line:274] - INFO: epoch 001:  13849 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2828, wps=100.4, ups=0.61, wpb=110, bsz=40, num_updates=13830, lr=4.48802e-05, gnorm=0.845, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74032
2023-01-10 10:27:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:44 - progress_bar.py[line:274] - INFO: epoch 001:  13859 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3846, wps=98.8, ups=0.6, wpb=109.4, bsz=40, num_updates=13840, lr=4.4875e-05, gnorm=0.582, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=74049
2023-01-10 10:27:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:27:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:01 - progress_bar.py[line:274] - INFO: epoch 001:  13869 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.216, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.4259, wps=103.5, ups=0.62, wpb=110.7, bsz=40, num_updates=13850, lr=4.48698e-05, gnorm=0.784, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74067
2023-01-10 10:28:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:17 - progress_bar.py[line:274] - INFO: epoch 001:  13879 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.283, wps=99.8, ups=0.61, wpb=108.8, bsz=40, num_updates=13860, lr=4.48646e-05, gnorm=1.011, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=74084
2023-01-10 10:28:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:34 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-10 10:28:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:36 - progress_bar.py[line:274] - INFO: epoch 001:  13890 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.688, nsentences=40, sample_size=109.688, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3358, wps=96.5, ups=0.55, wpb=109.7, bsz=40, num_updates=13870, lr=4.48594e-05, gnorm=0.931, clip=10, loss_scale=256, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=74102
2023-01-10 10:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:28:52 - progress_bar.py[line:274] - INFO: epoch 001:  13900 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3895, wps=102.7, ups=0.62, wpb=110.1, bsz=40, num_updates=13880, lr=4.48542e-05, gnorm=0.868, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=74118
2023-01-10 10:28:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:09 - progress_bar.py[line:274] - INFO: epoch 001:  13910 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3652, wps=99.8, ups=0.62, wpb=107.9, bsz=40, num_updates=13890, lr=4.4849e-05, gnorm=0.525, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74135
2023-01-10 10:29:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:25 - progress_bar.py[line:274] - INFO: epoch 001:  13920 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3152, wps=100.9, ups=0.61, wpb=109.5, bsz=40, num_updates=13900, lr=4.48438e-05, gnorm=0.64, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74151
2023-01-10 10:29:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:42 - progress_bar.py[line:274] - INFO: epoch 001:  13930 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.211, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3571, wps=100.7, ups=0.61, wpb=109.7, bsz=40, num_updates=13910, lr=4.48385e-05, gnorm=0.642, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74168
2023-01-10 10:29:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:29:59 - progress_bar.py[line:274] - INFO: epoch 001:  13940 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3494, wps=100.6, ups=0.61, wpb=109.9, bsz=40, num_updates=13920, lr=4.48333e-05, gnorm=0.493, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74185
2023-01-10 10:30:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:15 - progress_bar.py[line:274] - INFO: epoch 001:  13950 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.31, wps=100.8, ups=0.62, wpb=108.9, bsz=40, num_updates=13930, lr=4.48281e-05, gnorm=0.574, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74201
2023-01-10 10:30:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:32 - progress_bar.py[line:274] - INFO: epoch 001:  13960 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3462, wps=101, ups=0.62, wpb=109.2, bsz=40, num_updates=13940, lr=4.48229e-05, gnorm=0.925, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74218
2023-01-10 10:30:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:48 - progress_bar.py[line:274] - INFO: epoch 001:  13970 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3868, wps=104.2, ups=0.62, wpb=111.3, bsz=40, num_updates=13950, lr=4.48177e-05, gnorm=0.914, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74234
2023-01-10 10:30:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:30:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:05 - progress_bar.py[line:274] - INFO: epoch 001:  13980 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3824, wps=99.7, ups=0.61, wpb=109.7, bsz=40, num_updates=13960, lr=4.48125e-05, gnorm=1.253, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=74251
2023-01-10 10:31:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:21 - progress_bar.py[line:274] - INFO: epoch 001:  13990 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3158, wps=100.9, ups=0.62, wpb=109.1, bsz=40, num_updates=13970, lr=4.48073e-05, gnorm=0.808, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74267
2023-01-10 10:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:38 - progress_bar.py[line:274] - INFO: epoch 001:  14000 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.36, wps=100.2, ups=0.6, wpb=111.5, bsz=40, num_updates=13980, lr=4.48021e-05, gnorm=0.769, clip=20, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=74284
2023-01-10 10:31:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:31:55 - progress_bar.py[line:274] - INFO: epoch 001:  14010 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3455, wps=100.2, ups=0.61, wpb=109.6, bsz=40, num_updates=13990, lr=4.47969e-05, gnorm=0.458, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=74301
2023-01-10 10:32:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:32:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:32:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:32:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:32:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 10:32:11 - progress_bar.py[line:274] - INFO: epoch 001:  14020 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4062, wps=98.8, ups=0.61, wpb=108.9, bsz=40, num_updates=14000, lr=4.47917e-05, gnorm=0.838, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=74318
2023-01-10 10:32:11 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 10:32:13 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 10:32:13 - train.py[line:551] - INFO: load:1.28 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 10:34:44 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 10:34:44 - train.py[line:551] - INFO: load:1.30 valid_run:150.91 task_valid:147.97 collect_output:1.91
2023-01-10 10:37:12 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 10:37:12 - train.py[line:551] - INFO: load:1.32 valid_run:298.39 task_valid:291.11 collect_output:5.25
2023-01-10 10:39:43 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 10:39:43 - train.py[line:551] - INFO: load:1.35 valid_run:449.81 task_valid:434.28 collect_output:12.51
2023-01-10 10:42:12 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 10:42:12 - train.py[line:551] - INFO: load:1.37 valid_run:598.33 task_valid:579.22 collect_output:15.07
2023-01-10 10:44:43 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 10:44:43 - train.py[line:551] - INFO: load:1.39 valid_run:749.83 task_valid:726.72 collect_output:18.07
2023-01-10 10:47:14 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 10:47:14 - train.py[line:551] - INFO: load:1.42 valid_run:900.85 task_valid:872.19 collect_output:22.59
2023-01-10 10:49:47 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 10:49:47 - train.py[line:551] - INFO: load:1.44 valid_run:1053.31 task_valid:1018.40 collect_output:27.82
2023-01-10 10:52:17 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 10:52:17 - train.py[line:551] - INFO: load:1.47 valid_run:1203.64 task_valid:1159.65 collect_output:35.88
2023-01-10 10:54:46 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 10:54:46 - train.py[line:551] - INFO: load:1.49 valid_run:1352.27 task_valid:1304.37 collect_output:38.79
2023-01-10 10:57:14 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 10:57:14 - train.py[line:551] - INFO: load:1.52 valid_run:1499.93 task_valid:1447.47 collect_output:42.35
2023-01-10 10:59:43 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 10:59:43 - train.py[line:551] - INFO: load:1.54 valid_run:1649.29 task_valid:1592.58 collect_output:45.56
2023-01-10 11:02:12 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 11:02:12 - train.py[line:551] - INFO: load:1.57 valid_run:1798.39 task_valid:1737.45 collect_output:48.78
2023-01-10 11:04:41 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 11:04:41 - train.py[line:551] - INFO: load:1.59 valid_run:1947.38 task_valid:1879.30 collect_output:54.92
2023-01-10 11:07:12 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 11:07:12 - train.py[line:551] - INFO: load:1.62 valid_run:2097.50 task_valid:2025.07 collect_output:58.24
2023-01-10 11:09:42 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 11:09:42 - train.py[line:551] - INFO: load:1.64 valid_run:2247.62 task_valid:2171.90 collect_output:60.47
2023-01-10 11:12:11 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 11:12:11 - train.py[line:551] - INFO: load:1.67 valid_run:2396.75 task_valid:2316.06 collect_output:64.45
2023-01-10 11:14:41 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 11:14:41 - train.py[line:551] - INFO: load:1.69 valid_run:2547.19 task_valid:2461.72 collect_output:68.19
2023-01-10 11:17:12 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 11:17:12 - train.py[line:551] - INFO: load:1.72 valid_run:2697.56 task_valid:2609.02 collect_output:70.21
2023-01-10 11:19:40 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 11:19:40 - train.py[line:551] - INFO: load:1.74 valid_run:2845.14 task_valid:2750.81 collect_output:74.97
2023-01-10 11:22:09 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 11:22:09 - train.py[line:551] - INFO: load:1.77 valid_run:2994.62 task_valid:2896.02 collect_output:78.22
2023-01-10 11:24:40 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 11:24:40 - train.py[line:551] - INFO: load:1.79 valid_run:3145.26 task_valid:3040.98 collect_output:82.86
2023-01-10 11:27:09 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 11:27:09 - train.py[line:551] - INFO: load:1.82 valid_run:3294.00 task_valid:3185.81 collect_output:85.76
2023-01-10 11:29:40 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 11:29:40 - train.py[line:551] - INFO: load:1.84 valid_run:3444.24 task_valid:3332.01 collect_output:88.81
2023-01-10 11:32:10 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 11:32:10 - train.py[line:551] - INFO: load:1.87 valid_run:3594.73 task_valid:3478.53 collect_output:91.76

====================================================================================================
SGG eval:     R @ 50: 0.5376;     R @ 100: 0.6200;     R @ 500: 0.6767;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3383;    mR @ 100: 0.4057;    mR @ 500: 0.4561;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8750) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4032) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.8742) (says:0.0000) (sitting on:0.7378) (standing on:0.2993) (using:0.6000) (walking in:0.0000) (walking on:0.5856) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5376;     R @ 100: 0.6200;     R @ 500: 0.6767;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3383;    mR @ 100: 0.4057;    mR @ 500: 0.4561;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8750) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4032) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.8742) (says:0.0000) (sitting on:0.7378) (standing on:0.2993) (using:0.6000) (walking in:0.0000) (walking on:0.5856) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-10 11:34:41 - train.py[line:487] - INFO: 0.6200481792717087
2023-01-10 11:34:41 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 11:34:41 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.34 | loss_v1 0 | loss_v2 0 | nll_loss 0.187 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.620048 | ppl 1.14 | vqa_score 0.5653 | wps 119.7 | wpb 89.9 | bsz 30 | num_updates 14000 | best_R@100 0.69005
2023-01-10 11:34:41 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 14000 updates
2023-01-10 11:34:41 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt
2023-01-10 11:35:18 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt
2023-01-10 11:36:48 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_14000.pt (epoch 1 @ 14000 updates, score 0.6200481792717087) (writing took 126.59778930805624 seconds)
2023-01-10 11:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:36:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:36:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:05 - progress_bar.py[line:274] - INFO: epoch 001:  14030 / 100000 loss=0.355, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4144, wps=0.4, ups=0, wpb=107.6, bsz=40, num_updates=14010, lr=4.47865e-05, gnorm=1.191, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78211
2023-01-10 11:37:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:22 - progress_bar.py[line:274] - INFO: epoch 001:  14040 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3474, wps=99.2, ups=0.61, wpb=109.2, bsz=40, num_updates=14020, lr=4.47813e-05, gnorm=0.81, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78228
2023-01-10 11:37:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:38 - progress_bar.py[line:274] - INFO: epoch 001:  14050 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3196, wps=101.3, ups=0.61, wpb=110.8, bsz=40, num_updates=14030, lr=4.4776e-05, gnorm=0.567, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=78244
2023-01-10 11:37:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:37:55 - progress_bar.py[line:274] - INFO: epoch 001:  14060 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3529, wps=102.3, ups=0.62, wpb=110.8, bsz=40, num_updates=14040, lr=4.47708e-05, gnorm=0.679, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78261
2023-01-10 11:38:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:12 - progress_bar.py[line:274] - INFO: epoch 001:  14070 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3302, wps=97.9, ups=0.6, wpb=108.3, bsz=40, num_updates=14050, lr=4.47656e-05, gnorm=0.802, clip=30, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=78278
2023-01-10 11:38:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:28 - progress_bar.py[line:274] - INFO: epoch 001:  14080 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3333, wps=106.1, ups=0.64, wpb=110.5, bsz=40, num_updates=14060, lr=4.47604e-05, gnorm=0.763, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78294
2023-01-10 11:38:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:45 - progress_bar.py[line:274] - INFO: epoch 001:  14090 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3592, wps=99.4, ups=0.61, wpb=109.4, bsz=40, num_updates=14070, lr=4.47552e-05, gnorm=0.707, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78311
2023-01-10 11:38:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:38:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:01 - progress_bar.py[line:274] - INFO: epoch 001:  14100 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.46, wps=98.7, ups=0.61, wpb=108.7, bsz=40, num_updates=14080, lr=4.475e-05, gnorm=0.85, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=78328
2023-01-10 11:39:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:18 - progress_bar.py[line:274] - INFO: epoch 001:  14110 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3265, wps=101.8, ups=0.62, wpb=110.1, bsz=40, num_updates=14090, lr=4.47448e-05, gnorm=1.063, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=78344
2023-01-10 11:39:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:34 - progress_bar.py[line:274] - INFO: epoch 001:  14120 / 100000 loss=0.361, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.38, wps=103.5, ups=0.63, wpb=109.9, bsz=40, num_updates=14100, lr=4.47396e-05, gnorm=1.856, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78360
2023-01-10 11:39:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:51 - progress_bar.py[line:274] - INFO: epoch 001:  14130 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.413, wps=101.2, ups=0.61, wpb=110.7, bsz=40, num_updates=14110, lr=4.47344e-05, gnorm=0.86, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=78377
2023-01-10 11:39:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:39:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:07 - progress_bar.py[line:274] - INFO: epoch 001:  14140 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3056, wps=100.6, ups=0.61, wpb=109.9, bsz=40, num_updates=14120, lr=4.47292e-05, gnorm=0.537, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=78394
2023-01-10 11:40:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:24 - progress_bar.py[line:274] - INFO: epoch 001:  14150 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.41, wps=103.7, ups=0.62, wpb=111, bsz=40, num_updates=14130, lr=4.4724e-05, gnorm=0.731, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78410
2023-01-10 11:40:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:40 - progress_bar.py[line:274] - INFO: epoch 001:  14160 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.382, wps=102.6, ups=0.62, wpb=111.1, bsz=40, num_updates=14140, lr=4.47188e-05, gnorm=1.082, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=78426
2023-01-10 11:40:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:40:57 - progress_bar.py[line:274] - INFO: epoch 001:  14170 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3366, wps=101.4, ups=0.62, wpb=108.8, bsz=40, num_updates=14150, lr=4.47135e-05, gnorm=0.675, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78443
2023-01-10 11:41:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:13 - progress_bar.py[line:274] - INFO: epoch 001:  14180 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4273, wps=99.5, ups=0.61, wpb=108.3, bsz=40, num_updates=14160, lr=4.47083e-05, gnorm=0.841, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78459
2023-01-10 11:41:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:29 - progress_bar.py[line:274] - INFO: epoch 001:  14190 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3409, wps=103.5, ups=0.63, wpb=110.3, bsz=40, num_updates=14170, lr=4.47031e-05, gnorm=0.398, clip=0, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=78476
2023-01-10 11:41:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:46 - progress_bar.py[line:274] - INFO: epoch 001:  14200 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3333, wps=100.8, ups=0.61, wpb=109.8, bsz=40, num_updates=14180, lr=4.46979e-05, gnorm=1.353, clip=50, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78492
2023-01-10 11:41:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:41:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:03 - progress_bar.py[line:274] - INFO: epoch 001:  14210 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3398, wps=98.8, ups=0.61, wpb=108.7, bsz=40, num_updates=14190, lr=4.46927e-05, gnorm=0.539, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78509
2023-01-10 11:42:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:19 - progress_bar.py[line:274] - INFO: epoch 001:  14220 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4184, wps=102.3, ups=0.61, wpb=111, bsz=40, num_updates=14200, lr=4.46875e-05, gnorm=0.504, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78526
2023-01-10 11:42:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:36 - progress_bar.py[line:274] - INFO: epoch 001:  14230 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3137, wps=103, ups=0.63, wpb=109.6, bsz=40, num_updates=14210, lr=4.46823e-05, gnorm=0.574, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78542
2023-01-10 11:42:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:42:52 - progress_bar.py[line:274] - INFO: epoch 001:  14240 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3529, wps=101.7, ups=0.62, wpb=110.1, bsz=40, num_updates=14220, lr=4.46771e-05, gnorm=1.638, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78558
2023-01-10 11:42:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:08 - progress_bar.py[line:274] - INFO: epoch 001:  14250 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.214, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3273, wps=103.9, ups=0.63, wpb=110, bsz=40, num_updates=14230, lr=4.46719e-05, gnorm=0.86, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78575
2023-01-10 11:43:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:25 - progress_bar.py[line:274] - INFO: epoch 001:  14260 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3137, wps=101.5, ups=0.62, wpb=109.7, bsz=40, num_updates=14240, lr=4.46667e-05, gnorm=1.331, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78591
2023-01-10 11:43:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:41 - progress_bar.py[line:274] - INFO: epoch 001:  14270 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3738, wps=100.3, ups=0.61, wpb=109.3, bsz=40, num_updates=14250, lr=4.46615e-05, gnorm=0.819, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78608
2023-01-10 11:43:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:43:57 - progress_bar.py[line:274] - INFO: epoch 001:  14280 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3299, wps=104.3, ups=0.64, wpb=108.9, bsz=40, num_updates=14260, lr=4.46562e-05, gnorm=0.497, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78624
2023-01-10 11:44:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:14 - progress_bar.py[line:274] - INFO: epoch 001:  14290 / 100000 loss=0.366, loss_v1=0, loss_v2=0, nll_loss=0.215, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.4112, wps=103.4, ups=0.63, wpb=109.7, bsz=40, num_updates=14270, lr=4.4651e-05, gnorm=0.835, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78640
2023-01-10 11:44:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:31 - progress_bar.py[line:274] - INFO: epoch 001:  14300 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3271, wps=93.2, ups=0.57, wpb=109.5, bsz=40, num_updates=14280, lr=4.46458e-05, gnorm=0.61, clip=20, loss_scale=256, train_wall=18, gb_free=10.4, ema_decay=0.9999, wall=78658
2023-01-10 11:44:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:48 - progress_bar.py[line:274] - INFO: epoch 001:  14310 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3196, wps=99.3, ups=0.6, wpb=109.7, bsz=40, num_updates=14290, lr=4.46406e-05, gnorm=1.821, clip=20, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=78674
2023-01-10 11:44:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:44:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:05 - progress_bar.py[line:274] - INFO: epoch 001:  14320 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4118, wps=99.3, ups=0.6, wpb=109.7, bsz=40, num_updates=14300, lr=4.46354e-05, gnorm=0.994, clip=40, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=78691
2023-01-10 11:45:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:22 - progress_bar.py[line:274] - INFO: epoch 001:  14330 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3163, wps=101.5, ups=0.61, wpb=111.2, bsz=40, num_updates=14310, lr=4.46302e-05, gnorm=0.687, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78708
2023-01-10 11:45:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:39 - progress_bar.py[line:274] - INFO: epoch 001:  14340 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3409, wps=100, ups=0.6, wpb=110.9, bsz=40, num_updates=14320, lr=4.4625e-05, gnorm=0.569, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=78725
2023-01-10 11:45:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:45:55 - progress_bar.py[line:274] - INFO: epoch 001:  14350 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3444, wps=102.9, ups=0.62, wpb=109.8, bsz=40, num_updates=14330, lr=4.46198e-05, gnorm=0.769, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78741
2023-01-10 11:46:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:12 - progress_bar.py[line:274] - INFO: epoch 001:  14360 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2549, wps=99.9, ups=0.61, wpb=109.9, bsz=40, num_updates=14340, lr=4.46146e-05, gnorm=0.517, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=78758
2023-01-10 11:46:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:28 - progress_bar.py[line:274] - INFO: epoch 001:  14370 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3068, wps=105, ups=0.63, wpb=110.6, bsz=40, num_updates=14350, lr=4.46094e-05, gnorm=0.456, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78774
2023-01-10 11:46:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:45 - progress_bar.py[line:274] - INFO: epoch 001:  14380 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.203, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4103, wps=100.6, ups=0.61, wpb=109.6, bsz=40, num_updates=14360, lr=4.46042e-05, gnorm=1.055, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78791
2023-01-10 11:46:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:46:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:01 - progress_bar.py[line:274] - INFO: epoch 001:  14390 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3945, wps=101.3, ups=0.62, wpb=109.3, bsz=40, num_updates=14370, lr=4.4599e-05, gnorm=0.498, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78807
2023-01-10 11:47:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:18 - progress_bar.py[line:274] - INFO: epoch 001:  14400 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4854, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=14380, lr=4.45938e-05, gnorm=0.603, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=78824
2023-01-10 11:47:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:34 - progress_bar.py[line:274] - INFO: epoch 001:  14410 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3738, wps=99.8, ups=0.61, wpb=110, bsz=40, num_updates=14390, lr=4.45885e-05, gnorm=0.971, clip=40, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=78841
2023-01-10 11:47:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:51 - progress_bar.py[line:274] - INFO: epoch 001:  14420 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3333, wps=101.4, ups=0.62, wpb=109.6, bsz=40, num_updates=14400, lr=4.45833e-05, gnorm=0.544, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78857
2023-01-10 11:47:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:47:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:08 - progress_bar.py[line:274] - INFO: epoch 001:  14430 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3814, wps=100.1, ups=0.61, wpb=109.7, bsz=40, num_updates=14410, lr=4.45781e-05, gnorm=0.569, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78874
2023-01-10 11:48:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:24 - progress_bar.py[line:274] - INFO: epoch 001:  14440 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.301, wps=102.2, ups=0.62, wpb=110.5, bsz=40, num_updates=14420, lr=4.45729e-05, gnorm=0.661, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78890
2023-01-10 11:48:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:40 - progress_bar.py[line:274] - INFO: epoch 001:  14450 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3367, wps=103.8, ups=0.63, wpb=109.8, bsz=40, num_updates=14430, lr=4.45677e-05, gnorm=0.926, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78907
2023-01-10 11:48:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:48:57 - progress_bar.py[line:274] - INFO: epoch 001:  14460 / 100000 loss=0.359, loss_v1=0, loss_v2=0, nll_loss=0.206, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.42, wps=103.3, ups=0.63, wpb=109.8, bsz=40, num_updates=14440, lr=4.45625e-05, gnorm=0.829, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78923
2023-01-10 11:49:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:13 - progress_bar.py[line:274] - INFO: epoch 001:  14470 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3516, wps=101.9, ups=0.62, wpb=110.3, bsz=40, num_updates=14450, lr=4.45573e-05, gnorm=0.594, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=78939
2023-01-10 11:49:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:31 - progress_bar.py[line:274] - INFO: epoch 001:  14480 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=107.533, nsentences=40, sample_size=107.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3619, wps=98.3, ups=0.61, wpb=107.5, bsz=40, num_updates=14460, lr=4.45521e-05, gnorm=0.379, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=78956
2023-01-10 11:49:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:47 - progress_bar.py[line:274] - INFO: epoch 001:  14490 / 100000 loss=0.351, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2981, wps=101.5, ups=0.61, wpb=110.2, bsz=40, num_updates=14470, lr=4.45469e-05, gnorm=0.865, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78973
2023-01-10 11:49:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:49:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:04 - progress_bar.py[line:274] - INFO: epoch 001:  14500 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.209, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3434, wps=100.5, ups=0.61, wpb=109.1, bsz=40, num_updates=14480, lr=4.45417e-05, gnorm=0.84, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=78990
2023-01-10 11:50:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:21 - progress_bar.py[line:274] - INFO: epoch 001:  14510 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3232, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=14490, lr=4.45365e-05, gnorm=1.225, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=79007
2023-01-10 11:50:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:37 - progress_bar.py[line:274] - INFO: epoch 001:  14520 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4037, wps=103.3, ups=0.63, wpb=109.7, bsz=40, num_updates=14500, lr=4.45313e-05, gnorm=2.026, clip=50, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79023
2023-01-10 11:50:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:50:54 - progress_bar.py[line:274] - INFO: epoch 001:  14530 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3301, wps=99.9, ups=0.61, wpb=109, bsz=40, num_updates=14510, lr=4.4526e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=79040
2023-01-10 11:50:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:10 - progress_bar.py[line:274] - INFO: epoch 001:  14540 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3786, wps=99.3, ups=0.61, wpb=109, bsz=40, num_updates=14520, lr=4.45208e-05, gnorm=0.924, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79057
2023-01-10 11:51:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:27 - progress_bar.py[line:274] - INFO: epoch 001:  14550 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4623, wps=103.4, ups=0.63, wpb=109.7, bsz=40, num_updates=14530, lr=4.45156e-05, gnorm=0.751, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79073
2023-01-10 11:51:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:43 - progress_bar.py[line:274] - INFO: epoch 001:  14560 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4302, wps=100.8, ups=0.61, wpb=110.3, bsz=40, num_updates=14540, lr=4.45104e-05, gnorm=0.603, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=79089
2023-01-10 11:51:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:51:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:00 - progress_bar.py[line:274] - INFO: epoch 001:  14570 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.402, wps=100.8, ups=0.6, wpb=111.5, bsz=40, num_updates=14550, lr=4.45052e-05, gnorm=0.666, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=79106
2023-01-10 11:52:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:16 - progress_bar.py[line:274] - INFO: epoch 001:  14580 / 100000 loss=0.36, loss_v1=0, loss_v2=0, nll_loss=0.213, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3204, wps=102.5, ups=0.62, wpb=110, bsz=40, num_updates=14560, lr=4.45e-05, gnorm=0.793, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79123
2023-01-10 11:52:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:33 - progress_bar.py[line:274] - INFO: epoch 001:  14590 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3333, wps=97.8, ups=0.59, wpb=109.7, bsz=40, num_updates=14570, lr=4.44948e-05, gnorm=0.824, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=79140
2023-01-10 11:52:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:50 - progress_bar.py[line:274] - INFO: epoch 001:  14600 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4583, wps=97.6, ups=0.6, wpb=108.7, bsz=40, num_updates=14580, lr=4.44896e-05, gnorm=0.555, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=79157
2023-01-10 11:52:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:52:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:07 - progress_bar.py[line:274] - INFO: epoch 001:  14610 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4037, wps=101, ups=0.61, wpb=111.1, bsz=40, num_updates=14590, lr=4.44844e-05, gnorm=0.813, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79173
2023-01-10 11:53:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:23 - progress_bar.py[line:274] - INFO: epoch 001:  14620 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2816, wps=103.4, ups=0.63, wpb=109.9, bsz=40, num_updates=14600, lr=4.44792e-05, gnorm=0.447, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=79190
2023-01-10 11:53:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:40 - progress_bar.py[line:274] - INFO: epoch 001:  14630 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.38, wps=99.3, ups=0.6, wpb=110.3, bsz=40, num_updates=14610, lr=4.4474e-05, gnorm=0.632, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=79207
2023-01-10 11:53:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:53:57 - progress_bar.py[line:274] - INFO: epoch 001:  14640 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3444, wps=102.1, ups=0.61, wpb=112, bsz=40, num_updates=14620, lr=4.44688e-05, gnorm=0.466, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=79223
2023-01-10 11:54:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:14 - progress_bar.py[line:274] - INFO: epoch 001:  14650 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3434, wps=99.9, ups=0.6, wpb=110.7, bsz=40, num_updates=14630, lr=4.44635e-05, gnorm=0.476, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=79240
2023-01-10 11:54:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:31 - progress_bar.py[line:274] - INFO: epoch 001:  14660 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3684, wps=98.1, ups=0.6, wpb=109.4, bsz=40, num_updates=14640, lr=4.44583e-05, gnorm=1.685, clip=40, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=79257
2023-01-10 11:54:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:48 - progress_bar.py[line:274] - INFO: epoch 001:  14670 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3627, wps=99.7, ups=0.61, wpb=109.1, bsz=40, num_updates=14650, lr=4.44531e-05, gnorm=0.546, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=79274
2023-01-10 11:54:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:54:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:04 - progress_bar.py[line:274] - INFO: epoch 001:  14680 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=107.133, nsentences=40, sample_size=107.133, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.367, wps=101.4, ups=0.63, wpb=107.1, bsz=40, num_updates=14660, lr=4.44479e-05, gnorm=0.766, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79290
2023-01-10 11:55:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:22 - progress_bar.py[line:274] - INFO: epoch 001:  14690 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.422, wps=95.1, ups=0.57, wpb=110.9, bsz=40, num_updates=14670, lr=4.44427e-05, gnorm=0.619, clip=30, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=79308
2023-01-10 11:55:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:38 - progress_bar.py[line:274] - INFO: epoch 001:  14700 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3365, wps=101.9, ups=0.62, wpb=109.5, bsz=40, num_updates=14680, lr=4.44375e-05, gnorm=0.938, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=79324
2023-01-10 11:55:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:55:54 - progress_bar.py[line:274] - INFO: epoch 001:  14710 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3762, wps=101.9, ups=0.63, wpb=108.3, bsz=40, num_updates=14690, lr=4.44323e-05, gnorm=1.249, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=79340
2023-01-10 11:56:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:11 - progress_bar.py[line:274] - INFO: epoch 001:  14720 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2857, wps=103.7, ups=0.62, wpb=111.3, bsz=40, num_updates=14700, lr=4.44271e-05, gnorm=0.618, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79357
2023-01-10 11:56:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:27 - progress_bar.py[line:274] - INFO: epoch 001:  14730 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3981, wps=104, ups=0.63, wpb=110.1, bsz=40, num_updates=14710, lr=4.44219e-05, gnorm=1.025, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79373
2023-01-10 11:56:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:44 - progress_bar.py[line:274] - INFO: epoch 001:  14740 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3178, wps=100.7, ups=0.61, wpb=109.5, bsz=40, num_updates=14720, lr=4.44167e-05, gnorm=1.225, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79390
2023-01-10 11:56:49 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-10 11:56:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:56:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:01 - progress_bar.py[line:274] - INFO: epoch 001:  14751 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3434, wps=96.5, ups=0.58, wpb=110.1, bsz=40, num_updates=14730, lr=4.44115e-05, gnorm=0.973, clip=20, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=79407
2023-01-10 11:57:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:17 - progress_bar.py[line:274] - INFO: epoch 001:  14761 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4286, wps=100.2, ups=0.62, wpb=108.3, bsz=40, num_updates=14740, lr=4.44063e-05, gnorm=0.814, clip=40, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=79424
2023-01-10 11:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:34 - progress_bar.py[line:274] - INFO: epoch 001:  14771 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3958, wps=101.8, ups=0.62, wpb=108.9, bsz=40, num_updates=14750, lr=4.4401e-05, gnorm=0.76, clip=30, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=79440
2023-01-10 11:57:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:50 - progress_bar.py[line:274] - INFO: epoch 001:  14781 / 100000 loss=0.365, loss_v1=0, loss_v2=0, nll_loss=0.218, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.2342, wps=102.8, ups=0.64, wpb=107.8, bsz=40, num_updates=14760, lr=4.43958e-05, gnorm=0.838, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79456
2023-01-10 11:57:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:57:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:06 - progress_bar.py[line:274] - INFO: epoch 001:  14791 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3846, wps=101, ups=0.62, wpb=108.8, bsz=40, num_updates=14770, lr=4.43906e-05, gnorm=0.764, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=79472
2023-01-10 11:58:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:22 - progress_bar.py[line:274] - INFO: epoch 001:  14801 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3982, wps=102.9, ups=0.63, wpb=109.1, bsz=40, num_updates=14780, lr=4.43854e-05, gnorm=0.587, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79488
2023-01-10 11:58:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:39 - progress_bar.py[line:274] - INFO: epoch 001:  14811 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3854, wps=99.2, ups=0.6, wpb=109.5, bsz=40, num_updates=14790, lr=4.43802e-05, gnorm=0.753, clip=40, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=79505
2023-01-10 11:58:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:58:56 - progress_bar.py[line:274] - INFO: epoch 001:  14821 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.204, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3776, wps=101.4, ups=0.61, wpb=111.1, bsz=40, num_updates=14800, lr=4.4375e-05, gnorm=1.172, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79522
2023-01-10 11:59:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:13 - progress_bar.py[line:274] - INFO: epoch 001:  14831 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3925, wps=99.8, ups=0.6, wpb=110.2, bsz=40, num_updates=14810, lr=4.43698e-05, gnorm=0.735, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=79539
2023-01-10 11:59:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:30 - progress_bar.py[line:274] - INFO: epoch 001:  14841 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.41, wps=100.9, ups=0.6, wpb=111.3, bsz=40, num_updates=14820, lr=4.43646e-05, gnorm=0.532, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=79556
2023-01-10 11:59:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:46 - progress_bar.py[line:274] - INFO: epoch 001:  14851 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3187, wps=101, ups=0.61, wpb=110.8, bsz=40, num_updates=14830, lr=4.43594e-05, gnorm=0.438, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79572
2023-01-10 11:59:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 11:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:03 - progress_bar.py[line:274] - INFO: epoch 001:  14861 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3883, wps=97, ups=0.6, wpb=108.5, bsz=40, num_updates=14840, lr=4.43542e-05, gnorm=0.855, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=79589
2023-01-10 12:00:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:20 - progress_bar.py[line:274] - INFO: epoch 001:  14871 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3981, wps=98, ups=0.59, wpb=109.9, bsz=40, num_updates=14850, lr=4.4349e-05, gnorm=0.554, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=79606
2023-01-10 12:00:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:37 - progress_bar.py[line:274] - INFO: epoch 001:  14881 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3474, wps=103.5, ups=0.63, wpb=110.3, bsz=40, num_updates=14860, lr=4.43438e-05, gnorm=0.531, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79623
2023-01-10 12:00:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:00:53 - progress_bar.py[line:274] - INFO: epoch 001:  14891 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4096, wps=100.9, ups=0.61, wpb=110.6, bsz=40, num_updates=14870, lr=4.43385e-05, gnorm=0.809, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79639
2023-01-10 12:00:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:09 - progress_bar.py[line:274] - INFO: epoch 001:  14901 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4382, wps=103.5, ups=0.62, wpb=110.7, bsz=40, num_updates=14880, lr=4.43333e-05, gnorm=1.088, clip=50, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=79656
2023-01-10 12:01:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:26 - progress_bar.py[line:274] - INFO: epoch 001:  14911 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3368, wps=103.6, ups=0.63, wpb=110.3, bsz=40, num_updates=14890, lr=4.43281e-05, gnorm=0.728, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=79672
2023-01-10 12:01:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:42 - progress_bar.py[line:274] - INFO: epoch 001:  14921 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3723, wps=100.9, ups=0.61, wpb=110.3, bsz=40, num_updates=14900, lr=4.43229e-05, gnorm=2.169, clip=40, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=79689
2023-01-10 12:01:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:51 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-10 12:01:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:01:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:00 - progress_bar.py[line:274] - INFO: epoch 001:  14932 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3393, wps=92.8, ups=0.57, wpb=108.7, bsz=40, num_updates=14910, lr=4.43177e-05, gnorm=0.532, clip=10, loss_scale=128, train_wall=18, gb_free=10.1, ema_decay=0.9999, wall=79706
2023-01-10 12:02:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:16 - progress_bar.py[line:274] - INFO: epoch 001:  14942 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3725, wps=102.5, ups=0.62, wpb=109.5, bsz=40, num_updates=14920, lr=4.43125e-05, gnorm=0.717, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79723
2023-01-10 12:02:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:33 - progress_bar.py[line:274] - INFO: epoch 001:  14952 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3545, wps=101.2, ups=0.62, wpb=109.3, bsz=40, num_updates=14930, lr=4.43073e-05, gnorm=0.557, clip=10, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=79739
2023-01-10 12:02:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:49 - progress_bar.py[line:274] - INFO: epoch 001:  14962 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3925, wps=101.5, ups=0.61, wpb=110.2, bsz=40, num_updates=14940, lr=4.43021e-05, gnorm=0.753, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79756
2023-01-10 12:02:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:06 - progress_bar.py[line:274] - INFO: epoch 001:  14972 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4059, wps=101.6, ups=0.62, wpb=109.5, bsz=40, num_updates=14950, lr=4.42969e-05, gnorm=0.763, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79772
2023-01-10 12:03:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:22 - progress_bar.py[line:274] - INFO: epoch 001:  14982 / 100000 loss=0.357, loss_v1=0, loss_v2=0, nll_loss=0.208, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.16, vqa_score=0.3304, wps=103, ups=0.63, wpb=108.4, bsz=40, num_updates=14960, lr=4.42917e-05, gnorm=0.663, clip=20, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=79788
2023-01-10 12:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:39 - progress_bar.py[line:274] - INFO: epoch 001:  14992 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.201, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3366, wps=101.1, ups=0.61, wpb=110.5, bsz=40, num_updates=14970, lr=4.42865e-05, gnorm=0.828, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=79805
2023-01-10 12:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:03:55 - progress_bar.py[line:274] - INFO: epoch 001:  15002 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3402, wps=99.9, ups=0.6, wpb=110.7, bsz=40, num_updates=14980, lr=4.42813e-05, gnorm=0.486, clip=0, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=79822
2023-01-10 12:03:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:04:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:04:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:04:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:04:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:04:12 - progress_bar.py[line:274] - INFO: epoch 001:  15012 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3486, wps=98.2, ups=0.6, wpb=108.7, bsz=40, num_updates=14990, lr=4.4276e-05, gnorm=0.859, clip=20, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=79838
2023-01-10 12:04:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:04:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:04:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 12:04:29 - progress_bar.py[line:274] - INFO: epoch 001:  15022 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4184, wps=101.6, ups=0.61, wpb=111.1, bsz=40, num_updates=15000, lr=4.42708e-05, gnorm=0.374, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=79855
2023-01-10 12:04:29 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 12:04:30 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 12:04:30 - train.py[line:551] - INFO: load:1.21 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 12:07:02 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 12:07:02 - train.py[line:551] - INFO: load:1.24 valid_run:151.09 task_valid:148.11 collect_output:1.89
2023-01-10 12:09:29 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 12:09:29 - train.py[line:551] - INFO: load:1.26 valid_run:298.76 task_valid:291.35 collect_output:5.32
2023-01-10 12:12:00 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 12:12:01 - train.py[line:551] - INFO: load:1.29 valid_run:449.80 task_valid:434.34 collect_output:12.35
2023-01-10 12:14:29 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 12:14:29 - train.py[line:551] - INFO: load:1.31 valid_run:598.10 task_valid:579.16 collect_output:14.81
2023-01-10 12:17:00 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 12:17:00 - train.py[line:551] - INFO: load:1.34 valid_run:749.57 task_valid:726.53 collect_output:17.89
2023-01-10 12:19:31 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 12:19:31 - train.py[line:551] - INFO: load:1.36 valid_run:900.49 task_valid:872.13 collect_output:22.20
2023-01-10 12:22:04 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 12:22:04 - train.py[line:551] - INFO: load:1.39 valid_run:1052.46 task_valid:1017.98 collect_output:27.32
2023-01-10 12:24:34 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 12:24:34 - train.py[line:551] - INFO: load:1.42 valid_run:1202.46 task_valid:1159.21 collect_output:35.05
2023-01-10 12:27:02 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 12:27:02 - train.py[line:551] - INFO: load:1.44 valid_run:1351.13 task_valid:1304.01 collect_output:37.92
2023-01-10 12:29:30 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 12:29:30 - train.py[line:551] - INFO: load:1.47 valid_run:1499.00 task_valid:1447.28 collect_output:41.49
2023-01-10 12:32:00 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 12:32:00 - train.py[line:551] - INFO: load:1.49 valid_run:1648.06 task_valid:1592.30 collect_output:44.49
2023-01-10 12:34:29 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 12:34:29 - train.py[line:551] - INFO: load:1.52 valid_run:1797.09 task_valid:1737.10 collect_output:47.70
2023-01-10 12:36:58 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 12:36:58 - train.py[line:551] - INFO: load:1.54 valid_run:1945.88 task_valid:1878.82 collect_output:53.76
2023-01-10 12:39:28 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 12:39:28 - train.py[line:551] - INFO: load:1.57 valid_run:2096.11 task_valid:2024.54 collect_output:57.23
2023-01-10 12:41:58 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 12:41:58 - train.py[line:551] - INFO: load:1.60 valid_run:2245.85 task_valid:2171.25 collect_output:59.22
2023-01-10 12:44:27 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 12:44:27 - train.py[line:551] - INFO: load:1.62 valid_run:2395.06 task_valid:2315.60 collect_output:63.03
2023-01-10 12:46:58 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 12:46:58 - train.py[line:551] - INFO: load:1.65 valid_run:2545.47 task_valid:2461.23 collect_output:66.77
2023-01-10 12:49:28 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 12:49:28 - train.py[line:551] - INFO: load:1.67 valid_run:2695.74 task_valid:2608.43 collect_output:68.83
2023-01-10 12:51:55 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 12:51:55 - train.py[line:551] - INFO: load:1.70 valid_run:2842.95 task_valid:2750.08 collect_output:73.36
2023-01-10 12:54:24 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 12:54:24 - train.py[line:551] - INFO: load:1.72 valid_run:2992.17 task_valid:2895.20 collect_output:76.44
2023-01-10 12:56:55 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 12:56:55 - train.py[line:551] - INFO: load:1.75 valid_run:3142.66 task_valid:3039.96 collect_output:81.13
2023-01-10 12:59:24 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 12:59:24 - train.py[line:551] - INFO: load:1.77 valid_run:3291.54 task_valid:3184.79 collect_output:84.16
2023-01-10 13:01:54 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 13:01:54 - train.py[line:551] - INFO: load:1.80 valid_run:3441.79 task_valid:3331.17 collect_output:87.03
2023-01-10 13:04:25 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 13:04:25 - train.py[line:551] - INFO: load:1.82 valid_run:3592.21 task_valid:3477.69 collect_output:89.92

====================================================================================================
SGG eval:     R @ 50: 0.5269;     R @ 100: 0.6115;     R @ 500: 0.6670;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3331;    mR @ 100: 0.3992;    mR @ 500: 0.4484;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8125) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4032) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.8624) (says:0.0000) (sitting on:0.7191) (standing on:0.2843) (using:0.6000) (walking in:0.0000) (walking on:0.5631) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5269;     R @ 100: 0.6115;     R @ 500: 0.6670;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3331;    mR @ 100: 0.3992;    mR @ 500: 0.4484;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.8125) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4032) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.8624) (says:0.0000) (sitting on:0.7191) (standing on:0.2843) (using:0.6000) (walking in:0.0000) (walking on:0.5631) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-10 13:06:56 - train.py[line:487] - INFO: 0.6114624649859944
2023-01-10 13:06:56 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 13:06:56 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.35 | loss_v1 0 | loss_v2 0 | nll_loss 0.193 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.611462 | ppl 1.14 | vqa_score 0.5586 | wps 119.8 | wpb 89.9 | bsz 30 | num_updates 15000 | best_R@100 0.69005
2023-01-10 13:06:56 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 15000 updates
2023-01-10 13:06:56 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_15000.pt
2023-01-10 13:07:38 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_15000.pt
2023-01-10 13:09:01 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_15000.pt (epoch 1 @ 15000 updates, score 0.6114624649859944) (writing took 125.44629346206784 seconds)
2023-01-10 13:09:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:18 - progress_bar.py[line:274] - INFO: epoch 001:  15032 / 100000 loss=0.354, loss_v1=0, loss_v2=0, nll_loss=0.202, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3679, wps=0.4, ups=0, wpb=109, bsz=40, num_updates=15010, lr=4.42656e-05, gnorm=1.023, clip=10, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=83744
2023-01-10 13:09:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:35 - progress_bar.py[line:274] - INFO: epoch 001:  15042 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3608, wps=99.6, ups=0.6, wpb=110.3, bsz=40, num_updates=15020, lr=4.42604e-05, gnorm=0.852, clip=40, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=83761
2023-01-10 13:09:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:52 - progress_bar.py[line:274] - INFO: epoch 001:  15052 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.367, wps=98.7, ups=0.61, wpb=108.6, bsz=40, num_updates=15030, lr=4.42552e-05, gnorm=1.861, clip=50, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=83778
2023-01-10 13:09:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:09:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:09 - progress_bar.py[line:274] - INFO: epoch 001:  15062 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3763, wps=93.2, ups=0.57, wpb=109.2, bsz=40, num_updates=15040, lr=4.425e-05, gnorm=0.772, clip=30, loss_scale=128, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=83796
2023-01-10 13:10:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:26 - progress_bar.py[line:274] - INFO: epoch 001:  15072 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2857, wps=99, ups=0.6, wpb=110.5, bsz=40, num_updates=15050, lr=4.42448e-05, gnorm=0.524, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=83813
2023-01-10 13:10:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:43 - progress_bar.py[line:274] - INFO: epoch 001:  15082 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3636, wps=103.2, ups=0.63, wpb=109.9, bsz=40, num_updates=15060, lr=4.42396e-05, gnorm=0.352, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=83829
2023-01-10 13:10:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:10:59 - progress_bar.py[line:274] - INFO: epoch 001:  15092 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3762, wps=100.4, ups=0.6, wpb=110.7, bsz=40, num_updates=15070, lr=4.42344e-05, gnorm=0.511, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=83846
2023-01-10 13:11:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:16 - progress_bar.py[line:274] - INFO: epoch 001:  15102 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4021, wps=99.7, ups=0.6, wpb=110, bsz=40, num_updates=15080, lr=4.42292e-05, gnorm=0.48, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=83862
2023-01-10 13:11:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:32 - progress_bar.py[line:274] - INFO: epoch 001:  15112 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.32, wps=104.2, ups=0.63, wpb=109.9, bsz=40, num_updates=15090, lr=4.4224e-05, gnorm=0.999, clip=50, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=83879
2023-01-10 13:11:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:49 - progress_bar.py[line:274] - INFO: epoch 001:  15122 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3043, wps=100, ups=0.61, wpb=109.2, bsz=40, num_updates=15100, lr=4.42188e-05, gnorm=0.57, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=83895
2023-01-10 13:11:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:11:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:05 - progress_bar.py[line:274] - INFO: epoch 001:  15132 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3039, wps=103.3, ups=0.63, wpb=110, bsz=40, num_updates=15110, lr=4.42135e-05, gnorm=1.4, clip=40, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=83911
2023-01-10 13:12:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:22 - progress_bar.py[line:274] - INFO: epoch 001:  15142 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4623, wps=101, ups=0.62, wpb=109.3, bsz=40, num_updates=15120, lr=4.42083e-05, gnorm=0.696, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=83928
2023-01-10 13:12:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:38 - progress_bar.py[line:274] - INFO: epoch 001:  15152 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3558, wps=100.2, ups=0.6, wpb=110.5, bsz=40, num_updates=15130, lr=4.42031e-05, gnorm=0.739, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=83945
2023-01-10 13:12:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:12:55 - progress_bar.py[line:274] - INFO: epoch 001:  15162 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3678, wps=100.9, ups=0.61, wpb=110.2, bsz=40, num_updates=15140, lr=4.41979e-05, gnorm=0.978, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=83961
2023-01-10 13:12:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:11 - progress_bar.py[line:274] - INFO: epoch 001:  15172 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.198, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3091, wps=101.8, ups=0.62, wpb=109.3, bsz=40, num_updates=15150, lr=4.41927e-05, gnorm=0.586, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=83978
2023-01-10 13:13:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:28 - progress_bar.py[line:274] - INFO: epoch 001:  15182 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.42, wps=102.5, ups=0.62, wpb=110.3, bsz=40, num_updates=15160, lr=4.41875e-05, gnorm=0.951, clip=30, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=83994
2023-01-10 13:13:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:44 - progress_bar.py[line:274] - INFO: epoch 001:  15192 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3725, wps=101.5, ups=0.61, wpb=111.4, bsz=40, num_updates=15170, lr=4.41823e-05, gnorm=0.559, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=84011
2023-01-10 13:13:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:13:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:01 - progress_bar.py[line:274] - INFO: epoch 001:  15202 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3814, wps=100.7, ups=0.61, wpb=109.9, bsz=40, num_updates=15180, lr=4.41771e-05, gnorm=0.69, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=84027
2023-01-10 13:14:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:16 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-01-10 13:14:19 - progress_bar.py[line:274] - INFO: epoch 001:  15213 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3853, wps=95.2, ups=0.58, wpb=109.1, bsz=40, num_updates=15190, lr=4.41719e-05, gnorm=0.642, clip=10, loss_scale=64, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=84045
2023-01-10 13:14:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:35 - progress_bar.py[line:274] - INFO: epoch 001:  15223 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3302, wps=102.1, ups=0.62, wpb=109, bsz=40, num_updates=15200, lr=4.41667e-05, gnorm=0.793, clip=20, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84061
2023-01-10 13:14:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:51 - progress_bar.py[line:274] - INFO: epoch 001:  15233 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4206, wps=101.4, ups=0.62, wpb=109.1, bsz=40, num_updates=15210, lr=4.41615e-05, gnorm=0.46, clip=10, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84077
2023-01-10 13:14:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:14:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:08 - progress_bar.py[line:274] - INFO: epoch 001:  15243 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4245, wps=101, ups=0.62, wpb=108.4, bsz=40, num_updates=15220, lr=4.41562e-05, gnorm=0.477, clip=0, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84094
2023-01-10 13:15:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:24 - progress_bar.py[line:274] - INFO: epoch 001:  15253 / 100000 loss=0.35, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.4314, wps=103.1, ups=0.62, wpb=110.3, bsz=40, num_updates=15230, lr=4.4151e-05, gnorm=1.172, clip=10, loss_scale=64, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=84110
2023-01-10 13:15:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:41 - progress_bar.py[line:274] - INFO: epoch 001:  15263 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3854, wps=101.5, ups=0.61, wpb=111.1, bsz=40, num_updates=15240, lr=4.41458e-05, gnorm=1.689, clip=30, loss_scale=64, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=84127
2023-01-10 13:15:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:15:57 - progress_bar.py[line:274] - INFO: epoch 001:  15273 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3333, wps=99.1, ups=0.61, wpb=108.9, bsz=40, num_updates=15250, lr=4.41406e-05, gnorm=0.558, clip=0, loss_scale=64, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=84143
2023-01-10 13:16:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:14 - progress_bar.py[line:274] - INFO: epoch 001:  15283 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3486, wps=95.9, ups=0.59, wpb=107.9, bsz=40, num_updates=15260, lr=4.41354e-05, gnorm=0.882, clip=30, loss_scale=64, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=84161
2023-01-10 13:16:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:31 - progress_bar.py[line:274] - INFO: epoch 001:  15293 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4234, wps=102.2, ups=0.62, wpb=109.4, bsz=40, num_updates=15270, lr=4.41302e-05, gnorm=1.151, clip=10, loss_scale=64, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=84177
2023-01-10 13:16:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:47 - progress_bar.py[line:274] - INFO: epoch 001:  15303 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2245, wps=100.2, ups=0.61, wpb=109.8, bsz=40, num_updates=15280, lr=4.4125e-05, gnorm=0.773, clip=20, loss_scale=64, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=84194
2023-01-10 13:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:16:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:04 - progress_bar.py[line:274] - INFO: epoch 001:  15313 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4167, wps=102.7, ups=0.62, wpb=110.3, bsz=40, num_updates=15290, lr=4.41198e-05, gnorm=0.827, clip=30, loss_scale=64, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=84210
2023-01-10 13:17:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:20 - progress_bar.py[line:274] - INFO: epoch 001:  15323 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.32, wps=101.8, ups=0.62, wpb=109.7, bsz=40, num_updates=15300, lr=4.41146e-05, gnorm=0.404, clip=0, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84226
2023-01-10 13:17:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:36 - progress_bar.py[line:274] - INFO: epoch 001:  15333 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3297, wps=101.7, ups=0.62, wpb=108.9, bsz=40, num_updates=15310, lr=4.41094e-05, gnorm=0.504, clip=10, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84243
2023-01-10 13:17:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:54 - progress_bar.py[line:274] - INFO: epoch 001:  15343 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3626, wps=95.1, ups=0.57, wpb=110.3, bsz=40, num_updates=15320, lr=4.41042e-05, gnorm=0.411, clip=0, loss_scale=64, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=84260
2023-01-10 13:17:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:17:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:11 - progress_bar.py[line:274] - INFO: epoch 001:  15353 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2421, wps=100.9, ups=0.61, wpb=109.8, bsz=40, num_updates=15330, lr=4.4099e-05, gnorm=0.688, clip=20, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84277
2023-01-10 13:18:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:27 - progress_bar.py[line:274] - INFO: epoch 001:  15363 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4362, wps=103.3, ups=0.63, wpb=109.8, bsz=40, num_updates=15340, lr=4.40937e-05, gnorm=2.098, clip=30, loss_scale=64, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=84293
2023-01-10 13:18:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:43 - progress_bar.py[line:274] - INFO: epoch 001:  15373 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3684, wps=103.9, ups=0.63, wpb=110.4, bsz=40, num_updates=15350, lr=4.40885e-05, gnorm=0.632, clip=20, loss_scale=64, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=84309
2023-01-10 13:18:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:18:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:00 - progress_bar.py[line:274] - INFO: epoch 001:  15383 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3333, wps=99.3, ups=0.61, wpb=108.4, bsz=40, num_updates=15360, lr=4.40833e-05, gnorm=2.073, clip=20, loss_scale=64, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=84326
2023-01-10 13:19:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:16 - progress_bar.py[line:274] - INFO: epoch 001:  15393 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4468, wps=104.7, ups=0.63, wpb=111.1, bsz=40, num_updates=15370, lr=4.40781e-05, gnorm=0.423, clip=10, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84342
2023-01-10 13:19:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:33 - progress_bar.py[line:274] - INFO: epoch 001:  15403 / 100000 loss=0.349, loss_v1=0, loss_v2=0, nll_loss=0.2, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3419, wps=99.2, ups=0.61, wpb=107.9, bsz=40, num_updates=15380, lr=4.40729e-05, gnorm=1.169, clip=20, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84359
2023-01-10 13:19:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:50 - progress_bar.py[line:274] - INFO: epoch 001:  15413 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4271, wps=101, ups=0.6, wpb=111.3, bsz=40, num_updates=15390, lr=4.40677e-05, gnorm=0.484, clip=0, loss_scale=64, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=84376
2023-01-10 13:19:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:19:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:06 - progress_bar.py[line:274] - INFO: epoch 001:  15423 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4216, wps=102.1, ups=0.62, wpb=110.2, bsz=40, num_updates=15400, lr=4.40625e-05, gnorm=0.634, clip=30, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84392
2023-01-10 13:20:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:23 - progress_bar.py[line:274] - INFO: epoch 001:  15433 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3596, wps=102.1, ups=0.61, wpb=111.4, bsz=40, num_updates=15410, lr=4.40573e-05, gnorm=1, clip=30, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84409
2023-01-10 13:20:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:39 - progress_bar.py[line:274] - INFO: epoch 001:  15443 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3462, wps=103.4, ups=0.63, wpb=108.8, bsz=40, num_updates=15420, lr=4.40521e-05, gnorm=0.679, clip=20, loss_scale=64, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=84425
2023-01-10 13:20:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:55 - progress_bar.py[line:274] - INFO: epoch 001:  15453 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4062, wps=101.7, ups=0.62, wpb=110, bsz=40, num_updates=15430, lr=4.40469e-05, gnorm=0.904, clip=20, loss_scale=64, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=84441
2023-01-10 13:20:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:20:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:12 - progress_bar.py[line:274] - INFO: epoch 001:  15463 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.5094, wps=101.9, ups=0.62, wpb=109.5, bsz=40, num_updates=15440, lr=4.40417e-05, gnorm=1.015, clip=30, loss_scale=64, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=84458
2023-01-10 13:21:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:28 - progress_bar.py[line:274] - INFO: epoch 001:  15473 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.396, wps=102.5, ups=0.62, wpb=109.5, bsz=40, num_updates=15450, lr=4.40365e-05, gnorm=0.788, clip=30, loss_scale=64, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=84474
2023-01-10 13:21:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:45 - progress_bar.py[line:274] - INFO: epoch 001:  15483 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4444, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=15460, lr=4.40313e-05, gnorm=0.696, clip=20, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84491
2023-01-10 13:21:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:21:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:01 - progress_bar.py[line:274] - INFO: epoch 001:  15493 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3474, wps=100.4, ups=0.61, wpb=110.1, bsz=40, num_updates=15470, lr=4.4026e-05, gnorm=1.157, clip=40, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84507
2023-01-10 13:22:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:17 - progress_bar.py[line:274] - INFO: epoch 001:  15503 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3878, wps=103.6, ups=0.63, wpb=109.8, bsz=40, num_updates=15480, lr=4.40208e-05, gnorm=1.052, clip=30, loss_scale=64, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=84524
2023-01-10 13:22:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:34 - progress_bar.py[line:274] - INFO: epoch 001:  15513 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4175, wps=99.6, ups=0.6, wpb=110.3, bsz=40, num_updates=15490, lr=4.40156e-05, gnorm=0.642, clip=30, loss_scale=64, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=84540
2023-01-10 13:22:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:51 - progress_bar.py[line:274] - INFO: epoch 001:  15523 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4261, wps=99.2, ups=0.61, wpb=108.5, bsz=40, num_updates=15500, lr=4.40104e-05, gnorm=1.537, clip=30, loss_scale=64, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=84557
2023-01-10 13:22:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:22:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:07 - progress_bar.py[line:274] - INFO: epoch 001:  15533 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3627, wps=103.9, ups=0.63, wpb=109.8, bsz=40, num_updates=15510, lr=4.40052e-05, gnorm=1.351, clip=30, loss_scale=64, train_wall=16, gb_free=10, ema_decay=0.9999, wall=84573
2023-01-10 13:23:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:23 - progress_bar.py[line:274] - INFO: epoch 001:  15543 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3423, wps=102.5, ups=0.63, wpb=109.3, bsz=40, num_updates=15520, lr=4.4e-05, gnorm=0.403, clip=0, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84589
2023-01-10 13:23:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:39 - progress_bar.py[line:274] - INFO: epoch 001:  15553 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.2979, wps=103.1, ups=0.63, wpb=109.5, bsz=40, num_updates=15530, lr=4.39948e-05, gnorm=0.343, clip=0, loss_scale=64, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=84606
2023-01-10 13:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:23:56 - progress_bar.py[line:274] - INFO: epoch 001:  15563 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=112.333, nsentences=40, sample_size=112.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4023, wps=102.9, ups=0.61, wpb=112.3, bsz=40, num_updates=15540, lr=4.39896e-05, gnorm=0.506, clip=20, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84622
2023-01-10 13:23:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:13 - progress_bar.py[line:274] - INFO: epoch 001:  15573 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3529, wps=100.4, ups=0.61, wpb=109.7, bsz=40, num_updates=15550, lr=4.39844e-05, gnorm=0.715, clip=20, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84639
2023-01-10 13:24:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:29 - progress_bar.py[line:274] - INFO: epoch 001:  15583 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4135, wps=98.3, ups=0.61, wpb=108.1, bsz=40, num_updates=15560, lr=4.39792e-05, gnorm=0.459, clip=0, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84656
2023-01-10 13:24:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:46 - progress_bar.py[line:274] - INFO: epoch 001:  15593 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4021, wps=98.3, ups=0.6, wpb=109.6, bsz=40, num_updates=15570, lr=4.3974e-05, gnorm=1.599, clip=30, loss_scale=64, train_wall=17, gb_free=10, ema_decay=0.9999, wall=84673
2023-01-10 13:24:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:24:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:03 - progress_bar.py[line:274] - INFO: epoch 001:  15603 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.35, wps=102.4, ups=0.62, wpb=109.9, bsz=40, num_updates=15580, lr=4.39688e-05, gnorm=1.318, clip=20, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84689
2023-01-10 13:25:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:19 - progress_bar.py[line:274] - INFO: epoch 001:  15613 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2903, wps=100.7, ups=0.6, wpb=111.1, bsz=40, num_updates=15590, lr=4.39635e-05, gnorm=0.781, clip=30, loss_scale=64, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=84706
2023-01-10 13:25:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:36 - progress_bar.py[line:274] - INFO: epoch 001:  15623 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3905, wps=102.5, ups=0.63, wpb=109.3, bsz=40, num_updates=15600, lr=4.39583e-05, gnorm=0.663, clip=20, loss_scale=64, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=84722
2023-01-10 13:25:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:53 - progress_bar.py[line:274] - INFO: epoch 001:  15633 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4091, wps=101.9, ups=0.61, wpb=110.6, bsz=40, num_updates=15610, lr=4.39531e-05, gnorm=0.492, clip=0, loss_scale=64, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=84739
2023-01-10 13:25:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:25:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:09 - progress_bar.py[line:274] - INFO: epoch 001:  15643 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3107, wps=100.4, ups=0.61, wpb=109.1, bsz=40, num_updates=15620, lr=4.39479e-05, gnorm=0.549, clip=20, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84755
2023-01-10 13:26:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:25 - progress_bar.py[line:274] - INFO: epoch 001:  15653 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3883, wps=105, ups=0.63, wpb=110.4, bsz=40, num_updates=15630, lr=4.39427e-05, gnorm=0.357, clip=0, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84771
2023-01-10 13:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:42 - progress_bar.py[line:274] - INFO: epoch 001:  15663 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3694, wps=100.5, ups=0.62, wpb=108.5, bsz=40, num_updates=15640, lr=4.39375e-05, gnorm=0.756, clip=10, loss_scale=64, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=84788
2023-01-10 13:26:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:26:58 - progress_bar.py[line:274] - INFO: epoch 001:  15673 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4327, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=15650, lr=4.39323e-05, gnorm=1.154, clip=10, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84804
2023-01-10 13:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:15 - progress_bar.py[line:274] - INFO: epoch 001:  15683 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3469, wps=98.1, ups=0.6, wpb=109.7, bsz=40, num_updates=15660, lr=4.39271e-05, gnorm=0.661, clip=10, loss_scale=64, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=84821
2023-01-10 13:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:32 - progress_bar.py[line:274] - INFO: epoch 001:  15693 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.067, nsentences=40, sample_size=107.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4679, wps=99.1, ups=0.62, wpb=107.1, bsz=40, num_updates=15670, lr=4.39219e-05, gnorm=1.689, clip=10, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84838
2023-01-10 13:27:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:48 - progress_bar.py[line:274] - INFO: epoch 001:  15703 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4314, wps=103, ups=0.62, wpb=110.6, bsz=40, num_updates=15680, lr=4.39167e-05, gnorm=0.426, clip=10, loss_scale=64, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=84854
2023-01-10 13:27:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:27:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:04 - progress_bar.py[line:274] - INFO: epoch 001:  15713 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3265, wps=101.7, ups=0.61, wpb=110.3, bsz=40, num_updates=15690, lr=4.39115e-05, gnorm=0.673, clip=20, loss_scale=64, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=84871
2023-01-10 13:28:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:21 - progress_bar.py[line:274] - INFO: epoch 001:  15723 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4096, wps=105.3, ups=0.63, wpb=111.7, bsz=40, num_updates=15700, lr=4.39063e-05, gnorm=2.624, clip=60, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=84887
2023-01-10 13:28:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:37 - progress_bar.py[line:274] - INFO: epoch 001:  15733 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4019, wps=102.8, ups=0.63, wpb=109.4, bsz=40, num_updates=15710, lr=4.3901e-05, gnorm=1.214, clip=20, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=84903
2023-01-10 13:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:54 - progress_bar.py[line:274] - INFO: epoch 001:  15743 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3265, wps=97.9, ups=0.6, wpb=109.1, bsz=40, num_updates=15720, lr=4.38958e-05, gnorm=0.995, clip=20, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=84920
2023-01-10 13:28:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:28:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:10 - progress_bar.py[line:274] - INFO: epoch 001:  15753 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3936, wps=101.8, ups=0.61, wpb=110.9, bsz=40, num_updates=15730, lr=4.38906e-05, gnorm=0.441, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=84937
2023-01-10 13:29:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:27 - progress_bar.py[line:274] - INFO: epoch 001:  15763 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4167, wps=101.7, ups=0.61, wpb=110.5, bsz=40, num_updates=15740, lr=4.38854e-05, gnorm=1.543, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=84953
2023-01-10 13:29:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:43 - progress_bar.py[line:274] - INFO: epoch 001:  15773 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4062, wps=103.1, ups=0.62, wpb=110.9, bsz=40, num_updates=15750, lr=4.38802e-05, gnorm=0.641, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=84969
2023-01-10 13:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:29:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:00 - progress_bar.py[line:274] - INFO: epoch 001:  15783 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.381, wps=101.6, ups=0.62, wpb=110, bsz=40, num_updates=15760, lr=4.3875e-05, gnorm=0.978, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=84986
2023-01-10 13:30:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:17 - progress_bar.py[line:274] - INFO: epoch 001:  15793 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3398, wps=99.6, ups=0.6, wpb=110, bsz=40, num_updates=15770, lr=4.38698e-05, gnorm=0.754, clip=30, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=85003
2023-01-10 13:30:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:33 - progress_bar.py[line:274] - INFO: epoch 001:  15803 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.381, wps=101.2, ups=0.62, wpb=108, bsz=40, num_updates=15780, lr=4.38646e-05, gnorm=0.633, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85019
2023-01-10 13:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:50 - progress_bar.py[line:274] - INFO: epoch 001:  15813 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.33, wps=99.7, ups=0.61, wpb=109.3, bsz=40, num_updates=15790, lr=4.38594e-05, gnorm=0.55, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85036
2023-01-10 13:30:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:30:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:06 - progress_bar.py[line:274] - INFO: epoch 001:  15823 / 100000 loss=0.352, loss_v1=0, loss_v2=0, nll_loss=0.205, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2804, wps=99.7, ups=0.61, wpb=109.9, bsz=40, num_updates=15800, lr=4.38542e-05, gnorm=0.341, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85053
2023-01-10 13:31:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:23 - progress_bar.py[line:274] - INFO: epoch 001:  15833 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2947, wps=102.5, ups=0.62, wpb=110.6, bsz=40, num_updates=15810, lr=4.3849e-05, gnorm=0.492, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=85069
2023-01-10 13:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:39 - progress_bar.py[line:274] - INFO: epoch 001:  15843 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4043, wps=102.8, ups=0.62, wpb=111.3, bsz=40, num_updates=15820, lr=4.38437e-05, gnorm=0.643, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85085
2023-01-10 13:31:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:31:56 - progress_bar.py[line:274] - INFO: epoch 001:  15853 / 100000 loss=0.353, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.2816, wps=100.7, ups=0.62, wpb=108.9, bsz=40, num_updates=15830, lr=4.38385e-05, gnorm=0.714, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=85102
2023-01-10 13:31:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:12 - progress_bar.py[line:274] - INFO: epoch 001:  15863 / 100000 loss=0.345, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3826, wps=100.4, ups=0.61, wpb=108.9, bsz=40, num_updates=15840, lr=4.38333e-05, gnorm=1.069, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85118
2023-01-10 13:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:29 - progress_bar.py[line:274] - INFO: epoch 001:  15873 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4062, wps=102.4, ups=0.62, wpb=110.6, bsz=40, num_updates=15850, lr=4.38281e-05, gnorm=0.866, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85135
2023-01-10 13:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:45 - progress_bar.py[line:274] - INFO: epoch 001:  15883 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3152, wps=106.3, ups=0.63, wpb=111.7, bsz=40, num_updates=15860, lr=4.38229e-05, gnorm=0.798, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=85151
2023-01-10 13:32:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:32:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:01 - progress_bar.py[line:274] - INFO: epoch 001:  15893 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3913, wps=102.1, ups=0.62, wpb=109.7, bsz=40, num_updates=15870, lr=4.38177e-05, gnorm=0.606, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85167
2023-01-10 13:33:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:18 - progress_bar.py[line:274] - INFO: epoch 001:  15903 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.37, wps=101.7, ups=0.62, wpb=109.7, bsz=40, num_updates=15880, lr=4.38125e-05, gnorm=0.584, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85184
2023-01-10 13:33:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:34 - progress_bar.py[line:274] - INFO: epoch 001:  15913 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4062, wps=105.2, ups=0.63, wpb=111.5, bsz=40, num_updates=15890, lr=4.38073e-05, gnorm=0.689, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=85201
2023-01-10 13:33:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:51 - progress_bar.py[line:274] - INFO: epoch 001:  15923 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4245, wps=102.3, ups=0.62, wpb=110, bsz=40, num_updates=15900, lr=4.38021e-05, gnorm=1.123, clip=40, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=85217
2023-01-10 13:33:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:33:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:07 - progress_bar.py[line:274] - INFO: epoch 001:  15933 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3333, wps=100.7, ups=0.62, wpb=109.1, bsz=40, num_updates=15910, lr=4.37969e-05, gnorm=0.738, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85233
2023-01-10 13:34:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:24 - progress_bar.py[line:274] - INFO: epoch 001:  15943 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4273, wps=103.5, ups=0.62, wpb=110.8, bsz=40, num_updates=15920, lr=4.37917e-05, gnorm=0.675, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85250
2023-01-10 13:34:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:41 - progress_bar.py[line:274] - INFO: epoch 001:  15953 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4216, wps=97, ups=0.59, wpb=109.5, bsz=40, num_updates=15930, lr=4.37865e-05, gnorm=0.608, clip=10, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=85267
2023-01-10 13:34:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:34:57 - progress_bar.py[line:274] - INFO: epoch 001:  15963 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3774, wps=102.9, ups=0.62, wpb=110.1, bsz=40, num_updates=15940, lr=4.37813e-05, gnorm=1.407, clip=30, loss_scale=128, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=85283
2023-01-10 13:34:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:14 - progress_bar.py[line:274] - INFO: epoch 001:  15973 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3404, wps=100.7, ups=0.61, wpb=110.5, bsz=40, num_updates=15950, lr=4.3776e-05, gnorm=0.858, clip=30, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=85300
2023-01-10 13:35:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:30 - progress_bar.py[line:274] - INFO: epoch 001:  15983 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4045, wps=104.2, ups=0.62, wpb=111.6, bsz=40, num_updates=15960, lr=4.37708e-05, gnorm=0.646, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85316
2023-01-10 13:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:46 - progress_bar.py[line:274] - INFO: epoch 001:  15993 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3579, wps=102.8, ups=0.63, wpb=109.5, bsz=40, num_updates=15970, lr=4.37656e-05, gnorm=0.575, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85333
2023-01-10 13:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:35:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:36:03 - progress_bar.py[line:274] - INFO: epoch 001:  16003 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3696, wps=102.4, ups=0.62, wpb=110.7, bsz=40, num_updates=15980, lr=4.37604e-05, gnorm=0.385, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=85349
2023-01-10 13:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:36:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:36:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:36:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:36:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:36:19 - progress_bar.py[line:274] - INFO: epoch 001:  16013 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3333, wps=99, ups=0.61, wpb=108.8, bsz=40, num_updates=15990, lr=4.37552e-05, gnorm=1.545, clip=30, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=85366
2023-01-10 13:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:36:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:36:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:36:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:36:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 13:36:36 - progress_bar.py[line:274] - INFO: epoch 001:  16023 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3048, wps=103.6, ups=0.63, wpb=110.1, bsz=40, num_updates=16000, lr=4.375e-05, gnorm=0.595, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=85382
2023-01-10 13:36:36 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 13:36:37 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 13:36:37 - train.py[line:551] - INFO: load:1.39 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 13:39:08 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 13:39:08 - train.py[line:551] - INFO: load:1.42 valid_run:151.00 task_valid:148.09 collect_output:1.85
2023-01-10 13:41:36 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 13:41:36 - train.py[line:551] - INFO: load:1.44 valid_run:298.90 task_valid:291.34 collect_output:5.50
2023-01-10 13:44:08 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 13:44:08 - train.py[line:551] - INFO: load:1.47 valid_run:449.99 task_valid:434.39 collect_output:12.52
2023-01-10 13:46:36 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 13:46:36 - train.py[line:551] - INFO: load:1.49 valid_run:598.33 task_valid:579.28 collect_output:14.95
2023-01-10 13:49:08 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 13:49:08 - train.py[line:551] - INFO: load:1.52 valid_run:749.82 task_valid:726.64 collect_output:18.07
2023-01-10 13:51:39 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 13:51:39 - train.py[line:551] - INFO: load:1.54 valid_run:900.82 task_valid:872.30 collect_output:22.40
2023-01-10 13:54:10 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 13:54:10 - train.py[line:551] - INFO: load:1.57 valid_run:1052.53 task_valid:1018.34 collect_output:27.03
2023-01-10 13:56:40 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 13:56:40 - train.py[line:551] - INFO: load:1.60 valid_run:1202.29 task_valid:1159.37 collect_output:34.76
2023-01-10 13:59:09 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 13:59:09 - train.py[line:551] - INFO: load:1.62 valid_run:1350.92 task_valid:1303.96 collect_output:37.79
2023-01-10 14:01:37 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 14:01:37 - train.py[line:551] - INFO: load:1.65 valid_run:1498.98 task_valid:1447.65 collect_output:41.12
2023-01-10 14:04:06 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 14:04:06 - train.py[line:551] - INFO: load:1.68 valid_run:1647.82 task_valid:1592.48 collect_output:44.10
2023-01-10 14:06:35 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 14:06:35 - train.py[line:551] - INFO: load:1.71 valid_run:1796.86 task_valid:1737.42 collect_output:47.16
2023-01-10 14:09:04 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 14:09:04 - train.py[line:551] - INFO: load:1.73 valid_run:1945.40 task_valid:1879.20 collect_output:52.92
2023-01-10 14:11:34 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 14:11:34 - train.py[line:551] - INFO: load:1.76 valid_run:2095.48 task_valid:2025.00 collect_output:56.18
2023-01-10 14:14:03 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 14:14:03 - train.py[line:551] - INFO: load:1.79 valid_run:2244.97 task_valid:2171.49 collect_output:58.16
2023-01-10 14:16:33 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 14:16:33 - train.py[line:551] - INFO: load:1.81 valid_run:2394.08 task_valid:2315.59 collect_output:62.16
2023-01-10 14:19:03 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 14:19:03 - train.py[line:551] - INFO: load:1.84 valid_run:2544.35 task_valid:2460.96 collect_output:66.05
2023-01-10 14:21:33 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 14:21:33 - train.py[line:551] - INFO: load:1.87 valid_run:2694.56 task_valid:2608.05 collect_output:68.15
2023-01-10 14:24:00 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 14:24:00 - train.py[line:551] - INFO: load:1.89 valid_run:2841.62 task_valid:2749.59 collect_output:72.65
2023-01-10 14:26:30 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 14:26:30 - train.py[line:551] - INFO: load:1.92 valid_run:2990.96 task_valid:2894.78 collect_output:75.78
2023-01-10 14:29:00 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 14:29:00 - train.py[line:551] - INFO: load:1.95 valid_run:3141.48 task_valid:3039.63 collect_output:80.43
2023-01-10 14:31:29 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 14:31:29 - train.py[line:551] - INFO: load:1.97 valid_run:3290.00 task_valid:3184.15 collect_output:83.41
2023-01-10 14:33:59 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 14:33:59 - train.py[line:551] - INFO: load:2.00 valid_run:3440.11 task_valid:3330.40 collect_output:86.25
2023-01-10 14:36:30 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 14:36:30 - train.py[line:551] - INFO: load:2.03 valid_run:3590.57 task_valid:3476.97 collect_output:89.11

====================================================================================================
SGG eval:     R @ 50: 0.5095;     R @ 100: 0.6023;     R @ 500: 0.6597;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3231;    mR @ 100: 0.3884;    mR @ 500: 0.4437;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4355) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.8154) (says:0.0000) (sitting on:0.7217) (standing on:0.2743) (using:0.6000) (walking in:0.0000) (walking on:0.4955) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.5095;     R @ 100: 0.6023;     R @ 500: 0.6597;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3231;    mR @ 100: 0.3884;    mR @ 500: 0.4437;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4355) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.9583) (playing:0.0000) (riding:0.8154) (says:0.0000) (sitting on:0.7217) (standing on:0.2743) (using:0.6000) (walking in:0.0000) (walking on:0.4955) (watching:0.3889) 
--------------------------------------------------------
====================================================================================================

2023-01-10 14:39:01 - train.py[line:487] - INFO: 0.6023291316526611
2023-01-10 14:39:01 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 14:39:01 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.32 | loss_v1 0 | loss_v2 0 | nll_loss 0.162 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.602329 | ppl 1.12 | vqa_score 0.5484 | wps 119.8 | wpb 89.9 | bsz 30 | num_updates 16000 | best_R@100 0.69005
2023-01-10 14:39:01 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 16000 updates
2023-01-10 14:39:01 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt
2023-01-10 14:39:41 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt
2023-01-10 14:41:05 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_16000.pt (epoch 1 @ 16000 updates, score 0.6023291316526611) (writing took 124.10661963559687 seconds)
2023-01-10 14:41:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:22 - progress_bar.py[line:274] - INFO: epoch 001:  16033 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.35, wps=0.4, ups=0, wpb=110.7, bsz=40, num_updates=16010, lr=4.37448e-05, gnorm=0.566, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=89268
2023-01-10 14:41:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:38 - progress_bar.py[line:274] - INFO: epoch 001:  16043 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3727, wps=102.5, ups=0.62, wpb=110.7, bsz=40, num_updates=16020, lr=4.37396e-05, gnorm=1.379, clip=20, loss_scale=128, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=89285
2023-01-10 14:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:55 - progress_bar.py[line:274] - INFO: epoch 001:  16053 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3039, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=16030, lr=4.37344e-05, gnorm=0.675, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=89301
2023-01-10 14:41:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:41:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:11 - progress_bar.py[line:274] - INFO: epoch 001:  16063 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3619, wps=102.4, ups=0.61, wpb=111.1, bsz=40, num_updates=16040, lr=4.37292e-05, gnorm=0.438, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=89318
2023-01-10 14:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:28 - progress_bar.py[line:274] - INFO: epoch 001:  16073 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3146, wps=101.2, ups=0.61, wpb=110.9, bsz=40, num_updates=16050, lr=4.3724e-05, gnorm=0.418, clip=0, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=89334
2023-01-10 14:42:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:45 - progress_bar.py[line:274] - INFO: epoch 001:  16083 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3173, wps=99.2, ups=0.6, wpb=109.4, bsz=40, num_updates=16060, lr=4.37188e-05, gnorm=0.532, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=89351
2023-01-10 14:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:02 - progress_bar.py[line:274] - INFO: epoch 001:  16093 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3235, wps=100.2, ups=0.61, wpb=109.8, bsz=40, num_updates=16070, lr=4.37135e-05, gnorm=0.504, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=89368
2023-01-10 14:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:18 - progress_bar.py[line:274] - INFO: epoch 001:  16103 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4141, wps=103.4, ups=0.62, wpb=110.5, bsz=40, num_updates=16080, lr=4.37083e-05, gnorm=0.351, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=89384
2023-01-10 14:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:34 - progress_bar.py[line:274] - INFO: epoch 001:  16113 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.398, wps=102.2, ups=0.62, wpb=109.3, bsz=40, num_updates=16090, lr=4.37031e-05, gnorm=0.666, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=89401
2023-01-10 14:43:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:51 - progress_bar.py[line:274] - INFO: epoch 001:  16123 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3939, wps=103.6, ups=0.62, wpb=111.1, bsz=40, num_updates=16100, lr=4.36979e-05, gnorm=0.552, clip=10, loss_scale=128, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=89417
2023-01-10 14:43:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:43:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:07 - progress_bar.py[line:274] - INFO: epoch 001:  16133 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3976, wps=102.9, ups=0.62, wpb=110.5, bsz=40, num_updates=16110, lr=4.36927e-05, gnorm=0.911, clip=10, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=89433
2023-01-10 14:44:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:24 - progress_bar.py[line:274] - INFO: epoch 001:  16143 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3053, wps=98.8, ups=0.6, wpb=109.1, bsz=40, num_updates=16120, lr=4.36875e-05, gnorm=0.473, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=89450
2023-01-10 14:44:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:40 - progress_bar.py[line:274] - INFO: epoch 001:  16153 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3333, wps=100.5, ups=0.61, wpb=109.3, bsz=40, num_updates=16130, lr=4.36823e-05, gnorm=0.765, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=89467
2023-01-10 14:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:44:57 - progress_bar.py[line:274] - INFO: epoch 001:  16163 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3818, wps=102.6, ups=0.63, wpb=109.4, bsz=40, num_updates=16140, lr=4.36771e-05, gnorm=0.979, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=89483
2023-01-10 14:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:13 - progress_bar.py[line:274] - INFO: epoch 001:  16173 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3981, wps=98.1, ups=0.6, wpb=108.4, bsz=40, num_updates=16150, lr=4.36719e-05, gnorm=0.498, clip=0, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=89500
2023-01-10 14:45:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:30 - progress_bar.py[line:274] - INFO: epoch 001:  16183 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4118, wps=100.2, ups=0.61, wpb=109.7, bsz=40, num_updates=16160, lr=4.36667e-05, gnorm=0.557, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=89516
2023-01-10 14:45:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:46 - progress_bar.py[line:274] - INFO: epoch 001:  16193 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3684, wps=104.4, ups=0.63, wpb=110.5, bsz=40, num_updates=16170, lr=4.36615e-05, gnorm=0.965, clip=40, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=89532
2023-01-10 14:45:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:45:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:02 - progress_bar.py[line:274] - INFO: epoch 001:  16203 / 100000 loss=0.348, loss_v1=0, loss_v2=0, nll_loss=0.196, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3478, wps=102.2, ups=0.62, wpb=109.2, bsz=40, num_updates=16180, lr=4.36563e-05, gnorm=0.816, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=89549
2023-01-10 14:46:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:19 - progress_bar.py[line:274] - INFO: epoch 001:  16213 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3814, wps=102.6, ups=0.62, wpb=110.1, bsz=40, num_updates=16190, lr=4.3651e-05, gnorm=0.44, clip=0, loss_scale=128, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=89565
2023-01-10 14:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:35 - progress_bar.py[line:274] - INFO: epoch 001:  16223 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3301, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=16200, lr=4.36458e-05, gnorm=0.341, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=89581
2023-01-10 14:46:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:52 - progress_bar.py[line:274] - INFO: epoch 001:  16233 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3226, wps=103.9, ups=0.62, wpb=111.3, bsz=40, num_updates=16210, lr=4.36406e-05, gnorm=0.44, clip=0, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=89598
2023-01-10 14:46:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:08 - progress_bar.py[line:274] - INFO: epoch 001:  16243 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3429, wps=96.7, ups=0.6, wpb=107.6, bsz=40, num_updates=16220, lr=4.36354e-05, gnorm=1.175, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=89615
2023-01-10 14:47:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:25 - progress_bar.py[line:274] - INFO: epoch 001:  16253 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4151, wps=102.9, ups=0.63, wpb=108.8, bsz=40, num_updates=16230, lr=4.36302e-05, gnorm=0.757, clip=20, loss_scale=256, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=89631
2023-01-10 14:47:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:41 - progress_bar.py[line:274] - INFO: epoch 001:  16263 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.33, wps=105.9, ups=0.64, wpb=111, bsz=40, num_updates=16240, lr=4.3625e-05, gnorm=1.032, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=89647
2023-01-10 14:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:47:57 - progress_bar.py[line:274] - INFO: epoch 001:  16273 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3964, wps=99, ups=0.6, wpb=109.9, bsz=40, num_updates=16250, lr=4.36198e-05, gnorm=0.44, clip=10, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=89664
2023-01-10 14:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:14 - progress_bar.py[line:274] - INFO: epoch 001:  16283 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3469, wps=99, ups=0.6, wpb=110.1, bsz=40, num_updates=16260, lr=4.36146e-05, gnorm=0.578, clip=20, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=89681
2023-01-10 14:48:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:31 - progress_bar.py[line:274] - INFO: epoch 001:  16293 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3838, wps=99.4, ups=0.61, wpb=108.8, bsz=40, num_updates=16270, lr=4.36094e-05, gnorm=0.963, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=89697
2023-01-10 14:48:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:48 - progress_bar.py[line:274] - INFO: epoch 001:  16303 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4457, wps=102.2, ups=0.61, wpb=111.5, bsz=40, num_updates=16280, lr=4.36042e-05, gnorm=0.509, clip=20, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=89714
2023-01-10 14:48:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:48:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:05 - progress_bar.py[line:274] - INFO: epoch 001:  16313 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3824, wps=98.9, ups=0.6, wpb=110, bsz=40, num_updates=16290, lr=4.3599e-05, gnorm=0.384, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=89731
2023-01-10 14:49:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:21 - progress_bar.py[line:274] - INFO: epoch 001:  16323 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.193, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3551, wps=101.5, ups=0.62, wpb=108.9, bsz=40, num_updates=16300, lr=4.35937e-05, gnorm=0.777, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=89747
2023-01-10 14:49:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:37 - progress_bar.py[line:274] - INFO: epoch 001:  16333 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3636, wps=102.7, ups=0.62, wpb=110.9, bsz=40, num_updates=16310, lr=4.35885e-05, gnorm=0.876, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=89763
2023-01-10 14:49:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:53 - progress_bar.py[line:274] - INFO: epoch 001:  16343 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3854, wps=104, ups=0.63, wpb=110.9, bsz=40, num_updates=16320, lr=4.35833e-05, gnorm=1.059, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=89780
2023-01-10 14:49:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:49:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:10 - progress_bar.py[line:274] - INFO: epoch 001:  16353 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4021, wps=101.5, ups=0.61, wpb=110.9, bsz=40, num_updates=16330, lr=4.35781e-05, gnorm=0.696, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=89796
2023-01-10 14:50:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:27 - progress_bar.py[line:274] - INFO: epoch 001:  16363 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3564, wps=101.4, ups=0.61, wpb=110.2, bsz=40, num_updates=16340, lr=4.35729e-05, gnorm=1.072, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=89813
2023-01-10 14:50:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:44 - progress_bar.py[line:274] - INFO: epoch 001:  16373 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.494, wps=95.7, ups=0.58, wpb=110.5, bsz=40, num_updates=16350, lr=4.35677e-05, gnorm=0.915, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=89830
2023-01-10 14:50:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:50:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:00 - progress_bar.py[line:274] - INFO: epoch 001:  16383 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4128, wps=103.3, ups=0.63, wpb=109.1, bsz=40, num_updates=16360, lr=4.35625e-05, gnorm=0.336, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=89847
2023-01-10 14:51:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:17 - progress_bar.py[line:274] - INFO: epoch 001:  16393 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3737, wps=103.6, ups=0.62, wpb=110.9, bsz=40, num_updates=16370, lr=4.35573e-05, gnorm=0.388, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=89863
2023-01-10 14:51:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:34 - progress_bar.py[line:274] - INFO: epoch 001:  16403 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3608, wps=99, ups=0.6, wpb=109.9, bsz=40, num_updates=16380, lr=4.35521e-05, gnorm=0.672, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=89880
2023-01-10 14:51:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:50 - progress_bar.py[line:274] - INFO: epoch 001:  16413 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3137, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=16390, lr=4.35469e-05, gnorm=0.601, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=89896
2023-01-10 14:51:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:51:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:06 - progress_bar.py[line:274] - INFO: epoch 001:  16423 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3519, wps=101.5, ups=0.62, wpb=109.3, bsz=40, num_updates=16400, lr=4.35417e-05, gnorm=0.536, clip=20, loss_scale=256, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=89913
2023-01-10 14:52:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:23 - progress_bar.py[line:274] - INFO: epoch 001:  16433 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3238, wps=101.6, ups=0.62, wpb=109.4, bsz=40, num_updates=16410, lr=4.35365e-05, gnorm=2.772, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=89929
2023-01-10 14:52:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:40 - progress_bar.py[line:274] - INFO: epoch 001:  16443 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3654, wps=98.5, ups=0.61, wpb=108.2, bsz=40, num_updates=16420, lr=4.35313e-05, gnorm=0.59, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=89946
2023-01-10 14:52:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:52:56 - progress_bar.py[line:274] - INFO: epoch 001:  16453 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4054, wps=102.7, ups=0.62, wpb=110.1, bsz=40, num_updates=16430, lr=4.3526e-05, gnorm=0.595, clip=20, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=89962
2023-01-10 14:52:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:13 - progress_bar.py[line:274] - INFO: epoch 001:  16463 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3048, wps=100.8, ups=0.6, wpb=111.3, bsz=40, num_updates=16440, lr=4.35208e-05, gnorm=0.512, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=89979
2023-01-10 14:53:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:29 - progress_bar.py[line:274] - INFO: epoch 001:  16473 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.375, wps=103.1, ups=0.63, wpb=109.7, bsz=40, num_updates=16450, lr=4.35156e-05, gnorm=0.938, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=89995
2023-01-10 14:53:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:45 - progress_bar.py[line:274] - INFO: epoch 001:  16483 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3708, wps=102.8, ups=0.62, wpb=110.5, bsz=40, num_updates=16460, lr=4.35104e-05, gnorm=0.707, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90011
2023-01-10 14:53:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:53:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:02 - progress_bar.py[line:274] - INFO: epoch 001:  16493 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3883, wps=99.3, ups=0.61, wpb=108, bsz=40, num_updates=16470, lr=4.35052e-05, gnorm=2.385, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=90028
2023-01-10 14:54:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:18 - progress_bar.py[line:274] - INFO: epoch 001:  16503 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3571, wps=102.8, ups=0.62, wpb=110.2, bsz=40, num_updates=16480, lr=4.35e-05, gnorm=0.608, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90044
2023-01-10 14:54:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:34 - progress_bar.py[line:274] - INFO: epoch 001:  16513 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5408, wps=103.2, ups=0.62, wpb=111.1, bsz=40, num_updates=16490, lr=4.34948e-05, gnorm=0.586, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90061
2023-01-10 14:54:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:51 - progress_bar.py[line:274] - INFO: epoch 001:  16523 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3981, wps=99.3, ups=0.6, wpb=110.3, bsz=40, num_updates=16500, lr=4.34896e-05, gnorm=0.542, clip=20, loss_scale=256, train_wall=17, gb_free=10.8, ema_decay=0.9999, wall=90078
2023-01-10 14:54:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:54:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:08 - progress_bar.py[line:274] - INFO: epoch 001:  16533 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3524, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=16510, lr=4.34844e-05, gnorm=0.514, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90094
2023-01-10 14:55:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:25 - progress_bar.py[line:274] - INFO: epoch 001:  16543 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3523, wps=98.6, ups=0.59, wpb=110.9, bsz=40, num_updates=16520, lr=4.34792e-05, gnorm=0.585, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=90111
2023-01-10 14:55:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:41 - progress_bar.py[line:274] - INFO: epoch 001:  16553 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3038, wps=104.4, ups=0.63, wpb=110.9, bsz=40, num_updates=16530, lr=4.3474e-05, gnorm=0.54, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90127
2023-01-10 14:55:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:55:58 - progress_bar.py[line:274] - INFO: epoch 001:  16563 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3656, wps=101.6, ups=0.62, wpb=109.4, bsz=40, num_updates=16540, lr=4.34688e-05, gnorm=0.561, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90144
2023-01-10 14:56:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:14 - progress_bar.py[line:274] - INFO: epoch 001:  16573 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.31, wps=101.1, ups=0.61, wpb=109.8, bsz=40, num_updates=16550, lr=4.34635e-05, gnorm=0.456, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90160
2023-01-10 14:56:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:31 - progress_bar.py[line:274] - INFO: epoch 001:  16583 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3922, wps=98.3, ups=0.6, wpb=108.6, bsz=40, num_updates=16560, lr=4.34583e-05, gnorm=0.455, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=90177
2023-01-10 14:56:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:48 - progress_bar.py[line:274] - INFO: epoch 001:  16593 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3243, wps=99.6, ups=0.61, wpb=109.6, bsz=40, num_updates=16570, lr=4.34531e-05, gnorm=0.667, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90194
2023-01-10 14:56:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:56:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:04 - progress_bar.py[line:274] - INFO: epoch 001:  16603 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4257, wps=100.9, ups=0.61, wpb=109.5, bsz=40, num_updates=16580, lr=4.34479e-05, gnorm=0.388, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90210
2023-01-10 14:57:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:21 - progress_bar.py[line:274] - INFO: epoch 001:  16613 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4019, wps=98.6, ups=0.6, wpb=109.3, bsz=40, num_updates=16590, lr=4.34427e-05, gnorm=0.609, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=90227
2023-01-10 14:57:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:38 - progress_bar.py[line:274] - INFO: epoch 001:  16623 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.451, wps=99.5, ups=0.6, wpb=110.1, bsz=40, num_updates=16600, lr=4.34375e-05, gnorm=0.588, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=90244
2023-01-10 14:57:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:54 - progress_bar.py[line:274] - INFO: epoch 001:  16633 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3626, wps=101.9, ups=0.62, wpb=110.2, bsz=40, num_updates=16610, lr=4.34323e-05, gnorm=1.111, clip=30, loss_scale=256, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=90260
2023-01-10 14:57:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:57:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:11 - progress_bar.py[line:274] - INFO: epoch 001:  16643 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.38, wps=100.2, ups=0.61, wpb=109.7, bsz=40, num_updates=16620, lr=4.34271e-05, gnorm=0.541, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90277
2023-01-10 14:58:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:27 - progress_bar.py[line:274] - INFO: epoch 001:  16653 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3854, wps=104.5, ups=0.63, wpb=110.7, bsz=40, num_updates=16630, lr=4.34219e-05, gnorm=0.854, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90293
2023-01-10 14:58:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:44 - progress_bar.py[line:274] - INFO: epoch 001:  16663 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3663, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=16640, lr=4.34167e-05, gnorm=0.451, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90310
2023-01-10 14:58:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:58:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:00 - progress_bar.py[line:274] - INFO: epoch 001:  16673 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.375, wps=104.4, ups=0.63, wpb=111, bsz=40, num_updates=16650, lr=4.34115e-05, gnorm=0.888, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90326
2023-01-10 14:59:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:16 - progress_bar.py[line:274] - INFO: epoch 001:  16683 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3778, wps=104.7, ups=0.63, wpb=110.8, bsz=40, num_updates=16660, lr=4.34063e-05, gnorm=0.597, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90342
2023-01-10 14:59:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:32 - progress_bar.py[line:274] - INFO: epoch 001:  16693 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3714, wps=102.4, ups=0.62, wpb=110, bsz=40, num_updates=16670, lr=4.3401e-05, gnorm=0.623, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=90359
2023-01-10 14:59:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:49 - progress_bar.py[line:274] - INFO: epoch 001:  16703 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2826, wps=101.2, ups=0.61, wpb=110.1, bsz=40, num_updates=16680, lr=4.33958e-05, gnorm=0.489, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=90375
2023-01-10 14:59:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 14:59:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:06 - progress_bar.py[line:274] - INFO: epoch 001:  16713 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4259, wps=100.1, ups=0.61, wpb=109.8, bsz=40, num_updates=16690, lr=4.33906e-05, gnorm=0.775, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90392
2023-01-10 15:00:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:22 - progress_bar.py[line:274] - INFO: epoch 001:  16723 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.38, wps=103.4, ups=0.63, wpb=110.1, bsz=40, num_updates=16700, lr=4.33854e-05, gnorm=0.937, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90408
2023-01-10 15:00:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:38 - progress_bar.py[line:274] - INFO: epoch 001:  16733 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3269, wps=103.5, ups=0.63, wpb=109.1, bsz=40, num_updates=16710, lr=4.33802e-05, gnorm=0.504, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90424
2023-01-10 15:00:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:54 - progress_bar.py[line:274] - INFO: epoch 001:  16743 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3299, wps=102.6, ups=0.62, wpb=110.1, bsz=40, num_updates=16720, lr=4.3375e-05, gnorm=0.888, clip=30, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=90440
2023-01-10 15:00:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:00:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:10 - progress_bar.py[line:274] - INFO: epoch 001:  16753 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3558, wps=104.4, ups=0.63, wpb=110.1, bsz=40, num_updates=16730, lr=4.33698e-05, gnorm=0.496, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90456
2023-01-10 15:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:26 - progress_bar.py[line:274] - INFO: epoch 001:  16763 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2323, wps=103.3, ups=0.63, wpb=110.1, bsz=40, num_updates=16740, lr=4.33646e-05, gnorm=0.861, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90473
2023-01-10 15:01:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:43 - progress_bar.py[line:274] - INFO: epoch 001:  16773 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.133, nsentences=40, sample_size=107.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3611, wps=99, ups=0.62, wpb=107.1, bsz=40, num_updates=16750, lr=4.33594e-05, gnorm=0.25, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90489
2023-01-10 15:01:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:01:59 - progress_bar.py[line:274] - INFO: epoch 001:  16783 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3423, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=16760, lr=4.33542e-05, gnorm=0.484, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90506
2023-01-10 15:02:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:16 - progress_bar.py[line:274] - INFO: epoch 001:  16793 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3333, wps=102.3, ups=0.62, wpb=110.6, bsz=40, num_updates=16770, lr=4.3349e-05, gnorm=0.386, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90522
2023-01-10 15:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:22 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-10 15:02:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:33 - progress_bar.py[line:274] - INFO: epoch 001:  16804 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4646, wps=96.1, ups=0.58, wpb=110.3, bsz=40, num_updates=16780, lr=4.33438e-05, gnorm=0.567, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=90539
2023-01-10 15:02:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:50 - progress_bar.py[line:274] - INFO: epoch 001:  16814 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2788, wps=99.5, ups=0.61, wpb=108.8, bsz=40, num_updates=16790, lr=4.33385e-05, gnorm=0.483, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=90556
2023-01-10 15:02:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:06 - progress_bar.py[line:274] - INFO: epoch 001:  16824 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3505, wps=103, ups=0.62, wpb=110.4, bsz=40, num_updates=16800, lr=4.33333e-05, gnorm=0.411, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90572
2023-01-10 15:03:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:22 - progress_bar.py[line:274] - INFO: epoch 001:  16834 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3878, wps=104.4, ups=0.63, wpb=109.9, bsz=40, num_updates=16810, lr=4.33281e-05, gnorm=1.062, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90588
2023-01-10 15:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:38 - progress_bar.py[line:274] - INFO: epoch 001:  16844 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3778, wps=103.7, ups=0.64, wpb=108.6, bsz=40, num_updates=16820, lr=4.33229e-05, gnorm=0.757, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90604
2023-01-10 15:03:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:54 - progress_bar.py[line:274] - INFO: epoch 001:  16854 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3832, wps=104.9, ups=0.63, wpb=110.3, bsz=40, num_updates=16830, lr=4.33177e-05, gnorm=0.298, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90620
2023-01-10 15:03:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:03:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:11 - progress_bar.py[line:274] - INFO: epoch 001:  16864 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3964, wps=99.9, ups=0.61, wpb=109.1, bsz=40, num_updates=16840, lr=4.33125e-05, gnorm=0.369, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90637
2023-01-10 15:04:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:28 - progress_bar.py[line:274] - INFO: epoch 001:  16874 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.2647, wps=100.2, ups=0.6, wpb=110.5, bsz=40, num_updates=16850, lr=4.33073e-05, gnorm=0.412, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90654
2023-01-10 15:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:44 - progress_bar.py[line:274] - INFO: epoch 001:  16884 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3711, wps=100.9, ups=0.62, wpb=109.3, bsz=40, num_updates=16860, lr=4.33021e-05, gnorm=0.726, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=90670
2023-01-10 15:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:04:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:01 - progress_bar.py[line:274] - INFO: epoch 001:  16894 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3667, wps=102.2, ups=0.62, wpb=110.7, bsz=40, num_updates=16870, lr=4.32969e-05, gnorm=0.363, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90687
2023-01-10 15:05:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:17 - progress_bar.py[line:274] - INFO: epoch 001:  16904 / 100000 loss=0.356, loss_v1=0, loss_v2=0, nll_loss=0.207, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3578, wps=103.8, ups=0.63, wpb=109.5, bsz=40, num_updates=16880, lr=4.32917e-05, gnorm=2.536, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=90703
2023-01-10 15:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:33 - progress_bar.py[line:274] - INFO: epoch 001:  16914 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3871, wps=102, ups=0.61, wpb=111.3, bsz=40, num_updates=16890, lr=4.32865e-05, gnorm=0.414, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90719
2023-01-10 15:05:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:50 - progress_bar.py[line:274] - INFO: epoch 001:  16924 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3727, wps=99.3, ups=0.6, wpb=109.5, bsz=40, num_updates=16900, lr=4.32812e-05, gnorm=0.677, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90736
2023-01-10 15:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:05:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:07 - progress_bar.py[line:274] - INFO: epoch 001:  16934 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3654, wps=95.6, ups=0.58, wpb=109.7, bsz=40, num_updates=16910, lr=4.3276e-05, gnorm=0.696, clip=30, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=90754
2023-01-10 15:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:24 - progress_bar.py[line:274] - INFO: epoch 001:  16944 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3977, wps=99.5, ups=0.6, wpb=110.5, bsz=40, num_updates=16920, lr=4.32708e-05, gnorm=0.569, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=90770
2023-01-10 15:06:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:41 - progress_bar.py[line:274] - INFO: epoch 001:  16954 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4021, wps=101.5, ups=0.62, wpb=109.1, bsz=40, num_updates=16930, lr=4.32656e-05, gnorm=0.645, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90787
2023-01-10 15:06:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:06:57 - progress_bar.py[line:274] - INFO: epoch 001:  16964 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3402, wps=100.3, ups=0.62, wpb=108.3, bsz=40, num_updates=16940, lr=4.32604e-05, gnorm=0.576, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90803
2023-01-10 15:06:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:14 - progress_bar.py[line:274] - INFO: epoch 001:  16974 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2957, wps=98.4, ups=0.6, wpb=108.6, bsz=40, num_updates=16950, lr=4.32552e-05, gnorm=0.435, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=90820
2023-01-10 15:07:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:30 - progress_bar.py[line:274] - INFO: epoch 001:  16984 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4356, wps=102.5, ups=0.63, wpb=109.3, bsz=40, num_updates=16960, lr=4.325e-05, gnorm=0.398, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90836
2023-01-10 15:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:46 - progress_bar.py[line:274] - INFO: epoch 001:  16994 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4141, wps=100.4, ups=0.62, wpb=108.3, bsz=40, num_updates=16970, lr=4.32448e-05, gnorm=0.609, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90853
2023-01-10 15:07:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:07:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:08:03 - progress_bar.py[line:274] - INFO: epoch 001:  17004 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4494, wps=104.9, ups=0.63, wpb=111.5, bsz=40, num_updates=16980, lr=4.32396e-05, gnorm=0.476, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=90869
2023-01-10 15:08:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:08:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:08:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:08:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:08:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:08:19 - progress_bar.py[line:274] - INFO: epoch 001:  17014 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.425, wps=100.9, ups=0.62, wpb=108.8, bsz=40, num_updates=16990, lr=4.32344e-05, gnorm=0.865, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=90885
2023-01-10 15:08:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:08:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:08:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:08:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 15:08:36 - progress_bar.py[line:274] - INFO: epoch 001:  17024 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3789, wps=102.8, ups=0.61, wpb=111.9, bsz=40, num_updates=17000, lr=4.32292e-05, gnorm=0.808, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=90902
2023-01-10 15:08:36 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 15:08:37 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 15:08:37 - train.py[line:551] - INFO: load:1.26 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 15:08:53 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.21 GiB (GPU 0; 39.59 GiB total capacity; 9.30 GiB already allocated; 2.88 GiB free; 34.22 GiB reserved in total by PyTorch)
2023-01-10 15:08:53 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9520 MB |   14751 MB |   12790 TB |   12790 TB |
|       from large pool |    9346 MB |   14576 MB |   12785 TB |   12785 TB |
|       from small pool |     174 MB |     175 MB |       4 TB |       4 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9520 MB |   14751 MB |   12790 TB |   12790 TB |
|       from large pool |    9346 MB |   14576 MB |   12785 TB |   12785 TB |
|       from small pool |     174 MB |     175 MB |       4 TB |       4 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   35046 MB |   37646 MB |  282414 MB |  247368 MB |
|       from large pool |   34870 MB |   37464 MB |  282012 MB |  247142 MB |
|       from small pool |     176 MB |     182 MB |     402 MB |     226 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   25525 MB |   30070 MB |   13325 TB |   13325 TB |
|       from large pool |   25523 MB |   30067 MB |   13320 TB |   13320 TB |
|       from small pool |       1 MB |       3 MB |       4 TB |       4 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4634    |    4648    |  611133 K  |  611129 K  |
|       from large pool |     698    |     710    |  188519 K  |  188518 K  |
|       from small pool |    3936    |    3946    |  422614 K  |  422610 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4634    |    4648    |  611133 K  |  611129 K  |
|       from large pool |     698    |     710    |  188519 K  |  188518 K  |
|       from small pool |    3936    |    3946    |  422614 K  |  422610 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     186    |     197    |     855    |     669    |
|       from large pool |      98    |     106    |     654    |     556    |
|       from small pool |      88    |      91    |     201    |     113    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     138    |  442656 K  |  442656 K  |
|       from large pool |      70    |      75    |   71711 K  |   71711 K  |
|       from small pool |      56    |      70    |  370945 K  |  370945 K  |
|===========================================================================|

2023-01-10 15:08:53 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-10 15:08:53 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-01-10 15:11:09 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 15:11:09 - train.py[line:551] - INFO: load:1.29 valid_run:152.00 task_valid:147.97 collect_output:1.85
2023-01-10 15:13:37 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 15:13:38 - train.py[line:551] - INFO: load:1.31 valid_run:299.70 task_valid:291.14 collect_output:5.37
2023-01-10 15:16:10 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 15:16:10 - train.py[line:551] - INFO: load:1.34 valid_run:450.78 task_valid:433.93 collect_output:12.63
2023-01-10 15:18:38 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 15:18:38 - train.py[line:551] - INFO: load:1.37 valid_run:599.10 task_valid:578.66 collect_output:15.17
2023-01-10 15:21:09 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 15:21:09 - train.py[line:551] - INFO: load:1.40 valid_run:750.42 task_valid:725.90 collect_output:18.24
2023-01-10 15:23:40 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 15:23:40 - train.py[line:551] - INFO: load:1.42 valid_run:901.28 task_valid:871.54 collect_output:22.43
2023-01-10 15:26:13 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 15:26:13 - train.py[line:551] - INFO: load:1.45 valid_run:1053.54 task_valid:1017.78 collect_output:27.41
2023-01-10 15:28:43 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 15:28:43 - train.py[line:551] - INFO: load:1.48 valid_run:1203.55 task_valid:1158.83 collect_output:35.35
2023-01-10 15:31:12 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 15:31:12 - train.py[line:551] - INFO: load:1.50 valid_run:1352.40 task_valid:1303.77 collect_output:38.24
2023-01-10 15:33:39 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 15:33:39 - train.py[line:551] - INFO: load:1.53 valid_run:1499.94 task_valid:1446.72 collect_output:41.81
2023-01-10 15:36:08 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 15:36:08 - train.py[line:551] - INFO: load:1.56 valid_run:1648.86 task_valid:1591.58 collect_output:44.83
2023-01-10 15:38:38 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 15:38:38 - train.py[line:551] - INFO: load:1.58 valid_run:1797.97 task_valid:1736.41 collect_output:48.10
2023-01-10 15:41:06 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 15:41:06 - train.py[line:551] - INFO: load:1.61 valid_run:1946.75 task_valid:1878.52 collect_output:53.72
2023-01-10 15:43:36 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 15:43:36 - train.py[line:551] - INFO: load:1.63 valid_run:2096.45 task_valid:2023.92 collect_output:57.02
2023-01-10 15:46:06 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 15:46:06 - train.py[line:551] - INFO: load:1.66 valid_run:2245.91 task_valid:2170.31 collect_output:59.08
2023-01-10 15:48:35 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 15:48:35 - train.py[line:551] - INFO: load:1.69 valid_run:2394.95 task_valid:2314.40 collect_output:63.01
2023-01-10 15:51:05 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 15:51:05 - train.py[line:551] - INFO: load:1.71 valid_run:2545.37 task_valid:2459.98 collect_output:66.83
2023-01-10 15:53:35 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 15:53:35 - train.py[line:551] - INFO: load:1.74 valid_run:2695.40 task_valid:2606.91 collect_output:68.93
2023-01-10 15:56:03 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 15:56:03 - train.py[line:551] - INFO: load:1.77 valid_run:2842.63 task_valid:2748.48 collect_output:73.57
2023-01-10 15:58:32 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 15:58:32 - train.py[line:551] - INFO: load:1.79 valid_run:2991.77 task_valid:2893.42 collect_output:76.76
2023-01-10 16:01:03 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 16:01:03 - train.py[line:551] - INFO: load:1.82 valid_run:3142.29 task_valid:3038.10 collect_output:81.57
2023-01-10 16:03:32 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 16:03:32 - train.py[line:551] - INFO: load:1.84 valid_run:3290.93 task_valid:3182.74 collect_output:84.53
2023-01-10 16:06:02 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 16:06:02 - train.py[line:551] - INFO: load:1.87 valid_run:3441.01 task_valid:3329.13 collect_output:87.20
2023-01-10 16:08:33 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 16:08:33 - train.py[line:551] - INFO: load:1.90 valid_run:3591.47 task_valid:3475.79 collect_output:89.98

====================================================================================================
SGG eval:     R @ 50: 0.4964;     R @ 100: 0.5890;     R @ 500: 0.6469;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3165;    mR @ 100: 0.3726;    mR @ 500: 0.4323;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4355) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.8056) (says:0.0000) (sitting on:0.7217) (standing on:0.2843) (using:0.6000) (walking in:0.0000) (walking on:0.3874) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4964;     R @ 100: 0.5890;     R @ 500: 0.6469;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3165;    mR @ 100: 0.3726;    mR @ 500: 0.4323;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7561) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4355) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.8056) (says:0.0000) (sitting on:0.7217) (standing on:0.2843) (using:0.6000) (walking in:0.0000) (walking on:0.3874) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================

2023-01-10 16:11:03 - train.py[line:487] - INFO: 0.5889957983193278
2023-01-10 16:11:03 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 16:11:04 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.375 | loss_v1 0 | loss_v2 0 | nll_loss 0.219 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.588996 | ppl 1.16 | vqa_score 0.5439 | wps 119.7 | wpb 89.9 | bsz 30 | num_updates 17000 | best_R@100 0.69005
2023-01-10 16:11:04 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 17000 updates
2023-01-10 16:11:04 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_17000.pt
2023-01-10 16:11:49 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_17000.pt
2023-01-10 16:13:22 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_17000.pt (epoch 1 @ 17000 updates, score 0.5889957983193278) (writing took 137.48897225223482 seconds)
2023-01-10 16:13:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:13:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:13:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:13:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:13:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:13:38 - progress_bar.py[line:274] - INFO: epoch 001:  17034 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3587, wps=0.4, ups=0, wpb=111, bsz=40, num_updates=17010, lr=4.3224e-05, gnorm=0.69, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=94804
2023-01-10 16:13:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:13:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:13:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:13:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:13:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:13:55 - progress_bar.py[line:274] - INFO: epoch 001:  17044 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4066, wps=100.2, ups=0.61, wpb=109.9, bsz=40, num_updates=17020, lr=4.32188e-05, gnorm=0.46, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=94821
2023-01-10 16:13:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:13:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:13 - progress_bar.py[line:274] - INFO: epoch 001:  17054 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4023, wps=98.6, ups=0.59, wpb=110.9, bsz=40, num_updates=17030, lr=4.32135e-05, gnorm=0.509, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=94839
2023-01-10 16:14:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:30 - progress_bar.py[line:274] - INFO: epoch 001:  17064 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.199, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3604, wps=97.8, ups=0.6, wpb=108.3, bsz=40, num_updates=17040, lr=4.32083e-05, gnorm=1.091, clip=20, loss_scale=256, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=94856
2023-01-10 16:14:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:47 - progress_bar.py[line:274] - INFO: epoch 001:  17074 / 100000 loss=0.347, loss_v1=0, loss_v2=0, nll_loss=0.197, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3241, wps=99.2, ups=0.61, wpb=109, bsz=40, num_updates=17050, lr=4.32031e-05, gnorm=0.549, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=94873
2023-01-10 16:14:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:14:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:05 - progress_bar.py[line:274] - INFO: epoch 001:  17084 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.37, wps=99.1, ups=0.6, wpb=109.3, bsz=40, num_updates=17060, lr=4.31979e-05, gnorm=0.37, clip=0, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=94890
2023-01-10 16:15:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:21 - progress_bar.py[line:274] - INFO: epoch 001:  17094 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.413, wps=104.6, ups=0.63, wpb=110.3, bsz=40, num_updates=17070, lr=4.31927e-05, gnorm=0.36, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=94907
2023-01-10 16:15:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:38 - progress_bar.py[line:274] - INFO: epoch 001:  17104 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3942, wps=100, ups=0.6, wpb=110.7, bsz=40, num_updates=17080, lr=4.31875e-05, gnorm=0.406, clip=10, loss_scale=256, train_wall=17, gb_free=10.9, ema_decay=0.9999, wall=94924
2023-01-10 16:15:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:56 - progress_bar.py[line:274] - INFO: epoch 001:  17114 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3846, wps=99.9, ups=0.6, wpb=110.7, bsz=40, num_updates=17090, lr=4.31823e-05, gnorm=0.778, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=94941
2023-01-10 16:15:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:15:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:12 - progress_bar.py[line:274] - INFO: epoch 001:  17124 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3868, wps=103.6, ups=0.63, wpb=110.3, bsz=40, num_updates=17100, lr=4.31771e-05, gnorm=0.338, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=94958
2023-01-10 16:16:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:29 - progress_bar.py[line:274] - INFO: epoch 001:  17134 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.2604, wps=99.7, ups=0.6, wpb=110.6, bsz=40, num_updates=17110, lr=4.31719e-05, gnorm=0.422, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=94975
2023-01-10 16:16:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:46 - progress_bar.py[line:274] - INFO: epoch 001:  17144 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4536, wps=101.4, ups=0.62, wpb=109.7, bsz=40, num_updates=17120, lr=4.31667e-05, gnorm=0.567, clip=20, loss_scale=256, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=94992
2023-01-10 16:16:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:03 - progress_bar.py[line:274] - INFO: epoch 001:  17154 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3679, wps=99.3, ups=0.61, wpb=107.9, bsz=40, num_updates=17130, lr=4.31615e-05, gnorm=0.314, clip=0, loss_scale=256, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=95009
2023-01-10 16:17:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:20 - progress_bar.py[line:274] - INFO: epoch 001:  17164 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4314, wps=100.5, ups=0.61, wpb=110.4, bsz=40, num_updates=17140, lr=4.31563e-05, gnorm=0.695, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95026
2023-01-10 16:17:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:37 - progress_bar.py[line:274] - INFO: epoch 001:  17174 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3636, wps=105.1, ups=0.63, wpb=111.1, bsz=40, num_updates=17150, lr=4.3151e-05, gnorm=0.324, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95043
2023-01-10 16:17:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:54 - progress_bar.py[line:274] - INFO: epoch 001:  17184 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3689, wps=100.2, ups=0.61, wpb=108.6, bsz=40, num_updates=17160, lr=4.31458e-05, gnorm=1.158, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95060
2023-01-10 16:17:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:12 - progress_bar.py[line:274] - INFO: epoch 001:  17194 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3556, wps=98.8, ups=0.6, wpb=110.4, bsz=40, num_updates=17170, lr=4.31406e-05, gnorm=0.754, clip=30, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=95077
2023-01-10 16:18:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:29 - progress_bar.py[line:274] - INFO: epoch 001:  17204 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4128, wps=98.1, ups=0.6, wpb=109, bsz=40, num_updates=17180, lr=4.31354e-05, gnorm=0.588, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=95095
2023-01-10 16:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:46 - progress_bar.py[line:274] - INFO: epoch 001:  17214 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3605, wps=102.2, ups=0.61, wpb=110.8, bsz=40, num_updates=17190, lr=4.31302e-05, gnorm=1.32, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95112
2023-01-10 16:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:18:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:03 - progress_bar.py[line:274] - INFO: epoch 001:  17224 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4524, wps=105, ups=0.63, wpb=110.4, bsz=40, num_updates=17200, lr=4.3125e-05, gnorm=1.047, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95128
2023-01-10 16:19:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:20 - progress_bar.py[line:274] - INFO: epoch 001:  17234 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3491, wps=99.8, ups=0.6, wpb=110.3, bsz=40, num_updates=17210, lr=4.31198e-05, gnorm=0.463, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=95146
2023-01-10 16:19:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:37 - progress_bar.py[line:274] - INFO: epoch 001:  17244 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3486, wps=97.8, ups=0.6, wpb=108.5, bsz=40, num_updates=17220, lr=4.31146e-05, gnorm=1.209, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=95163
2023-01-10 16:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:54 - progress_bar.py[line:274] - INFO: epoch 001:  17254 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2427, wps=100.6, ups=0.62, wpb=108.8, bsz=40, num_updates=17230, lr=4.31094e-05, gnorm=0.423, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=95180
2023-01-10 16:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:19:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:11 - progress_bar.py[line:274] - INFO: epoch 001:  17264 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4688, wps=101.9, ups=0.61, wpb=110.8, bsz=40, num_updates=17240, lr=4.31042e-05, gnorm=0.651, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95197
2023-01-10 16:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:27 - progress_bar.py[line:274] - INFO: epoch 001:  17274 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3814, wps=105.3, ups=0.64, wpb=109.4, bsz=40, num_updates=17250, lr=4.3099e-05, gnorm=0.377, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=95213
2023-01-10 16:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:44 - progress_bar.py[line:274] - INFO: epoch 001:  17284 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4071, wps=99.2, ups=0.61, wpb=108.9, bsz=40, num_updates=17260, lr=4.30938e-05, gnorm=1.021, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=95230
2023-01-10 16:20:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:20:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:02 - progress_bar.py[line:274] - INFO: epoch 001:  17294 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3478, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=17270, lr=4.30885e-05, gnorm=0.496, clip=10, loss_scale=256, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=95247
2023-01-10 16:21:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:19 - progress_bar.py[line:274] - INFO: epoch 001:  17304 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4062, wps=99.4, ups=0.6, wpb=109.9, bsz=40, num_updates=17280, lr=4.30833e-05, gnorm=0.662, clip=10, loss_scale=256, train_wall=17, gb_free=10.8, ema_decay=0.9999, wall=95265
2023-01-10 16:21:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:36 - progress_bar.py[line:274] - INFO: epoch 001:  17314 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3516, wps=102.4, ups=0.61, wpb=111.4, bsz=40, num_updates=17290, lr=4.30781e-05, gnorm=0.406, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=95282
2023-01-10 16:21:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:53 - progress_bar.py[line:274] - INFO: epoch 001:  17324 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3874, wps=100.9, ups=0.62, wpb=109, bsz=40, num_updates=17300, lr=4.30729e-05, gnorm=0.678, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=95298
2023-01-10 16:21:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:21:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:10 - progress_bar.py[line:274] - INFO: epoch 001:  17334 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3398, wps=99.7, ups=0.61, wpb=109.7, bsz=40, num_updates=17310, lr=4.30677e-05, gnorm=1.113, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=95315
2023-01-10 16:22:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:26 - progress_bar.py[line:274] - INFO: epoch 001:  17344 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3846, wps=103.1, ups=0.62, wpb=110.4, bsz=40, num_updates=17320, lr=4.30625e-05, gnorm=1.002, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=95332
2023-01-10 16:22:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:43 - progress_bar.py[line:274] - INFO: epoch 001:  17354 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.398, wps=99.2, ups=0.61, wpb=108.4, bsz=40, num_updates=17330, lr=4.30573e-05, gnorm=0.572, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=95349
2023-01-10 16:22:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:22:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:00 - progress_bar.py[line:274] - INFO: epoch 001:  17364 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.41, wps=101.6, ups=0.61, wpb=110.1, bsz=40, num_updates=17340, lr=4.30521e-05, gnorm=0.528, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=95366
2023-01-10 16:23:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:17 - progress_bar.py[line:274] - INFO: epoch 001:  17374 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.402, wps=102.1, ups=0.62, wpb=109.4, bsz=40, num_updates=17350, lr=4.30469e-05, gnorm=0.441, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=95383
2023-01-10 16:23:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:29 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-10 16:23:36 - progress_bar.py[line:274] - INFO: epoch 001:  17385 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3789, wps=94.8, ups=0.57, wpb=110.9, bsz=40, num_updates=17360, lr=4.30417e-05, gnorm=0.566, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=95401
2023-01-10 16:23:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:52 - progress_bar.py[line:274] - INFO: epoch 001:  17395 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4082, wps=101.7, ups=0.62, wpb=109.8, bsz=40, num_updates=17370, lr=4.30365e-05, gnorm=0.424, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=95418
2023-01-10 16:23:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:23:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:09 - progress_bar.py[line:274] - INFO: epoch 001:  17405 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3429, wps=101.7, ups=0.62, wpb=109.8, bsz=40, num_updates=17380, lr=4.30312e-05, gnorm=0.843, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=95435
2023-01-10 16:24:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:27 - progress_bar.py[line:274] - INFO: epoch 001:  17415 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=111.933, nsentences=40, sample_size=111.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3438, wps=100.3, ups=0.6, wpb=111.9, bsz=40, num_updates=17390, lr=4.3026e-05, gnorm=0.755, clip=40, loss_scale=256, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=95453
2023-01-10 16:24:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:44 - progress_bar.py[line:274] - INFO: epoch 001:  17425 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3163, wps=99.5, ups=0.61, wpb=109.6, bsz=40, num_updates=17400, lr=4.30208e-05, gnorm=1.211, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=95470
2023-01-10 16:24:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:24:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:01 - progress_bar.py[line:274] - INFO: epoch 001:  17435 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2913, wps=100.9, ups=0.62, wpb=108.3, bsz=40, num_updates=17410, lr=4.30156e-05, gnorm=0.956, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=95487
2023-01-10 16:25:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:18 - progress_bar.py[line:274] - INFO: epoch 001:  17445 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4, wps=101.3, ups=0.61, wpb=111.5, bsz=40, num_updates=17420, lr=4.30104e-05, gnorm=0.401, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95504
2023-01-10 16:25:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:35 - progress_bar.py[line:274] - INFO: epoch 001:  17455 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3564, wps=98.3, ups=0.59, wpb=110.3, bsz=40, num_updates=17430, lr=4.30052e-05, gnorm=0.376, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=95521
2023-01-10 16:25:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:53 - progress_bar.py[line:274] - INFO: epoch 001:  17465 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.15, vqa_score=0.3617, wps=99.7, ups=0.6, wpb=110.7, bsz=40, num_updates=17440, lr=4.3e-05, gnorm=1.235, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=95539
2023-01-10 16:25:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:25:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:10 - progress_bar.py[line:274] - INFO: epoch 001:  17475 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3125, wps=101.3, ups=0.6, wpb=111.8, bsz=40, num_updates=17450, lr=4.29948e-05, gnorm=0.71, clip=20, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=95556
2023-01-10 16:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:28 - progress_bar.py[line:274] - INFO: epoch 001:  17485 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3889, wps=98.8, ups=0.6, wpb=109.2, bsz=40, num_updates=17460, lr=4.29896e-05, gnorm=0.396, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=95573
2023-01-10 16:26:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:45 - progress_bar.py[line:274] - INFO: epoch 001:  17495 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3918, wps=101.1, ups=0.61, wpb=110, bsz=40, num_updates=17470, lr=4.29844e-05, gnorm=0.399, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=95590
2023-01-10 16:26:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:02 - progress_bar.py[line:274] - INFO: epoch 001:  17505 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3362, wps=98, ups=0.6, wpb=108.8, bsz=40, num_updates=17480, lr=4.29792e-05, gnorm=0.84, clip=40, loss_scale=256, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=95608
2023-01-10 16:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:19 - progress_bar.py[line:274] - INFO: epoch 001:  17515 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2736, wps=102.8, ups=0.62, wpb=110.9, bsz=40, num_updates=17490, lr=4.2974e-05, gnorm=1.061, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=95625
2023-01-10 16:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:36 - progress_bar.py[line:274] - INFO: epoch 001:  17525 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3925, wps=100.7, ups=0.61, wpb=110.4, bsz=40, num_updates=17500, lr=4.29688e-05, gnorm=0.572, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95642
2023-01-10 16:27:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:53 - progress_bar.py[line:274] - INFO: epoch 001:  17535 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3958, wps=100.4, ups=0.61, wpb=110.4, bsz=40, num_updates=17510, lr=4.29635e-05, gnorm=0.584, clip=20, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=95659
2023-01-10 16:27:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:27:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:11 - progress_bar.py[line:274] - INFO: epoch 001:  17545 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.367, wps=98.1, ups=0.61, wpb=107.2, bsz=40, num_updates=17520, lr=4.29583e-05, gnorm=0.503, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=95676
2023-01-10 16:28:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:28 - progress_bar.py[line:274] - INFO: epoch 001:  17555 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4513, wps=99.2, ups=0.61, wpb=109.3, bsz=40, num_updates=17530, lr=4.29531e-05, gnorm=0.953, clip=40, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=95694
2023-01-10 16:28:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:46 - progress_bar.py[line:274] - INFO: epoch 001:  17565 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3529, wps=96.6, ups=0.59, wpb=108.9, bsz=40, num_updates=17540, lr=4.29479e-05, gnorm=0.704, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=95712
2023-01-10 16:28:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:28:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:03 - progress_bar.py[line:274] - INFO: epoch 001:  17575 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3956, wps=100, ups=0.61, wpb=109.7, bsz=40, num_updates=17550, lr=4.29427e-05, gnorm=0.325, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=95729
2023-01-10 16:29:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:20 - progress_bar.py[line:274] - INFO: epoch 001:  17585 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.37, wps=100.3, ups=0.61, wpb=109.1, bsz=40, num_updates=17560, lr=4.29375e-05, gnorm=0.566, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=95746
2023-01-10 16:29:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:37 - progress_bar.py[line:274] - INFO: epoch 001:  17595 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3222, wps=99.5, ups=0.6, wpb=111.5, bsz=40, num_updates=17570, lr=4.29323e-05, gnorm=0.785, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=95763
2023-01-10 16:29:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:54 - progress_bar.py[line:274] - INFO: epoch 001:  17605 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.46, wps=101.2, ups=0.61, wpb=110.1, bsz=40, num_updates=17580, lr=4.29271e-05, gnorm=0.408, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=95780
2023-01-10 16:29:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:29:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:11 - progress_bar.py[line:274] - INFO: epoch 001:  17615 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4086, wps=99.9, ups=0.6, wpb=110.8, bsz=40, num_updates=17590, lr=4.29219e-05, gnorm=0.932, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=95797
2023-01-10 16:30:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:27 - progress_bar.py[line:274] - INFO: epoch 001:  17625 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4455, wps=103, ups=0.63, wpb=109.9, bsz=40, num_updates=17600, lr=4.29167e-05, gnorm=1.281, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95813
2023-01-10 16:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:43 - progress_bar.py[line:274] - INFO: epoch 001:  17635 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4038, wps=101.3, ups=0.62, wpb=108.2, bsz=40, num_updates=17610, lr=4.29115e-05, gnorm=0.687, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95830
2023-01-10 16:30:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:30:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:00 - progress_bar.py[line:274] - INFO: epoch 001:  17645 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3571, wps=103.8, ups=0.63, wpb=110.7, bsz=40, num_updates=17620, lr=4.29063e-05, gnorm=0.508, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95846
2023-01-10 16:31:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:16 - progress_bar.py[line:274] - INFO: epoch 001:  17655 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4646, wps=103.4, ups=0.63, wpb=109.7, bsz=40, num_updates=17630, lr=4.2901e-05, gnorm=0.377, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=95862
2023-01-10 16:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:33 - progress_bar.py[line:274] - INFO: epoch 001:  17665 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4444, wps=99.4, ups=0.6, wpb=109.7, bsz=40, num_updates=17640, lr=4.28958e-05, gnorm=0.563, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=95879
2023-01-10 16:31:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:49 - progress_bar.py[line:274] - INFO: epoch 001:  17675 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4021, wps=103.5, ups=0.63, wpb=109.1, bsz=40, num_updates=17650, lr=4.28906e-05, gnorm=0.549, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=95895
2023-01-10 16:31:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:31:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:06 - progress_bar.py[line:274] - INFO: epoch 001:  17685 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.367, wps=98.5, ups=0.6, wpb=109.5, bsz=40, num_updates=17660, lr=4.28854e-05, gnorm=0.635, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=95912
2023-01-10 16:32:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:22 - progress_bar.py[line:274] - INFO: epoch 001:  17695 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4598, wps=104.1, ups=0.62, wpb=112.2, bsz=40, num_updates=17670, lr=4.28802e-05, gnorm=0.952, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=95928
2023-01-10 16:32:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:39 - progress_bar.py[line:274] - INFO: epoch 001:  17705 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3455, wps=99.2, ups=0.61, wpb=109.2, bsz=40, num_updates=17680, lr=4.2875e-05, gnorm=1.106, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=95945
2023-01-10 16:32:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:55 - progress_bar.py[line:274] - INFO: epoch 001:  17715 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3462, wps=100.8, ups=0.61, wpb=110.9, bsz=40, num_updates=17690, lr=4.28698e-05, gnorm=0.44, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=95962
2023-01-10 16:32:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:32:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:12 - progress_bar.py[line:274] - INFO: epoch 001:  17725 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.188, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3529, wps=98.1, ups=0.6, wpb=108.4, bsz=40, num_updates=17700, lr=4.28646e-05, gnorm=1.26, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=95978
2023-01-10 16:33:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:29 - progress_bar.py[line:274] - INFO: epoch 001:  17735 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4242, wps=99.4, ups=0.6, wpb=110.7, bsz=40, num_updates=17710, lr=4.28594e-05, gnorm=0.435, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=95995
2023-01-10 16:33:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:46 - progress_bar.py[line:274] - INFO: epoch 001:  17745 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.34, wps=103.5, ups=0.62, wpb=111.8, bsz=40, num_updates=17720, lr=4.28542e-05, gnorm=0.443, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96012
2023-01-10 16:33:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:02 - progress_bar.py[line:274] - INFO: epoch 001:  17755 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3551, wps=97.9, ups=0.6, wpb=108.1, bsz=40, num_updates=17730, lr=4.2849e-05, gnorm=0.388, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=96029
2023-01-10 16:34:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:19 - progress_bar.py[line:274] - INFO: epoch 001:  17765 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4694, wps=99, ups=0.61, wpb=108.9, bsz=40, num_updates=17740, lr=4.28438e-05, gnorm=0.993, clip=30, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=96045
2023-01-10 16:34:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:36 - progress_bar.py[line:274] - INFO: epoch 001:  17775 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4167, wps=99.6, ups=0.61, wpb=108.5, bsz=40, num_updates=17750, lr=4.28385e-05, gnorm=0.631, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=96062
2023-01-10 16:34:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:52 - progress_bar.py[line:274] - INFO: epoch 001:  17785 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4369, wps=102.9, ups=0.63, wpb=109.1, bsz=40, num_updates=17760, lr=4.28333e-05, gnorm=0.445, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96078
2023-01-10 16:34:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:34:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:08 - progress_bar.py[line:274] - INFO: epoch 001:  17795 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4019, wps=100.8, ups=0.62, wpb=108.7, bsz=40, num_updates=17770, lr=4.28281e-05, gnorm=0.317, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=96094
2023-01-10 16:35:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:25 - progress_bar.py[line:274] - INFO: epoch 001:  17805 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4231, wps=101, ups=0.62, wpb=109.3, bsz=40, num_updates=17780, lr=4.28229e-05, gnorm=0.569, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96111
2023-01-10 16:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:41 - progress_bar.py[line:274] - INFO: epoch 001:  17815 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3429, wps=102.1, ups=0.62, wpb=110.6, bsz=40, num_updates=17790, lr=4.28177e-05, gnorm=0.515, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=96127
2023-01-10 16:35:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:35:58 - progress_bar.py[line:274] - INFO: epoch 001:  17825 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3736, wps=100.5, ups=0.61, wpb=109.1, bsz=40, num_updates=17800, lr=4.28125e-05, gnorm=0.695, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96144
2023-01-10 16:35:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:12 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-10 16:36:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:16 - progress_bar.py[line:274] - INFO: epoch 001:  17836 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=110.625, nsentences=40, sample_size=110.625, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4488, wps=95.3, ups=0.54, wpb=110.6, bsz=40, num_updates=17810, lr=4.28073e-05, gnorm=0.878, clip=20, loss_scale=128, train_wall=19, gb_free=10.3, ema_decay=0.9999, wall=96163
2023-01-10 16:36:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:33 - progress_bar.py[line:274] - INFO: epoch 001:  17846 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=107.333, nsentences=40, sample_size=107.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4019, wps=97.7, ups=0.61, wpb=107.3, bsz=40, num_updates=17820, lr=4.28021e-05, gnorm=0.406, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96179
2023-01-10 16:36:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:50 - progress_bar.py[line:274] - INFO: epoch 001:  17856 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4643, wps=99.9, ups=0.62, wpb=108.3, bsz=40, num_updates=17830, lr=4.27969e-05, gnorm=0.782, clip=20, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=96196
2023-01-10 16:36:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:36:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:06 - progress_bar.py[line:274] - INFO: epoch 001:  17866 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3235, wps=102.1, ups=0.62, wpb=110.5, bsz=40, num_updates=17840, lr=4.27917e-05, gnorm=1.09, clip=40, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96212
2023-01-10 16:37:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:23 - progress_bar.py[line:274] - INFO: epoch 001:  17876 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3964, wps=99.1, ups=0.6, wpb=109.2, bsz=40, num_updates=17850, lr=4.27865e-05, gnorm=0.526, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96229
2023-01-10 16:37:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:39 - progress_bar.py[line:274] - INFO: epoch 001:  17886 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3143, wps=99.2, ups=0.61, wpb=107.8, bsz=40, num_updates=17860, lr=4.27812e-05, gnorm=0.669, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96246
2023-01-10 16:37:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:56 - progress_bar.py[line:274] - INFO: epoch 001:  17896 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4062, wps=102, ups=0.61, wpb=111, bsz=40, num_updates=17870, lr=4.2776e-05, gnorm=2.217, clip=30, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=96262
2023-01-10 16:37:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:37:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:12 - progress_bar.py[line:274] - INFO: epoch 001:  17906 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3261, wps=102.4, ups=0.62, wpb=110.6, bsz=40, num_updates=17880, lr=4.27708e-05, gnorm=0.504, clip=20, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=96278
2023-01-10 16:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:29 - progress_bar.py[line:274] - INFO: epoch 001:  17916 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3636, wps=102.7, ups=0.63, wpb=108.7, bsz=40, num_updates=17890, lr=4.27656e-05, gnorm=0.539, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=96295
2023-01-10 16:38:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:45 - progress_bar.py[line:274] - INFO: epoch 001:  17926 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4343, wps=99.1, ups=0.6, wpb=109.9, bsz=40, num_updates=17900, lr=4.27604e-05, gnorm=1.433, clip=30, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=96312
2023-01-10 16:38:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:38:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:02 - progress_bar.py[line:274] - INFO: epoch 001:  17936 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4369, wps=98.3, ups=0.6, wpb=108.5, bsz=40, num_updates=17910, lr=4.27552e-05, gnorm=0.575, clip=20, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=96328
2023-01-10 16:39:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:19 - progress_bar.py[line:274] - INFO: epoch 001:  17946 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3646, wps=102.6, ups=0.62, wpb=110.7, bsz=40, num_updates=17920, lr=4.275e-05, gnorm=0.474, clip=20, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=96345
2023-01-10 16:39:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:35 - progress_bar.py[line:274] - INFO: epoch 001:  17956 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3578, wps=102.7, ups=0.62, wpb=110.3, bsz=40, num_updates=17930, lr=4.27448e-05, gnorm=1.107, clip=40, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96361
2023-01-10 16:39:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:52 - progress_bar.py[line:274] - INFO: epoch 001:  17966 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4468, wps=100.3, ups=0.61, wpb=110.3, bsz=40, num_updates=17940, lr=4.27396e-05, gnorm=1.318, clip=30, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96378
2023-01-10 16:39:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:39:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:08 - progress_bar.py[line:274] - INFO: epoch 001:  17976 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4066, wps=102.8, ups=0.62, wpb=110.8, bsz=40, num_updates=17950, lr=4.27344e-05, gnorm=0.539, clip=10, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=96394
2023-01-10 16:40:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:25 - progress_bar.py[line:274] - INFO: epoch 001:  17986 / 100000 loss=0.338, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3619, wps=98.6, ups=0.6, wpb=108.9, bsz=40, num_updates=17960, lr=4.27292e-05, gnorm=1.175, clip=50, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=96411
2023-01-10 16:40:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:41 - progress_bar.py[line:274] - INFO: epoch 001:  17996 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4082, wps=103.4, ups=0.63, wpb=108.7, bsz=40, num_updates=17970, lr=4.2724e-05, gnorm=1.025, clip=30, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96427
2023-01-10 16:40:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:40:57 - progress_bar.py[line:274] - INFO: epoch 001:  18006 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3736, wps=103.6, ups=0.62, wpb=110.9, bsz=40, num_updates=17980, lr=4.27188e-05, gnorm=0.812, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=96444
2023-01-10 16:40:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:41:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:41:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:41:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:41:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:41:14 - progress_bar.py[line:274] - INFO: epoch 001:  18016 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3838, wps=100.7, ups=0.61, wpb=110.9, bsz=40, num_updates=17990, lr=4.27135e-05, gnorm=0.39, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=96460
2023-01-10 16:41:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:41:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:41:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:41:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:41:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 16:41:31 - progress_bar.py[line:274] - INFO: epoch 001:  18026 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3542, wps=101.2, ups=0.61, wpb=109.9, bsz=40, num_updates=18000, lr=4.27083e-05, gnorm=1.164, clip=30, loss_scale=128, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=96477
2023-01-10 16:41:31 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 16:41:32 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 16:41:32 - train.py[line:551] - INFO: load:1.24 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 16:44:04 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 16:44:04 - train.py[line:551] - INFO: load:1.27 valid_run:151.60 task_valid:148.63 collect_output:1.91
2023-01-10 16:46:32 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 16:46:32 - train.py[line:551] - INFO: load:1.29 valid_run:299.09 task_valid:291.52 collect_output:5.52
2023-01-10 16:49:03 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 16:49:03 - train.py[line:551] - INFO: load:1.31 valid_run:450.46 task_valid:434.72 collect_output:12.70
2023-01-10 16:51:32 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 16:51:32 - train.py[line:551] - INFO: load:1.34 valid_run:598.99 task_valid:579.89 collect_output:14.99
2023-01-10 16:54:04 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 16:54:04 - train.py[line:551] - INFO: load:1.37 valid_run:750.95 task_valid:727.76 collect_output:18.04
2023-01-10 16:56:35 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 16:56:35 - train.py[line:551] - INFO: load:1.39 valid_run:901.89 task_valid:873.50 collect_output:22.19
2023-01-10 16:59:07 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 16:59:07 - train.py[line:551] - INFO: load:1.42 valid_run:1054.34 task_valid:1019.89 collect_output:27.22
2023-01-10 17:01:37 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 17:01:37 - train.py[line:551] - INFO: load:1.45 valid_run:1204.55 task_valid:1161.19 collect_output:35.05
2023-01-10 17:04:06 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 17:04:06 - train.py[line:551] - INFO: load:1.47 valid_run:1353.38 task_valid:1306.09 collect_output:37.96
2023-01-10 17:06:34 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 17:06:34 - train.py[line:551] - INFO: load:1.50 valid_run:1501.24 task_valid:1449.55 collect_output:41.30
2023-01-10 17:09:04 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 17:09:04 - train.py[line:551] - INFO: load:1.52 valid_run:1650.65 task_valid:1594.79 collect_output:44.44
2023-01-10 17:11:33 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 17:11:33 - train.py[line:551] - INFO: load:1.55 valid_run:1799.76 task_valid:1739.91 collect_output:47.38
2023-01-10 17:14:02 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 17:14:02 - train.py[line:551] - INFO: load:1.57 valid_run:1948.60 task_valid:1881.82 collect_output:53.25
2023-01-10 17:16:32 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 17:16:32 - train.py[line:551] - INFO: load:1.60 valid_run:2098.55 task_valid:2027.53 collect_output:56.45
2023-01-10 17:19:02 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 17:19:02 - train.py[line:551] - INFO: load:1.63 valid_run:2248.47 task_valid:2174.44 collect_output:58.44
2023-01-10 17:21:31 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 17:21:31 - train.py[line:551] - INFO: load:1.65 valid_run:2397.78 task_valid:2318.87 collect_output:62.26
2023-01-10 17:24:02 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 17:24:02 - train.py[line:551] - INFO: load:1.68 valid_run:2548.16 task_valid:2464.57 collect_output:65.92
2023-01-10 17:26:32 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 17:26:32 - train.py[line:551] - INFO: load:1.70 valid_run:2698.25 task_valid:2611.78 collect_output:67.76
2023-01-10 17:29:00 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 17:29:00 - train.py[line:551] - INFO: load:1.73 valid_run:2845.79 task_valid:2753.78 collect_output:72.25
2023-01-10 17:31:29 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 17:31:29 - train.py[line:551] - INFO: load:1.76 valid_run:2995.09 task_valid:2899.13 collect_output:75.16
2023-01-10 17:33:59 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 17:33:59 - train.py[line:551] - INFO: load:1.78 valid_run:3145.47 task_valid:3043.89 collect_output:79.73
2023-01-10 17:36:29 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 17:36:29 - train.py[line:551] - INFO: load:1.81 valid_run:3294.58 task_valid:3189.30 collect_output:82.38
2023-01-10 17:38:59 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 17:38:59 - train.py[line:551] - INFO: load:1.83 valid_run:3445.22 task_valid:3336.11 collect_output:85.18
2023-01-10 17:41:30 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 17:41:30 - train.py[line:551] - INFO: load:1.86 valid_run:3595.83 task_valid:3482.83 collect_output:88.04

====================================================================================================
SGG eval:     R @ 50: 0.4917;     R @ 100: 0.5788;     R @ 500: 0.6378;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3174;    mR @ 100: 0.3755;    mR @ 500: 0.4324;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7317) (covered in:0.8125) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4355) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.8154) (says:0.0000) (sitting on:0.6978) (standing on:0.2693) (using:0.6000) (walking in:0.0000) (walking on:0.3739) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4917;     R @ 100: 0.5788;     R @ 500: 0.6378;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3174;    mR @ 100: 0.3755;    mR @ 500: 0.4324;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7317) (covered in:0.8125) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.2500) (hanging from:0.4355) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.8154) (says:0.0000) (sitting on:0.6978) (standing on:0.2693) (using:0.6000) (walking in:0.0000) (walking on:0.3739) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================

2023-01-10 17:44:01 - train.py[line:487] - INFO: 0.5788291316526611
2023-01-10 17:44:01 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 17:44:01 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.314 | loss_v1 0 | loss_v2 0 | nll_loss 0.158 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.578829 | ppl 1.12 | vqa_score 0.536 | wps 119.7 | wpb 89.9 | bsz 30 | num_updates 18000 | best_R@100 0.69005
2023-01-10 17:44:01 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 18000 updates
2023-01-10 17:44:01 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_18000.pt
2023-01-10 17:44:43 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_18000.pt
2023-01-10 17:46:13 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_18000.pt (epoch 1 @ 18000 updates, score 0.5788291316526611) (writing took 131.84811290353537 seconds)
2023-01-10 17:46:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:30 - progress_bar.py[line:274] - INFO: epoch 001:  18036 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3232, wps=0.4, ups=0, wpb=109.4, bsz=40, num_updates=18010, lr=4.27031e-05, gnorm=0.715, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=100376
2023-01-10 17:46:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:47 - progress_bar.py[line:274] - INFO: epoch 001:  18046 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3302, wps=98.2, ups=0.6, wpb=109.9, bsz=40, num_updates=18020, lr=4.26979e-05, gnorm=0.675, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=100393
2023-01-10 17:46:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:46:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:04 - progress_bar.py[line:274] - INFO: epoch 001:  18056 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3636, wps=100.4, ups=0.61, wpb=110.3, bsz=40, num_updates=18030, lr=4.26927e-05, gnorm=0.452, clip=10, loss_scale=128, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=100410
2023-01-10 17:47:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:20 - progress_bar.py[line:274] - INFO: epoch 001:  18066 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4066, wps=102.5, ups=0.61, wpb=111.3, bsz=40, num_updates=18040, lr=4.26875e-05, gnorm=0.417, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=100427
2023-01-10 17:47:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:37 - progress_bar.py[line:274] - INFO: epoch 001:  18076 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3673, wps=101.6, ups=0.61, wpb=110.9, bsz=40, num_updates=18050, lr=4.26823e-05, gnorm=1.279, clip=50, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=100443
2023-01-10 17:47:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:54 - progress_bar.py[line:274] - INFO: epoch 001:  18086 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4227, wps=101.5, ups=0.62, wpb=109.8, bsz=40, num_updates=18060, lr=4.26771e-05, gnorm=2.048, clip=50, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=100460
2023-01-10 17:47:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:47:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:10 - progress_bar.py[line:274] - INFO: epoch 001:  18096 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4074, wps=101.9, ups=0.62, wpb=109.3, bsz=40, num_updates=18070, lr=4.26719e-05, gnorm=0.463, clip=10, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=100476
2023-01-10 17:48:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:27 - progress_bar.py[line:274] - INFO: epoch 001:  18106 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3654, wps=98.6, ups=0.6, wpb=109.1, bsz=40, num_updates=18080, lr=4.26667e-05, gnorm=0.772, clip=20, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=100493
2023-01-10 17:48:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:43 - progress_bar.py[line:274] - INFO: epoch 001:  18116 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3587, wps=102.4, ups=0.62, wpb=109.8, bsz=40, num_updates=18090, lr=4.26615e-05, gnorm=0.873, clip=30, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=100509
2023-01-10 17:48:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:48:59 - progress_bar.py[line:274] - INFO: epoch 001:  18126 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4078, wps=105.6, ups=0.65, wpb=109, bsz=40, num_updates=18100, lr=4.26563e-05, gnorm=0.453, clip=0, loss_scale=128, train_wall=15, gb_free=10.3, ema_decay=0.9999, wall=100525
2023-01-10 17:48:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:15 - progress_bar.py[line:274] - INFO: epoch 001:  18136 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3663, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=18110, lr=4.2651e-05, gnorm=0.537, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=100542
2023-01-10 17:49:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:32 - progress_bar.py[line:274] - INFO: epoch 001:  18146 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4747, wps=98.4, ups=0.6, wpb=109.4, bsz=40, num_updates=18120, lr=4.26458e-05, gnorm=0.47, clip=10, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=100559
2023-01-10 17:49:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:49 - progress_bar.py[line:274] - INFO: epoch 001:  18156 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3469, wps=100.1, ups=0.61, wpb=109.6, bsz=40, num_updates=18130, lr=4.26406e-05, gnorm=0.578, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=100575
2023-01-10 17:49:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:49:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:06 - progress_bar.py[line:274] - INFO: epoch 001:  18166 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4021, wps=100.5, ups=0.61, wpb=110.1, bsz=40, num_updates=18140, lr=4.26354e-05, gnorm=0.456, clip=10, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=100592
2023-01-10 17:50:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:23 - progress_bar.py[line:274] - INFO: epoch 001:  18176 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4327, wps=97, ups=0.59, wpb=108.9, bsz=40, num_updates=18150, lr=4.26302e-05, gnorm=0.392, clip=0, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=100609
2023-01-10 17:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:41 - progress_bar.py[line:274] - INFO: epoch 001:  18186 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3846, wps=98, ups=0.6, wpb=109.3, bsz=40, num_updates=18160, lr=4.2625e-05, gnorm=0.459, clip=0, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=100626
2023-01-10 17:50:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:50:57 - progress_bar.py[line:274] - INFO: epoch 001:  18196 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3696, wps=101, ups=0.61, wpb=110.4, bsz=40, num_updates=18170, lr=4.26198e-05, gnorm=2.532, clip=30, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=100644
2023-01-10 17:50:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:14 - progress_bar.py[line:274] - INFO: epoch 001:  18206 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.383, wps=102.2, ups=0.62, wpb=110.4, bsz=40, num_updates=18180, lr=4.26146e-05, gnorm=0.672, clip=20, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=100660
2023-01-10 17:51:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:31 - progress_bar.py[line:274] - INFO: epoch 001:  18216 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4, wps=100.3, ups=0.61, wpb=110.4, bsz=40, num_updates=18190, lr=4.26094e-05, gnorm=0.579, clip=10, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=100677
2023-01-10 17:51:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:47 - progress_bar.py[line:274] - INFO: epoch 001:  18226 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4792, wps=104.5, ups=0.62, wpb=111.5, bsz=40, num_updates=18200, lr=4.26042e-05, gnorm=0.459, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=100693
2023-01-10 17:51:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:51:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:04 - progress_bar.py[line:274] - INFO: epoch 001:  18236 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4239, wps=101.7, ups=0.61, wpb=111.2, bsz=40, num_updates=18210, lr=4.2599e-05, gnorm=0.393, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=100710
2023-01-10 17:52:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:20 - progress_bar.py[line:274] - INFO: epoch 001:  18246 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4423, wps=103.8, ups=0.63, wpb=109.6, bsz=40, num_updates=18220, lr=4.25938e-05, gnorm=0.648, clip=30, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=100726
2023-01-10 17:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:36 - progress_bar.py[line:274] - INFO: epoch 001:  18256 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3611, wps=100.7, ups=0.62, wpb=109, bsz=40, num_updates=18230, lr=4.25885e-05, gnorm=0.636, clip=20, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=100743
2023-01-10 17:52:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:53 - progress_bar.py[line:274] - INFO: epoch 001:  18266 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4268, wps=103, ups=0.61, wpb=112.2, bsz=40, num_updates=18240, lr=4.25833e-05, gnorm=0.532, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=100759
2023-01-10 17:52:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:52:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:09 - progress_bar.py[line:274] - INFO: epoch 001:  18276 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3146, wps=103.7, ups=0.62, wpb=111, bsz=40, num_updates=18250, lr=4.25781e-05, gnorm=0.485, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=100776
2023-01-10 17:53:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:26 - progress_bar.py[line:274] - INFO: epoch 001:  18286 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.451, wps=99.2, ups=0.6, wpb=110.1, bsz=40, num_updates=18260, lr=4.25729e-05, gnorm=0.454, clip=10, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=100792
2023-01-10 17:53:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:43 - progress_bar.py[line:274] - INFO: epoch 001:  18296 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.41, wps=100.8, ups=0.61, wpb=110.6, bsz=40, num_updates=18270, lr=4.25677e-05, gnorm=0.375, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=100809
2023-01-10 17:53:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:53:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:00 - progress_bar.py[line:274] - INFO: epoch 001:  18306 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3596, wps=99.7, ups=0.6, wpb=111, bsz=40, num_updates=18280, lr=4.25625e-05, gnorm=0.63, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=100826
2023-01-10 17:54:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:17 - progress_bar.py[line:274] - INFO: epoch 001:  18316 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3465, wps=100.8, ups=0.61, wpb=110, bsz=40, num_updates=18290, lr=4.25573e-05, gnorm=0.421, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=100843
2023-01-10 17:54:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:33 - progress_bar.py[line:274] - INFO: epoch 001:  18326 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3368, wps=100.2, ups=0.61, wpb=109.9, bsz=40, num_updates=18300, lr=4.25521e-05, gnorm=0.44, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=100860
2023-01-10 17:54:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:50 - progress_bar.py[line:274] - INFO: epoch 001:  18336 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3684, wps=99.2, ups=0.61, wpb=109.1, bsz=40, num_updates=18310, lr=4.25469e-05, gnorm=0.703, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=100876
2023-01-10 17:54:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:54:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:07 - progress_bar.py[line:274] - INFO: epoch 001:  18346 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3913, wps=102, ups=0.61, wpb=110.8, bsz=40, num_updates=18320, lr=4.25417e-05, gnorm=0.587, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=100893
2023-01-10 17:55:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:24 - progress_bar.py[line:274] - INFO: epoch 001:  18356 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.34, wps=97.9, ups=0.6, wpb=109.3, bsz=40, num_updates=18330, lr=4.25365e-05, gnorm=0.283, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=100910
2023-01-10 17:55:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:41 - progress_bar.py[line:274] - INFO: epoch 001:  18366 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4536, wps=101.4, ups=0.61, wpb=110.9, bsz=40, num_updates=18340, lr=4.25313e-05, gnorm=0.646, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=100927
2023-01-10 17:55:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:57 - progress_bar.py[line:274] - INFO: epoch 001:  18376 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3905, wps=101.5, ups=0.62, wpb=109.9, bsz=40, num_updates=18350, lr=4.2526e-05, gnorm=0.574, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=100943
2023-01-10 17:55:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:55:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:14 - progress_bar.py[line:274] - INFO: epoch 001:  18386 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4174, wps=101, ups=0.61, wpb=110.2, bsz=40, num_updates=18360, lr=4.25208e-05, gnorm=0.512, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=100960
2023-01-10 17:56:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:31 - progress_bar.py[line:274] - INFO: epoch 001:  18396 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3735, wps=99.9, ups=0.6, wpb=110.2, bsz=40, num_updates=18370, lr=4.25156e-05, gnorm=0.424, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=100977
2023-01-10 17:56:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:47 - progress_bar.py[line:274] - INFO: epoch 001:  18406 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4396, wps=104.4, ups=0.63, wpb=110.4, bsz=40, num_updates=18380, lr=4.25104e-05, gnorm=0.369, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=100993
2023-01-10 17:56:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:56:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:04 - progress_bar.py[line:274] - INFO: epoch 001:  18416 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3814, wps=103.3, ups=0.62, wpb=111.7, bsz=40, num_updates=18390, lr=4.25052e-05, gnorm=1.518, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101010
2023-01-10 17:57:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:20 - progress_bar.py[line:274] - INFO: epoch 001:  18426 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3298, wps=101.2, ups=0.61, wpb=110.7, bsz=40, num_updates=18400, lr=4.25e-05, gnorm=1.063, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=101027
2023-01-10 17:57:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:37 - progress_bar.py[line:274] - INFO: epoch 001:  18436 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4592, wps=99.2, ups=0.6, wpb=110.3, bsz=40, num_updates=18410, lr=4.24948e-05, gnorm=1.479, clip=50, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=101043
2023-01-10 17:57:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:54 - progress_bar.py[line:274] - INFO: epoch 001:  18446 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3587, wps=103.3, ups=0.62, wpb=110.3, bsz=40, num_updates=18420, lr=4.24896e-05, gnorm=1.266, clip=40, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=101060
2023-01-10 17:57:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:57:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:11 - progress_bar.py[line:274] - INFO: epoch 001:  18456 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4537, wps=99.8, ups=0.61, wpb=109.9, bsz=40, num_updates=18430, lr=4.24844e-05, gnorm=0.48, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101077
2023-01-10 17:58:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:27 - progress_bar.py[line:274] - INFO: epoch 001:  18466 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=112.4, nsentences=40, sample_size=112.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3939, wps=104.2, ups=0.62, wpb=112.4, bsz=40, num_updates=18440, lr=4.24792e-05, gnorm=0.833, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101093
2023-01-10 17:58:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:44 - progress_bar.py[line:274] - INFO: epoch 001:  18476 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.354, wps=99.6, ups=0.6, wpb=109.9, bsz=40, num_updates=18450, lr=4.2474e-05, gnorm=1.042, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=101110
2023-01-10 17:58:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:58:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:01 - progress_bar.py[line:274] - INFO: epoch 001:  18486 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4356, wps=100.4, ups=0.61, wpb=109.1, bsz=40, num_updates=18460, lr=4.24687e-05, gnorm=0.308, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=101127
2023-01-10 17:59:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:17 - progress_bar.py[line:274] - INFO: epoch 001:  18496 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4369, wps=102, ups=0.62, wpb=110.3, bsz=40, num_updates=18470, lr=4.24635e-05, gnorm=1.104, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=101143
2023-01-10 17:59:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:34 - progress_bar.py[line:274] - INFO: epoch 001:  18506 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4479, wps=99.5, ups=0.6, wpb=110.7, bsz=40, num_updates=18480, lr=4.24583e-05, gnorm=0.721, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=101160
2023-01-10 17:59:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:51 - progress_bar.py[line:274] - INFO: epoch 001:  18516 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.433, wps=101.6, ups=0.62, wpb=109.9, bsz=40, num_updates=18490, lr=4.24531e-05, gnorm=0.503, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101177
2023-01-10 17:59:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 17:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:07 - progress_bar.py[line:274] - INFO: epoch 001:  18526 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3158, wps=104.4, ups=0.63, wpb=111, bsz=40, num_updates=18500, lr=4.24479e-05, gnorm=0.284, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101193
2023-01-10 18:00:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:24 - progress_bar.py[line:274] - INFO: epoch 001:  18536 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4272, wps=99.9, ups=0.61, wpb=108.9, bsz=40, num_updates=18510, lr=4.24427e-05, gnorm=0.47, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=101210
2023-01-10 18:00:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:40 - progress_bar.py[line:274] - INFO: epoch 001:  18546 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4086, wps=101.4, ups=0.61, wpb=110.3, bsz=40, num_updates=18520, lr=4.24375e-05, gnorm=0.472, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101226
2023-01-10 18:00:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:57 - progress_bar.py[line:274] - INFO: epoch 001:  18556 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3558, wps=99.9, ups=0.61, wpb=109.5, bsz=40, num_updates=18530, lr=4.24323e-05, gnorm=0.252, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=101243
2023-01-10 18:00:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:00:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:14 - progress_bar.py[line:274] - INFO: epoch 001:  18566 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3854, wps=102, ups=0.62, wpb=109.9, bsz=40, num_updates=18540, lr=4.24271e-05, gnorm=0.432, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=101260
2023-01-10 18:01:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:30 - progress_bar.py[line:274] - INFO: epoch 001:  18576 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4563, wps=99.7, ups=0.6, wpb=110.2, bsz=40, num_updates=18550, lr=4.24219e-05, gnorm=0.676, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=101277
2023-01-10 18:01:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:47 - progress_bar.py[line:274] - INFO: epoch 001:  18586 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4851, wps=103.7, ups=0.63, wpb=110.5, bsz=40, num_updates=18560, lr=4.24167e-05, gnorm=0.412, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=101293
2023-01-10 18:01:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:01:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:03 - progress_bar.py[line:274] - INFO: epoch 001:  18596 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4272, wps=102.6, ups=0.62, wpb=109.7, bsz=40, num_updates=18570, lr=4.24115e-05, gnorm=0.474, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101309
2023-01-10 18:02:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:19 - progress_bar.py[line:274] - INFO: epoch 001:  18606 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3474, wps=100.5, ups=0.61, wpb=109.7, bsz=40, num_updates=18580, lr=4.24063e-05, gnorm=0.746, clip=20, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=101326
2023-01-10 18:02:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:36 - progress_bar.py[line:274] - INFO: epoch 001:  18616 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3263, wps=102.4, ups=0.62, wpb=110.6, bsz=40, num_updates=18590, lr=4.2401e-05, gnorm=0.637, clip=20, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=101342
2023-01-10 18:02:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:53 - progress_bar.py[line:274] - INFO: epoch 001:  18626 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3469, wps=100.9, ups=0.61, wpb=109.7, bsz=40, num_updates=18600, lr=4.23958e-05, gnorm=0.339, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=101359
2023-01-10 18:02:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:10 - progress_bar.py[line:274] - INFO: epoch 001:  18636 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.5259, wps=98.2, ups=0.6, wpb=109.7, bsz=40, num_updates=18610, lr=4.23906e-05, gnorm=0.893, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=101376
2023-01-10 18:03:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:26 - progress_bar.py[line:274] - INFO: epoch 001:  18646 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3455, wps=100.1, ups=0.62, wpb=108.1, bsz=40, num_updates=18620, lr=4.23854e-05, gnorm=0.414, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101392
2023-01-10 18:03:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:43 - progress_bar.py[line:274] - INFO: epoch 001:  18656 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.175, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4444, wps=99.2, ups=0.6, wpb=110.1, bsz=40, num_updates=18630, lr=4.23802e-05, gnorm=0.544, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=101409
2023-01-10 18:03:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:03:59 - progress_bar.py[line:274] - INFO: epoch 001:  18666 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3431, wps=104.3, ups=0.63, wpb=109.9, bsz=40, num_updates=18640, lr=4.2375e-05, gnorm=0.454, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101425
2023-01-10 18:03:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:15 - progress_bar.py[line:274] - INFO: epoch 001:  18676 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4537, wps=101.5, ups=0.62, wpb=109.4, bsz=40, num_updates=18650, lr=4.23698e-05, gnorm=0.522, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=101442
2023-01-10 18:04:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:32 - progress_bar.py[line:274] - INFO: epoch 001:  18686 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4052, wps=101.1, ups=0.62, wpb=108.9, bsz=40, num_updates=18660, lr=4.23646e-05, gnorm=0.631, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101458
2023-01-10 18:04:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:48 - progress_bar.py[line:274] - INFO: epoch 001:  18696 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3667, wps=100.3, ups=0.61, wpb=109.8, bsz=40, num_updates=18670, lr=4.23594e-05, gnorm=0.829, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101475
2023-01-10 18:04:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:05 - progress_bar.py[line:274] - INFO: epoch 001:  18706 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3939, wps=100.3, ups=0.61, wpb=110.1, bsz=40, num_updates=18680, lr=4.23542e-05, gnorm=1.17, clip=20, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=101491
2023-01-10 18:05:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:22 - progress_bar.py[line:274] - INFO: epoch 001:  18716 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2917, wps=101.4, ups=0.61, wpb=110.9, bsz=40, num_updates=18690, lr=4.2349e-05, gnorm=0.506, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101508
2023-01-10 18:05:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:38 - progress_bar.py[line:274] - INFO: epoch 001:  18726 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3423, wps=100.7, ups=0.62, wpb=108.9, bsz=40, num_updates=18700, lr=4.23438e-05, gnorm=0.842, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101524
2023-01-10 18:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:55 - progress_bar.py[line:274] - INFO: epoch 001:  18736 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4792, wps=100.6, ups=0.61, wpb=110, bsz=40, num_updates=18710, lr=4.23385e-05, gnorm=0.642, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101541
2023-01-10 18:05:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:05:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:11 - progress_bar.py[line:274] - INFO: epoch 001:  18746 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4, wps=101.9, ups=0.61, wpb=110.6, bsz=40, num_updates=18720, lr=4.23333e-05, gnorm=1.508, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101558
2023-01-10 18:06:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:28 - progress_bar.py[line:274] - INFO: epoch 001:  18756 / 100000 loss=0.346, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4052, wps=98.3, ups=0.61, wpb=107.7, bsz=40, num_updates=18730, lr=4.23281e-05, gnorm=0.528, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101574
2023-01-10 18:06:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:45 - progress_bar.py[line:274] - INFO: epoch 001:  18766 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4362, wps=98.8, ups=0.6, wpb=109.9, bsz=40, num_updates=18740, lr=4.23229e-05, gnorm=0.985, clip=20, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=101591
2023-01-10 18:06:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:06:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:01 - progress_bar.py[line:274] - INFO: epoch 001:  18776 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3407, wps=102.5, ups=0.62, wpb=110.3, bsz=40, num_updates=18750, lr=4.23177e-05, gnorm=0.712, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=101608
2023-01-10 18:07:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:18 - progress_bar.py[line:274] - INFO: epoch 001:  18786 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4605, wps=104.6, ups=0.63, wpb=111, bsz=40, num_updates=18760, lr=4.23125e-05, gnorm=0.916, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=101624
2023-01-10 18:07:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:34 - progress_bar.py[line:274] - INFO: epoch 001:  18796 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5169, wps=102.6, ups=0.62, wpb=111.1, bsz=40, num_updates=18770, lr=4.23073e-05, gnorm=0.62, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101640
2023-01-10 18:07:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:51 - progress_bar.py[line:274] - INFO: epoch 001:  18806 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3883, wps=100.4, ups=0.61, wpb=110.1, bsz=40, num_updates=18780, lr=4.23021e-05, gnorm=0.413, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=101657
2023-01-10 18:07:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:07:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:08 - progress_bar.py[line:274] - INFO: epoch 001:  18816 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.402, wps=101.4, ups=0.61, wpb=110.1, bsz=40, num_updates=18790, lr=4.22969e-05, gnorm=0.676, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101674
2023-01-10 18:08:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:24 - progress_bar.py[line:274] - INFO: epoch 001:  18826 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4336, wps=100.5, ups=0.62, wpb=108.3, bsz=40, num_updates=18800, lr=4.22917e-05, gnorm=0.6, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=101690
2023-01-10 18:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:41 - progress_bar.py[line:274] - INFO: epoch 001:  18836 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.2967, wps=101.3, ups=0.61, wpb=110.5, bsz=40, num_updates=18810, lr=4.22865e-05, gnorm=0.479, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101707
2023-01-10 18:08:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:57 - progress_bar.py[line:274] - INFO: epoch 001:  18846 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4524, wps=101.7, ups=0.61, wpb=110.7, bsz=40, num_updates=18820, lr=4.22813e-05, gnorm=0.435, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101723
2023-01-10 18:08:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:08:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:14 - progress_bar.py[line:274] - INFO: epoch 001:  18856 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.5, wps=99.3, ups=0.61, wpb=109.3, bsz=40, num_updates=18830, lr=4.2276e-05, gnorm=0.846, clip=30, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=101740
2023-01-10 18:09:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:30 - progress_bar.py[line:274] - INFO: epoch 001:  18866 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3333, wps=103.9, ups=0.63, wpb=110.7, bsz=40, num_updates=18840, lr=4.22708e-05, gnorm=1.112, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=101756
2023-01-10 18:09:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:47 - progress_bar.py[line:274] - INFO: epoch 001:  18876 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4519, wps=100, ups=0.6, wpb=110.7, bsz=40, num_updates=18850, lr=4.22656e-05, gnorm=0.51, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=101773
2023-01-10 18:09:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:09:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:03 - progress_bar.py[line:274] - INFO: epoch 001:  18886 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3478, wps=104.3, ups=0.63, wpb=110.8, bsz=40, num_updates=18860, lr=4.22604e-05, gnorm=0.377, clip=0, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=101790
2023-01-10 18:10:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:20 - progress_bar.py[line:274] - INFO: epoch 001:  18896 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.38, wps=102.5, ups=0.62, wpb=109.5, bsz=40, num_updates=18870, lr=4.22552e-05, gnorm=0.46, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101806
2023-01-10 18:10:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:36 - progress_bar.py[line:274] - INFO: epoch 001:  18906 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.337, wps=102.2, ups=0.62, wpb=110.4, bsz=40, num_updates=18880, lr=4.225e-05, gnorm=0.454, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101822
2023-01-10 18:10:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:53 - progress_bar.py[line:274] - INFO: epoch 001:  18916 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4021, wps=101.2, ups=0.61, wpb=109.9, bsz=40, num_updates=18890, lr=4.22448e-05, gnorm=0.353, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=101839
2023-01-10 18:10:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:10:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:09 - progress_bar.py[line:274] - INFO: epoch 001:  18926 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3652, wps=97.5, ups=0.6, wpb=108.1, bsz=40, num_updates=18900, lr=4.22396e-05, gnorm=0.639, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=101856
2023-01-10 18:11:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:26 - progress_bar.py[line:274] - INFO: epoch 001:  18936 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4271, wps=102.7, ups=0.62, wpb=110.2, bsz=40, num_updates=18910, lr=4.22344e-05, gnorm=0.664, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101872
2023-01-10 18:11:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:33 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-10 18:11:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:45 - progress_bar.py[line:274] - INFO: epoch 001:  18947 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.062, nsentences=40, sample_size=109.062, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4118, wps=94.2, ups=0.54, wpb=109.1, bsz=40, num_updates=18920, lr=4.22292e-05, gnorm=0.38, clip=0, loss_scale=256, train_wall=18, gb_free=10.1, ema_decay=0.9999, wall=101891
2023-01-10 18:11:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:11:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:01 - progress_bar.py[line:274] - INFO: epoch 001:  18957 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.333, nsentences=40, sample_size=107.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3898, wps=100.8, ups=0.63, wpb=107.3, bsz=40, num_updates=18930, lr=4.2224e-05, gnorm=0.36, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=101907
2023-01-10 18:12:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:18 - progress_bar.py[line:274] - INFO: epoch 001:  18967 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3667, wps=100.9, ups=0.61, wpb=111.1, bsz=40, num_updates=18940, lr=4.22187e-05, gnorm=0.863, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101924
2023-01-10 18:12:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:34 - progress_bar.py[line:274] - INFO: epoch 001:  18977 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3739, wps=99.2, ups=0.6, wpb=109.5, bsz=40, num_updates=18950, lr=4.22135e-05, gnorm=1.675, clip=20, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=101941
2023-01-10 18:12:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:51 - progress_bar.py[line:274] - INFO: epoch 001:  18987 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3495, wps=100.4, ups=0.61, wpb=108.9, bsz=40, num_updates=18960, lr=4.22083e-05, gnorm=0.382, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101957
2023-01-10 18:12:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:12:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:08 - progress_bar.py[line:274] - INFO: epoch 001:  18997 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3478, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=18970, lr=4.22031e-05, gnorm=1.632, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=101974
2023-01-10 18:13:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:24 - progress_bar.py[line:274] - INFO: epoch 001:  19007 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.34, wps=100.8, ups=0.61, wpb=110.4, bsz=40, num_updates=18980, lr=4.21979e-05, gnorm=1.233, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=101990
2023-01-10 18:13:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:40 - progress_bar.py[line:274] - INFO: epoch 001:  19017 / 100000 loss=0.337, loss_v1=0, loss_v2=0, nll_loss=0.19, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3793, wps=102.2, ups=0.63, wpb=108.7, bsz=40, num_updates=18990, lr=4.21927e-05, gnorm=0.526, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=102007
2023-01-10 18:13:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 18:13:57 - progress_bar.py[line:274] - INFO: epoch 001:  19027 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3762, wps=102.5, ups=0.62, wpb=109.9, bsz=40, num_updates=19000, lr=4.21875e-05, gnorm=0.597, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=102023
2023-01-10 18:13:57 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 18:13:59 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 18:13:59 - train.py[line:551] - INFO: load:1.51 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 18:16:30 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 18:16:30 - train.py[line:551] - INFO: load:1.54 valid_run:151.15 task_valid:148.34 collect_output:1.69
2023-01-10 18:18:58 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 18:18:58 - train.py[line:551] - INFO: load:1.57 valid_run:299.30 task_valid:292.03 collect_output:5.05
2023-01-10 18:21:29 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 18:21:29 - train.py[line:551] - INFO: load:1.59 valid_run:450.50 task_valid:435.31 collect_output:11.89
2023-01-10 18:23:58 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 18:23:58 - train.py[line:551] - INFO: load:1.62 valid_run:599.19 task_valid:580.55 collect_output:14.31
2023-01-10 18:26:30 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 18:26:30 - train.py[line:551] - INFO: load:1.64 valid_run:751.09 task_valid:728.32 collect_output:17.34
2023-01-10 18:29:02 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 18:29:02 - train.py[line:551] - INFO: load:1.67 valid_run:902.37 task_valid:874.16 collect_output:21.73
2023-01-10 18:31:33 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 18:31:34 - train.py[line:551] - INFO: load:1.69 valid_run:1054.23 task_valid:1020.55 collect_output:26.11
2023-01-10 18:34:03 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 18:34:03 - train.py[line:551] - INFO: load:1.72 valid_run:1204.07 task_valid:1161.79 collect_output:33.67
2023-01-10 18:36:32 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 18:36:32 - train.py[line:551] - INFO: load:1.75 valid_run:1352.97 task_valid:1306.77 collect_output:36.54
2023-01-10 18:39:00 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 18:39:00 - train.py[line:551] - INFO: load:1.77 valid_run:1500.91 task_valid:1450.28 collect_output:39.94
2023-01-10 18:41:30 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 18:41:30 - train.py[line:551] - INFO: load:1.80 valid_run:1650.13 task_valid:1595.51 collect_output:42.84
2023-01-10 18:43:59 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 18:43:59 - train.py[line:551] - INFO: load:1.82 valid_run:1799.31 task_valid:1740.63 collect_output:45.86
2023-01-10 18:46:28 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 18:46:28 - train.py[line:551] - INFO: load:1.85 valid_run:1948.22 task_valid:1882.57 collect_output:51.81
2023-01-10 18:48:58 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 18:48:58 - train.py[line:551] - INFO: load:1.88 valid_run:2098.39 task_valid:2028.65 collect_output:54.84
2023-01-10 18:51:28 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 18:51:28 - train.py[line:551] - INFO: load:1.90 valid_run:2248.22 task_valid:2175.46 collect_output:56.80
2023-01-10 18:53:58 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 18:53:58 - train.py[line:551] - INFO: load:1.93 valid_run:2397.52 task_valid:2319.88 collect_output:60.62
2023-01-10 18:56:28 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 18:56:28 - train.py[line:551] - INFO: load:1.95 valid_run:2548.16 task_valid:2465.59 collect_output:64.51
2023-01-10 18:58:59 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 18:58:59 - train.py[line:551] - INFO: load:1.98 valid_run:2698.42 task_valid:2613.03 collect_output:66.28
2023-01-10 19:01:26 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 19:01:26 - train.py[line:551] - INFO: load:2.01 valid_run:2845.89 task_valid:2755.18 collect_output:70.51
2023-01-10 19:03:56 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 19:03:56 - train.py[line:551] - INFO: load:2.03 valid_run:2995.51 task_valid:2900.83 collect_output:73.39
2023-01-10 19:06:27 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 19:06:27 - train.py[line:551] - INFO: load:2.06 valid_run:3146.22 task_valid:3045.87 collect_output:77.97
2023-01-10 19:08:56 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 19:08:56 - train.py[line:551] - INFO: load:2.09 valid_run:3295.23 task_valid:3191.11 collect_output:80.68
2023-01-10 19:11:26 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 19:11:26 - train.py[line:551] - INFO: load:2.12 valid_run:3445.55 task_valid:3337.71 collect_output:83.34
2023-01-10 19:13:57 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 19:13:57 - train.py[line:551] - INFO: load:2.14 valid_run:3596.26 task_valid:3484.50 collect_output:86.22

====================================================================================================
SGG eval:     R @ 50: 0.4853;     R @ 100: 0.5737;     R @ 500: 0.6311;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3088;    mR @ 100: 0.3614;    mR @ 500: 0.4232;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7488) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.4355) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.7859) (says:0.0000) (sitting on:0.7157) (standing on:0.2543) (using:0.6000) (walking in:0.0000) (walking on:0.3514) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4853;     R @ 100: 0.5737;     R @ 500: 0.6311;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3088;    mR @ 100: 0.3614;    mR @ 500: 0.4232;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7488) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.4355) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.7859) (says:0.0000) (sitting on:0.7157) (standing on:0.2543) (using:0.6000) (walking in:0.0000) (walking on:0.3514) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================

2023-01-10 19:16:28 - train.py[line:487] - INFO: 0.5736957983193277
2023-01-10 19:16:29 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 19:16:29 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.333 | loss_v1 0 | loss_v2 0 | nll_loss 0.175 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.573696 | ppl 1.13 | vqa_score 0.5394 | wps 119.6 | wpb 89.9 | bsz 30 | num_updates 19000 | best_R@100 0.69005
2023-01-10 19:16:29 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 19000 updates
2023-01-10 19:16:29 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_19000.pt
2023-01-10 19:17:14 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_19000.pt
2023-01-10 19:18:45 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_19000.pt (epoch 1 @ 19000 updates, score 0.5736957983193277) (writing took 135.88609115965664 seconds)
2023-01-10 19:18:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:18:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:18:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:02 - progress_bar.py[line:274] - INFO: epoch 001:  19037 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3846, wps=0.4, ups=0, wpb=109.4, bsz=40, num_updates=19010, lr=4.21823e-05, gnorm=0.695, clip=10, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=105928
2023-01-10 19:19:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:18 - progress_bar.py[line:274] - INFO: epoch 001:  19047 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4747, wps=102.5, ups=0.62, wpb=109.9, bsz=40, num_updates=19020, lr=4.21771e-05, gnorm=1.253, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=105944
2023-01-10 19:19:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:34 - progress_bar.py[line:274] - INFO: epoch 001:  19057 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3762, wps=103.5, ups=0.62, wpb=110.7, bsz=40, num_updates=19030, lr=4.21719e-05, gnorm=0.696, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=105960
2023-01-10 19:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:51 - progress_bar.py[line:274] - INFO: epoch 001:  19067 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3654, wps=98.3, ups=0.6, wpb=109.7, bsz=40, num_updates=19040, lr=4.21667e-05, gnorm=0.364, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=105977
2023-01-10 19:19:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:07 - progress_bar.py[line:274] - INFO: epoch 001:  19077 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4646, wps=101.8, ups=0.62, wpb=109.6, bsz=40, num_updates=19050, lr=4.21615e-05, gnorm=0.544, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=105994
2023-01-10 19:20:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:24 - progress_bar.py[line:274] - INFO: epoch 001:  19087 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4571, wps=101.5, ups=0.61, wpb=110.3, bsz=40, num_updates=19060, lr=4.21563e-05, gnorm=0.409, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106010
2023-01-10 19:20:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:40 - progress_bar.py[line:274] - INFO: epoch 001:  19097 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.36, wps=102.8, ups=0.62, wpb=109.9, bsz=40, num_updates=19070, lr=4.2151e-05, gnorm=0.53, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=106026
2023-01-10 19:20:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:57 - progress_bar.py[line:274] - INFO: epoch 001:  19107 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3939, wps=99.9, ups=0.61, wpb=109.3, bsz=40, num_updates=19080, lr=4.21458e-05, gnorm=0.337, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106043
2023-01-10 19:20:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:20:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:13 - progress_bar.py[line:274] - INFO: epoch 001:  19117 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3034, wps=101.6, ups=0.61, wpb=110.5, bsz=40, num_updates=19090, lr=4.21406e-05, gnorm=0.422, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106059
2023-01-10 19:21:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:30 - progress_bar.py[line:274] - INFO: epoch 001:  19127 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4135, wps=98.8, ups=0.6, wpb=109.5, bsz=40, num_updates=19100, lr=4.21354e-05, gnorm=0.368, clip=0, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=106076
2023-01-10 19:21:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:47 - progress_bar.py[line:274] - INFO: epoch 001:  19137 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3774, wps=100.6, ups=0.61, wpb=110.4, bsz=40, num_updates=19110, lr=4.21302e-05, gnorm=0.387, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106093
2023-01-10 19:21:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:21:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:03 - progress_bar.py[line:274] - INFO: epoch 001:  19147 / 100000 loss=0.343, loss_v1=0, loss_v2=0, nll_loss=0.192, ntokens=107.533, nsentences=40, sample_size=107.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3805, wps=101.7, ups=0.63, wpb=107.5, bsz=40, num_updates=19120, lr=4.2125e-05, gnorm=0.401, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106109
2023-01-10 19:22:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:19 - progress_bar.py[line:274] - INFO: epoch 001:  19157 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3592, wps=101, ups=0.62, wpb=109.3, bsz=40, num_updates=19130, lr=4.21198e-05, gnorm=0.294, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106126
2023-01-10 19:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:36 - progress_bar.py[line:274] - INFO: epoch 001:  19167 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4423, wps=99.2, ups=0.61, wpb=107.7, bsz=40, num_updates=19140, lr=4.21146e-05, gnorm=0.461, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106142
2023-01-10 19:22:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:52 - progress_bar.py[line:274] - INFO: epoch 001:  19177 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4636, wps=100.2, ups=0.62, wpb=107.4, bsz=40, num_updates=19150, lr=4.21094e-05, gnorm=0.786, clip=40, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=106159
2023-01-10 19:22:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:09 - progress_bar.py[line:274] - INFO: epoch 001:  19187 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4815, wps=100.8, ups=0.62, wpb=108.4, bsz=40, num_updates=19160, lr=4.21042e-05, gnorm=0.894, clip=20, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=106175
2023-01-10 19:23:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:26 - progress_bar.py[line:274] - INFO: epoch 001:  19197 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4149, wps=98.3, ups=0.6, wpb=109.7, bsz=40, num_updates=19170, lr=4.2099e-05, gnorm=0.374, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=106192
2023-01-10 19:23:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:42 - progress_bar.py[line:274] - INFO: epoch 001:  19207 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4167, wps=101.4, ups=0.62, wpb=108.9, bsz=40, num_updates=19180, lr=4.20938e-05, gnorm=1.259, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106208
2023-01-10 19:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:23:59 - progress_bar.py[line:274] - INFO: epoch 001:  19217 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.407, wps=102.2, ups=0.62, wpb=109.2, bsz=40, num_updates=19190, lr=4.20885e-05, gnorm=0.812, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106225
2023-01-10 19:23:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:15 - progress_bar.py[line:274] - INFO: epoch 001:  19227 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4078, wps=101.6, ups=0.62, wpb=109.7, bsz=40, num_updates=19200, lr=4.20833e-05, gnorm=0.354, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106241
2023-01-10 19:24:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:31 - progress_bar.py[line:274] - INFO: epoch 001:  19237 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4086, wps=104.1, ups=0.63, wpb=110.6, bsz=40, num_updates=19210, lr=4.20781e-05, gnorm=0.438, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=106257
2023-01-10 19:24:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:48 - progress_bar.py[line:274] - INFO: epoch 001:  19247 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3776, wps=99.1, ups=0.6, wpb=109.3, bsz=40, num_updates=19220, lr=4.20729e-05, gnorm=1.109, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106274
2023-01-10 19:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:24:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:05 - progress_bar.py[line:274] - INFO: epoch 001:  19257 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4057, wps=99.7, ups=0.6, wpb=110, bsz=40, num_updates=19230, lr=4.20677e-05, gnorm=0.407, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=106291
2023-01-10 19:25:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:21 - progress_bar.py[line:274] - INFO: epoch 001:  19267 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3226, wps=100.2, ups=0.61, wpb=108.9, bsz=40, num_updates=19240, lr=4.20625e-05, gnorm=1.213, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=106307
2023-01-10 19:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:38 - progress_bar.py[line:274] - INFO: epoch 001:  19277 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4409, wps=100.8, ups=0.61, wpb=110.2, bsz=40, num_updates=19250, lr=4.20573e-05, gnorm=0.509, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106324
2023-01-10 19:25:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:54 - progress_bar.py[line:274] - INFO: epoch 001:  19287 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3645, wps=103.4, ups=0.63, wpb=110.2, bsz=40, num_updates=19260, lr=4.20521e-05, gnorm=0.331, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106340
2023-01-10 19:25:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:25:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:10 - progress_bar.py[line:274] - INFO: epoch 001:  19297 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.433, wps=101.1, ups=0.62, wpb=108.9, bsz=40, num_updates=19270, lr=4.20469e-05, gnorm=0.665, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106357
2023-01-10 19:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:27 - progress_bar.py[line:274] - INFO: epoch 001:  19307 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4021, wps=101.1, ups=0.62, wpb=109.5, bsz=40, num_updates=19280, lr=4.20417e-05, gnorm=0.942, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=106373
2023-01-10 19:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:44 - progress_bar.py[line:274] - INFO: epoch 001:  19317 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4393, wps=100.2, ups=0.61, wpb=110.1, bsz=40, num_updates=19290, lr=4.20365e-05, gnorm=0.77, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=106390
2023-01-10 19:26:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:26:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:00 - progress_bar.py[line:274] - INFO: epoch 001:  19327 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.466, wps=103.6, ups=0.63, wpb=110.1, bsz=40, num_updates=19300, lr=4.20313e-05, gnorm=0.457, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=106406
2023-01-10 19:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:17 - progress_bar.py[line:274] - INFO: epoch 001:  19337 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3721, wps=99.7, ups=0.6, wpb=109.9, bsz=40, num_updates=19310, lr=4.2026e-05, gnorm=0.986, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=106423
2023-01-10 19:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:33 - progress_bar.py[line:274] - INFO: epoch 001:  19347 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5319, wps=100.1, ups=0.61, wpb=109.3, bsz=40, num_updates=19320, lr=4.20208e-05, gnorm=0.915, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106439
2023-01-10 19:27:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:50 - progress_bar.py[line:274] - INFO: epoch 001:  19357 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4433, wps=99.9, ups=0.6, wpb=110.3, bsz=40, num_updates=19330, lr=4.20156e-05, gnorm=0.34, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=106456
2023-01-10 19:27:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:27:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:07 - progress_bar.py[line:274] - INFO: epoch 001:  19367 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.413, wps=99.9, ups=0.6, wpb=110.3, bsz=40, num_updates=19340, lr=4.20104e-05, gnorm=0.225, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=106473
2023-01-10 19:28:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:23 - progress_bar.py[line:274] - INFO: epoch 001:  19377 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4362, wps=101.7, ups=0.62, wpb=109.5, bsz=40, num_updates=19350, lr=4.20052e-05, gnorm=0.466, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106489
2023-01-10 19:28:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:39 - progress_bar.py[line:274] - INFO: epoch 001:  19387 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4135, wps=102.2, ups=0.63, wpb=108.9, bsz=40, num_updates=19360, lr=4.2e-05, gnorm=0.462, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=106505
2023-01-10 19:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:55 - progress_bar.py[line:274] - INFO: epoch 001:  19397 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4545, wps=104, ups=0.63, wpb=110.7, bsz=40, num_updates=19370, lr=4.19948e-05, gnorm=0.384, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=106522
2023-01-10 19:28:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:28:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:12 - progress_bar.py[line:274] - INFO: epoch 001:  19407 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4196, wps=100.7, ups=0.61, wpb=109.2, bsz=40, num_updates=19380, lr=4.19896e-05, gnorm=0.339, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106538
2023-01-10 19:29:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:28 - progress_bar.py[line:274] - INFO: epoch 001:  19417 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4824, wps=100.7, ups=0.61, wpb=109.9, bsz=40, num_updates=19390, lr=4.19844e-05, gnorm=0.431, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=106555
2023-01-10 19:29:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:45 - progress_bar.py[line:274] - INFO: epoch 001:  19427 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3981, wps=102.5, ups=0.62, wpb=110.5, bsz=40, num_updates=19400, lr=4.19792e-05, gnorm=0.824, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106571
2023-01-10 19:29:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:29:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:02 - progress_bar.py[line:274] - INFO: epoch 001:  19437 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4954, wps=99.4, ups=0.6, wpb=109.9, bsz=40, num_updates=19410, lr=4.1974e-05, gnorm=0.462, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=106588
2023-01-10 19:30:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:19 - progress_bar.py[line:274] - INFO: epoch 001:  19447 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4175, wps=98.8, ups=0.6, wpb=110.2, bsz=40, num_updates=19420, lr=4.19688e-05, gnorm=0.488, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=106605
2023-01-10 19:30:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:35 - progress_bar.py[line:274] - INFO: epoch 001:  19457 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4273, wps=99.7, ups=0.6, wpb=110.3, bsz=40, num_updates=19430, lr=4.19635e-05, gnorm=0.571, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=106622
2023-01-10 19:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:52 - progress_bar.py[line:274] - INFO: epoch 001:  19467 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=107.733, nsentences=40, sample_size=107.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3879, wps=100.5, ups=0.62, wpb=107.7, bsz=40, num_updates=19440, lr=4.19583e-05, gnorm=0.597, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106638
2023-01-10 19:30:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:30:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:08 - progress_bar.py[line:274] - INFO: epoch 001:  19477 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3486, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=19450, lr=4.19531e-05, gnorm=0.356, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=106655
2023-01-10 19:31:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:25 - progress_bar.py[line:274] - INFO: epoch 001:  19487 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3913, wps=101.3, ups=0.61, wpb=110.7, bsz=40, num_updates=19460, lr=4.19479e-05, gnorm=0.468, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106671
2023-01-10 19:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:42 - progress_bar.py[line:274] - INFO: epoch 001:  19497 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3796, wps=98.5, ups=0.6, wpb=109, bsz=40, num_updates=19470, lr=4.19427e-05, gnorm=0.393, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=106688
2023-01-10 19:31:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:31:58 - progress_bar.py[line:274] - INFO: epoch 001:  19507 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3913, wps=103.4, ups=0.62, wpb=110.3, bsz=40, num_updates=19480, lr=4.19375e-05, gnorm=0.386, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106704
2023-01-10 19:31:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:14 - progress_bar.py[line:274] - INFO: epoch 001:  19517 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5217, wps=101.6, ups=0.62, wpb=109.7, bsz=40, num_updates=19490, lr=4.19323e-05, gnorm=0.508, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106721
2023-01-10 19:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:31 - progress_bar.py[line:274] - INFO: epoch 001:  19527 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3939, wps=100.3, ups=0.61, wpb=109.5, bsz=40, num_updates=19500, lr=4.19271e-05, gnorm=1.066, clip=50, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106737
2023-01-10 19:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:48 - progress_bar.py[line:274] - INFO: epoch 001:  19537 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4216, wps=99, ups=0.61, wpb=108.6, bsz=40, num_updates=19510, lr=4.19219e-05, gnorm=0.305, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=106754
2023-01-10 19:32:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:32:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:04 - progress_bar.py[line:274] - INFO: epoch 001:  19547 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.43, wps=102.6, ups=0.62, wpb=110.4, bsz=40, num_updates=19520, lr=4.19167e-05, gnorm=0.325, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106770
2023-01-10 19:33:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:21 - progress_bar.py[line:274] - INFO: epoch 001:  19557 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4255, wps=100.1, ups=0.6, wpb=110.3, bsz=40, num_updates=19530, lr=4.19115e-05, gnorm=0.833, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106787
2023-01-10 19:33:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:37 - progress_bar.py[line:274] - INFO: epoch 001:  19567 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.451, wps=99.7, ups=0.61, wpb=108.9, bsz=40, num_updates=19540, lr=4.19063e-05, gnorm=0.672, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106804
2023-01-10 19:33:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:54 - progress_bar.py[line:274] - INFO: epoch 001:  19577 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3776, wps=100.2, ups=0.6, wpb=110.6, bsz=40, num_updates=19550, lr=4.1901e-05, gnorm=1.284, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=106820
2023-01-10 19:33:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:33:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:11 - progress_bar.py[line:274] - INFO: epoch 001:  19587 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3945, wps=100.6, ups=0.61, wpb=110.4, bsz=40, num_updates=19560, lr=4.18958e-05, gnorm=0.315, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106837
2023-01-10 19:34:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:27 - progress_bar.py[line:274] - INFO: epoch 001:  19597 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.375, wps=99.6, ups=0.61, wpb=108.3, bsz=40, num_updates=19570, lr=4.18906e-05, gnorm=0.841, clip=40, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=106854
2023-01-10 19:34:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:44 - progress_bar.py[line:274] - INFO: epoch 001:  19607 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4343, wps=99.9, ups=0.61, wpb=109.9, bsz=40, num_updates=19580, lr=4.18854e-05, gnorm=0.45, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=106870
2023-01-10 19:34:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:34:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:01 - progress_bar.py[line:274] - INFO: epoch 001:  19617 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4906, wps=99.5, ups=0.61, wpb=109.2, bsz=40, num_updates=19590, lr=4.18802e-05, gnorm=0.59, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=106887
2023-01-10 19:35:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:18 - progress_bar.py[line:274] - INFO: epoch 001:  19627 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5326, wps=99.1, ups=0.6, wpb=109.9, bsz=40, num_updates=19600, lr=4.1875e-05, gnorm=0.297, clip=0, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=106904
2023-01-10 19:35:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:34 - progress_bar.py[line:274] - INFO: epoch 001:  19637 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4579, wps=106.6, ups=0.64, wpb=111.2, bsz=40, num_updates=19610, lr=4.18698e-05, gnorm=0.327, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=106920
2023-01-10 19:35:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:50 - progress_bar.py[line:274] - INFO: epoch 001:  19647 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4074, wps=100.7, ups=0.6, wpb=111.3, bsz=40, num_updates=19620, lr=4.18646e-05, gnorm=0.594, clip=20, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=106937
2023-01-10 19:35:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:35:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:07 - progress_bar.py[line:274] - INFO: epoch 001:  19657 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4205, wps=98.8, ups=0.6, wpb=110.1, bsz=40, num_updates=19630, lr=4.18594e-05, gnorm=0.334, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=106953
2023-01-10 19:36:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:24 - progress_bar.py[line:274] - INFO: epoch 001:  19667 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3861, wps=100.3, ups=0.6, wpb=110.9, bsz=40, num_updates=19640, lr=4.18542e-05, gnorm=0.646, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=106970
2023-01-10 19:36:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:41 - progress_bar.py[line:274] - INFO: epoch 001:  19677 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4022, wps=103, ups=0.62, wpb=111.5, bsz=40, num_updates=19650, lr=4.1849e-05, gnorm=0.306, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=106987
2023-01-10 19:36:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:57 - progress_bar.py[line:274] - INFO: epoch 001:  19687 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3977, wps=103.6, ups=0.63, wpb=109.9, bsz=40, num_updates=19660, lr=4.18438e-05, gnorm=0.752, clip=30, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=107003
2023-01-10 19:36:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:36:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:13 - progress_bar.py[line:274] - INFO: epoch 001:  19697 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4074, wps=101.4, ups=0.62, wpb=109.3, bsz=40, num_updates=19670, lr=4.18385e-05, gnorm=0.956, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=107020
2023-01-10 19:37:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:30 - progress_bar.py[line:274] - INFO: epoch 001:  19707 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4706, wps=102.1, ups=0.61, wpb=110.9, bsz=40, num_updates=19680, lr=4.18333e-05, gnorm=0.388, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=107036
2023-01-10 19:37:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:47 - progress_bar.py[line:274] - INFO: epoch 001:  19717 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4792, wps=99.2, ups=0.6, wpb=110.9, bsz=40, num_updates=19690, lr=4.18281e-05, gnorm=0.464, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=107053
2023-01-10 19:37:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:37:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:04 - progress_bar.py[line:274] - INFO: epoch 001:  19727 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3962, wps=99, ups=0.61, wpb=108.7, bsz=40, num_updates=19700, lr=4.18229e-05, gnorm=0.339, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=107070
2023-01-10 19:38:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:20 - progress_bar.py[line:274] - INFO: epoch 001:  19737 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4135, wps=101.3, ups=0.62, wpb=109.1, bsz=40, num_updates=19710, lr=4.18177e-05, gnorm=0.668, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=107086
2023-01-10 19:38:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:37 - progress_bar.py[line:274] - INFO: epoch 001:  19747 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4537, wps=101.8, ups=0.61, wpb=110.7, bsz=40, num_updates=19720, lr=4.18125e-05, gnorm=0.772, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=107103
2023-01-10 19:38:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:53 - progress_bar.py[line:274] - INFO: epoch 001:  19757 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4206, wps=101.4, ups=0.62, wpb=109.6, bsz=40, num_updates=19730, lr=4.18073e-05, gnorm=0.4, clip=10, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=107119
2023-01-10 19:38:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:09 - progress_bar.py[line:274] - INFO: epoch 001:  19767 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4118, wps=103.8, ups=0.63, wpb=109.6, bsz=40, num_updates=19740, lr=4.18021e-05, gnorm=0.588, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=107135
2023-01-10 19:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:26 - progress_bar.py[line:274] - INFO: epoch 001:  19777 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.5052, wps=98.7, ups=0.6, wpb=109.9, bsz=40, num_updates=19750, lr=4.17969e-05, gnorm=0.442, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=107152
2023-01-10 19:39:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:43 - progress_bar.py[line:274] - INFO: epoch 001:  19787 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4206, wps=100.9, ups=0.61, wpb=110.3, bsz=40, num_updates=19760, lr=4.17917e-05, gnorm=0.285, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=107169
2023-01-10 19:39:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:39:59 - progress_bar.py[line:274] - INFO: epoch 001:  19797 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3299, wps=102.5, ups=0.62, wpb=110, bsz=40, num_updates=19770, lr=4.17865e-05, gnorm=0.469, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=107185
2023-01-10 19:39:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:15 - progress_bar.py[line:274] - INFO: epoch 001:  19807 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3879, wps=101.8, ups=0.63, wpb=108, bsz=40, num_updates=19780, lr=4.17813e-05, gnorm=0.484, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=107201
2023-01-10 19:40:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:32 - progress_bar.py[line:274] - INFO: epoch 001:  19817 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3942, wps=99.1, ups=0.61, wpb=108.9, bsz=40, num_updates=19790, lr=4.1776e-05, gnorm=0.33, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=107218
2023-01-10 19:40:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:48 - progress_bar.py[line:274] - INFO: epoch 001:  19827 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3905, wps=101.3, ups=0.62, wpb=109.6, bsz=40, num_updates=19800, lr=4.17708e-05, gnorm=0.437, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=107234
2023-01-10 19:40:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:40:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:05 - progress_bar.py[line:274] - INFO: epoch 001:  19837 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.451, wps=102, ups=0.62, wpb=109.7, bsz=40, num_updates=19810, lr=4.17656e-05, gnorm=0.665, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=107251
2023-01-10 19:41:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:21 - progress_bar.py[line:274] - INFO: epoch 001:  19847 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3178, wps=100, ups=0.61, wpb=109.1, bsz=40, num_updates=19820, lr=4.17604e-05, gnorm=0.553, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=107267
2023-01-10 19:41:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:38 - progress_bar.py[line:274] - INFO: epoch 001:  19857 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.47, wps=99.2, ups=0.61, wpb=109.1, bsz=40, num_updates=19830, lr=4.17552e-05, gnorm=1.761, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=107284
2023-01-10 19:41:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:54 - progress_bar.py[line:274] - INFO: epoch 001:  19867 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4545, wps=101.5, ups=0.62, wpb=109.6, bsz=40, num_updates=19840, lr=4.175e-05, gnorm=0.582, clip=20, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=107301
2023-01-10 19:41:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:41:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:11 - progress_bar.py[line:274] - INFO: epoch 001:  19877 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4386, wps=97.5, ups=0.59, wpb=109.7, bsz=40, num_updates=19850, lr=4.17448e-05, gnorm=0.415, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=107318
2023-01-10 19:42:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:28 - progress_bar.py[line:274] - INFO: epoch 001:  19887 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4444, wps=99.8, ups=0.6, wpb=111.5, bsz=40, num_updates=19860, lr=4.17396e-05, gnorm=0.473, clip=10, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=107335
2023-01-10 19:42:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:33 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-10 19:42:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:47 - progress_bar.py[line:274] - INFO: epoch 001:  19898 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.375, nsentences=40, sample_size=110.375, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4228, wps=95.7, ups=0.54, wpb=110.4, bsz=40, num_updates=19870, lr=4.17344e-05, gnorm=0.672, clip=20, loss_scale=256, train_wall=18, gb_free=10, ema_decay=0.9999, wall=107353
2023-01-10 19:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:42:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:04 - progress_bar.py[line:274] - INFO: epoch 001:  19908 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3846, wps=99, ups=0.6, wpb=109.2, bsz=40, num_updates=19880, lr=4.17292e-05, gnorm=1.115, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=107370
2023-01-10 19:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:20 - progress_bar.py[line:274] - INFO: epoch 001:  19918 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4235, wps=101.9, ups=0.61, wpb=110.5, bsz=40, num_updates=19890, lr=4.1724e-05, gnorm=0.963, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=107387
2023-01-10 19:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:37 - progress_bar.py[line:274] - INFO: epoch 001:  19928 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4314, wps=98.7, ups=0.6, wpb=109.9, bsz=40, num_updates=19900, lr=4.17188e-05, gnorm=0.455, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=107404
2023-01-10 19:43:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:54 - progress_bar.py[line:274] - INFO: epoch 001:  19938 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3714, wps=100.5, ups=0.62, wpb=108.8, bsz=40, num_updates=19910, lr=4.17135e-05, gnorm=0.683, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=107420
2023-01-10 19:43:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:43:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:10 - progress_bar.py[line:274] - INFO: epoch 001:  19948 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4259, wps=102.7, ups=0.62, wpb=109.9, bsz=40, num_updates=19920, lr=4.17083e-05, gnorm=0.299, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=107436
2023-01-10 19:44:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:27 - progress_bar.py[line:274] - INFO: epoch 001:  19958 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4118, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=19930, lr=4.17031e-05, gnorm=0.765, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=107453
2023-01-10 19:44:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:43 - progress_bar.py[line:274] - INFO: epoch 001:  19968 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3551, wps=101.5, ups=0.62, wpb=109.6, bsz=40, num_updates=19940, lr=4.16979e-05, gnorm=0.737, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=107469
2023-01-10 19:44:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:44:59 - progress_bar.py[line:274] - INFO: epoch 001:  19978 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4419, wps=102.1, ups=0.62, wpb=109.7, bsz=40, num_updates=19950, lr=4.16927e-05, gnorm=0.89, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=107486
2023-01-10 19:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:16 - progress_bar.py[line:274] - INFO: epoch 001:  19988 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5312, wps=101.4, ups=0.61, wpb=110.3, bsz=40, num_updates=19960, lr=4.16875e-05, gnorm=0.256, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=107502
2023-01-10 19:45:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:33 - progress_bar.py[line:274] - INFO: epoch 001:  19998 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4, wps=99, ups=0.6, wpb=109.9, bsz=40, num_updates=19970, lr=4.16823e-05, gnorm=0.548, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=107519
2023-01-10 19:45:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:50 - progress_bar.py[line:274] - INFO: epoch 001:  20008 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4141, wps=98.7, ups=0.6, wpb=109.5, bsz=40, num_updates=19980, lr=4.16771e-05, gnorm=1.203, clip=40, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=107536
2023-01-10 19:45:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:45:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:46:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:46:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:46:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:46:07 - progress_bar.py[line:274] - INFO: epoch 001:  20018 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5258, wps=97.5, ups=0.59, wpb=109.3, bsz=40, num_updates=19990, lr=4.16719e-05, gnorm=0.634, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=107553
2023-01-10 19:46:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:46:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:46:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:46:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 19:46:23 - progress_bar.py[line:274] - INFO: epoch 001:  20028 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4563, wps=101.1, ups=0.61, wpb=110.1, bsz=40, num_updates=20000, lr=4.16667e-05, gnorm=0.777, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=107570
2023-01-10 19:46:23 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 19:46:25 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 19:46:25 - train.py[line:551] - INFO: load:1.26 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 19:48:56 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 19:48:56 - train.py[line:551] - INFO: load:1.28 valid_run:151.28 task_valid:148.40 collect_output:1.81
2023-01-10 19:51:24 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 19:51:24 - train.py[line:551] - INFO: load:1.31 valid_run:298.90 task_valid:291.67 collect_output:5.14
2023-01-10 19:53:56 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 19:53:56 - train.py[line:551] - INFO: load:1.33 valid_run:450.42 task_valid:435.18 collect_output:12.08
2023-01-10 19:56:24 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 19:56:24 - train.py[line:551] - INFO: load:1.36 valid_run:599.17 task_valid:580.54 collect_output:14.38
2023-01-10 19:58:56 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 19:58:56 - train.py[line:551] - INFO: load:1.38 valid_run:750.97 task_valid:728.33 collect_output:17.30
2023-01-10 20:01:27 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 20:01:27 - train.py[line:551] - INFO: load:1.41 valid_run:901.72 task_valid:873.97 collect_output:21.38
2023-01-10 20:03:59 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 20:03:59 - train.py[line:551] - INFO: load:1.43 valid_run:1053.49 task_valid:1020.07 collect_output:26.03
2023-01-10 20:06:29 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 20:06:29 - train.py[line:551] - INFO: load:1.45 valid_run:1203.25 task_valid:1161.33 collect_output:33.49
2023-01-10 20:08:58 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 20:08:58 - train.py[line:551] - INFO: load:1.48 valid_run:1352.03 task_valid:1306.32 collect_output:36.24
2023-01-10 20:11:26 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 20:11:26 - train.py[line:551] - INFO: load:1.50 valid_run:1499.84 task_valid:1449.71 collect_output:39.60
2023-01-10 20:13:55 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 20:13:55 - train.py[line:551] - INFO: load:1.53 valid_run:1649.39 task_valid:1595.16 collect_output:42.68
2023-01-10 20:16:25 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 20:16:25 - train.py[line:551] - INFO: load:1.55 valid_run:1798.65 task_valid:1740.39 collect_output:45.64
2023-01-10 20:18:53 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 20:18:53 - train.py[line:551] - INFO: load:1.58 valid_run:1947.42 task_valid:1882.48 collect_output:51.30
2023-01-10 20:21:24 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 20:21:24 - train.py[line:551] - INFO: load:1.60 valid_run:2097.39 task_valid:2028.20 collect_output:54.49
2023-01-10 20:23:54 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 20:23:54 - train.py[line:551] - INFO: load:1.63 valid_run:2247.46 task_valid:2175.07 collect_output:56.62
2023-01-10 20:26:23 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 20:26:23 - train.py[line:551] - INFO: load:1.65 valid_run:2396.92 task_valid:2319.84 collect_output:60.23
2023-01-10 20:28:54 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 20:28:54 - train.py[line:551] - INFO: load:1.68 valid_run:2547.49 task_valid:2465.78 collect_output:63.83
2023-01-10 20:31:24 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 20:31:24 - train.py[line:551] - INFO: load:1.70 valid_run:2697.93 task_valid:2613.45 collect_output:65.53
2023-01-10 20:33:52 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 20:33:52 - train.py[line:551] - INFO: load:1.73 valid_run:2845.57 task_valid:2755.79 collect_output:69.76
2023-01-10 20:36:22 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 20:36:22 - train.py[line:551] - INFO: load:1.75 valid_run:2995.23 task_valid:2901.42 collect_output:72.72
2023-01-10 20:38:53 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 20:38:53 - train.py[line:551] - INFO: load:1.78 valid_run:3145.92 task_valid:3046.74 collect_output:77.02
2023-01-10 20:41:22 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 20:41:22 - train.py[line:551] - INFO: load:1.80 valid_run:3294.78 task_valid:3191.89 collect_output:79.64
2023-01-10 20:43:53 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 20:43:53 - train.py[line:551] - INFO: load:1.83 valid_run:3445.67 task_valid:3338.94 collect_output:82.39
2023-01-10 20:46:24 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 20:46:24 - train.py[line:551] - INFO: load:1.85 valid_run:3596.55 task_valid:3485.95 collect_output:85.21

====================================================================================================
SGG eval:     R @ 50: 0.4865;     R @ 100: 0.5703;     R @ 500: 0.6285;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3094;    mR @ 100: 0.3608;    mR @ 500: 0.4218;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7537) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.4355) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.7663) (says:0.0000) (sitting on:0.7140) (standing on:0.2593) (using:0.6000) (walking in:0.0000) (walking on:0.3514) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4865;     R @ 100: 0.5703;     R @ 500: 0.6285;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3094;    mR @ 100: 0.3608;    mR @ 500: 0.4218;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7537) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.4355) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.7663) (says:0.0000) (sitting on:0.7140) (standing on:0.2593) (using:0.6000) (walking in:0.0000) (walking on:0.3514) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================

2023-01-10 20:48:55 - train.py[line:487] - INFO: 0.5702624649859944
2023-01-10 20:48:55 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 20:48:55 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.354 | loss_v1 0 | loss_v2 0 | nll_loss 0.199 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.570262 | ppl 1.15 | vqa_score 0.5372 | wps 119.6 | wpb 89.9 | bsz 30 | num_updates 20000 | best_R@100 0.69005
2023-01-10 20:48:55 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 20000 updates
2023-01-10 20:48:55 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_20000.pt
2023-01-10 20:49:40 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_20000.pt
2023-01-10 20:51:12 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_20000.pt (epoch 1 @ 20000 updates, score 0.5702624649859944) (writing took 136.23991753347218 seconds)
2023-01-10 20:51:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:28 - progress_bar.py[line:274] - INFO: epoch 001:  20038 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4732, wps=0.4, ups=0, wpb=108.6, bsz=40, num_updates=20010, lr=4.16615e-05, gnorm=0.691, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=111474
2023-01-10 20:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:44 - progress_bar.py[line:274] - INFO: epoch 001:  20048 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4433, wps=101.1, ups=0.62, wpb=108.8, bsz=40, num_updates=20020, lr=4.16562e-05, gnorm=0.493, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=111490
2023-01-10 20:51:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:51:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:00 - progress_bar.py[line:274] - INFO: epoch 001:  20058 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3978, wps=103.5, ups=0.62, wpb=110.5, bsz=40, num_updates=20030, lr=4.1651e-05, gnorm=0.48, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=111507
2023-01-10 20:52:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:17 - progress_bar.py[line:274] - INFO: epoch 001:  20068 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4082, wps=102.6, ups=0.62, wpb=110.5, bsz=40, num_updates=20040, lr=4.16458e-05, gnorm=0.398, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=111523
2023-01-10 20:52:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:33 - progress_bar.py[line:274] - INFO: epoch 001:  20078 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4066, wps=102.1, ups=0.62, wpb=110.1, bsz=40, num_updates=20050, lr=4.16406e-05, gnorm=0.458, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=111539
2023-01-10 20:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:50 - progress_bar.py[line:274] - INFO: epoch 001:  20088 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3762, wps=101.5, ups=0.62, wpb=109.8, bsz=40, num_updates=20060, lr=4.16354e-05, gnorm=0.582, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=111556
2023-01-10 20:52:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:52:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:06 - progress_bar.py[line:274] - INFO: epoch 001:  20098 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.433, wps=102.3, ups=0.61, wpb=111.9, bsz=40, num_updates=20070, lr=4.16302e-05, gnorm=1.163, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=111573
2023-01-10 20:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:23 - progress_bar.py[line:274] - INFO: epoch 001:  20108 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4681, wps=100.3, ups=0.61, wpb=109.1, bsz=40, num_updates=20080, lr=4.1625e-05, gnorm=0.449, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=111589
2023-01-10 20:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:40 - progress_bar.py[line:274] - INFO: epoch 001:  20118 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.41, wps=99.9, ups=0.61, wpb=108.3, bsz=40, num_updates=20090, lr=4.16198e-05, gnorm=0.533, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=111606
2023-01-10 20:53:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:56 - progress_bar.py[line:274] - INFO: epoch 001:  20128 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4286, wps=99.9, ups=0.61, wpb=109.9, bsz=40, num_updates=20100, lr=4.16146e-05, gnorm=0.688, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=111623
2023-01-10 20:53:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:53:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:13 - progress_bar.py[line:274] - INFO: epoch 001:  20138 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3173, wps=101.3, ups=0.62, wpb=108.2, bsz=40, num_updates=20110, lr=4.16094e-05, gnorm=0.605, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=111639
2023-01-10 20:54:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:29 - progress_bar.py[line:274] - INFO: epoch 001:  20148 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4412, wps=100.7, ups=0.6, wpb=111.3, bsz=40, num_updates=20120, lr=4.16042e-05, gnorm=0.402, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=111656
2023-01-10 20:54:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:46 - progress_bar.py[line:274] - INFO: epoch 001:  20158 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.44, wps=98.5, ups=0.6, wpb=108.7, bsz=40, num_updates=20130, lr=4.1599e-05, gnorm=0.55, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=111672
2023-01-10 20:54:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:54:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:03 - progress_bar.py[line:274] - INFO: epoch 001:  20168 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.191, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4206, wps=102.4, ups=0.62, wpb=110.8, bsz=40, num_updates=20140, lr=4.15938e-05, gnorm=0.731, clip=20, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=111689
2023-01-10 20:55:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:20 - progress_bar.py[line:274] - INFO: epoch 001:  20178 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3366, wps=99.6, ups=0.6, wpb=110.9, bsz=40, num_updates=20150, lr=4.15885e-05, gnorm=0.65, clip=30, loss_scale=256, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=111706
2023-01-10 20:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:37 - progress_bar.py[line:274] - INFO: epoch 001:  20188 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5102, wps=99.1, ups=0.6, wpb=109.9, bsz=40, num_updates=20160, lr=4.15833e-05, gnorm=0.426, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=111723
2023-01-10 20:55:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:53 - progress_bar.py[line:274] - INFO: epoch 001:  20198 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4423, wps=102.9, ups=0.63, wpb=109.1, bsz=40, num_updates=20170, lr=4.15781e-05, gnorm=0.6, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=111739
2023-01-10 20:55:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:09 - progress_bar.py[line:274] - INFO: epoch 001:  20208 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4423, wps=99.6, ups=0.61, wpb=109.1, bsz=40, num_updates=20180, lr=4.15729e-05, gnorm=0.35, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=111756
2023-01-10 20:56:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:26 - progress_bar.py[line:274] - INFO: epoch 001:  20218 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.179, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3426, wps=101, ups=0.61, wpb=110.5, bsz=40, num_updates=20190, lr=4.15677e-05, gnorm=0.572, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=111772
2023-01-10 20:56:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:43 - progress_bar.py[line:274] - INFO: epoch 001:  20228 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4818, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=20200, lr=4.15625e-05, gnorm=0.391, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=111789
2023-01-10 20:56:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:56:59 - progress_bar.py[line:274] - INFO: epoch 001:  20238 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4091, wps=103.4, ups=0.62, wpb=110.3, bsz=40, num_updates=20210, lr=4.15573e-05, gnorm=0.403, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=111805
2023-01-10 20:56:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:16 - progress_bar.py[line:274] - INFO: epoch 001:  20248 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4444, wps=98.7, ups=0.6, wpb=110.5, bsz=40, num_updates=20220, lr=4.15521e-05, gnorm=0.542, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=111822
2023-01-10 20:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:33 - progress_bar.py[line:274] - INFO: epoch 001:  20258 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4556, wps=99.5, ups=0.61, wpb=109.7, bsz=40, num_updates=20230, lr=4.15469e-05, gnorm=0.394, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=111839
2023-01-10 20:57:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:49 - progress_bar.py[line:274] - INFO: epoch 001:  20268 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.44, wps=102.3, ups=0.62, wpb=110.1, bsz=40, num_updates=20240, lr=4.15417e-05, gnorm=0.594, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=111855
2023-01-10 20:57:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:57:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:05 - progress_bar.py[line:274] - INFO: epoch 001:  20278 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4144, wps=102.5, ups=0.63, wpb=108.9, bsz=40, num_updates=20250, lr=4.15365e-05, gnorm=0.667, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=111871
2023-01-10 20:58:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:22 - progress_bar.py[line:274] - INFO: epoch 001:  20288 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3725, wps=98.5, ups=0.6, wpb=109.8, bsz=40, num_updates=20260, lr=4.15312e-05, gnorm=0.54, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=111888
2023-01-10 20:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:39 - progress_bar.py[line:274] - INFO: epoch 001:  20298 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3626, wps=103.5, ups=0.62, wpb=110.7, bsz=40, num_updates=20270, lr=4.1526e-05, gnorm=0.5, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=111905
2023-01-10 20:58:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:56 - progress_bar.py[line:274] - INFO: epoch 001:  20308 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3889, wps=97.8, ups=0.6, wpb=108.2, bsz=40, num_updates=20280, lr=4.15208e-05, gnorm=0.427, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=111922
2023-01-10 20:58:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:58:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:12 - progress_bar.py[line:274] - INFO: epoch 001:  20318 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4537, wps=98.6, ups=0.6, wpb=108.9, bsz=40, num_updates=20290, lr=4.15156e-05, gnorm=0.737, clip=30, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=111939
2023-01-10 20:59:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:29 - progress_bar.py[line:274] - INFO: epoch 001:  20328 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3689, wps=99.5, ups=0.6, wpb=111, bsz=40, num_updates=20300, lr=4.15104e-05, gnorm=0.996, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=111955
2023-01-10 20:59:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:46 - progress_bar.py[line:274] - INFO: epoch 001:  20338 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.449, wps=103, ups=0.63, wpb=109.7, bsz=40, num_updates=20310, lr=4.15052e-05, gnorm=0.511, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=111972
2023-01-10 20:59:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 20:59:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:02 - progress_bar.py[line:274] - INFO: epoch 001:  20348 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4947, wps=101.8, ups=0.61, wpb=111.1, bsz=40, num_updates=20320, lr=4.15e-05, gnorm=0.298, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=111988
2023-01-10 21:00:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:18 - progress_bar.py[line:274] - INFO: epoch 001:  20358 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4343, wps=101.6, ups=0.62, wpb=108.5, bsz=40, num_updates=20330, lr=4.14948e-05, gnorm=0.572, clip=30, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=112005
2023-01-10 21:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:35 - progress_bar.py[line:274] - INFO: epoch 001:  20368 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3814, wps=99, ups=0.6, wpb=109.3, bsz=40, num_updates=20340, lr=4.14896e-05, gnorm=0.549, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=112021
2023-01-10 21:00:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:52 - progress_bar.py[line:274] - INFO: epoch 001:  20378 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4947, wps=101.8, ups=0.61, wpb=111.6, bsz=40, num_updates=20350, lr=4.14844e-05, gnorm=2.061, clip=60, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=112038
2023-01-10 21:00:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:00:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:08 - progress_bar.py[line:274] - INFO: epoch 001:  20388 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4771, wps=99.3, ups=0.61, wpb=108.7, bsz=40, num_updates=20360, lr=4.14792e-05, gnorm=0.408, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112055
2023-01-10 21:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:25 - progress_bar.py[line:274] - INFO: epoch 001:  20398 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4175, wps=102.3, ups=0.62, wpb=109.7, bsz=40, num_updates=20370, lr=4.1474e-05, gnorm=0.435, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=112071
2023-01-10 21:01:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:41 - progress_bar.py[line:274] - INFO: epoch 001:  20408 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5106, wps=102.1, ups=0.61, wpb=111.5, bsz=40, num_updates=20380, lr=4.14688e-05, gnorm=0.436, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112088
2023-01-10 21:01:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:01:58 - progress_bar.py[line:274] - INFO: epoch 001:  20418 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4712, wps=100.7, ups=0.61, wpb=109.5, bsz=40, num_updates=20390, lr=4.14635e-05, gnorm=0.313, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=112104
2023-01-10 21:01:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:14 - progress_bar.py[line:274] - INFO: epoch 001:  20428 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4747, wps=102.2, ups=0.62, wpb=110.5, bsz=40, num_updates=20400, lr=4.14583e-05, gnorm=0.501, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112121
2023-01-10 21:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:31 - progress_bar.py[line:274] - INFO: epoch 001:  20438 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.2991, wps=99.9, ups=0.61, wpb=108.3, bsz=40, num_updates=20410, lr=4.14531e-05, gnorm=0.344, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=112137
2023-01-10 21:02:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:48 - progress_bar.py[line:274] - INFO: epoch 001:  20448 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4545, wps=98.6, ups=0.6, wpb=110.1, bsz=40, num_updates=20420, lr=4.14479e-05, gnorm=0.849, clip=40, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=112154
2023-01-10 21:02:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:04 - progress_bar.py[line:274] - INFO: epoch 001:  20458 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5556, wps=102.6, ups=0.62, wpb=109.7, bsz=40, num_updates=20430, lr=4.14427e-05, gnorm=1.756, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112170
2023-01-10 21:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:21 - progress_bar.py[line:274] - INFO: epoch 001:  20468 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.5, wps=102.2, ups=0.62, wpb=110.7, bsz=40, num_updates=20440, lr=4.14375e-05, gnorm=0.443, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112187
2023-01-10 21:03:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:37 - progress_bar.py[line:274] - INFO: epoch 001:  20478 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4118, wps=105.2, ups=0.63, wpb=110.8, bsz=40, num_updates=20450, lr=4.14323e-05, gnorm=0.403, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=112203
2023-01-10 21:03:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:53 - progress_bar.py[line:274] - INFO: epoch 001:  20488 / 100000 loss=0.329, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=106.933, nsentences=40, sample_size=106.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.5172, wps=99, ups=0.62, wpb=106.9, bsz=40, num_updates=20460, lr=4.14271e-05, gnorm=0.428, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112219
2023-01-10 21:03:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:03:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:09 - progress_bar.py[line:274] - INFO: epoch 001:  20498 / 100000 loss=0.341, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4762, wps=102.3, ups=0.62, wpb=110.1, bsz=40, num_updates=20470, lr=4.14219e-05, gnorm=1.465, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112236
2023-01-10 21:04:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:26 - progress_bar.py[line:274] - INFO: epoch 001:  20508 / 100000 loss=0.342, loss_v1=0, loss_v2=0, nll_loss=0.194, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3925, wps=97.9, ups=0.6, wpb=108.4, bsz=40, num_updates=20480, lr=4.14167e-05, gnorm=1.305, clip=50, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=112253
2023-01-10 21:04:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:31 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-10 21:04:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:45 - progress_bar.py[line:274] - INFO: epoch 001:  20519 / 100000 loss=0.336, loss_v1=0, loss_v2=0, nll_loss=0.186, ntokens=107.125, nsentences=40, sample_size=107.125, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4931, wps=92.5, ups=0.54, wpb=107.1, bsz=40, num_updates=20490, lr=4.14115e-05, gnorm=0.379, clip=0, loss_scale=256, train_wall=18, gb_free=10.5, ema_decay=0.9999, wall=112271
2023-01-10 21:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:04:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:02 - progress_bar.py[line:274] - INFO: epoch 001:  20529 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4851, wps=102.3, ups=0.61, wpb=110.9, bsz=40, num_updates=20500, lr=4.14063e-05, gnorm=0.559, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112288
2023-01-10 21:05:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:18 - progress_bar.py[line:274] - INFO: epoch 001:  20539 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3838, wps=98.9, ups=0.61, wpb=108, bsz=40, num_updates=20510, lr=4.1401e-05, gnorm=0.698, clip=20, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=112304
2023-01-10 21:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:35 - progress_bar.py[line:274] - INFO: epoch 001:  20549 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3936, wps=99.6, ups=0.61, wpb=109.4, bsz=40, num_updates=20520, lr=4.13958e-05, gnorm=0.592, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112321
2023-01-10 21:05:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:52 - progress_bar.py[line:274] - INFO: epoch 001:  20559 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4486, wps=99.4, ups=0.61, wpb=108.6, bsz=40, num_updates=20530, lr=4.13906e-05, gnorm=0.558, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=112338
2023-01-10 21:05:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:05:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:08 - progress_bar.py[line:274] - INFO: epoch 001:  20569 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3981, wps=102.3, ups=0.62, wpb=109.6, bsz=40, num_updates=20540, lr=4.13854e-05, gnorm=0.518, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112354
2023-01-10 21:06:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:25 - progress_bar.py[line:274] - INFO: epoch 001:  20579 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4845, wps=99.5, ups=0.61, wpb=109.4, bsz=40, num_updates=20550, lr=4.13802e-05, gnorm=0.574, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=112371
2023-01-10 21:06:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:41 - progress_bar.py[line:274] - INFO: epoch 001:  20589 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4545, wps=98.8, ups=0.6, wpb=109.9, bsz=40, num_updates=20560, lr=4.1375e-05, gnorm=1.15, clip=10, loss_scale=256, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=112388
2023-01-10 21:06:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:06:58 - progress_bar.py[line:274] - INFO: epoch 001:  20599 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3878, wps=103.1, ups=0.62, wpb=110.6, bsz=40, num_updates=20570, lr=4.13698e-05, gnorm=0.428, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=112404
2023-01-10 21:06:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:14 - progress_bar.py[line:274] - INFO: epoch 001:  20609 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3953, wps=104.1, ups=0.62, wpb=111.7, bsz=40, num_updates=20580, lr=4.13646e-05, gnorm=0.469, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112420
2023-01-10 21:07:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:31 - progress_bar.py[line:274] - INFO: epoch 001:  20619 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4141, wps=102.4, ups=0.62, wpb=110.6, bsz=40, num_updates=20590, lr=4.13594e-05, gnorm=0.359, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=112437
2023-01-10 21:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:47 - progress_bar.py[line:274] - INFO: epoch 001:  20629 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4727, wps=99.6, ups=0.61, wpb=108.3, bsz=40, num_updates=20600, lr=4.13542e-05, gnorm=0.458, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=112453
2023-01-10 21:07:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:07:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:03 - progress_bar.py[line:274] - INFO: epoch 001:  20639 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3939, wps=103.6, ups=0.63, wpb=109.3, bsz=40, num_updates=20610, lr=4.1349e-05, gnorm=0.404, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=112470
2023-01-10 21:08:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:20 - progress_bar.py[line:274] - INFO: epoch 001:  20649 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3762, wps=101.1, ups=0.62, wpb=109.3, bsz=40, num_updates=20620, lr=4.13438e-05, gnorm=0.537, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=112486
2023-01-10 21:08:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:36 - progress_bar.py[line:274] - INFO: epoch 001:  20659 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4271, wps=102.4, ups=0.62, wpb=110.3, bsz=40, num_updates=20630, lr=4.13385e-05, gnorm=0.836, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112502
2023-01-10 21:08:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:08:53 - progress_bar.py[line:274] - INFO: epoch 001:  20669 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4554, wps=101.7, ups=0.62, wpb=108.5, bsz=40, num_updates=20640, lr=4.13333e-05, gnorm=0.665, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=112519
2023-01-10 21:08:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:09 - progress_bar.py[line:274] - INFO: epoch 001:  20679 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4248, wps=100, ups=0.61, wpb=109.4, bsz=40, num_updates=20650, lr=4.13281e-05, gnorm=0.435, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112535
2023-01-10 21:09:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:26 - progress_bar.py[line:274] - INFO: epoch 001:  20689 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.396, wps=101.6, ups=0.62, wpb=109.3, bsz=40, num_updates=20660, lr=4.13229e-05, gnorm=0.368, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=112552
2023-01-10 21:09:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:42 - progress_bar.py[line:274] - INFO: epoch 001:  20699 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4409, wps=104.5, ups=0.63, wpb=110.1, bsz=40, num_updates=20670, lr=4.13177e-05, gnorm=0.459, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112568
2023-01-10 21:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:09:58 - progress_bar.py[line:274] - INFO: epoch 001:  20709 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3981, wps=99.5, ups=0.61, wpb=108.3, bsz=40, num_updates=20680, lr=4.13125e-05, gnorm=0.58, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=112584
2023-01-10 21:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:12 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-10 21:10:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:17 - progress_bar.py[line:274] - INFO: epoch 001:  20720 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4098, wps=93.7, ups=0.53, wpb=110, bsz=40, num_updates=20690, lr=4.13073e-05, gnorm=0.474, clip=0, loss_scale=128, train_wall=19, gb_free=10, ema_decay=0.9999, wall=112603
2023-01-10 21:10:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:34 - progress_bar.py[line:274] - INFO: epoch 001:  20730 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4112, wps=101.9, ups=0.62, wpb=110.3, bsz=40, num_updates=20700, lr=4.13021e-05, gnorm=0.239, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=112620
2023-01-10 21:10:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:51 - progress_bar.py[line:274] - INFO: epoch 001:  20740 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3103, wps=97.8, ups=0.6, wpb=109.5, bsz=40, num_updates=20710, lr=4.12969e-05, gnorm=2.102, clip=30, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=112637
2023-01-10 21:10:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:10:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:08 - progress_bar.py[line:274] - INFO: epoch 001:  20750 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4314, wps=99.3, ups=0.61, wpb=108.9, bsz=40, num_updates=20720, lr=4.12917e-05, gnorm=0.531, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112654
2023-01-10 21:11:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:24 - progress_bar.py[line:274] - INFO: epoch 001:  20760 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4444, wps=102.6, ups=0.62, wpb=110.8, bsz=40, num_updates=20730, lr=4.12865e-05, gnorm=1.893, clip=20, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=112670
2023-01-10 21:11:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:40 - progress_bar.py[line:274] - INFO: epoch 001:  20770 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3981, wps=103, ups=0.63, wpb=109.7, bsz=40, num_updates=20740, lr=4.12813e-05, gnorm=0.367, clip=0, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=112687
2023-01-10 21:11:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:11:57 - progress_bar.py[line:274] - INFO: epoch 001:  20780 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.375, wps=101.7, ups=0.62, wpb=109.7, bsz=40, num_updates=20750, lr=4.1276e-05, gnorm=0.71, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112703
2023-01-10 21:12:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:13 - progress_bar.py[line:274] - INFO: epoch 001:  20790 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4653, wps=101.8, ups=0.62, wpb=109.3, bsz=40, num_updates=20760, lr=4.12708e-05, gnorm=0.399, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112719
2023-01-10 21:12:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:30 - progress_bar.py[line:274] - INFO: epoch 001:  20800 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4158, wps=98.8, ups=0.6, wpb=110.1, bsz=40, num_updates=20770, lr=4.12656e-05, gnorm=0.392, clip=10, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=112736
2023-01-10 21:12:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:47 - progress_bar.py[line:274] - INFO: epoch 001:  20810 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3725, wps=99.5, ups=0.61, wpb=109.5, bsz=40, num_updates=20780, lr=4.12604e-05, gnorm=0.756, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=112753
2023-01-10 21:12:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:12:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:03 - progress_bar.py[line:274] - INFO: epoch 001:  20820 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4388, wps=102.9, ups=0.62, wpb=110.2, bsz=40, num_updates=20790, lr=4.12552e-05, gnorm=0.839, clip=20, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=112769
2023-01-10 21:13:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:20 - progress_bar.py[line:274] - INFO: epoch 001:  20830 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.451, wps=99.8, ups=0.6, wpb=111.3, bsz=40, num_updates=20800, lr=4.125e-05, gnorm=0.414, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=112786
2023-01-10 21:13:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:37 - progress_bar.py[line:274] - INFO: epoch 001:  20840 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4118, wps=102.4, ups=0.62, wpb=109.7, bsz=40, num_updates=20810, lr=4.12448e-05, gnorm=1.128, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112803
2023-01-10 21:13:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:13:53 - progress_bar.py[line:274] - INFO: epoch 001:  20850 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4272, wps=101.6, ups=0.62, wpb=108.9, bsz=40, num_updates=20820, lr=4.12396e-05, gnorm=0.696, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=112819
2023-01-10 21:13:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:09 - progress_bar.py[line:274] - INFO: epoch 001:  20860 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3235, wps=103, ups=0.62, wpb=110.3, bsz=40, num_updates=20830, lr=4.12344e-05, gnorm=0.425, clip=10, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=112835
2023-01-10 21:14:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:26 - progress_bar.py[line:274] - INFO: epoch 001:  20870 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4, wps=100.2, ups=0.62, wpb=108.3, bsz=40, num_updates=20840, lr=4.12292e-05, gnorm=0.608, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112852
2023-01-10 21:14:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:42 - progress_bar.py[line:274] - INFO: epoch 001:  20880 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4271, wps=101.5, ups=0.61, wpb=110.8, bsz=40, num_updates=20850, lr=4.1224e-05, gnorm=0.65, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=112868
2023-01-10 21:14:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:14:59 - progress_bar.py[line:274] - INFO: epoch 001:  20890 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4021, wps=98.9, ups=0.6, wpb=109, bsz=40, num_updates=20860, lr=4.12188e-05, gnorm=0.62, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=112885
2023-01-10 21:15:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:16 - progress_bar.py[line:274] - INFO: epoch 001:  20900 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5104, wps=101.2, ups=0.61, wpb=111.4, bsz=40, num_updates=20870, lr=4.12135e-05, gnorm=1.14, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=112902
2023-01-10 21:15:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:32 - progress_bar.py[line:274] - INFO: epoch 001:  20910 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3563, wps=101.4, ups=0.61, wpb=109.9, bsz=40, num_updates=20880, lr=4.12083e-05, gnorm=0.466, clip=0, loss_scale=128, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=112918
2023-01-10 21:15:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:49 - progress_bar.py[line:274] - INFO: epoch 001:  20920 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.41, wps=100.8, ups=0.61, wpb=109.5, bsz=40, num_updates=20890, lr=4.12031e-05, gnorm=0.353, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=112935
2023-01-10 21:15:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:15:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:05 - progress_bar.py[line:274] - INFO: epoch 001:  20930 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3939, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=20900, lr=4.11979e-05, gnorm=0.506, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=112952
2023-01-10 21:16:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:22 - progress_bar.py[line:274] - INFO: epoch 001:  20940 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3868, wps=100.4, ups=0.61, wpb=108.9, bsz=40, num_updates=20910, lr=4.11927e-05, gnorm=0.702, clip=20, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=112968
2023-01-10 21:16:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:39 - progress_bar.py[line:274] - INFO: epoch 001:  20950 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3838, wps=100.5, ups=0.61, wpb=110.7, bsz=40, num_updates=20920, lr=4.11875e-05, gnorm=0.426, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=112985
2023-01-10 21:16:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:16:55 - progress_bar.py[line:274] - INFO: epoch 001:  20960 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4646, wps=103.3, ups=0.62, wpb=110.8, bsz=40, num_updates=20930, lr=4.11823e-05, gnorm=0.877, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=113001
2023-01-10 21:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:11 - progress_bar.py[line:274] - INFO: epoch 001:  20970 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3824, wps=103.1, ups=0.62, wpb=110.9, bsz=40, num_updates=20940, lr=4.11771e-05, gnorm=0.675, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=113018
2023-01-10 21:17:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:28 - progress_bar.py[line:274] - INFO: epoch 001:  20980 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.29, wps=103.5, ups=0.62, wpb=110.5, bsz=40, num_updates=20950, lr=4.11719e-05, gnorm=0.453, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=113034
2023-01-10 21:17:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:44 - progress_bar.py[line:274] - INFO: epoch 001:  20990 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4505, wps=101.9, ups=0.61, wpb=111.1, bsz=40, num_updates=20960, lr=4.11667e-05, gnorm=2.128, clip=50, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=113050
2023-01-10 21:17:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:17:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:01 - progress_bar.py[line:274] - INFO: epoch 001:  21000 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4556, wps=103.6, ups=0.62, wpb=111.5, bsz=40, num_updates=20970, lr=4.11615e-05, gnorm=1.598, clip=50, loss_scale=128, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=113067
2023-01-10 21:18:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:17 - progress_bar.py[line:274] - INFO: epoch 001:  21010 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4216, wps=101.3, ups=0.61, wpb=110.6, bsz=40, num_updates=20980, lr=4.11563e-05, gnorm=0.888, clip=30, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=113083
2023-01-10 21:18:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:34 - progress_bar.py[line:274] - INFO: epoch 001:  21020 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3958, wps=102.8, ups=0.62, wpb=110.6, bsz=40, num_updates=20990, lr=4.1151e-05, gnorm=0.511, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=113100
2023-01-10 21:18:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 21:18:50 - progress_bar.py[line:274] - INFO: epoch 001:  21030 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4579, wps=100.9, ups=0.61, wpb=109.5, bsz=40, num_updates=21000, lr=4.11458e-05, gnorm=0.334, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=113116
2023-01-10 21:18:50 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 21:18:52 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 21:18:52 - train.py[line:551] - INFO: load:1.22 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 21:21:23 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 21:21:23 - train.py[line:551] - INFO: load:1.24 valid_run:151.28 task_valid:148.29 collect_output:1.92
2023-01-10 21:23:51 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 21:23:51 - train.py[line:551] - INFO: load:1.27 valid_run:299.26 task_valid:291.98 collect_output:5.12
2023-01-10 21:26:23 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 21:26:23 - train.py[line:551] - INFO: load:1.29 valid_run:450.88 task_valid:435.90 collect_output:11.74
2023-01-10 21:28:52 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 21:28:52 - train.py[line:551] - INFO: load:1.32 valid_run:599.53 task_valid:581.02 collect_output:14.25
2023-01-10 21:31:24 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 21:31:24 - train.py[line:551] - INFO: load:1.34 valid_run:751.47 task_valid:728.92 collect_output:17.22
2023-01-10 21:33:55 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 21:33:55 - train.py[line:551] - INFO: load:1.37 valid_run:902.45 task_valid:874.68 collect_output:21.39
2023-01-10 21:36:27 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 21:36:27 - train.py[line:551] - INFO: load:1.39 valid_run:1054.75 task_valid:1021.07 collect_output:26.26
2023-01-10 21:38:57 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 21:38:57 - train.py[line:551] - INFO: load:1.41 valid_run:1204.77 task_valid:1162.27 collect_output:34.06
2023-01-10 21:41:26 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 21:41:27 - train.py[line:551] - INFO: load:1.44 valid_run:1353.73 task_valid:1307.35 collect_output:36.91
2023-01-10 21:43:55 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 21:43:55 - train.py[line:551] - INFO: load:1.46 valid_run:1501.61 task_valid:1450.77 collect_output:40.33
2023-01-10 21:46:24 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 21:46:24 - train.py[line:551] - INFO: load:1.49 valid_run:1651.23 task_valid:1596.38 collect_output:43.28
2023-01-10 21:48:54 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 21:48:54 - train.py[line:551] - INFO: load:1.51 valid_run:1800.83 task_valid:1741.88 collect_output:46.34
2023-01-10 21:51:23 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 21:51:23 - train.py[line:551] - INFO: load:1.54 valid_run:1949.82 task_valid:1884.05 collect_output:52.10
2023-01-10 21:53:53 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 21:53:53 - train.py[line:551] - INFO: load:1.56 valid_run:2100.06 task_valid:2029.85 collect_output:55.52
2023-01-10 21:56:24 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 21:56:24 - train.py[line:551] - INFO: load:1.59 valid_run:2250.18 task_valid:2176.99 collect_output:57.44
2023-01-10 21:58:53 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 21:58:53 - train.py[line:551] - INFO: load:1.61 valid_run:2399.74 task_valid:2321.69 collect_output:61.26
2023-01-10 22:01:24 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 22:01:24 - train.py[line:551] - INFO: load:1.64 valid_run:2550.28 task_valid:2467.42 collect_output:65.02
2023-01-10 22:03:54 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 22:03:54 - train.py[line:551] - INFO: load:1.66 valid_run:2700.45 task_valid:2614.54 collect_output:67.05
2023-01-10 22:06:22 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 22:06:22 - train.py[line:551] - INFO: load:1.69 valid_run:2847.91 task_valid:2756.63 collect_output:71.38
2023-01-10 22:08:51 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 22:08:51 - train.py[line:551] - INFO: load:1.71 valid_run:2997.41 task_valid:2902.09 collect_output:74.37
2023-01-10 22:11:22 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 22:11:22 - train.py[line:551] - INFO: load:1.74 valid_run:3148.00 task_valid:3047.21 collect_output:78.78
2023-01-10 22:13:51 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 22:13:51 - train.py[line:551] - INFO: load:1.76 valid_run:3296.96 task_valid:3192.09 collect_output:81.82
2023-01-10 22:16:21 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 22:16:21 - train.py[line:551] - INFO: load:1.79 valid_run:3447.38 task_valid:3338.70 collect_output:84.61
2023-01-10 22:18:52 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 22:18:52 - train.py[line:551] - INFO: load:1.81 valid_run:3598.05 task_valid:3485.38 collect_output:87.57

====================================================================================================
SGG eval:     R @ 50: 0.4883;     R @ 100: 0.5693;     R @ 500: 0.6216;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3094;    mR @ 100: 0.3595;    mR @ 500: 0.4176;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7585) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3968) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.7663) (says:0.0000) (sitting on:0.7208) (standing on:0.2593) (using:0.6000) (walking in:0.0000) (walking on:0.3514) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4883;     R @ 100: 0.5693;     R @ 500: 0.6216;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3094;    mR @ 100: 0.3595;    mR @ 500: 0.4176;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7585) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3968) (lying on:0.2000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7917) (playing:0.0000) (riding:0.7663) (says:0.0000) (sitting on:0.7208) (standing on:0.2593) (using:0.6000) (walking in:0.0000) (walking on:0.3514) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================

2023-01-10 22:21:23 - train.py[line:487] - INFO: 0.5692624649859943
2023-01-10 22:21:23 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 22:21:24 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.342 | loss_v1 0 | loss_v2 0 | nll_loss 0.185 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.569262 | ppl 1.14 | vqa_score 0.5338 | wps 119.6 | wpb 89.9 | bsz 30 | num_updates 21000 | best_R@100 0.69005
2023-01-10 22:21:24 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 21000 updates
2023-01-10 22:21:24 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_21000.pt
2023-01-10 22:22:03 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_21000.pt
2023-01-10 22:23:28 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_21000.pt (epoch 1 @ 21000 updates, score 0.5692624649859943) (writing took 124.85457495227456 seconds)
2023-01-10 22:23:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:23:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:23:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:23:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:23:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:23:45 - progress_bar.py[line:274] - INFO: epoch 001:  21040 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4632, wps=0.4, ups=0, wpb=108.9, bsz=40, num_updates=21010, lr=4.11406e-05, gnorm=0.444, clip=10, loss_scale=128, train_wall=17, gb_free=10.8, ema_decay=0.9999, wall=117012
2023-01-10 22:23:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:23:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:23:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:23:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:02 - progress_bar.py[line:274] - INFO: epoch 001:  21050 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4141, wps=101, ups=0.61, wpb=110.2, bsz=40, num_updates=21020, lr=4.11354e-05, gnorm=0.439, clip=0, loss_scale=128, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=117028
2023-01-10 22:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:19 - progress_bar.py[line:274] - INFO: epoch 001:  21060 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4091, wps=98.1, ups=0.6, wpb=109.1, bsz=40, num_updates=21030, lr=4.11302e-05, gnorm=0.669, clip=20, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=117045
2023-01-10 22:24:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:36 - progress_bar.py[line:274] - INFO: epoch 001:  21070 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5464, wps=100.5, ups=0.61, wpb=110.3, bsz=40, num_updates=21040, lr=4.1125e-05, gnorm=0.345, clip=10, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=117062
2023-01-10 22:24:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:24:52 - progress_bar.py[line:274] - INFO: epoch 001:  21080 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3333, wps=101, ups=0.62, wpb=108.4, bsz=40, num_updates=21050, lr=4.11198e-05, gnorm=0.635, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117078
2023-01-10 22:24:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:08 - progress_bar.py[line:274] - INFO: epoch 001:  21090 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4021, wps=103.6, ups=0.62, wpb=110.7, bsz=40, num_updates=21060, lr=4.11146e-05, gnorm=0.529, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117095
2023-01-10 22:25:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:25 - progress_bar.py[line:274] - INFO: epoch 001:  21100 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.466, wps=100.7, ups=0.62, wpb=108.7, bsz=40, num_updates=21070, lr=4.11094e-05, gnorm=0.501, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117111
2023-01-10 22:25:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:42 - progress_bar.py[line:274] - INFO: epoch 001:  21110 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3214, wps=100.7, ups=0.61, wpb=109.3, bsz=40, num_updates=21080, lr=4.11042e-05, gnorm=0.544, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=117128
2023-01-10 22:25:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:25:59 - progress_bar.py[line:274] - INFO: epoch 001:  21120 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.35, wps=97.7, ups=0.6, wpb=109.1, bsz=40, num_updates=21090, lr=4.1099e-05, gnorm=0.611, clip=20, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=117145
2023-01-10 22:26:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:14 - progress_bar.py[line:274] - INFO: epoch 001:  21130 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4356, wps=105.9, ups=0.64, wpb=110.7, bsz=40, num_updates=21100, lr=4.10938e-05, gnorm=0.371, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117161
2023-01-10 22:26:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:31 - progress_bar.py[line:274] - INFO: epoch 001:  21140 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3714, wps=103.1, ups=0.62, wpb=110.5, bsz=40, num_updates=21110, lr=4.10885e-05, gnorm=0.433, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117177
2023-01-10 22:26:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:48 - progress_bar.py[line:274] - INFO: epoch 001:  21150 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3694, wps=99.4, ups=0.6, wpb=110.1, bsz=40, num_updates=21120, lr=4.10833e-05, gnorm=0.752, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=117194
2023-01-10 22:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:26:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:04 - progress_bar.py[line:274] - INFO: epoch 001:  21160 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4158, wps=101.6, ups=0.62, wpb=109.5, bsz=40, num_updates=21130, lr=4.10781e-05, gnorm=0.629, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117210
2023-01-10 22:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:21 - progress_bar.py[line:274] - INFO: epoch 001:  21170 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3962, wps=99.8, ups=0.61, wpb=109.7, bsz=40, num_updates=21140, lr=4.10729e-05, gnorm=0.376, clip=10, loss_scale=128, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=117227
2023-01-10 22:27:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:37 - progress_bar.py[line:274] - INFO: epoch 001:  21180 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.36, wps=100.7, ups=0.61, wpb=109.4, bsz=40, num_updates=21150, lr=4.10677e-05, gnorm=0.368, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117244
2023-01-10 22:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:27:54 - progress_bar.py[line:274] - INFO: epoch 001:  21190 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.44, wps=102.1, ups=0.61, wpb=111.6, bsz=40, num_updates=21160, lr=4.10625e-05, gnorm=0.378, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117260
2023-01-10 22:27:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:11 - progress_bar.py[line:274] - INFO: epoch 001:  21200 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.42, wps=100, ups=0.61, wpb=110, bsz=40, num_updates=21170, lr=4.10573e-05, gnorm=0.364, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117277
2023-01-10 22:28:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:27 - progress_bar.py[line:274] - INFO: epoch 001:  21210 / 100000 loss=0.339, loss_v1=0, loss_v2=0, nll_loss=0.187, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4182, wps=99.2, ups=0.61, wpb=108.5, bsz=40, num_updates=21180, lr=4.10521e-05, gnorm=0.479, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=117294
2023-01-10 22:28:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:44 - progress_bar.py[line:274] - INFO: epoch 001:  21220 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3878, wps=102.6, ups=0.63, wpb=109, bsz=40, num_updates=21190, lr=4.10469e-05, gnorm=1.609, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117310
2023-01-10 22:28:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:28:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:00 - progress_bar.py[line:274] - INFO: epoch 001:  21230 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4118, wps=102.1, ups=0.62, wpb=110.5, bsz=40, num_updates=21200, lr=4.10417e-05, gnorm=0.415, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117326
2023-01-10 22:29:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:17 - progress_bar.py[line:274] - INFO: epoch 001:  21240 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4253, wps=95.8, ups=0.58, wpb=110, bsz=40, num_updates=21210, lr=4.10365e-05, gnorm=0.451, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=117344
2023-01-10 22:29:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:34 - progress_bar.py[line:274] - INFO: epoch 001:  21250 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4286, wps=102.4, ups=0.63, wpb=108.9, bsz=40, num_updates=21220, lr=4.10312e-05, gnorm=0.508, clip=10, loss_scale=256, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=117360
2023-01-10 22:29:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:50 - progress_bar.py[line:274] - INFO: epoch 001:  21260 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4, wps=100.3, ups=0.61, wpb=109.9, bsz=40, num_updates=21230, lr=4.1026e-05, gnorm=0.474, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=117377
2023-01-10 22:29:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:29:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:07 - progress_bar.py[line:274] - INFO: epoch 001:  21270 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4579, wps=99.3, ups=0.61, wpb=108.6, bsz=40, num_updates=21240, lr=4.10208e-05, gnorm=0.692, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=117393
2023-01-10 22:30:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:24 - progress_bar.py[line:274] - INFO: epoch 001:  21280 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4608, wps=100.3, ups=0.61, wpb=110.3, bsz=40, num_updates=21250, lr=4.10156e-05, gnorm=1.319, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117410
2023-01-10 22:30:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:40 - progress_bar.py[line:274] - INFO: epoch 001:  21290 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.398, wps=102.3, ups=0.62, wpb=109.5, bsz=40, num_updates=21260, lr=4.10104e-05, gnorm=1.276, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=117426
2023-01-10 22:30:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:30:56 - progress_bar.py[line:274] - INFO: epoch 001:  21300 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3908, wps=104.5, ups=0.63, wpb=111.3, bsz=40, num_updates=21270, lr=4.10052e-05, gnorm=0.805, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117442
2023-01-10 22:31:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:13 - progress_bar.py[line:274] - INFO: epoch 001:  21310 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.5306, wps=99.8, ups=0.61, wpb=109.9, bsz=40, num_updates=21280, lr=4.1e-05, gnorm=0.432, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117459
2023-01-10 22:31:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:30 - progress_bar.py[line:274] - INFO: epoch 001:  21320 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.189, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3661, wps=99.9, ups=0.61, wpb=108.9, bsz=40, num_updates=21290, lr=4.09948e-05, gnorm=0.693, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117476
2023-01-10 22:31:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:46 - progress_bar.py[line:274] - INFO: epoch 001:  21330 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3868, wps=101.2, ups=0.62, wpb=109.1, bsz=40, num_updates=21300, lr=4.09896e-05, gnorm=0.381, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117492
2023-01-10 22:31:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:31:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:02 - progress_bar.py[line:274] - INFO: epoch 001:  21340 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3596, wps=102.3, ups=0.62, wpb=110.3, bsz=40, num_updates=21310, lr=4.09844e-05, gnorm=0.288, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117509
2023-01-10 22:32:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:19 - progress_bar.py[line:274] - INFO: epoch 001:  21350 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5114, wps=101, ups=0.61, wpb=110.7, bsz=40, num_updates=21320, lr=4.09792e-05, gnorm=0.414, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117525
2023-01-10 22:32:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:36 - progress_bar.py[line:274] - INFO: epoch 001:  21360 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4388, wps=101.1, ups=0.61, wpb=110.7, bsz=40, num_updates=21330, lr=4.0974e-05, gnorm=0.454, clip=20, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=117542
2023-01-10 22:32:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:32:52 - progress_bar.py[line:274] - INFO: epoch 001:  21370 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=107.733, nsentences=40, sample_size=107.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.486, wps=98.6, ups=0.61, wpb=107.7, bsz=40, num_updates=21340, lr=4.09688e-05, gnorm=0.279, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=117559
2023-01-10 22:32:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:09 - progress_bar.py[line:274] - INFO: epoch 001:  21380 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4, wps=101.5, ups=0.61, wpb=111.2, bsz=40, num_updates=21350, lr=4.09635e-05, gnorm=0.903, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=117575
2023-01-10 22:33:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:26 - progress_bar.py[line:274] - INFO: epoch 001:  21390 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4842, wps=98.8, ups=0.6, wpb=110.5, bsz=40, num_updates=21360, lr=4.09583e-05, gnorm=0.633, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=117592
2023-01-10 22:33:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:43 - progress_bar.py[line:274] - INFO: epoch 001:  21400 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4375, wps=101.2, ups=0.61, wpb=110.7, bsz=40, num_updates=21370, lr=4.09531e-05, gnorm=0.506, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=117609
2023-01-10 22:33:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:33:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:00 - progress_bar.py[line:274] - INFO: epoch 001:  21410 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3711, wps=100.3, ups=0.61, wpb=109.8, bsz=40, num_updates=21380, lr=4.09479e-05, gnorm=0.556, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117626
2023-01-10 22:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:17 - progress_bar.py[line:274] - INFO: epoch 001:  21420 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3333, wps=99.5, ups=0.6, wpb=109.7, bsz=40, num_updates=21390, lr=4.09427e-05, gnorm=0.728, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=117643
2023-01-10 22:34:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:34 - progress_bar.py[line:274] - INFO: epoch 001:  21430 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5354, wps=97, ups=0.58, wpb=110.9, bsz=40, num_updates=21400, lr=4.09375e-05, gnorm=0.461, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=117660
2023-01-10 22:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:50 - progress_bar.py[line:274] - INFO: epoch 001:  21440 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.404, wps=102.5, ups=0.62, wpb=110.3, bsz=40, num_updates=21410, lr=4.09323e-05, gnorm=0.432, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=117677
2023-01-10 22:34:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:34:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:07 - progress_bar.py[line:274] - INFO: epoch 001:  21450 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3571, wps=97.7, ups=0.6, wpb=109.2, bsz=40, num_updates=21420, lr=4.09271e-05, gnorm=0.62, clip=20, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=117694
2023-01-10 22:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:24 - progress_bar.py[line:274] - INFO: epoch 001:  21460 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3644, wps=99.2, ups=0.61, wpb=109, bsz=40, num_updates=21430, lr=4.09219e-05, gnorm=0.468, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117710
2023-01-10 22:35:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:41 - progress_bar.py[line:274] - INFO: epoch 001:  21470 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.5, wps=98.9, ups=0.61, wpb=108.8, bsz=40, num_updates=21440, lr=4.09167e-05, gnorm=1.154, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117727
2023-01-10 22:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:35:57 - progress_bar.py[line:274] - INFO: epoch 001:  21480 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3725, wps=100, ups=0.61, wpb=109.6, bsz=40, num_updates=21450, lr=4.09115e-05, gnorm=0.378, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=117744
2023-01-10 22:36:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:14 - progress_bar.py[line:274] - INFO: epoch 001:  21490 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.176, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3229, wps=100.9, ups=0.61, wpb=109.5, bsz=40, num_updates=21460, lr=4.09063e-05, gnorm=0.535, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=117760
2023-01-10 22:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:30 - progress_bar.py[line:274] - INFO: epoch 001:  21500 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4118, wps=103, ups=0.62, wpb=110.4, bsz=40, num_updates=21470, lr=4.0901e-05, gnorm=0.748, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117777
2023-01-10 22:36:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:47 - progress_bar.py[line:274] - INFO: epoch 001:  21510 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4259, wps=100.2, ups=0.62, wpb=108.3, bsz=40, num_updates=21480, lr=4.08958e-05, gnorm=0.364, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=117793
2023-01-10 22:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:36:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:04 - progress_bar.py[line:274] - INFO: epoch 001:  21520 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.37, wps=99.6, ups=0.61, wpb=109.5, bsz=40, num_updates=21490, lr=4.08906e-05, gnorm=0.529, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117810
2023-01-10 22:37:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:20 - progress_bar.py[line:274] - INFO: epoch 001:  21530 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3592, wps=100.5, ups=0.61, wpb=109.9, bsz=40, num_updates=21500, lr=4.08854e-05, gnorm=0.736, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=117826
2023-01-10 22:37:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:36 - progress_bar.py[line:274] - INFO: epoch 001:  21540 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3947, wps=102.5, ups=0.63, wpb=107.9, bsz=40, num_updates=21510, lr=4.08802e-05, gnorm=0.401, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=117842
2023-01-10 22:37:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:37:52 - progress_bar.py[line:274] - INFO: epoch 001:  21550 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4257, wps=103.7, ups=0.63, wpb=109.7, bsz=40, num_updates=21520, lr=4.0875e-05, gnorm=0.717, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117859
2023-01-10 22:37:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:09 - progress_bar.py[line:274] - INFO: epoch 001:  21560 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4712, wps=103.2, ups=0.62, wpb=110.3, bsz=40, num_updates=21530, lr=4.08698e-05, gnorm=0.491, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117875
2023-01-10 22:38:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:25 - progress_bar.py[line:274] - INFO: epoch 001:  21570 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4842, wps=103.7, ups=0.62, wpb=110.8, bsz=40, num_updates=21540, lr=4.08646e-05, gnorm=1.03, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117891
2023-01-10 22:38:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:41 - progress_bar.py[line:274] - INFO: epoch 001:  21580 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3763, wps=103.3, ups=0.62, wpb=111.6, bsz=40, num_updates=21550, lr=4.08594e-05, gnorm=0.392, clip=0, loss_scale=256, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=117908
2023-01-10 22:38:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:38:58 - progress_bar.py[line:274] - INFO: epoch 001:  21590 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4176, wps=101.7, ups=0.61, wpb=111.3, bsz=40, num_updates=21560, lr=4.08542e-05, gnorm=0.625, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=117924
2023-01-10 22:39:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:14 - progress_bar.py[line:274] - INFO: epoch 001:  21600 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3146, wps=102.9, ups=0.62, wpb=110.5, bsz=40, num_updates=21570, lr=4.0849e-05, gnorm=0.506, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117941
2023-01-10 22:39:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:31 - progress_bar.py[line:274] - INFO: epoch 001:  21610 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4479, wps=101.3, ups=0.61, wpb=110.5, bsz=40, num_updates=21580, lr=4.08438e-05, gnorm=0.333, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=117957
2023-01-10 22:39:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:47 - progress_bar.py[line:274] - INFO: epoch 001:  21620 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4545, wps=103.2, ups=0.63, wpb=108.6, bsz=40, num_updates=21590, lr=4.08385e-05, gnorm=0.841, clip=50, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117973
2023-01-10 22:39:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:39:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:03 - progress_bar.py[line:274] - INFO: epoch 001:  21630 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4375, wps=101.8, ups=0.62, wpb=108.9, bsz=40, num_updates=21600, lr=4.08333e-05, gnorm=0.634, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=117990
2023-01-10 22:40:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:20 - progress_bar.py[line:274] - INFO: epoch 001:  21640 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3402, wps=100.9, ups=0.61, wpb=109.7, bsz=40, num_updates=21610, lr=4.08281e-05, gnorm=1.661, clip=40, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=118006
2023-01-10 22:40:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:36 - progress_bar.py[line:274] - INFO: epoch 001:  21650 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4623, wps=101.2, ups=0.61, wpb=110, bsz=40, num_updates=21620, lr=4.08229e-05, gnorm=0.839, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=118023
2023-01-10 22:40:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:40:53 - progress_bar.py[line:274] - INFO: epoch 001:  21660 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5047, wps=98.9, ups=0.6, wpb=109.3, bsz=40, num_updates=21630, lr=4.08177e-05, gnorm=0.687, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=118039
2023-01-10 22:40:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:10 - progress_bar.py[line:274] - INFO: epoch 001:  21670 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4272, wps=101.9, ups=0.62, wpb=110.3, bsz=40, num_updates=21640, lr=4.08125e-05, gnorm=0.635, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=118056
2023-01-10 22:41:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:26 - progress_bar.py[line:274] - INFO: epoch 001:  21680 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3366, wps=99.5, ups=0.61, wpb=108.6, bsz=40, num_updates=21650, lr=4.08073e-05, gnorm=0.55, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=118073
2023-01-10 22:41:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:43 - progress_bar.py[line:274] - INFO: epoch 001:  21690 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3878, wps=102.1, ups=0.61, wpb=110.8, bsz=40, num_updates=21660, lr=4.08021e-05, gnorm=1.324, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=118089
2023-01-10 22:41:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:41:59 - progress_bar.py[line:274] - INFO: epoch 001:  21700 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3704, wps=101.5, ups=0.62, wpb=109.3, bsz=40, num_updates=21670, lr=4.07969e-05, gnorm=0.408, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=118106
2023-01-10 22:42:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:16 - progress_bar.py[line:274] - INFO: epoch 001:  21710 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4151, wps=98.2, ups=0.6, wpb=108.7, bsz=40, num_updates=21680, lr=4.07917e-05, gnorm=0.478, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=118123
2023-01-10 22:42:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:33 - progress_bar.py[line:274] - INFO: epoch 001:  21720 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.494, wps=101.9, ups=0.61, wpb=111.3, bsz=40, num_updates=21690, lr=4.07865e-05, gnorm=0.298, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=118139
2023-01-10 22:42:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:51 - progress_bar.py[line:274] - INFO: epoch 001:  21730 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4176, wps=97.2, ups=0.58, wpb=110.8, bsz=40, num_updates=21700, lr=4.07813e-05, gnorm=0.385, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=118157
2023-01-10 22:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:42:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:08 - progress_bar.py[line:274] - INFO: epoch 001:  21740 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4796, wps=98.6, ups=0.59, wpb=110.6, bsz=40, num_updates=21710, lr=4.0776e-05, gnorm=0.374, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=118174
2023-01-10 22:43:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:25 - progress_bar.py[line:274] - INFO: epoch 001:  21750 / 100000 loss=0.333, loss_v1=0, loss_v2=0, nll_loss=0.183, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3486, wps=101.2, ups=0.61, wpb=110.3, bsz=40, num_updates=21720, lr=4.07708e-05, gnorm=0.538, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=118191
2023-01-10 22:43:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:41 - progress_bar.py[line:274] - INFO: epoch 001:  21760 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4455, wps=102.8, ups=0.62, wpb=110.9, bsz=40, num_updates=21730, lr=4.07656e-05, gnorm=0.86, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=118207
2023-01-10 22:43:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:43:58 - progress_bar.py[line:274] - INFO: epoch 001:  21770 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3854, wps=99.7, ups=0.6, wpb=110.1, bsz=40, num_updates=21740, lr=4.07604e-05, gnorm=0.396, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=118224
2023-01-10 22:44:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:15 - progress_bar.py[line:274] - INFO: epoch 001:  21780 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.42, wps=100.6, ups=0.6, wpb=111.3, bsz=40, num_updates=21750, lr=4.07552e-05, gnorm=0.543, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=118241
2023-01-10 22:44:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:32 - progress_bar.py[line:274] - INFO: epoch 001:  21790 / 100000 loss=0.332, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.3839, wps=99.5, ups=0.61, wpb=109.1, bsz=40, num_updates=21760, lr=4.075e-05, gnorm=0.88, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=118258
2023-01-10 22:44:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:49 - progress_bar.py[line:274] - INFO: epoch 001:  21800 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4752, wps=99, ups=0.6, wpb=109.7, bsz=40, num_updates=21770, lr=4.07448e-05, gnorm=0.409, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=118275
2023-01-10 22:44:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:06 - progress_bar.py[line:274] - INFO: epoch 001:  21810 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3824, wps=99.8, ups=0.6, wpb=110.1, bsz=40, num_updates=21780, lr=4.07396e-05, gnorm=0.628, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=118292
2023-01-10 22:45:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:23 - progress_bar.py[line:274] - INFO: epoch 001:  21820 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.52, wps=98.4, ups=0.6, wpb=108.5, bsz=40, num_updates=21790, lr=4.07344e-05, gnorm=0.299, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=118309
2023-01-10 22:45:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:40 - progress_bar.py[line:274] - INFO: epoch 001:  21830 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3889, wps=101.2, ups=0.61, wpb=110.4, bsz=40, num_updates=21800, lr=4.07292e-05, gnorm=0.468, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=118326
2023-01-10 22:45:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:45:57 - progress_bar.py[line:274] - INFO: epoch 001:  21840 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4176, wps=98.3, ups=0.6, wpb=109, bsz=40, num_updates=21810, lr=4.0724e-05, gnorm=1.064, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=118343
2023-01-10 22:46:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:13 - progress_bar.py[line:274] - INFO: epoch 001:  21850 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3469, wps=102.1, ups=0.62, wpb=109.7, bsz=40, num_updates=21820, lr=4.07187e-05, gnorm=0.435, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=118359
2023-01-10 22:46:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:31 - progress_bar.py[line:274] - INFO: epoch 001:  21860 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4545, wps=97.7, ups=0.6, wpb=109, bsz=40, num_updates=21830, lr=4.07135e-05, gnorm=0.472, clip=20, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=118377
2023-01-10 22:46:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:47 - progress_bar.py[line:274] - INFO: epoch 001:  21870 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4382, wps=103.9, ups=0.62, wpb=111.3, bsz=40, num_updates=21840, lr=4.07083e-05, gnorm=1.362, clip=50, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=118393
2023-01-10 22:46:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:46:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:04 - progress_bar.py[line:274] - INFO: epoch 001:  21880 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4216, wps=98.4, ups=0.59, wpb=110.6, bsz=40, num_updates=21850, lr=4.07031e-05, gnorm=1.286, clip=20, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=118410
2023-01-10 22:47:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:21 - progress_bar.py[line:274] - INFO: epoch 001:  21890 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.402, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=21860, lr=4.06979e-05, gnorm=0.621, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=118427
2023-01-10 22:47:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:37 - progress_bar.py[line:274] - INFO: epoch 001:  21900 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4902, wps=103.4, ups=0.63, wpb=108.7, bsz=40, num_updates=21870, lr=4.06927e-05, gnorm=0.521, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=118443
2023-01-10 22:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:47:54 - progress_bar.py[line:274] - INFO: epoch 001:  21910 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=107, nsentences=40, sample_size=107, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4649, wps=97.4, ups=0.61, wpb=107, bsz=40, num_updates=21880, lr=4.06875e-05, gnorm=0.637, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=118460
2023-01-10 22:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:11 - progress_bar.py[line:274] - INFO: epoch 001:  21920 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4898, wps=102.2, ups=0.62, wpb=110.4, bsz=40, num_updates=21890, lr=4.06823e-05, gnorm=0.427, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=118477
2023-01-10 22:48:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:27 - progress_bar.py[line:274] - INFO: epoch 001:  21930 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3913, wps=100.5, ups=0.61, wpb=109.3, bsz=40, num_updates=21900, lr=4.06771e-05, gnorm=0.608, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=118494
2023-01-10 22:48:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:44 - progress_bar.py[line:274] - INFO: epoch 001:  21940 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4587, wps=99.1, ups=0.6, wpb=109.6, bsz=40, num_updates=21910, lr=4.06719e-05, gnorm=0.826, clip=30, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=118510
2023-01-10 22:48:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:48:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:01 - progress_bar.py[line:274] - INFO: epoch 001:  21950 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4476, wps=100, ups=0.6, wpb=110.9, bsz=40, num_updates=21920, lr=4.06667e-05, gnorm=0.494, clip=20, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=118528
2023-01-10 22:49:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:18 - progress_bar.py[line:274] - INFO: epoch 001:  21960 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4, wps=100.3, ups=0.61, wpb=109.6, bsz=40, num_updates=21930, lr=4.06615e-05, gnorm=0.49, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=118544
2023-01-10 22:49:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:35 - progress_bar.py[line:274] - INFO: epoch 001:  21970 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4231, wps=99.5, ups=0.6, wpb=109.7, bsz=40, num_updates=21940, lr=4.06563e-05, gnorm=0.553, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=118561
2023-01-10 22:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:49:52 - progress_bar.py[line:274] - INFO: epoch 001:  21980 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.43, wps=100.4, ups=0.61, wpb=110.3, bsz=40, num_updates=21950, lr=4.0651e-05, gnorm=0.497, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=118578
2023-01-10 22:49:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:09 - progress_bar.py[line:274] - INFO: epoch 001:  21990 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4583, wps=99.2, ups=0.6, wpb=109.9, bsz=40, num_updates=21960, lr=4.06458e-05, gnorm=0.943, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=118595
2023-01-10 22:50:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:26 - progress_bar.py[line:274] - INFO: epoch 001:  22000 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3942, wps=100.1, ups=0.62, wpb=108.2, bsz=40, num_updates=21970, lr=4.06406e-05, gnorm=0.82, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=118612
2023-01-10 22:50:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:42 - progress_bar.py[line:274] - INFO: epoch 001:  22010 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3818, wps=101.9, ups=0.62, wpb=109.1, bsz=40, num_updates=21980, lr=4.06354e-05, gnorm=0.545, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=118628
2023-01-10 22:50:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:50:59 - progress_bar.py[line:274] - INFO: epoch 001:  22020 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4286, wps=102.9, ups=0.62, wpb=110.5, bsz=40, num_updates=21990, lr=4.06302e-05, gnorm=1.491, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=118645
2023-01-10 22:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:51:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:51:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:51:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:51:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 22:51:15 - progress_bar.py[line:274] - INFO: epoch 001:  22030 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3804, wps=103.4, ups=0.63, wpb=110.2, bsz=40, num_updates=22000, lr=4.0625e-05, gnorm=0.452, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=118661
2023-01-10 22:51:15 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-10 22:51:17 - train.py[line:549] - INFO: 0 / 4988
2023-01-10 22:51:17 - train.py[line:551] - INFO: load:1.18 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-10 22:53:48 - train.py[line:549] - INFO: 200 / 4988
2023-01-10 22:53:48 - train.py[line:551] - INFO: load:1.21 valid_run:151.12 task_valid:148.18 collect_output:1.88
2023-01-10 22:56:16 - train.py[line:549] - INFO: 400 / 4988
2023-01-10 22:56:16 - train.py[line:551] - INFO: load:1.23 valid_run:298.84 task_valid:291.19 collect_output:5.56
2023-01-10 22:58:47 - train.py[line:549] - INFO: 600 / 4988
2023-01-10 22:58:47 - train.py[line:551] - INFO: load:1.26 valid_run:450.11 task_valid:434.37 collect_output:12.63
2023-01-10 23:01:16 - train.py[line:549] - INFO: 800 / 4988
2023-01-10 23:01:16 - train.py[line:551] - INFO: load:1.29 valid_run:598.42 task_valid:579.17 collect_output:15.15
2023-01-10 23:03:47 - train.py[line:549] - INFO: 1000 / 4988
2023-01-10 23:03:47 - train.py[line:551] - INFO: load:1.31 valid_run:749.94 task_valid:726.51 collect_output:18.34
2023-01-10 23:06:18 - train.py[line:549] - INFO: 1200 / 4988
2023-01-10 23:06:18 - train.py[line:551] - INFO: load:1.34 valid_run:901.11 task_valid:872.26 collect_output:22.67
2023-01-10 23:08:51 - train.py[line:549] - INFO: 1400 / 4988
2023-01-10 23:08:51 - train.py[line:551] - INFO: load:1.37 valid_run:1053.48 task_valid:1019.18 collect_output:27.08
2023-01-10 23:11:21 - train.py[line:549] - INFO: 1600 / 4988
2023-01-10 23:11:21 - train.py[line:551] - INFO: load:1.40 valid_run:1203.25 task_valid:1160.52 collect_output:34.45
2023-01-10 23:13:49 - train.py[line:549] - INFO: 1800 / 4988
2023-01-10 23:13:49 - train.py[line:551] - INFO: load:1.42 valid_run:1351.96 task_valid:1305.39 collect_output:37.24
2023-01-10 23:16:17 - train.py[line:549] - INFO: 2000 / 4988
2023-01-10 23:16:17 - train.py[line:551] - INFO: load:1.45 valid_run:1499.68 task_valid:1448.51 collect_output:40.82
2023-01-10 23:18:47 - train.py[line:549] - INFO: 2200 / 4988
2023-01-10 23:18:47 - train.py[line:551] - INFO: load:1.47 valid_run:1649.05 task_valid:1593.79 collect_output:43.81
2023-01-10 23:21:16 - train.py[line:549] - INFO: 2400 / 4988
2023-01-10 23:21:16 - train.py[line:551] - INFO: load:1.50 valid_run:1798.18 task_valid:1738.85 collect_output:46.85
2023-01-10 23:23:45 - train.py[line:549] - INFO: 2600 / 4988
2023-01-10 23:23:45 - train.py[line:551] - INFO: load:1.53 valid_run:1946.95 task_valid:1880.77 collect_output:52.68
2023-01-10 23:26:15 - train.py[line:549] - INFO: 2800 / 4988
2023-01-10 23:26:15 - train.py[line:551] - INFO: load:1.55 valid_run:2097.14 task_valid:2026.60 collect_output:55.97
2023-01-10 23:28:45 - train.py[line:549] - INFO: 3000 / 4988
2023-01-10 23:28:45 - train.py[line:551] - INFO: load:1.58 valid_run:2246.90 task_valid:2173.36 collect_output:57.92
2023-01-10 23:31:14 - train.py[line:549] - INFO: 3200 / 4988
2023-01-10 23:31:14 - train.py[line:551] - INFO: load:1.60 valid_run:2396.13 task_valid:2317.63 collect_output:61.82
2023-01-10 23:33:45 - train.py[line:549] - INFO: 3400 / 4988
2023-01-10 23:33:45 - train.py[line:551] - INFO: load:1.63 valid_run:2546.55 task_valid:2463.30 collect_output:65.54
2023-01-10 23:36:15 - train.py[line:549] - INFO: 3600 / 4988
2023-01-10 23:36:15 - train.py[line:551] - INFO: load:1.66 valid_run:2696.73 task_valid:2610.43 collect_output:67.56
2023-01-10 23:38:43 - train.py[line:549] - INFO: 3800 / 4988
2023-01-10 23:38:43 - train.py[line:551] - INFO: load:1.68 valid_run:2844.21 task_valid:2752.35 collect_output:72.07
2023-01-10 23:41:12 - train.py[line:549] - INFO: 4000 / 4988
2023-01-10 23:41:12 - train.py[line:551] - INFO: load:1.71 valid_run:2993.57 task_valid:2897.71 collect_output:75.04
2023-01-10 23:43:42 - train.py[line:549] - INFO: 4200 / 4988
2023-01-10 23:43:42 - train.py[line:551] - INFO: load:1.73 valid_run:3143.93 task_valid:3042.47 collect_output:79.60
2023-01-10 23:46:11 - train.py[line:549] - INFO: 4400 / 4988
2023-01-10 23:46:11 - train.py[line:551] - INFO: load:1.76 valid_run:3292.72 task_valid:3187.13 collect_output:82.69
2023-01-10 23:48:42 - train.py[line:549] - INFO: 4600 / 4988
2023-01-10 23:48:42 - train.py[line:551] - INFO: load:1.78 valid_run:3443.02 task_valid:3333.60 collect_output:85.50
2023-01-10 23:51:12 - train.py[line:549] - INFO: 4800 / 4988
2023-01-10 23:51:12 - train.py[line:551] - INFO: load:1.81 valid_run:3593.48 task_valid:3480.12 collect_output:88.41

====================================================================================================
SGG eval:     R @ 50: 0.4787;     R @ 100: 0.5652;     R @ 500: 0.6134;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3009;    mR @ 100: 0.3529;    mR @ 500: 0.4206;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6875) (covering:0.3714) (eating:0.6765) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3903) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7708) (playing:0.0000) (riding:0.7654) (says:0.0000) (sitting on:0.7276) (standing on:0.2360) (using:0.6000) (walking in:0.0000) (walking on:0.3243) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4787;     R @ 100: 0.5652;     R @ 500: 0.6134;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.3009;    mR @ 100: 0.3529;    mR @ 500: 0.4206;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6875) (covering:0.3714) (eating:0.6765) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3903) (lying on:0.1000) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7708) (playing:0.0000) (riding:0.7654) (says:0.0000) (sitting on:0.7276) (standing on:0.2360) (using:0.6000) (walking in:0.0000) (walking on:0.3243) (watching:0.3472) 
--------------------------------------------------------
====================================================================================================

2023-01-10 23:53:43 - train.py[line:487] - INFO: 0.5652291316526611
2023-01-10 23:53:43 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-10 23:53:43 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.353 | loss_v1 0 | loss_v2 0 | nll_loss 0.199 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.565229 | ppl 1.15 | vqa_score 0.527 | wps 119.7 | wpb 89.9 | bsz 30 | num_updates 22000 | best_R@100 0.69005
2023-01-10 23:53:43 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 22000 updates
2023-01-10 23:53:44 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_22000.pt
2023-01-10 23:54:23 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_22000.pt
2023-01-10 23:55:49 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_22000.pt (epoch 1 @ 22000 updates, score 0.5652291316526611) (writing took 125.41305693611503 seconds)
2023-01-10 23:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:55:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:55:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:05 - progress_bar.py[line:274] - INFO: epoch 001:  22040 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3579, wps=0.4, ups=0, wpb=110.1, bsz=40, num_updates=22010, lr=4.06198e-05, gnorm=0.69, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=122551
2023-01-10 23:56:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:22 - progress_bar.py[line:274] - INFO: epoch 001:  22050 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4333, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=22020, lr=4.06146e-05, gnorm=0.635, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=122568
2023-01-10 23:56:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:38 - progress_bar.py[line:274] - INFO: epoch 001:  22060 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4316, wps=100.2, ups=0.61, wpb=110.3, bsz=40, num_updates=22030, lr=4.06094e-05, gnorm=0.53, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=122585
2023-01-10 23:56:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:56:55 - progress_bar.py[line:274] - INFO: epoch 001:  22070 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.47, wps=100.4, ups=0.61, wpb=110.1, bsz=40, num_updates=22040, lr=4.06042e-05, gnorm=0.869, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=122601
2023-01-10 23:57:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:12 - progress_bar.py[line:274] - INFO: epoch 001:  22080 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4423, wps=100.3, ups=0.61, wpb=110.2, bsz=40, num_updates=22050, lr=4.0599e-05, gnorm=0.447, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=122618
2023-01-10 23:57:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:28 - progress_bar.py[line:274] - INFO: epoch 001:  22090 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4159, wps=101.7, ups=0.62, wpb=109.7, bsz=40, num_updates=22060, lr=4.05938e-05, gnorm=0.428, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=122635
2023-01-10 23:57:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:45 - progress_bar.py[line:274] - INFO: epoch 001:  22100 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4444, wps=102.7, ups=0.62, wpb=110, bsz=40, num_updates=22070, lr=4.05885e-05, gnorm=0.445, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=122651
2023-01-10 23:57:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:57:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:01 - progress_bar.py[line:274] - INFO: epoch 001:  22110 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3918, wps=99.7, ups=0.61, wpb=109.3, bsz=40, num_updates=22080, lr=4.05833e-05, gnorm=0.451, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=122668
2023-01-10 23:58:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:18 - progress_bar.py[line:274] - INFO: epoch 001:  22120 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4211, wps=102.4, ups=0.62, wpb=109.7, bsz=40, num_updates=22090, lr=4.05781e-05, gnorm=0.268, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=122684
2023-01-10 23:58:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:35 - progress_bar.py[line:274] - INFO: epoch 001:  22130 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5408, wps=100.8, ups=0.61, wpb=110.5, bsz=40, num_updates=22100, lr=4.05729e-05, gnorm=0.341, clip=10, loss_scale=512, train_wall=16, gb_free=10.9, ema_decay=0.9999, wall=122701
2023-01-10 23:58:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:51 - progress_bar.py[line:274] - INFO: epoch 001:  22140 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4747, wps=101.8, ups=0.61, wpb=111.3, bsz=40, num_updates=22110, lr=4.05677e-05, gnorm=0.517, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=122718
2023-01-10 23:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:58:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:08 - progress_bar.py[line:274] - INFO: epoch 001:  22150 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4468, wps=98.5, ups=0.6, wpb=108.7, bsz=40, num_updates=22120, lr=4.05625e-05, gnorm=0.383, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=122734
2023-01-10 23:59:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:25 - progress_bar.py[line:274] - INFO: epoch 001:  22160 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5052, wps=100.7, ups=0.61, wpb=110.2, bsz=40, num_updates=22130, lr=4.05573e-05, gnorm=0.33, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=122751
2023-01-10 23:59:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:41 - progress_bar.py[line:274] - INFO: epoch 001:  22170 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.38, wps=100.7, ups=0.61, wpb=109.7, bsz=40, num_updates=22140, lr=4.05521e-05, gnorm=0.321, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=122768
2023-01-10 23:59:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-10 23:59:59 - progress_bar.py[line:274] - INFO: epoch 001:  22180 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3956, wps=97.9, ups=0.59, wpb=109.7, bsz=40, num_updates=22150, lr=4.05469e-05, gnorm=0.591, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=122785
2023-01-11 00:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:11 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 00:00:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:16 - progress_bar.py[line:274] - INFO: epoch 001:  22191 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=107.333, nsentences=40, sample_size=107.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4537, wps=91.8, ups=0.57, wpb=107.3, bsz=40, num_updates=22160, lr=4.05417e-05, gnorm=0.276, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=122802
2023-01-11 00:00:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:33 - progress_bar.py[line:274] - INFO: epoch 001:  22201 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4476, wps=99.6, ups=0.61, wpb=108.3, bsz=40, num_updates=22170, lr=4.05365e-05, gnorm=2.102, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=122819
2023-01-11 00:00:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:49 - progress_bar.py[line:274] - INFO: epoch 001:  22211 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4894, wps=99.8, ups=0.61, wpb=108.7, bsz=40, num_updates=22180, lr=4.05313e-05, gnorm=0.394, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=122836
2023-01-11 00:00:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:00:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:06 - progress_bar.py[line:274] - INFO: epoch 001:  22221 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4592, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=22190, lr=4.0526e-05, gnorm=0.484, clip=20, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=122852
2023-01-11 00:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:22 - progress_bar.py[line:274] - INFO: epoch 001:  22231 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3667, wps=104.8, ups=0.63, wpb=111.5, bsz=40, num_updates=22200, lr=4.05208e-05, gnorm=0.602, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=122868
2023-01-11 00:01:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:39 - progress_bar.py[line:274] - INFO: epoch 001:  22241 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.41, wps=101.4, ups=0.61, wpb=111, bsz=40, num_updates=22210, lr=4.05156e-05, gnorm=0.314, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=122885
2023-01-11 00:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:01:56 - progress_bar.py[line:274] - INFO: epoch 001:  22251 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3977, wps=99.4, ups=0.6, wpb=110.3, bsz=40, num_updates=22220, lr=4.05104e-05, gnorm=0.341, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=122902
2023-01-11 00:02:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:12 - progress_bar.py[line:274] - INFO: epoch 001:  22261 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4466, wps=100.4, ups=0.62, wpb=108.5, bsz=40, num_updates=22230, lr=4.05052e-05, gnorm=0.515, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=122918
2023-01-11 00:02:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:29 - progress_bar.py[line:274] - INFO: epoch 001:  22271 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3981, wps=98.5, ups=0.6, wpb=109.7, bsz=40, num_updates=22240, lr=4.05e-05, gnorm=0.448, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=122935
2023-01-11 00:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:46 - progress_bar.py[line:274] - INFO: epoch 001:  22281 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.171, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4273, wps=100.8, ups=0.62, wpb=108.3, bsz=40, num_updates=22250, lr=4.04948e-05, gnorm=0.405, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=122952
2023-01-11 00:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:02 - progress_bar.py[line:274] - INFO: epoch 001:  22291 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4854, wps=99, ups=0.6, wpb=110.1, bsz=40, num_updates=22260, lr=4.04896e-05, gnorm=0.634, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=122969
2023-01-11 00:03:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:19 - progress_bar.py[line:274] - INFO: epoch 001:  22301 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4021, wps=101.6, ups=0.62, wpb=110.1, bsz=40, num_updates=22270, lr=4.04844e-05, gnorm=0.631, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=122985
2023-01-11 00:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:35 - progress_bar.py[line:274] - INFO: epoch 001:  22311 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4135, wps=102.3, ups=0.63, wpb=108.6, bsz=40, num_updates=22280, lr=4.04792e-05, gnorm=0.274, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=123001
2023-01-11 00:03:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:51 - progress_bar.py[line:274] - INFO: epoch 001:  22321 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4316, wps=101.6, ups=0.62, wpb=109.3, bsz=40, num_updates=22290, lr=4.0474e-05, gnorm=0.533, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=123018
2023-01-11 00:03:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:03:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:08 - progress_bar.py[line:274] - INFO: epoch 001:  22331 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4245, wps=99.5, ups=0.61, wpb=109.3, bsz=40, num_updates=22300, lr=4.04687e-05, gnorm=0.364, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123034
2023-01-11 00:04:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:25 - progress_bar.py[line:274] - INFO: epoch 001:  22341 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4216, wps=98, ups=0.6, wpb=109.1, bsz=40, num_updates=22310, lr=4.04635e-05, gnorm=0.799, clip=20, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=123051
2023-01-11 00:04:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:42 - progress_bar.py[line:274] - INFO: epoch 001:  22351 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4343, wps=98.7, ups=0.6, wpb=110.1, bsz=40, num_updates=22320, lr=4.04583e-05, gnorm=0.665, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=123068
2023-01-11 00:04:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:04:59 - progress_bar.py[line:274] - INFO: epoch 001:  22361 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4845, wps=99.6, ups=0.61, wpb=109.3, bsz=40, num_updates=22330, lr=4.04531e-05, gnorm=0.273, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123085
2023-01-11 00:05:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:15 - progress_bar.py[line:274] - INFO: epoch 001:  22371 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4086, wps=99, ups=0.61, wpb=108.9, bsz=40, num_updates=22340, lr=4.04479e-05, gnorm=0.327, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=123102
2023-01-11 00:05:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:32 - progress_bar.py[line:274] - INFO: epoch 001:  22381 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4792, wps=103.9, ups=0.63, wpb=110.8, bsz=40, num_updates=22350, lr=4.04427e-05, gnorm=0.642, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=123118
2023-01-11 00:05:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:49 - progress_bar.py[line:274] - INFO: epoch 001:  22391 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.402, wps=99.2, ups=0.6, wpb=110.5, bsz=40, num_updates=22360, lr=4.04375e-05, gnorm=0.346, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=123135
2023-01-11 00:05:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:05:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:05 - progress_bar.py[line:274] - INFO: epoch 001:  22401 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4526, wps=99.7, ups=0.61, wpb=109.3, bsz=40, num_updates=22370, lr=4.04323e-05, gnorm=0.43, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123151
2023-01-11 00:06:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:21 - progress_bar.py[line:274] - INFO: epoch 001:  22411 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5109, wps=104.2, ups=0.63, wpb=110.7, bsz=40, num_updates=22380, lr=4.04271e-05, gnorm=0.568, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123168
2023-01-11 00:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:38 - progress_bar.py[line:274] - INFO: epoch 001:  22421 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4316, wps=104.6, ups=0.63, wpb=110.9, bsz=40, num_updates=22390, lr=4.04219e-05, gnorm=0.416, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123184
2023-01-11 00:06:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:06:54 - progress_bar.py[line:274] - INFO: epoch 001:  22431 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3714, wps=101.3, ups=0.62, wpb=108.9, bsz=40, num_updates=22400, lr=4.04167e-05, gnorm=0.329, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=123200
2023-01-11 00:06:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:11 - progress_bar.py[line:274] - INFO: epoch 001:  22441 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3663, wps=100.5, ups=0.61, wpb=110, bsz=40, num_updates=22410, lr=4.04115e-05, gnorm=0.759, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=123217
2023-01-11 00:07:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:27 - progress_bar.py[line:274] - INFO: epoch 001:  22451 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.5043, wps=98.2, ups=0.61, wpb=107.7, bsz=40, num_updates=22420, lr=4.04063e-05, gnorm=0.743, clip=20, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=123233
2023-01-11 00:07:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:44 - progress_bar.py[line:274] - INFO: epoch 001:  22461 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.2947, wps=103, ups=0.62, wpb=110.9, bsz=40, num_updates=22430, lr=4.0401e-05, gnorm=0.604, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=123250
2023-01-11 00:07:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:07:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:00 - progress_bar.py[line:274] - INFO: epoch 001:  22471 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4579, wps=102, ups=0.62, wpb=109.9, bsz=40, num_updates=22440, lr=4.03958e-05, gnorm=0.735, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=123266
2023-01-11 00:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:17 - progress_bar.py[line:274] - INFO: epoch 001:  22481 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4409, wps=99.2, ups=0.61, wpb=109.3, bsz=40, num_updates=22450, lr=4.03906e-05, gnorm=0.517, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=123283
2023-01-11 00:08:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:33 - progress_bar.py[line:274] - INFO: epoch 001:  22491 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4495, wps=100.5, ups=0.61, wpb=109.7, bsz=40, num_updates=22460, lr=4.03854e-05, gnorm=1.603, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123300
2023-01-11 00:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:50 - progress_bar.py[line:274] - INFO: epoch 001:  22501 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4432, wps=101.1, ups=0.61, wpb=110.7, bsz=40, num_updates=22470, lr=4.03802e-05, gnorm=0.621, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123316
2023-01-11 00:08:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:08:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:07 - progress_bar.py[line:274] - INFO: epoch 001:  22511 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4286, wps=102.3, ups=0.62, wpb=110.7, bsz=40, num_updates=22480, lr=4.0375e-05, gnorm=0.59, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=123333
2023-01-11 00:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:23 - progress_bar.py[line:274] - INFO: epoch 001:  22521 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4316, wps=100.3, ups=0.61, wpb=109.7, bsz=40, num_updates=22490, lr=4.03698e-05, gnorm=0.391, clip=0, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=123349
2023-01-11 00:09:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:40 - progress_bar.py[line:274] - INFO: epoch 001:  22531 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4486, wps=101.2, ups=0.61, wpb=110.3, bsz=40, num_updates=22500, lr=4.03646e-05, gnorm=1.097, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=123366
2023-01-11 00:09:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:09:56 - progress_bar.py[line:274] - INFO: epoch 001:  22541 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=103.5, ups=0.62, wpb=111.1, bsz=40, num_updates=22510, lr=4.03594e-05, gnorm=0.635, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123382
2023-01-11 00:10:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:13 - progress_bar.py[line:274] - INFO: epoch 001:  22551 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.44, wps=103.6, ups=0.62, wpb=110.7, bsz=40, num_updates=22520, lr=4.03542e-05, gnorm=0.813, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=123399
2023-01-11 00:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:29 - progress_bar.py[line:274] - INFO: epoch 001:  22561 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4368, wps=101.9, ups=0.61, wpb=110.7, bsz=40, num_updates=22530, lr=4.0349e-05, gnorm=0.846, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=123415
2023-01-11 00:10:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:46 - progress_bar.py[line:274] - INFO: epoch 001:  22571 / 100000 loss=0.335, loss_v1=0, loss_v2=0, nll_loss=0.184, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4653, wps=101.7, ups=0.62, wpb=109.8, bsz=40, num_updates=22540, lr=4.03438e-05, gnorm=1.748, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=123432
2023-01-11 00:10:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:10:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:02 - progress_bar.py[line:274] - INFO: epoch 001:  22581 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4375, wps=102, ups=0.62, wpb=109.7, bsz=40, num_updates=22550, lr=4.03385e-05, gnorm=0.816, clip=20, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=123448
2023-01-11 00:11:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:19 - progress_bar.py[line:274] - INFO: epoch 001:  22591 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3939, wps=99.1, ups=0.6, wpb=109.8, bsz=40, num_updates=22560, lr=4.03333e-05, gnorm=0.369, clip=0, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=123465
2023-01-11 00:11:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:35 - progress_bar.py[line:274] - INFO: epoch 001:  22601 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4211, wps=101.1, ups=0.61, wpb=110.4, bsz=40, num_updates=22570, lr=4.03281e-05, gnorm=0.691, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=123482
2023-01-11 00:11:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:52 - progress_bar.py[line:274] - INFO: epoch 001:  22611 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=103, ups=0.63, wpb=109.7, bsz=40, num_updates=22580, lr=4.03229e-05, gnorm=0.312, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=123498
2023-01-11 00:11:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:11:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:08 - progress_bar.py[line:274] - INFO: epoch 001:  22621 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3441, wps=100.8, ups=0.62, wpb=108.9, bsz=40, num_updates=22590, lr=4.03177e-05, gnorm=0.517, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=123514
2023-01-11 00:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:24 - progress_bar.py[line:274] - INFO: epoch 001:  22631 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4314, wps=102.2, ups=0.62, wpb=110.7, bsz=40, num_updates=22600, lr=4.03125e-05, gnorm=2.53, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=123531
2023-01-11 00:12:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:41 - progress_bar.py[line:274] - INFO: epoch 001:  22641 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.5, wps=103.5, ups=0.62, wpb=110.9, bsz=40, num_updates=22610, lr=4.03073e-05, gnorm=0.414, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=123547
2023-01-11 00:12:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:12:58 - progress_bar.py[line:274] - INFO: epoch 001:  22651 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4563, wps=101.2, ups=0.61, wpb=111.3, bsz=40, num_updates=22620, lr=4.03021e-05, gnorm=0.408, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123564
2023-01-11 00:13:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:14 - progress_bar.py[line:274] - INFO: epoch 001:  22661 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4563, wps=99.9, ups=0.61, wpb=108.4, bsz=40, num_updates=22630, lr=4.02969e-05, gnorm=0.651, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=123580
2023-01-11 00:13:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:31 - progress_bar.py[line:274] - INFO: epoch 001:  22671 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4167, wps=100.6, ups=0.61, wpb=110.3, bsz=40, num_updates=22640, lr=4.02917e-05, gnorm=1.052, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123597
2023-01-11 00:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:48 - progress_bar.py[line:274] - INFO: epoch 001:  22681 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3902, wps=102.8, ups=0.61, wpb=112.2, bsz=40, num_updates=22650, lr=4.02865e-05, gnorm=0.403, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=123614
2023-01-11 00:13:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:13:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:05 - progress_bar.py[line:274] - INFO: epoch 001:  22691 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4624, wps=99.3, ups=0.6, wpb=109.9, bsz=40, num_updates=22660, lr=4.02813e-05, gnorm=0.447, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=123631
2023-01-11 00:14:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:21 - progress_bar.py[line:274] - INFO: epoch 001:  22701 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5664, wps=101.4, ups=0.62, wpb=108.7, bsz=40, num_updates=22670, lr=4.0276e-05, gnorm=0.4, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123647
2023-01-11 00:14:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:37 - progress_bar.py[line:274] - INFO: epoch 001:  22711 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.404, wps=101.2, ups=0.62, wpb=109.6, bsz=40, num_updates=22680, lr=4.02708e-05, gnorm=1.001, clip=30, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=123664
2023-01-11 00:14:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:14:54 - progress_bar.py[line:274] - INFO: epoch 001:  22721 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4712, wps=101.3, ups=0.62, wpb=108.3, bsz=40, num_updates=22690, lr=4.02656e-05, gnorm=0.507, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=123680
2023-01-11 00:14:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:10 - progress_bar.py[line:274] - INFO: epoch 001:  22731 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4343, wps=102.1, ups=0.62, wpb=110.4, bsz=40, num_updates=22700, lr=4.02604e-05, gnorm=0.529, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123696
2023-01-11 00:15:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:27 - progress_bar.py[line:274] - INFO: epoch 001:  22741 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4943, wps=101.4, ups=0.61, wpb=110.9, bsz=40, num_updates=22710, lr=4.02552e-05, gnorm=0.599, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=123713
2023-01-11 00:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:43 - progress_bar.py[line:274] - INFO: epoch 001:  22751 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3962, wps=100.4, ups=0.61, wpb=109.3, bsz=40, num_updates=22720, lr=4.025e-05, gnorm=0.527, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123730
2023-01-11 00:15:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:15:59 - progress_bar.py[line:274] - INFO: epoch 001:  22761 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4151, wps=102.1, ups=0.63, wpb=107.9, bsz=40, num_updates=22730, lr=4.02448e-05, gnorm=0.39, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123746
2023-01-11 00:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:16 - progress_bar.py[line:274] - INFO: epoch 001:  22771 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3776, wps=101.6, ups=0.62, wpb=109.3, bsz=40, num_updates=22740, lr=4.02396e-05, gnorm=0.622, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=123762
2023-01-11 00:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:33 - progress_bar.py[line:274] - INFO: epoch 001:  22781 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4286, wps=99.7, ups=0.6, wpb=110.3, bsz=40, num_updates=22750, lr=4.02344e-05, gnorm=0.802, clip=20, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=123779
2023-01-11 00:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:49 - progress_bar.py[line:274] - INFO: epoch 001:  22791 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4167, wps=99.3, ups=0.61, wpb=108.3, bsz=40, num_updates=22760, lr=4.02292e-05, gnorm=0.684, clip=30, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=123795
2023-01-11 00:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:16:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:06 - progress_bar.py[line:274] - INFO: epoch 001:  22801 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3956, wps=102.4, ups=0.62, wpb=110.1, bsz=40, num_updates=22770, lr=4.0224e-05, gnorm=0.691, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=123812
2023-01-11 00:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:23 - progress_bar.py[line:274] - INFO: epoch 001:  22811 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3977, wps=98.1, ups=0.6, wpb=109.4, bsz=40, num_updates=22780, lr=4.02188e-05, gnorm=0.313, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=123829
2023-01-11 00:17:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:39 - progress_bar.py[line:274] - INFO: epoch 001:  22821 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4954, wps=102, ups=0.62, wpb=108.9, bsz=40, num_updates=22790, lr=4.02135e-05, gnorm=0.601, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=123845
2023-01-11 00:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:17:56 - progress_bar.py[line:274] - INFO: epoch 001:  22831 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4301, wps=98.9, ups=0.6, wpb=109.3, bsz=40, num_updates=22800, lr=4.02083e-05, gnorm=0.651, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=123862
2023-01-11 00:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:12 - progress_bar.py[line:274] - INFO: epoch 001:  22841 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4583, wps=101.4, ups=0.61, wpb=111.4, bsz=40, num_updates=22810, lr=4.02031e-05, gnorm=0.465, clip=0, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=123879
2023-01-11 00:18:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:28 - progress_bar.py[line:274] - INFO: epoch 001:  22851 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.396, wps=104.8, ups=0.63, wpb=110.1, bsz=40, num_updates=22820, lr=4.01979e-05, gnorm=0.831, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123895
2023-01-11 00:18:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:45 - progress_bar.py[line:274] - INFO: epoch 001:  22861 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4078, wps=100.7, ups=0.61, wpb=110.2, bsz=40, num_updates=22830, lr=4.01927e-05, gnorm=0.815, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=123911
2023-01-11 00:18:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:18:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:02 - progress_bar.py[line:274] - INFO: epoch 001:  22871 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4946, wps=100.6, ups=0.61, wpb=110.3, bsz=40, num_updates=22840, lr=4.01875e-05, gnorm=0.484, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=123928
2023-01-11 00:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:19 - progress_bar.py[line:274] - INFO: epoch 001:  22881 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4747, wps=99.4, ups=0.61, wpb=108.8, bsz=40, num_updates=22850, lr=4.01823e-05, gnorm=0.451, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=123945
2023-01-11 00:19:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:35 - progress_bar.py[line:274] - INFO: epoch 001:  22891 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4257, wps=106.5, ups=0.64, wpb=110.4, bsz=40, num_updates=22860, lr=4.01771e-05, gnorm=0.342, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=123961
2023-01-11 00:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:51 - progress_bar.py[line:274] - INFO: epoch 001:  22901 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4889, wps=102.7, ups=0.62, wpb=111.1, bsz=40, num_updates=22870, lr=4.01719e-05, gnorm=0.833, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=123977
2023-01-11 00:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:19:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:08 - progress_bar.py[line:274] - INFO: epoch 001:  22911 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3441, wps=100.6, ups=0.62, wpb=108.9, bsz=40, num_updates=22880, lr=4.01667e-05, gnorm=0.355, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=123994
2023-01-11 00:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:24 - progress_bar.py[line:274] - INFO: epoch 001:  22921 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5049, wps=101.5, ups=0.61, wpb=110.9, bsz=40, num_updates=22890, lr=4.01615e-05, gnorm=0.263, clip=0, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=124010
2023-01-11 00:20:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:41 - progress_bar.py[line:274] - INFO: epoch 001:  22931 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4206, wps=100.6, ups=0.62, wpb=108.3, bsz=40, num_updates=22900, lr=4.01563e-05, gnorm=1.034, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=124027
2023-01-11 00:20:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:20:57 - progress_bar.py[line:274] - INFO: epoch 001:  22941 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4536, wps=102.4, ups=0.62, wpb=110.1, bsz=40, num_updates=22910, lr=4.0151e-05, gnorm=0.435, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=124043
2023-01-11 00:21:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:14 - progress_bar.py[line:274] - INFO: epoch 001:  22951 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.449, wps=99.9, ups=0.61, wpb=109.1, bsz=40, num_updates=22920, lr=4.01458e-05, gnorm=0.27, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=124060
2023-01-11 00:21:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:30 - progress_bar.py[line:274] - INFO: epoch 001:  22961 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4706, wps=104.1, ups=0.63, wpb=110.3, bsz=40, num_updates=22930, lr=4.01406e-05, gnorm=0.891, clip=40, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=124076
2023-01-11 00:21:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:46 - progress_bar.py[line:274] - INFO: epoch 001:  22971 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4696, wps=102.4, ups=0.63, wpb=108.4, bsz=40, num_updates=22940, lr=4.01354e-05, gnorm=0.498, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=124092
2023-01-11 00:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:21:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:02 - progress_bar.py[line:274] - INFO: epoch 001:  22981 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4176, wps=104, ups=0.63, wpb=110.3, bsz=40, num_updates=22950, lr=4.01302e-05, gnorm=0.467, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=124108
2023-01-11 00:22:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:18 - progress_bar.py[line:274] - INFO: epoch 001:  22991 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3805, wps=100.9, ups=0.62, wpb=108.9, bsz=40, num_updates=22960, lr=4.0125e-05, gnorm=0.611, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=124125
2023-01-11 00:22:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:35 - progress_bar.py[line:274] - INFO: epoch 001:  23001 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3434, wps=98.3, ups=0.6, wpb=108.9, bsz=40, num_updates=22970, lr=4.01198e-05, gnorm=0.495, clip=10, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=124141
2023-01-11 00:22:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:52 - progress_bar.py[line:274] - INFO: epoch 001:  23011 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4639, wps=101.6, ups=0.61, wpb=110.5, bsz=40, num_updates=22980, lr=4.01146e-05, gnorm=1.655, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=124158
2023-01-11 00:22:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:23:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:23:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:23:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:23:09 - progress_bar.py[line:274] - INFO: epoch 001:  23021 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4316, wps=101.2, ups=0.61, wpb=111.3, bsz=40, num_updates=22990, lr=4.01094e-05, gnorm=0.361, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=124175
2023-01-11 00:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:23:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:23:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:23:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:23:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 00:23:25 - progress_bar.py[line:274] - INFO: epoch 001:  23031 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.45, wps=100.4, ups=0.6, wpb=110.8, bsz=40, num_updates=23000, lr=4.01042e-05, gnorm=0.872, clip=40, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=124192
2023-01-11 00:23:25 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 00:23:27 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 00:23:27 - train.py[line:551] - INFO: load:1.52 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 00:25:58 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 00:25:58 - train.py[line:551] - INFO: load:1.55 valid_run:151.12 task_valid:148.24 collect_output:1.81
2023-01-11 00:28:26 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 00:28:26 - train.py[line:551] - INFO: load:1.57 valid_run:298.72 task_valid:291.44 collect_output:5.17
2023-01-11 00:30:57 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 00:30:58 - train.py[line:551] - INFO: load:1.60 valid_run:450.01 task_valid:434.63 collect_output:12.21
2023-01-11 00:33:26 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 00:33:26 - train.py[line:551] - INFO: load:1.62 valid_run:598.63 task_valid:579.94 collect_output:14.50
2023-01-11 00:35:58 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 00:35:58 - train.py[line:551] - INFO: load:1.65 valid_run:750.35 task_valid:727.42 collect_output:17.71
2023-01-11 00:38:29 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 00:38:29 - train.py[line:551] - INFO: load:1.67 valid_run:901.27 task_valid:872.99 collect_output:22.04
2023-01-11 00:41:01 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 00:41:01 - train.py[line:551] - INFO: load:1.70 valid_run:1053.31 task_valid:1019.18 collect_output:26.88
2023-01-11 00:43:31 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 00:43:31 - train.py[line:551] - INFO: load:1.72 valid_run:1203.07 task_valid:1160.46 collect_output:34.33
2023-01-11 00:46:00 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 00:46:00 - train.py[line:551] - INFO: load:1.75 valid_run:1351.95 task_valid:1305.34 collect_output:37.27
2023-01-11 00:48:28 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 00:48:28 - train.py[line:551] - INFO: load:1.77 valid_run:1499.72 task_valid:1448.57 collect_output:40.74
2023-01-11 00:50:57 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 00:50:57 - train.py[line:551] - INFO: load:1.80 valid_run:1649.05 task_valid:1593.73 collect_output:43.88
2023-01-11 00:53:27 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 00:53:27 - train.py[line:551] - INFO: load:1.82 valid_run:1798.38 task_valid:1739.00 collect_output:46.89
2023-01-11 00:55:56 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 00:55:56 - train.py[line:551] - INFO: load:1.85 valid_run:1947.27 task_valid:1881.03 collect_output:52.70
2023-01-11 00:58:26 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 00:58:26 - train.py[line:551] - INFO: load:1.87 valid_run:2097.14 task_valid:2026.71 collect_output:55.85
2023-01-11 01:00:56 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 01:00:56 - train.py[line:551] - INFO: load:1.90 valid_run:2247.17 task_valid:2173.64 collect_output:57.90
2023-01-11 01:03:25 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 01:03:25 - train.py[line:551] - INFO: load:1.93 valid_run:2396.49 task_valid:2318.03 collect_output:61.79
2023-01-11 01:05:56 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 01:05:56 - train.py[line:551] - INFO: load:1.95 valid_run:2546.97 task_valid:2463.71 collect_output:65.57
2023-01-11 01:08:26 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 01:08:26 - train.py[line:551] - INFO: load:1.98 valid_run:2697.04 task_valid:2610.83 collect_output:67.50
2023-01-11 01:10:53 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 01:10:53 - train.py[line:551] - INFO: load:2.00 valid_run:2844.64 task_valid:2752.72 collect_output:72.12
2023-01-11 01:13:23 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 01:13:23 - train.py[line:551] - INFO: load:2.03 valid_run:2994.12 task_valid:2897.95 collect_output:75.34
2023-01-11 01:15:54 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 01:15:54 - train.py[line:551] - INFO: load:2.06 valid_run:3144.61 task_valid:3042.75 collect_output:79.99
2023-01-11 01:18:22 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 01:18:22 - train.py[line:551] - INFO: load:2.08 valid_run:3293.32 task_valid:3187.54 collect_output:82.87
2023-01-11 01:20:53 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 01:20:53 - train.py[line:551] - INFO: load:2.11 valid_run:3444.10 task_valid:3334.26 collect_output:85.86
2023-01-11 01:23:24 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 01:23:24 - train.py[line:551] - INFO: load:2.13 valid_run:3594.75 task_valid:3481.03 collect_output:88.69

====================================================================================================
SGG eval:     R @ 50: 0.4743;     R @ 100: 0.5500;     R @ 500: 0.5932;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2970;    mR @ 100: 0.3541;    mR @ 500: 0.4013;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6875) (covering:0.3714) (eating:0.6765) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3903) (lying on:0.1000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7917) (playing:0.0000) (riding:0.7046) (says:0.0000) (sitting on:0.7222) (standing on:0.2310) (using:0.6000) (walking in:0.0000) (walking on:0.3333) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4743;     R @ 100: 0.5500;     R @ 500: 0.5932;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2970;    mR @ 100: 0.3541;    mR @ 500: 0.4013;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7927) (covered in:0.6875) (covering:0.3714) (eating:0.6765) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3903) (lying on:0.1000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7917) (playing:0.0000) (riding:0.7046) (says:0.0000) (sitting on:0.7222) (standing on:0.2310) (using:0.6000) (walking in:0.0000) (walking on:0.3333) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-11 01:25:55 - train.py[line:487] - INFO: 0.550029131652661
2023-01-11 01:25:55 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 01:25:55 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.372 | loss_v1 0 | loss_v2 0 | nll_loss 0.219 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.550029 | ppl 1.16 | vqa_score 0.5011 | wps 119.7 | wpb 89.9 | bsz 30 | num_updates 23000 | best_R@100 0.69005
2023-01-11 01:25:55 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 23000 updates
2023-01-11 01:25:55 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_23000.pt
2023-01-11 01:26:34 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_23000.pt
2023-01-11 01:27:53 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_23000.pt (epoch 1 @ 23000 updates, score 0.550029131652661) (writing took 117.54380331747234 seconds)
2023-01-11 01:27:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:10 - progress_bar.py[line:274] - INFO: epoch 001:  23041 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3933, wps=0.4, ups=0, wpb=108.3, bsz=40, num_updates=23010, lr=4.0099e-05, gnorm=0.451, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=128076
2023-01-11 01:28:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:26 - progress_bar.py[line:274] - INFO: epoch 001:  23051 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4554, wps=98.9, ups=0.6, wpb=110.2, bsz=40, num_updates=23020, lr=4.00938e-05, gnorm=0.609, clip=10, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=128093
2023-01-11 01:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:43 - progress_bar.py[line:274] - INFO: epoch 001:  23061 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4167, wps=100.3, ups=0.6, wpb=111.3, bsz=40, num_updates=23030, lr=4.00885e-05, gnorm=0.439, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=128110
2023-01-11 01:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:28:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:00 - progress_bar.py[line:274] - INFO: epoch 001:  23071 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.398, wps=102.2, ups=0.61, wpb=111.4, bsz=40, num_updates=23040, lr=4.00833e-05, gnorm=0.285, clip=0, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=128126
2023-01-11 01:29:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:16 - progress_bar.py[line:274] - INFO: epoch 001:  23081 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4818, wps=100.1, ups=0.61, wpb=109.1, bsz=40, num_updates=23050, lr=4.00781e-05, gnorm=0.522, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=128143
2023-01-11 01:29:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:33 - progress_bar.py[line:274] - INFO: epoch 001:  23091 / 100000 loss=0.331, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3704, wps=101.8, ups=0.62, wpb=108.6, bsz=40, num_updates=23060, lr=4.00729e-05, gnorm=0.385, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128159
2023-01-11 01:29:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:49 - progress_bar.py[line:274] - INFO: epoch 001:  23101 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3846, wps=101.3, ups=0.61, wpb=111.2, bsz=40, num_updates=23070, lr=4.00677e-05, gnorm=0.359, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=128176
2023-01-11 01:29:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:29:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:06 - progress_bar.py[line:274] - INFO: epoch 001:  23111 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.36, wps=101.3, ups=0.62, wpb=109.6, bsz=40, num_updates=23080, lr=4.00625e-05, gnorm=0.713, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128192
2023-01-11 01:30:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:22 - progress_bar.py[line:274] - INFO: epoch 001:  23121 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5222, wps=104, ups=0.63, wpb=110.8, bsz=40, num_updates=23090, lr=4.00573e-05, gnorm=0.599, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128208
2023-01-11 01:30:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:39 - progress_bar.py[line:274] - INFO: epoch 001:  23131 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4432, wps=101.3, ups=0.61, wpb=111.1, bsz=40, num_updates=23100, lr=4.00521e-05, gnorm=0.478, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128225
2023-01-11 01:30:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:30:54 - progress_bar.py[line:274] - INFO: epoch 001:  23141 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.48, wps=105.8, ups=0.64, wpb=109.8, bsz=40, num_updates=23110, lr=4.00469e-05, gnorm=0.553, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=128241
2023-01-11 01:30:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:11 - progress_bar.py[line:274] - INFO: epoch 001:  23151 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=100.4, ups=0.61, wpb=109.6, bsz=40, num_updates=23120, lr=4.00417e-05, gnorm=1.007, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=128257
2023-01-11 01:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:28 - progress_bar.py[line:274] - INFO: epoch 001:  23161 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3402, wps=101.7, ups=0.61, wpb=110.9, bsz=40, num_updates=23130, lr=4.00365e-05, gnorm=0.395, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128274
2023-01-11 01:31:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:44 - progress_bar.py[line:274] - INFO: epoch 001:  23171 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5294, wps=102.2, ups=0.61, wpb=110.8, bsz=40, num_updates=23140, lr=4.00313e-05, gnorm=0.553, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=128290
2023-01-11 01:31:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:31:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:00 - progress_bar.py[line:274] - INFO: epoch 001:  23181 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.17, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3832, wps=100.8, ups=0.62, wpb=108.5, bsz=40, num_updates=23150, lr=4.0026e-05, gnorm=0.655, clip=20, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=128307
2023-01-11 01:32:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:17 - progress_bar.py[line:274] - INFO: epoch 001:  23191 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.382, wps=99.2, ups=0.6, wpb=109.5, bsz=40, num_updates=23160, lr=4.00208e-05, gnorm=0.316, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=128323
2023-01-11 01:32:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:34 - progress_bar.py[line:274] - INFO: epoch 001:  23201 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.402, wps=101, ups=0.62, wpb=109.1, bsz=40, num_updates=23170, lr=4.00156e-05, gnorm=0.356, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128340
2023-01-11 01:32:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:50 - progress_bar.py[line:274] - INFO: epoch 001:  23211 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3366, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=23180, lr=4.00104e-05, gnorm=0.528, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128356
2023-01-11 01:32:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:32:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:07 - progress_bar.py[line:274] - INFO: epoch 001:  23221 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.178, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4786, wps=101.1, ups=0.62, wpb=109.3, bsz=40, num_updates=23190, lr=4.00052e-05, gnorm=0.578, clip=10, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128373
2023-01-11 01:33:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:23 - progress_bar.py[line:274] - INFO: epoch 001:  23231 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.375, wps=101.1, ups=0.61, wpb=110.1, bsz=40, num_updates=23200, lr=4e-05, gnorm=0.333, clip=0, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=128389
2023-01-11 01:33:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:40 - progress_bar.py[line:274] - INFO: epoch 001:  23241 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4286, wps=99.8, ups=0.6, wpb=110.3, bsz=40, num_updates=23210, lr=3.99948e-05, gnorm=0.304, clip=0, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=128406
2023-01-11 01:33:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:33:56 - progress_bar.py[line:274] - INFO: epoch 001:  23251 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4216, wps=105, ups=0.63, wpb=110.8, bsz=40, num_updates=23220, lr=3.99896e-05, gnorm=0.405, clip=10, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128422
2023-01-11 01:34:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:13 - progress_bar.py[line:274] - INFO: epoch 001:  23261 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4556, wps=101.7, ups=0.61, wpb=110.8, bsz=40, num_updates=23230, lr=3.99844e-05, gnorm=0.34, clip=0, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=128439
2023-01-11 01:34:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:29 - progress_bar.py[line:274] - INFO: epoch 001:  23271 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4299, wps=101.9, ups=0.62, wpb=109.1, bsz=40, num_updates=23240, lr=3.99792e-05, gnorm=0.312, clip=0, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128455
2023-01-11 01:34:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:45 - progress_bar.py[line:274] - INFO: epoch 001:  23281 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3548, wps=102.1, ups=0.62, wpb=110.1, bsz=40, num_updates=23250, lr=3.9974e-05, gnorm=0.422, clip=0, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128471
2023-01-11 01:34:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:34:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:02 - progress_bar.py[line:274] - INFO: epoch 001:  23291 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4368, wps=101.7, ups=0.61, wpb=111.5, bsz=40, num_updates=23260, lr=3.99687e-05, gnorm=0.496, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128488
2023-01-11 01:35:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:18 - progress_bar.py[line:274] - INFO: epoch 001:  23301 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3889, wps=101.8, ups=0.62, wpb=109.9, bsz=40, num_updates=23270, lr=3.99635e-05, gnorm=0.426, clip=10, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=128504
2023-01-11 01:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:35 - progress_bar.py[line:274] - INFO: epoch 001:  23311 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4486, wps=100.4, ups=0.61, wpb=110.1, bsz=40, num_updates=23280, lr=3.99583e-05, gnorm=0.544, clip=10, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128521
2023-01-11 01:35:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:51 - progress_bar.py[line:274] - INFO: epoch 001:  23321 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4434, wps=102.6, ups=0.62, wpb=110.2, bsz=40, num_updates=23290, lr=3.99531e-05, gnorm=0.669, clip=10, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=128537
2023-01-11 01:35:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:35:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:08 - progress_bar.py[line:274] - INFO: epoch 001:  23331 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4211, wps=101.4, ups=0.62, wpb=109.5, bsz=40, num_updates=23300, lr=3.99479e-05, gnorm=0.356, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128554
2023-01-11 01:36:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:24 - progress_bar.py[line:274] - INFO: epoch 001:  23341 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.46, wps=100.8, ups=0.61, wpb=110.1, bsz=40, num_updates=23310, lr=3.99427e-05, gnorm=0.346, clip=0, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128570
2023-01-11 01:36:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:41 - progress_bar.py[line:274] - INFO: epoch 001:  23351 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3871, wps=101.9, ups=0.62, wpb=110.1, bsz=40, num_updates=23320, lr=3.99375e-05, gnorm=0.393, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128587
2023-01-11 01:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:36:57 - progress_bar.py[line:274] - INFO: epoch 001:  23361 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3868, wps=101.8, ups=0.62, wpb=110.2, bsz=40, num_updates=23330, lr=3.99323e-05, gnorm=0.322, clip=0, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=128603
2023-01-11 01:37:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:13 - progress_bar.py[line:274] - INFO: epoch 001:  23371 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4356, wps=104, ups=0.62, wpb=111.6, bsz=40, num_updates=23340, lr=3.99271e-05, gnorm=0.438, clip=10, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=128620
2023-01-11 01:37:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:30 - progress_bar.py[line:274] - INFO: epoch 001:  23381 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4688, wps=99.9, ups=0.61, wpb=109.5, bsz=40, num_updates=23350, lr=3.99219e-05, gnorm=0.45, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128636
2023-01-11 01:37:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:47 - progress_bar.py[line:274] - INFO: epoch 001:  23391 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4409, wps=101.4, ups=0.61, wpb=110.5, bsz=40, num_updates=23360, lr=3.99167e-05, gnorm=0.391, clip=0, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=128653
2023-01-11 01:37:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:37:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:03 - progress_bar.py[line:274] - INFO: epoch 001:  23401 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4118, wps=99.5, ups=0.6, wpb=109.9, bsz=40, num_updates=23370, lr=3.99115e-05, gnorm=0.347, clip=10, loss_scale=1024, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=128670
2023-01-11 01:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:20 - progress_bar.py[line:274] - INFO: epoch 001:  23411 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.537, wps=98.7, ups=0.61, wpb=108.3, bsz=40, num_updates=23380, lr=3.99062e-05, gnorm=0.253, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128686
2023-01-11 01:38:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:37 - progress_bar.py[line:274] - INFO: epoch 001:  23421 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3832, wps=99, ups=0.61, wpb=108, bsz=40, num_updates=23390, lr=3.9901e-05, gnorm=0.409, clip=10, loss_scale=1024, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=128703
2023-01-11 01:38:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:38:54 - progress_bar.py[line:274] - INFO: epoch 001:  23431 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4021, wps=98.5, ups=0.6, wpb=109.5, bsz=40, num_updates=23400, lr=3.98958e-05, gnorm=0.442, clip=10, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=128720
2023-01-11 01:38:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:00 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 01:39:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:11 - progress_bar.py[line:274] - INFO: epoch 001:  23442 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4444, wps=93.9, ups=0.57, wpb=110.2, bsz=40, num_updates=23410, lr=3.98906e-05, gnorm=0.218, clip=0, loss_scale=512, train_wall=18, gb_free=10.5, ema_decay=0.9999, wall=128738
2023-01-11 01:39:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:28 - progress_bar.py[line:274] - INFO: epoch 001:  23452 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4486, wps=100.9, ups=0.62, wpb=108.8, bsz=40, num_updates=23420, lr=3.98854e-05, gnorm=0.455, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128754
2023-01-11 01:39:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:44 - progress_bar.py[line:274] - INFO: epoch 001:  23462 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4327, wps=102.9, ups=0.63, wpb=109.2, bsz=40, num_updates=23430, lr=3.98802e-05, gnorm=0.896, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128770
2023-01-11 01:39:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:39:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:01 - progress_bar.py[line:274] - INFO: epoch 001:  23472 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4904, wps=99.4, ups=0.61, wpb=108.6, bsz=40, num_updates=23440, lr=3.9875e-05, gnorm=0.536, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128787
2023-01-11 01:40:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:17 - progress_bar.py[line:274] - INFO: epoch 001:  23482 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4444, wps=100.7, ups=0.6, wpb=111.4, bsz=40, num_updates=23450, lr=3.98698e-05, gnorm=1.656, clip=20, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=128804
2023-01-11 01:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:34 - progress_bar.py[line:274] - INFO: epoch 001:  23492 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4149, wps=101.3, ups=0.62, wpb=109.4, bsz=40, num_updates=23460, lr=3.98646e-05, gnorm=0.431, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128820
2023-01-11 01:40:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:51 - progress_bar.py[line:274] - INFO: epoch 001:  23502 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.375, wps=100.4, ups=0.61, wpb=110.1, bsz=40, num_updates=23470, lr=3.98594e-05, gnorm=0.681, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=128837
2023-01-11 01:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:40:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:07 - progress_bar.py[line:274] - INFO: epoch 001:  23512 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4272, wps=102.6, ups=0.62, wpb=109.8, bsz=40, num_updates=23480, lr=3.98542e-05, gnorm=0.242, clip=0, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=128853
2023-01-11 01:41:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:24 - progress_bar.py[line:274] - INFO: epoch 001:  23522 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4545, wps=99.8, ups=0.61, wpb=108.9, bsz=40, num_updates=23490, lr=3.9849e-05, gnorm=0.467, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=128870
2023-01-11 01:41:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:40 - progress_bar.py[line:274] - INFO: epoch 001:  23532 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4479, wps=100.8, ups=0.62, wpb=109.2, bsz=40, num_updates=23500, lr=3.98438e-05, gnorm=0.405, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128886
2023-01-11 01:41:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:41:57 - progress_bar.py[line:274] - INFO: epoch 001:  23542 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.177, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3874, wps=101, ups=0.61, wpb=111.2, bsz=40, num_updates=23510, lr=3.98385e-05, gnorm=0.696, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128903
2023-01-11 01:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:13 - progress_bar.py[line:274] - INFO: epoch 001:  23552 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4078, wps=101.7, ups=0.62, wpb=109.3, bsz=40, num_updates=23520, lr=3.98333e-05, gnorm=0.362, clip=10, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=128919
2023-01-11 01:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:29 - progress_bar.py[line:274] - INFO: epoch 001:  23562 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5149, wps=102.4, ups=0.62, wpb=109.7, bsz=40, num_updates=23530, lr=3.98281e-05, gnorm=0.511, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=128936
2023-01-11 01:42:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:45 - progress_bar.py[line:274] - INFO: epoch 001:  23572 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3578, wps=103.6, ups=0.63, wpb=109.3, bsz=40, num_updates=23540, lr=3.98229e-05, gnorm=0.286, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=128952
2023-01-11 01:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:42:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:02 - progress_bar.py[line:274] - INFO: epoch 001:  23582 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4746, wps=98.5, ups=0.61, wpb=107.9, bsz=40, num_updates=23550, lr=3.98177e-05, gnorm=0.358, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=128968
2023-01-11 01:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:18 - progress_bar.py[line:274] - INFO: epoch 001:  23592 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3889, wps=103.3, ups=0.64, wpb=108.5, bsz=40, num_updates=23560, lr=3.98125e-05, gnorm=0.496, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=128984
2023-01-11 01:43:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:35 - progress_bar.py[line:274] - INFO: epoch 001:  23602 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4536, wps=100.3, ups=0.61, wpb=109.9, bsz=40, num_updates=23570, lr=3.98073e-05, gnorm=0.486, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129001
2023-01-11 01:43:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:51 - progress_bar.py[line:274] - INFO: epoch 001:  23612 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5, wps=100.7, ups=0.61, wpb=110.4, bsz=40, num_updates=23580, lr=3.98021e-05, gnorm=0.529, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129018
2023-01-11 01:43:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:43:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:08 - progress_bar.py[line:274] - INFO: epoch 001:  23622 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=106.933, nsentences=40, sample_size=106.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4316, wps=99.1, ups=0.62, wpb=106.9, bsz=40, num_updates=23590, lr=3.97969e-05, gnorm=0.268, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129034
2023-01-11 01:44:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:24 - progress_bar.py[line:274] - INFO: epoch 001:  23632 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4118, wps=100.8, ups=0.61, wpb=109.6, bsz=40, num_updates=23600, lr=3.97917e-05, gnorm=0.254, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=129050
2023-01-11 01:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:41 - progress_bar.py[line:274] - INFO: epoch 001:  23642 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4722, wps=100.6, ups=0.61, wpb=110.3, bsz=40, num_updates=23610, lr=3.97865e-05, gnorm=0.617, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=129067
2023-01-11 01:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:44:57 - progress_bar.py[line:274] - INFO: epoch 001:  23652 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.43, wps=100.5, ups=0.61, wpb=109.5, bsz=40, num_updates=23620, lr=3.97813e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=129084
2023-01-11 01:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:14 - progress_bar.py[line:274] - INFO: epoch 001:  23662 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4095, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=23630, lr=3.9776e-05, gnorm=0.449, clip=20, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=129100
2023-01-11 01:45:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:30 - progress_bar.py[line:274] - INFO: epoch 001:  23672 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4182, wps=102.7, ups=0.63, wpb=108.8, bsz=40, num_updates=23640, lr=3.97708e-05, gnorm=0.39, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=129116
2023-01-11 01:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:47 - progress_bar.py[line:274] - INFO: epoch 001:  23682 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3883, wps=98.7, ups=0.6, wpb=109, bsz=40, num_updates=23650, lr=3.97656e-05, gnorm=0.331, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=129133
2023-01-11 01:45:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:45:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:04 - progress_bar.py[line:274] - INFO: epoch 001:  23692 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.44, wps=99.4, ups=0.61, wpb=109, bsz=40, num_updates=23660, lr=3.97604e-05, gnorm=0.364, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=129150
2023-01-11 01:46:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:20 - progress_bar.py[line:274] - INFO: epoch 001:  23702 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4167, wps=102.6, ups=0.62, wpb=110.9, bsz=40, num_updates=23670, lr=3.97552e-05, gnorm=0.385, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=129166
2023-01-11 01:46:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:37 - progress_bar.py[line:274] - INFO: epoch 001:  23712 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3832, wps=102, ups=0.61, wpb=110.7, bsz=40, num_updates=23680, lr=3.975e-05, gnorm=0.329, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129183
2023-01-11 01:46:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:53 - progress_bar.py[line:274] - INFO: epoch 001:  23722 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3737, wps=100.4, ups=0.61, wpb=110.1, bsz=40, num_updates=23690, lr=3.97448e-05, gnorm=0.497, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=129199
2023-01-11 01:46:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:46:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:09 - progress_bar.py[line:274] - INFO: epoch 001:  23732 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=107.533, nsentences=40, sample_size=107.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4563, wps=102, ups=0.63, wpb=107.5, bsz=40, num_updates=23700, lr=3.97396e-05, gnorm=0.295, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=129215
2023-01-11 01:47:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:26 - progress_bar.py[line:274] - INFO: epoch 001:  23742 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3871, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=23710, lr=3.97344e-05, gnorm=1.146, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129232
2023-01-11 01:47:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:42 - progress_bar.py[line:274] - INFO: epoch 001:  23752 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4245, wps=102.5, ups=0.62, wpb=110.3, bsz=40, num_updates=23720, lr=3.97292e-05, gnorm=0.456, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=129248
2023-01-11 01:47:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:47:59 - progress_bar.py[line:274] - INFO: epoch 001:  23762 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4747, wps=100.5, ups=0.61, wpb=110.3, bsz=40, num_updates=23730, lr=3.9724e-05, gnorm=0.576, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129265
2023-01-11 01:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:15 - progress_bar.py[line:274] - INFO: epoch 001:  23772 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4423, wps=101.8, ups=0.62, wpb=109, bsz=40, num_updates=23740, lr=3.97188e-05, gnorm=0.468, clip=10, loss_scale=512, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=129281
2023-01-11 01:48:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:31 - progress_bar.py[line:274] - INFO: epoch 001:  23782 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4835, wps=104.9, ups=0.63, wpb=110.9, bsz=40, num_updates=23750, lr=3.97135e-05, gnorm=1.002, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129297
2023-01-11 01:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:48 - progress_bar.py[line:274] - INFO: epoch 001:  23792 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4615, wps=100.5, ups=0.61, wpb=110.2, bsz=40, num_updates=23760, lr=3.97083e-05, gnorm=0.736, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129314
2023-01-11 01:48:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:48:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:05 - progress_bar.py[line:274] - INFO: epoch 001:  23802 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4045, wps=101, ups=0.6, wpb=111.4, bsz=40, num_updates=23770, lr=3.97031e-05, gnorm=0.913, clip=40, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129331
2023-01-11 01:49:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:21 - progress_bar.py[line:274] - INFO: epoch 001:  23812 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3636, wps=102.8, ups=0.62, wpb=111, bsz=40, num_updates=23780, lr=3.96979e-05, gnorm=0.602, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=129347
2023-01-11 01:49:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:38 - progress_bar.py[line:274] - INFO: epoch 001:  23822 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4951, wps=96.3, ups=0.59, wpb=108.9, bsz=40, num_updates=23790, lr=3.96927e-05, gnorm=0.417, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=129365
2023-01-11 01:49:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:49:55 - progress_bar.py[line:274] - INFO: epoch 001:  23832 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4468, wps=102, ups=0.61, wpb=111.7, bsz=40, num_updates=23800, lr=3.96875e-05, gnorm=0.607, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129381
2023-01-11 01:49:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:11 - progress_bar.py[line:274] - INFO: epoch 001:  23842 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.38, wps=102.1, ups=0.62, wpb=110.3, bsz=40, num_updates=23810, lr=3.96823e-05, gnorm=0.345, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129398
2023-01-11 01:50:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:25 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 01:50:29 - progress_bar.py[line:274] - INFO: epoch 001:  23853 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4348, wps=94, ups=0.57, wpb=109.7, bsz=40, num_updates=23820, lr=3.96771e-05, gnorm=0.228, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=129415
2023-01-11 01:50:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:45 - progress_bar.py[line:274] - INFO: epoch 001:  23863 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.55, wps=101, ups=0.62, wpb=108.2, bsz=40, num_updates=23830, lr=3.96719e-05, gnorm=0.534, clip=10, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=129432
2023-01-11 01:50:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:50:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:02 - progress_bar.py[line:274] - INFO: epoch 001:  23873 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3789, wps=99, ups=0.6, wpb=110.7, bsz=40, num_updates=23840, lr=3.96667e-05, gnorm=2.476, clip=30, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=129449
2023-01-11 01:51:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:19 - progress_bar.py[line:274] - INFO: epoch 001:  23883 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4608, wps=100.4, ups=0.61, wpb=110.4, bsz=40, num_updates=23850, lr=3.96615e-05, gnorm=1.039, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129465
2023-01-11 01:51:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:35 - progress_bar.py[line:274] - INFO: epoch 001:  23893 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4679, wps=102.4, ups=0.63, wpb=107.9, bsz=40, num_updates=23860, lr=3.96562e-05, gnorm=1.138, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=129481
2023-01-11 01:51:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:52 - progress_bar.py[line:274] - INFO: epoch 001:  23903 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.434, wps=98.7, ups=0.6, wpb=110.4, bsz=40, num_updates=23870, lr=3.9651e-05, gnorm=0.336, clip=0, loss_scale=256, train_wall=17, gb_free=10.7, ema_decay=0.9999, wall=129498
2023-01-11 01:51:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:51:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:09 - progress_bar.py[line:274] - INFO: epoch 001:  23913 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4706, wps=100.2, ups=0.61, wpb=108.8, bsz=40, num_updates=23880, lr=3.96458e-05, gnorm=0.419, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129515
2023-01-11 01:52:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:25 - progress_bar.py[line:274] - INFO: epoch 001:  23923 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4574, wps=100.8, ups=0.61, wpb=110.6, bsz=40, num_updates=23890, lr=3.96406e-05, gnorm=1.246, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=129531
2023-01-11 01:52:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:42 - progress_bar.py[line:274] - INFO: epoch 001:  23933 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4245, wps=102.3, ups=0.62, wpb=109.9, bsz=40, num_updates=23900, lr=3.96354e-05, gnorm=0.33, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=129548
2023-01-11 01:52:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:52:58 - progress_bar.py[line:274] - INFO: epoch 001:  23943 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.46, wps=103.8, ups=0.63, wpb=110.7, bsz=40, num_updates=23910, lr=3.96302e-05, gnorm=0.513, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129564
2023-01-11 01:53:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:14 - progress_bar.py[line:274] - INFO: epoch 001:  23953 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3942, wps=101.5, ups=0.62, wpb=109.9, bsz=40, num_updates=23920, lr=3.9625e-05, gnorm=0.404, clip=0, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=129580
2023-01-11 01:53:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:31 - progress_bar.py[line:274] - INFO: epoch 001:  23963 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4659, wps=100, ups=0.6, wpb=111.1, bsz=40, num_updates=23930, lr=3.96198e-05, gnorm=0.516, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=129597
2023-01-11 01:53:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:48 - progress_bar.py[line:274] - INFO: epoch 001:  23973 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3962, wps=100.9, ups=0.61, wpb=110.9, bsz=40, num_updates=23940, lr=3.96146e-05, gnorm=0.229, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=129614
2023-01-11 01:53:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:53:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:04 - progress_bar.py[line:274] - INFO: epoch 001:  23983 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.469, wps=99.7, ups=0.61, wpb=109.1, bsz=40, num_updates=23950, lr=3.96094e-05, gnorm=0.378, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=129631
2023-01-11 01:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:21 - progress_bar.py[line:274] - INFO: epoch 001:  23993 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4526, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=23960, lr=3.96042e-05, gnorm=0.534, clip=20, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=129647
2023-01-11 01:54:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:38 - progress_bar.py[line:274] - INFO: epoch 001:  24003 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4851, wps=99.3, ups=0.6, wpb=109.9, bsz=40, num_updates=23970, lr=3.9599e-05, gnorm=0.295, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=129664
2023-01-11 01:54:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:55 - progress_bar.py[line:274] - INFO: epoch 001:  24013 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4421, wps=101.1, ups=0.61, wpb=110.9, bsz=40, num_updates=23980, lr=3.95938e-05, gnorm=0.275, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=129681
2023-01-11 01:54:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:54:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:55:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:55:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:55:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:55:11 - progress_bar.py[line:274] - INFO: epoch 001:  24023 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3918, wps=102.3, ups=0.63, wpb=109.1, bsz=40, num_updates=23990, lr=3.95885e-05, gnorm=0.391, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=129697
2023-01-11 01:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:55:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:55:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:55:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:55:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 01:55:27 - progress_bar.py[line:274] - INFO: epoch 001:  24033 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4194, wps=100.6, ups=0.61, wpb=110, bsz=40, num_updates=24000, lr=3.95833e-05, gnorm=0.53, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=129714
2023-01-11 01:55:27 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 01:55:29 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 01:55:29 - train.py[line:551] - INFO: load:1.51 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 01:58:00 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 01:58:00 - train.py[line:551] - INFO: load:1.54 valid_run:150.99 task_valid:148.07 collect_output:1.86
2023-01-11 02:00:29 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 02:00:29 - train.py[line:551] - INFO: load:1.56 valid_run:299.05 task_valid:291.55 collect_output:5.37
2023-01-11 02:03:00 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 02:03:00 - train.py[line:551] - INFO: load:1.59 valid_run:450.20 task_valid:434.68 collect_output:12.36
2023-01-11 02:05:28 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 02:05:28 - train.py[line:551] - INFO: load:1.61 valid_run:598.67 task_valid:579.69 collect_output:14.78
2023-01-11 02:08:00 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 02:08:00 - train.py[line:551] - INFO: load:1.64 valid_run:750.30 task_valid:727.09 collect_output:17.99
2023-01-11 02:10:31 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 02:10:31 - train.py[line:551] - INFO: load:1.66 valid_run:901.43 task_valid:872.89 collect_output:22.28
2023-01-11 02:13:03 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 02:13:03 - train.py[line:551] - INFO: load:1.69 valid_run:1053.34 task_valid:1018.81 collect_output:27.24
2023-01-11 02:15:33 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 02:15:33 - train.py[line:551] - INFO: load:1.71 valid_run:1203.29 task_valid:1159.75 collect_output:35.25
2023-01-11 02:18:02 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 02:18:02 - train.py[line:551] - INFO: load:1.74 valid_run:1352.37 task_valid:1304.71 collect_output:38.34
2023-01-11 02:20:30 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 02:20:30 - train.py[line:551] - INFO: load:1.77 valid_run:1500.00 task_valid:1447.86 collect_output:41.78
2023-01-11 02:22:59 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 02:22:59 - train.py[line:551] - INFO: load:1.79 valid_run:1649.05 task_valid:1592.88 collect_output:44.77
2023-01-11 02:25:28 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 02:25:28 - train.py[line:551] - INFO: load:1.82 valid_run:1798.05 task_valid:1737.73 collect_output:47.91
2023-01-11 02:27:57 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 02:27:57 - train.py[line:551] - INFO: load:1.84 valid_run:1947.03 task_valid:1879.74 collect_output:53.86
2023-01-11 02:30:27 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 02:30:27 - train.py[line:551] - INFO: load:1.87 valid_run:2097.01 task_valid:2025.69 collect_output:56.84
2023-01-11 02:32:57 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 02:32:57 - train.py[line:551] - INFO: load:1.90 valid_run:2246.56 task_valid:2172.23 collect_output:58.83
2023-01-11 02:35:26 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 02:35:26 - train.py[line:551] - INFO: load:1.92 valid_run:2395.73 task_valid:2316.41 collect_output:62.79
2023-01-11 02:37:57 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 02:37:57 - train.py[line:551] - INFO: load:1.95 valid_run:2546.46 task_valid:2462.16 collect_output:66.75
2023-01-11 02:40:27 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 02:40:27 - train.py[line:551] - INFO: load:1.97 valid_run:2696.52 task_valid:2609.35 collect_output:68.58
2023-01-11 02:42:55 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 02:42:55 - train.py[line:551] - INFO: load:2.00 valid_run:2843.82 task_valid:2751.05 collect_output:73.14
2023-01-11 02:45:24 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 02:45:24 - train.py[line:551] - INFO: load:2.03 valid_run:2993.14 task_valid:2896.29 collect_output:76.15
2023-01-11 02:47:55 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 02:47:55 - train.py[line:551] - INFO: load:2.05 valid_run:3143.76 task_valid:3041.24 collect_output:80.81
2023-01-11 02:50:23 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 02:50:23 - train.py[line:551] - INFO: load:2.08 valid_run:3292.34 task_valid:3185.91 collect_output:83.69
2023-01-11 02:52:54 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 02:52:54 - train.py[line:551] - INFO: load:2.10 valid_run:3442.40 task_valid:3332.09 collect_output:86.54
2023-01-11 02:55:24 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 02:55:24 - train.py[line:551] - INFO: load:2.13 valid_run:3592.92 task_valid:3478.73 collect_output:89.39

====================================================================================================
SGG eval:     R @ 50: 0.4699;     R @ 100: 0.5469;     R @ 500: 0.5848;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2885;    mR @ 100: 0.3467;    mR @ 500: 0.3855;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6458) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3774) (lying on:0.0500) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7812) (playing:0.0000) (riding:0.6850) (says:0.0000) (sitting on:0.7418) (standing on:0.2210) (using:0.6000) (walking in:0.0000) (walking on:0.3063) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4699;     R @ 100: 0.5469;     R @ 500: 0.5848;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2885;    mR @ 100: 0.3467;    mR @ 500: 0.3855;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.6458) (covering:0.3714) (eating:0.7059) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3774) (lying on:0.0500) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7812) (playing:0.0000) (riding:0.6850) (says:0.0000) (sitting on:0.7418) (standing on:0.2210) (using:0.6000) (walking in:0.0000) (walking on:0.3063) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-11 02:57:55 - train.py[line:487] - INFO: 0.5468624649859944
2023-01-11 02:57:55 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 02:57:56 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.358 | loss_v1 0 | loss_v2 0 | nll_loss 0.206 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.546862 | ppl 1.15 | vqa_score 0.4854 | wps 119.7 | wpb 89.9 | bsz 30 | num_updates 24000 | best_R@100 0.69005
2023-01-11 02:57:56 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 24000 updates
2023-01-11 02:57:56 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_24000.pt
2023-01-11 02:58:37 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_24000.pt
2023-01-11 03:00:02 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_24000.pt (epoch 1 @ 24000 updates, score 0.5468624649859944) (writing took 126.33920153789222 seconds)
2023-01-11 03:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:19 - progress_bar.py[line:274] - INFO: epoch 001:  24043 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5106, wps=0.4, ups=0, wpb=109.7, bsz=40, num_updates=24010, lr=3.95781e-05, gnorm=0.364, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=133605
2023-01-11 03:00:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:36 - progress_bar.py[line:274] - INFO: epoch 001:  24053 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4851, wps=98.9, ups=0.6, wpb=109.7, bsz=40, num_updates=24020, lr=3.95729e-05, gnorm=0.568, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=133622
2023-01-11 03:00:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:53 - progress_bar.py[line:274] - INFO: epoch 001:  24063 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5253, wps=98.6, ups=0.6, wpb=109.2, bsz=40, num_updates=24030, lr=3.95677e-05, gnorm=0.319, clip=0, loss_scale=256, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=133639
2023-01-11 03:00:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:00:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:10 - progress_bar.py[line:274] - INFO: epoch 001:  24073 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4891, wps=99.3, ups=0.6, wpb=110.6, bsz=40, num_updates=24040, lr=3.95625e-05, gnorm=0.384, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=133656
2023-01-11 03:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:26 - progress_bar.py[line:274] - INFO: epoch 001:  24083 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4196, wps=101.9, ups=0.62, wpb=109.8, bsz=40, num_updates=24050, lr=3.95573e-05, gnorm=0.553, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=133672
2023-01-11 03:01:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:43 - progress_bar.py[line:274] - INFO: epoch 001:  24093 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3861, wps=103.5, ups=0.63, wpb=110.1, bsz=40, num_updates=24060, lr=3.95521e-05, gnorm=0.275, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=133689
2023-01-11 03:01:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:01:59 - progress_bar.py[line:274] - INFO: epoch 001:  24103 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5, wps=100.2, ups=0.61, wpb=109.9, bsz=40, num_updates=24070, lr=3.95469e-05, gnorm=0.384, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=133705
2023-01-11 03:02:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:16 - progress_bar.py[line:274] - INFO: epoch 001:  24113 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4423, wps=101.1, ups=0.62, wpb=109.3, bsz=40, num_updates=24080, lr=3.95417e-05, gnorm=0.646, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=133722
2023-01-11 03:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:32 - progress_bar.py[line:274] - INFO: epoch 001:  24123 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4423, wps=103.1, ups=0.63, wpb=109.9, bsz=40, num_updates=24090, lr=3.95365e-05, gnorm=0.422, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=133738
2023-01-11 03:02:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:47 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-11 03:02:49 - progress_bar.py[line:274] - INFO: epoch 001:  24134 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4158, wps=93.8, ups=0.58, wpb=108.7, bsz=40, num_updates=24100, lr=3.95313e-05, gnorm=0.82, clip=10, loss_scale=128, train_wall=17, gb_free=10, ema_decay=0.9999, wall=133756
2023-01-11 03:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:06 - progress_bar.py[line:274] - INFO: epoch 001:  24144 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4381, wps=100.5, ups=0.61, wpb=110.2, bsz=40, num_updates=24110, lr=3.9526e-05, gnorm=0.422, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=133772
2023-01-11 03:03:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:23 - progress_bar.py[line:274] - INFO: epoch 001:  24154 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4608, wps=99.6, ups=0.6, wpb=109.8, bsz=40, num_updates=24120, lr=3.95208e-05, gnorm=0.379, clip=0, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=133789
2023-01-11 03:03:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:39 - progress_bar.py[line:274] - INFO: epoch 001:  24164 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4321, wps=101.9, ups=0.61, wpb=111.1, bsz=40, num_updates=24130, lr=3.95156e-05, gnorm=0.378, clip=10, loss_scale=128, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=133806
2023-01-11 03:03:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:03:56 - progress_bar.py[line:274] - INFO: epoch 001:  24174 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.43, wps=98.8, ups=0.6, wpb=109.3, bsz=40, num_updates=24140, lr=3.95104e-05, gnorm=0.388, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=133822
2023-01-11 03:03:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:13 - progress_bar.py[line:274] - INFO: epoch 001:  24184 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4, wps=101.6, ups=0.62, wpb=108.7, bsz=40, num_updates=24150, lr=3.95052e-05, gnorm=0.658, clip=20, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=133839
2023-01-11 03:04:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:29 - progress_bar.py[line:274] - INFO: epoch 001:  24194 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5196, wps=101.4, ups=0.62, wpb=109.2, bsz=40, num_updates=24160, lr=3.95e-05, gnorm=0.288, clip=0, loss_scale=128, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=133855
2023-01-11 03:04:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:45 - progress_bar.py[line:274] - INFO: epoch 001:  24204 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4608, wps=101.5, ups=0.62, wpb=109.5, bsz=40, num_updates=24170, lr=3.94948e-05, gnorm=0.373, clip=10, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=133872
2023-01-11 03:04:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:01 - progress_bar.py[line:274] - INFO: epoch 001:  24214 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4851, wps=104, ups=0.63, wpb=110.9, bsz=40, num_updates=24180, lr=3.94896e-05, gnorm=0.716, clip=20, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=133888
2023-01-11 03:05:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:18 - progress_bar.py[line:274] - INFO: epoch 001:  24224 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3725, wps=104.1, ups=0.63, wpb=110.1, bsz=40, num_updates=24190, lr=3.94844e-05, gnorm=0.431, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=133904
2023-01-11 03:05:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:34 - progress_bar.py[line:274] - INFO: epoch 001:  24234 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4762, wps=102.5, ups=0.63, wpb=109.1, bsz=40, num_updates=24200, lr=3.94792e-05, gnorm=0.469, clip=0, loss_scale=128, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=133920
2023-01-11 03:05:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:50 - progress_bar.py[line:274] - INFO: epoch 001:  24244 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4792, wps=103.3, ups=0.62, wpb=110.6, bsz=40, num_updates=24210, lr=3.9474e-05, gnorm=0.331, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=133936
2023-01-11 03:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:05:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:07 - progress_bar.py[line:274] - INFO: epoch 001:  24254 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4771, wps=100.2, ups=0.61, wpb=108.8, bsz=40, num_updates=24220, lr=3.94688e-05, gnorm=0.522, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=133953
2023-01-11 03:06:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:23 - progress_bar.py[line:274] - INFO: epoch 001:  24264 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.449, wps=101.1, ups=0.61, wpb=110.1, bsz=40, num_updates=24230, lr=3.94635e-05, gnorm=1.006, clip=40, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=133969
2023-01-11 03:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:39 - progress_bar.py[line:274] - INFO: epoch 001:  24274 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4673, wps=102.3, ups=0.62, wpb=109.5, bsz=40, num_updates=24240, lr=3.94583e-05, gnorm=0.297, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=133986
2023-01-11 03:06:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:56 - progress_bar.py[line:274] - INFO: epoch 001:  24284 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4444, wps=102.5, ups=0.61, wpb=111.3, bsz=40, num_updates=24250, lr=3.94531e-05, gnorm=0.646, clip=20, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=134002
2023-01-11 03:06:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:06:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:12 - progress_bar.py[line:274] - INFO: epoch 001:  24294 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4423, wps=100.4, ups=0.61, wpb=108.9, bsz=40, num_updates=24260, lr=3.94479e-05, gnorm=0.463, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134019
2023-01-11 03:07:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:28 - progress_bar.py[line:274] - INFO: epoch 001:  24304 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=106, ups=0.64, wpb=109.7, bsz=40, num_updates=24270, lr=3.94427e-05, gnorm=0.466, clip=20, loss_scale=128, train_wall=15, gb_free=10.2, ema_decay=0.9999, wall=134034
2023-01-11 03:07:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:45 - progress_bar.py[line:274] - INFO: epoch 001:  24314 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4409, wps=99.9, ups=0.6, wpb=110.1, bsz=40, num_updates=24280, lr=3.94375e-05, gnorm=0.259, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134051
2023-01-11 03:07:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:07:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:01 - progress_bar.py[line:274] - INFO: epoch 001:  24324 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4902, wps=102.7, ups=0.62, wpb=110.8, bsz=40, num_updates=24290, lr=3.94323e-05, gnorm=0.36, clip=0, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=134067
2023-01-11 03:08:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:18 - progress_bar.py[line:274] - INFO: epoch 001:  24334 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4421, wps=100.8, ups=0.61, wpb=110.4, bsz=40, num_updates=24300, lr=3.94271e-05, gnorm=0.434, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=134084
2023-01-11 03:08:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:34 - progress_bar.py[line:274] - INFO: epoch 001:  24344 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4062, wps=102.7, ups=0.62, wpb=110.5, bsz=40, num_updates=24310, lr=3.94219e-05, gnorm=0.35, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134100
2023-01-11 03:08:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:51 - progress_bar.py[line:274] - INFO: epoch 001:  24354 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4086, wps=101.9, ups=0.62, wpb=110.1, bsz=40, num_updates=24320, lr=3.94167e-05, gnorm=0.326, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134117
2023-01-11 03:08:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:08:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:07 - progress_bar.py[line:274] - INFO: epoch 001:  24364 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4333, wps=103.1, ups=0.62, wpb=111.5, bsz=40, num_updates=24330, lr=3.94115e-05, gnorm=0.575, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134133
2023-01-11 03:09:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:24 - progress_bar.py[line:274] - INFO: epoch 001:  24374 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4369, wps=98.7, ups=0.61, wpb=108.2, bsz=40, num_updates=24340, lr=3.94062e-05, gnorm=0.513, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134150
2023-01-11 03:09:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:41 - progress_bar.py[line:274] - INFO: epoch 001:  24384 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4565, wps=99.9, ups=0.6, wpb=110.5, bsz=40, num_updates=24350, lr=3.9401e-05, gnorm=0.249, clip=0, loss_scale=128, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=134167
2023-01-11 03:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:09:57 - progress_bar.py[line:274] - INFO: epoch 001:  24394 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5055, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=24360, lr=3.93958e-05, gnorm=0.638, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134183
2023-01-11 03:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:14 - progress_bar.py[line:274] - INFO: epoch 001:  24404 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4198, wps=103.1, ups=0.62, wpb=111.3, bsz=40, num_updates=24370, lr=3.93906e-05, gnorm=0.575, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134200
2023-01-11 03:10:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:30 - progress_bar.py[line:274] - INFO: epoch 001:  24414 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4369, wps=101.9, ups=0.62, wpb=109, bsz=40, num_updates=24380, lr=3.93854e-05, gnorm=0.916, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134216
2023-01-11 03:10:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:46 - progress_bar.py[line:274] - INFO: epoch 001:  24424 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4609, wps=99.5, ups=0.61, wpb=108.8, bsz=40, num_updates=24390, lr=3.93802e-05, gnorm=0.54, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134233
2023-01-11 03:10:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:10:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:03 - progress_bar.py[line:274] - INFO: epoch 001:  24434 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4731, wps=100, ups=0.6, wpb=110.4, bsz=40, num_updates=24400, lr=3.9375e-05, gnorm=0.458, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=134249
2023-01-11 03:11:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:20 - progress_bar.py[line:274] - INFO: epoch 001:  24444 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3918, wps=100.1, ups=0.61, wpb=109.1, bsz=40, num_updates=24410, lr=3.93698e-05, gnorm=0.341, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134266
2023-01-11 03:11:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:36 - progress_bar.py[line:274] - INFO: epoch 001:  24454 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4343, wps=102.7, ups=0.62, wpb=110, bsz=40, num_updates=24420, lr=3.93646e-05, gnorm=0.348, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=134282
2023-01-11 03:11:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:53 - progress_bar.py[line:274] - INFO: epoch 001:  24464 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4457, wps=101.5, ups=0.61, wpb=110.7, bsz=40, num_updates=24430, lr=3.93594e-05, gnorm=0.334, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134299
2023-01-11 03:11:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:11:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:09 - progress_bar.py[line:274] - INFO: epoch 001:  24474 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4, wps=100, ups=0.61, wpb=108.4, bsz=40, num_updates=24440, lr=3.93542e-05, gnorm=0.289, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=134315
2023-01-11 03:12:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:25 - progress_bar.py[line:274] - INFO: epoch 001:  24484 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.486, wps=103.6, ups=0.63, wpb=110.1, bsz=40, num_updates=24450, lr=3.9349e-05, gnorm=0.855, clip=20, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=134332
2023-01-11 03:12:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:42 - progress_bar.py[line:274] - INFO: epoch 001:  24494 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4105, wps=100.9, ups=0.6, wpb=111.5, bsz=40, num_updates=24460, lr=3.93438e-05, gnorm=0.569, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=134348
2023-01-11 03:12:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:12:59 - progress_bar.py[line:274] - INFO: epoch 001:  24504 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4409, wps=100.8, ups=0.61, wpb=110.8, bsz=40, num_updates=24470, lr=3.93385e-05, gnorm=0.236, clip=0, loss_scale=128, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=134365
2023-01-11 03:13:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:16 - progress_bar.py[line:274] - INFO: epoch 001:  24514 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=24480, lr=3.93333e-05, gnorm=0.178, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134382
2023-01-11 03:13:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:32 - progress_bar.py[line:274] - INFO: epoch 001:  24524 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5102, wps=99.2, ups=0.61, wpb=108.7, bsz=40, num_updates=24490, lr=3.93281e-05, gnorm=0.382, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=134398
2023-01-11 03:13:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:49 - progress_bar.py[line:274] - INFO: epoch 001:  24534 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3818, wps=102.5, ups=0.62, wpb=110.5, bsz=40, num_updates=24500, lr=3.93229e-05, gnorm=0.284, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134415
2023-01-11 03:13:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:13:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:05 - progress_bar.py[line:274] - INFO: epoch 001:  24544 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3617, wps=102.9, ups=0.61, wpb=111.7, bsz=40, num_updates=24510, lr=3.93177e-05, gnorm=0.7, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134431
2023-01-11 03:14:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:21 - progress_bar.py[line:274] - INFO: epoch 001:  24554 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=105, ups=0.63, wpb=110.6, bsz=40, num_updates=24520, lr=3.93125e-05, gnorm=0.333, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134447
2023-01-11 03:14:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:38 - progress_bar.py[line:274] - INFO: epoch 001:  24564 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5455, wps=98.5, ups=0.6, wpb=109.4, bsz=40, num_updates=24530, lr=3.93073e-05, gnorm=0.486, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=134464
2023-01-11 03:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:55 - progress_bar.py[line:274] - INFO: epoch 001:  24574 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4455, wps=100.8, ups=0.61, wpb=110.5, bsz=40, num_updates=24540, lr=3.93021e-05, gnorm=0.324, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134481
2023-01-11 03:14:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:14:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:11 - progress_bar.py[line:274] - INFO: epoch 001:  24584 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4434, wps=100.9, ups=0.61, wpb=109.8, bsz=40, num_updates=24550, lr=3.92969e-05, gnorm=0.47, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134498
2023-01-11 03:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:28 - progress_bar.py[line:274] - INFO: epoch 001:  24594 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.42, wps=99.9, ups=0.61, wpb=108.9, bsz=40, num_updates=24560, lr=3.92917e-05, gnorm=0.339, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=134514
2023-01-11 03:15:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:44 - progress_bar.py[line:274] - INFO: epoch 001:  24604 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4, wps=102.5, ups=0.62, wpb=110.1, bsz=40, num_updates=24570, lr=3.92865e-05, gnorm=0.398, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134530
2023-01-11 03:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:15:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:01 - progress_bar.py[line:274] - INFO: epoch 001:  24614 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4681, wps=98.5, ups=0.6, wpb=109.7, bsz=40, num_updates=24580, lr=3.92813e-05, gnorm=0.526, clip=20, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=134547
2023-01-11 03:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:17 - progress_bar.py[line:274] - INFO: epoch 001:  24624 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3939, wps=105.5, ups=0.64, wpb=109.6, bsz=40, num_updates=24590, lr=3.9276e-05, gnorm=0.173, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=134563
2023-01-11 03:16:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:34 - progress_bar.py[line:274] - INFO: epoch 001:  24634 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5049, wps=98.3, ups=0.6, wpb=108.5, bsz=40, num_updates=24600, lr=3.92708e-05, gnorm=0.347, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134580
2023-01-11 03:16:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:50 - progress_bar.py[line:274] - INFO: epoch 001:  24644 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4722, wps=101.8, ups=0.62, wpb=110.2, bsz=40, num_updates=24610, lr=3.92656e-05, gnorm=0.367, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=134597
2023-01-11 03:16:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:16:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:07 - progress_bar.py[line:274] - INFO: epoch 001:  24654 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4234, wps=100, ups=0.61, wpb=109.2, bsz=40, num_updates=24620, lr=3.92604e-05, gnorm=0.261, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134613
2023-01-11 03:17:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:23 - progress_bar.py[line:274] - INFO: epoch 001:  24664 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=101.4, ups=0.62, wpb=109.4, bsz=40, num_updates=24630, lr=3.92552e-05, gnorm=0.309, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=134630
2023-01-11 03:17:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:40 - progress_bar.py[line:274] - INFO: epoch 001:  24674 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4348, wps=99.1, ups=0.6, wpb=109.5, bsz=40, num_updates=24640, lr=3.925e-05, gnorm=0.443, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=134646
2023-01-11 03:17:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:17:57 - progress_bar.py[line:274] - INFO: epoch 001:  24684 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5054, wps=102.1, ups=0.61, wpb=110.9, bsz=40, num_updates=24650, lr=3.92448e-05, gnorm=0.28, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=134663
2023-01-11 03:17:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:13 - progress_bar.py[line:274] - INFO: epoch 001:  24694 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3721, wps=104.8, ups=0.63, wpb=111.4, bsz=40, num_updates=24660, lr=3.92396e-05, gnorm=0.424, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=134679
2023-01-11 03:18:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:29 - progress_bar.py[line:274] - INFO: epoch 001:  24704 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3645, wps=100.7, ups=0.62, wpb=107.6, bsz=40, num_updates=24670, lr=3.92344e-05, gnorm=0.445, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134695
2023-01-11 03:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:46 - progress_bar.py[line:274] - INFO: epoch 001:  24714 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4563, wps=98.7, ups=0.6, wpb=109.5, bsz=40, num_updates=24680, lr=3.92292e-05, gnorm=0.324, clip=10, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=134712
2023-01-11 03:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:18:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:03 - progress_bar.py[line:274] - INFO: epoch 001:  24724 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4299, wps=98.8, ups=0.6, wpb=109.2, bsz=40, num_updates=24690, lr=3.9224e-05, gnorm=0.551, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=134729
2023-01-11 03:19:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:20 - progress_bar.py[line:274] - INFO: epoch 001:  24734 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4457, wps=99.4, ups=0.6, wpb=109.9, bsz=40, num_updates=24700, lr=3.92188e-05, gnorm=0.243, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=134746
2023-01-11 03:19:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:36 - progress_bar.py[line:274] - INFO: epoch 001:  24744 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4889, wps=102.4, ups=0.61, wpb=111.1, bsz=40, num_updates=24710, lr=3.92135e-05, gnorm=0.347, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=134762
2023-01-11 03:19:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:53 - progress_bar.py[line:274] - INFO: epoch 001:  24754 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.44, wps=100.7, ups=0.61, wpb=110.7, bsz=40, num_updates=24720, lr=3.92083e-05, gnorm=1.115, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=134779
2023-01-11 03:19:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:19:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:09 - progress_bar.py[line:274] - INFO: epoch 001:  24764 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4839, wps=102.9, ups=0.62, wpb=110.5, bsz=40, num_updates=24730, lr=3.92031e-05, gnorm=0.245, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134795
2023-01-11 03:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:26 - progress_bar.py[line:274] - INFO: epoch 001:  24774 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4574, wps=102.8, ups=0.62, wpb=111.1, bsz=40, num_updates=24740, lr=3.91979e-05, gnorm=0.277, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134812
2023-01-11 03:20:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:42 - progress_bar.py[line:274] - INFO: epoch 001:  24784 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4792, wps=102.8, ups=0.62, wpb=110.9, bsz=40, num_updates=24750, lr=3.91927e-05, gnorm=0.239, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=134828
2023-01-11 03:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:20:58 - progress_bar.py[line:274] - INFO: epoch 001:  24794 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3824, wps=100.8, ups=0.61, wpb=109.7, bsz=40, num_updates=24760, lr=3.91875e-05, gnorm=0.22, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=134845
2023-01-11 03:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:15 - progress_bar.py[line:274] - INFO: epoch 001:  24804 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3894, wps=100.9, ups=0.62, wpb=109.1, bsz=40, num_updates=24770, lr=3.91823e-05, gnorm=0.337, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=134861
2023-01-11 03:21:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:31 - progress_bar.py[line:274] - INFO: epoch 001:  24814 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3148, wps=103.5, ups=0.63, wpb=109.9, bsz=40, num_updates=24780, lr=3.91771e-05, gnorm=0.333, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134877
2023-01-11 03:21:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:47 - progress_bar.py[line:274] - INFO: epoch 001:  24824 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=112.133, nsentences=40, sample_size=112.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4684, wps=104.6, ups=0.62, wpb=112.1, bsz=40, num_updates=24790, lr=3.91719e-05, gnorm=0.328, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=134894
2023-01-11 03:21:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:21:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:04 - progress_bar.py[line:274] - INFO: epoch 001:  24834 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4021, wps=100.6, ups=0.61, wpb=109.4, bsz=40, num_updates=24800, lr=3.91667e-05, gnorm=0.459, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=134910
2023-01-11 03:22:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:20 - progress_bar.py[line:274] - INFO: epoch 001:  24844 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3945, wps=102.3, ups=0.62, wpb=109.1, bsz=40, num_updates=24810, lr=3.91615e-05, gnorm=0.397, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=134926
2023-01-11 03:22:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:36 - progress_bar.py[line:274] - INFO: epoch 001:  24854 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5185, wps=103.5, ups=0.63, wpb=109.9, bsz=40, num_updates=24820, lr=3.91562e-05, gnorm=0.769, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=134943
2023-01-11 03:22:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:53 - progress_bar.py[line:274] - INFO: epoch 001:  24864 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4433, wps=102.2, ups=0.61, wpb=112.2, bsz=40, num_updates=24830, lr=3.9151e-05, gnorm=0.273, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=134959
2023-01-11 03:22:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:10 - progress_bar.py[line:274] - INFO: epoch 001:  24874 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4333, wps=98.1, ups=0.6, wpb=108.3, bsz=40, num_updates=24840, lr=3.91458e-05, gnorm=0.458, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=134976
2023-01-11 03:23:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:27 - progress_bar.py[line:274] - INFO: epoch 001:  24884 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4369, wps=98.9, ups=0.6, wpb=109.7, bsz=40, num_updates=24850, lr=3.91406e-05, gnorm=0.464, clip=10, loss_scale=256, train_wall=17, gb_free=10.7, ema_decay=0.9999, wall=134993
2023-01-11 03:23:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:44 - progress_bar.py[line:274] - INFO: epoch 001:  24894 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5294, wps=100.7, ups=0.6, wpb=111.3, bsz=40, num_updates=24860, lr=3.91354e-05, gnorm=0.491, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=135010
2023-01-11 03:23:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:23:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:00 - progress_bar.py[line:274] - INFO: epoch 001:  24904 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3448, wps=102.1, ups=0.62, wpb=110.5, bsz=40, num_updates=24870, lr=3.91302e-05, gnorm=0.346, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=135026
2023-01-11 03:24:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:17 - progress_bar.py[line:274] - INFO: epoch 001:  24914 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4771, wps=99.9, ups=0.61, wpb=108.9, bsz=40, num_updates=24880, lr=3.9125e-05, gnorm=0.498, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=135043
2023-01-11 03:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:33 - progress_bar.py[line:274] - INFO: epoch 001:  24924 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5098, wps=99.4, ups=0.61, wpb=109.2, bsz=40, num_updates=24890, lr=3.91198e-05, gnorm=0.386, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=135059
2023-01-11 03:24:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:50 - progress_bar.py[line:274] - INFO: epoch 001:  24934 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4848, wps=100.1, ups=0.61, wpb=110, bsz=40, num_updates=24900, lr=3.91146e-05, gnorm=1.039, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=135076
2023-01-11 03:24:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:24:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:07 - progress_bar.py[line:274] - INFO: epoch 001:  24944 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5446, wps=101.3, ups=0.62, wpb=109.7, bsz=40, num_updates=24910, lr=3.91094e-05, gnorm=0.278, clip=0, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=135093
2023-01-11 03:25:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:23 - progress_bar.py[line:274] - INFO: epoch 001:  24954 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3678, wps=103.4, ups=0.63, wpb=109.9, bsz=40, num_updates=24920, lr=3.91042e-05, gnorm=0.636, clip=20, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=135109
2023-01-11 03:25:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:40 - progress_bar.py[line:274] - INFO: epoch 001:  24964 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.52, wps=99.4, ups=0.6, wpb=110.1, bsz=40, num_updates=24930, lr=3.9099e-05, gnorm=0.311, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=135126
2023-01-11 03:25:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:25:57 - progress_bar.py[line:274] - INFO: epoch 001:  24974 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4205, wps=101.1, ups=0.61, wpb=110.6, bsz=40, num_updates=24940, lr=3.90937e-05, gnorm=0.464, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=135143
2023-01-11 03:25:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:13 - progress_bar.py[line:274] - INFO: epoch 001:  24984 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3611, wps=100.2, ups=0.61, wpb=109.7, bsz=40, num_updates=24950, lr=3.90885e-05, gnorm=0.951, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=135159
2023-01-11 03:26:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:29 - progress_bar.py[line:274] - INFO: epoch 001:  24994 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4571, wps=102, ups=0.63, wpb=107.7, bsz=40, num_updates=24960, lr=3.90833e-05, gnorm=0.404, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=135176
2023-01-11 03:26:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:46 - progress_bar.py[line:274] - INFO: epoch 001:  25004 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4062, wps=102.5, ups=0.62, wpb=110.8, bsz=40, num_updates=24970, lr=3.90781e-05, gnorm=1.866, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=135192
2023-01-11 03:26:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:26:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:27:03 - progress_bar.py[line:274] - INFO: epoch 001:  25014 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4, wps=100.4, ups=0.61, wpb=110, bsz=40, num_updates=24980, lr=3.90729e-05, gnorm=1.116, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=135209
2023-01-11 03:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:27:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:27:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:27:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:27:19 - progress_bar.py[line:274] - INFO: epoch 001:  25024 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3404, wps=100.9, ups=0.61, wpb=110.4, bsz=40, num_updates=24990, lr=3.90677e-05, gnorm=0.525, clip=10, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=135226
2023-01-11 03:27:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:27:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:27:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:27:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:27:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 03:27:37 - progress_bar.py[line:274] - INFO: epoch 001:  25034 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4646, wps=100.3, ups=0.61, wpb=110.3, bsz=40, num_updates=25000, lr=3.90625e-05, gnorm=0.468, clip=10, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=135242
2023-01-11 03:27:37 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 03:27:38 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 03:27:38 - train.py[line:551] - INFO: load:1.39 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 03:30:09 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 03:30:09 - train.py[line:551] - INFO: load:1.41 valid_run:151.01 task_valid:148.10 collect_output:1.81
2023-01-11 03:32:37 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 03:32:37 - train.py[line:551] - INFO: load:1.44 valid_run:298.69 task_valid:291.44 collect_output:5.09
2023-01-11 03:35:09 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 03:35:09 - train.py[line:551] - INFO: load:1.46 valid_run:450.17 task_valid:434.85 collect_output:12.11
2023-01-11 03:37:37 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 03:37:37 - train.py[line:551] - INFO: load:1.49 valid_run:598.83 task_valid:580.02 collect_output:14.54
2023-01-11 03:40:09 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 03:40:09 - train.py[line:551] - INFO: load:1.51 valid_run:750.58 task_valid:727.68 collect_output:17.58
2023-01-11 03:42:40 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 03:42:40 - train.py[line:551] - INFO: load:1.54 valid_run:901.62 task_valid:873.43 collect_output:21.82
2023-01-11 03:45:13 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 03:45:13 - train.py[line:551] - INFO: load:1.57 valid_run:1053.76 task_valid:1019.70 collect_output:26.66
2023-01-11 03:47:43 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 03:47:43 - train.py[line:551] - INFO: load:1.59 valid_run:1203.77 task_valid:1160.93 collect_output:34.39
2023-01-11 03:50:12 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 03:50:12 - train.py[line:551] - INFO: load:1.62 valid_run:1352.56 task_valid:1305.84 collect_output:37.22
2023-01-11 03:52:39 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 03:52:39 - train.py[line:551] - INFO: load:1.64 valid_run:1500.22 task_valid:1449.16 collect_output:40.50
2023-01-11 03:55:09 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 03:55:09 - train.py[line:551] - INFO: load:1.67 valid_run:1649.60 task_valid:1594.35 collect_output:43.66
2023-01-11 03:57:38 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 03:57:38 - train.py[line:551] - INFO: load:1.69 valid_run:1798.67 task_valid:1739.40 collect_output:46.64
2023-01-11 04:00:07 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 04:00:07 - train.py[line:551] - INFO: load:1.72 valid_run:1947.29 task_valid:1881.20 collect_output:52.44
2023-01-11 04:02:37 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 04:02:37 - train.py[line:551] - INFO: load:1.74 valid_run:2097.19 task_valid:2026.80 collect_output:55.72
2023-01-11 04:05:07 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 04:05:07 - train.py[line:551] - INFO: load:1.77 valid_run:2247.11 task_valid:2173.59 collect_output:57.81
2023-01-11 04:07:36 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 04:07:36 - train.py[line:551] - INFO: load:1.79 valid_run:2396.33 task_valid:2317.88 collect_output:61.71
2023-01-11 04:10:06 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 04:10:06 - train.py[line:551] - INFO: load:1.82 valid_run:2546.65 task_valid:2463.36 collect_output:65.53
2023-01-11 04:12:36 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 04:12:36 - train.py[line:551] - INFO: load:1.84 valid_run:2696.63 task_valid:2610.45 collect_output:67.36
2023-01-11 04:15:04 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 04:15:04 - train.py[line:551] - INFO: load:1.87 valid_run:2844.16 task_valid:2752.42 collect_output:71.89
2023-01-11 04:17:33 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 04:17:33 - train.py[line:551] - INFO: load:1.90 valid_run:2993.42 task_valid:2897.49 collect_output:75.03
2023-01-11 04:20:04 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 04:20:04 - train.py[line:551] - INFO: load:1.92 valid_run:3143.83 task_valid:3042.19 collect_output:79.71
2023-01-11 04:22:33 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 04:22:33 - train.py[line:551] - INFO: load:1.95 valid_run:3292.63 task_valid:3187.08 collect_output:82.53
2023-01-11 04:25:03 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 04:25:03 - train.py[line:551] - INFO: load:1.97 valid_run:3443.12 task_valid:3333.67 collect_output:85.36
2023-01-11 04:27:34 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 04:27:34 - train.py[line:551] - INFO: load:2.00 valid_run:3593.70 task_valid:3480.38 collect_output:88.12

====================================================================================================
SGG eval:     R @ 50: 0.4692;     R @ 100: 0.5455;     R @ 500: 0.5811;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2835;    mR @ 100: 0.3357;    mR @ 500: 0.3758;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.5833) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3774) (lying on:0.0500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7604) (playing:0.0000) (riding:0.6899) (says:0.0000) (sitting on:0.7418) (standing on:0.2210) (using:0.6000) (walking in:0.0000) (walking on:0.3063) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4692;     R @ 100: 0.5455;     R @ 500: 0.5811;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2835;    mR @ 100: 0.3357;    mR @ 500: 0.3758;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.5833) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3774) (lying on:0.0500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7604) (playing:0.0000) (riding:0.6899) (says:0.0000) (sitting on:0.7418) (standing on:0.2210) (using:0.6000) (walking in:0.0000) (walking on:0.3063) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-11 04:30:05 - train.py[line:487] - INFO: 0.5455291316526611
2023-01-11 04:30:05 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 04:30:06 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.368 | loss_v1 0 | loss_v2 0 | nll_loss 0.22 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.545529 | ppl 1.16 | vqa_score 0.4797 | wps 119.7 | wpb 89.9 | bsz 30 | num_updates 25000 | best_R@100 0.69005
2023-01-11 04:30:06 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 25000 updates
2023-01-11 04:30:06 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_25000.pt
2023-01-11 04:30:50 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_25000.pt
2023-01-11 04:32:20 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_25000.pt (epoch 1 @ 25000 updates, score 0.5455291316526611) (writing took 133.9988869857043 seconds)
2023-01-11 04:32:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:37 - progress_bar.py[line:274] - INFO: epoch 001:  25044 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3889, wps=0.4, ups=0, wpb=110.7, bsz=40, num_updates=25010, lr=3.90573e-05, gnorm=1.013, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=139143
2023-01-11 04:32:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:54 - progress_bar.py[line:274] - INFO: epoch 001:  25054 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5094, wps=100.1, ups=0.61, wpb=109.9, bsz=40, num_updates=25020, lr=3.90521e-05, gnorm=0.423, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=139160
2023-01-11 04:32:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:32:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:10 - progress_bar.py[line:274] - INFO: epoch 001:  25064 / 100000 loss=0.344, loss_v1=0, loss_v2=0, nll_loss=0.195, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.367, wps=101, ups=0.62, wpb=108.3, bsz=40, num_updates=25030, lr=3.90469e-05, gnorm=1.173, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=139176
2023-01-11 04:33:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:26 - progress_bar.py[line:274] - INFO: epoch 001:  25074 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5116, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=25040, lr=3.90417e-05, gnorm=1.838, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=139193
2023-01-11 04:33:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:43 - progress_bar.py[line:274] - INFO: epoch 001:  25084 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3667, wps=100, ups=0.6, wpb=110.8, bsz=40, num_updates=25050, lr=3.90365e-05, gnorm=2.042, clip=20, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=139209
2023-01-11 04:33:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:33:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:00 - progress_bar.py[line:274] - INFO: epoch 001:  25094 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4369, wps=102.2, ups=0.62, wpb=109.1, bsz=40, num_updates=25060, lr=3.90313e-05, gnorm=0.533, clip=10, loss_scale=256, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=139226
2023-01-11 04:34:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:16 - progress_bar.py[line:274] - INFO: epoch 001:  25104 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4124, wps=99.2, ups=0.6, wpb=109.9, bsz=40, num_updates=25070, lr=3.9026e-05, gnorm=0.455, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=139243
2023-01-11 04:34:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:33 - progress_bar.py[line:274] - INFO: epoch 001:  25114 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4118, wps=99.5, ups=0.61, wpb=108.4, bsz=40, num_updates=25080, lr=3.90208e-05, gnorm=0.532, clip=20, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=139259
2023-01-11 04:34:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:49 - progress_bar.py[line:274] - INFO: epoch 001:  25124 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.463, wps=101.9, ups=0.62, wpb=109.3, bsz=40, num_updates=25090, lr=3.90156e-05, gnorm=0.393, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=139275
2023-01-11 04:34:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:34:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:06 - progress_bar.py[line:274] - INFO: epoch 001:  25134 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4271, wps=101.2, ups=0.62, wpb=109.4, bsz=40, num_updates=25100, lr=3.90104e-05, gnorm=1.058, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=139292
2023-01-11 04:35:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:22 - progress_bar.py[line:274] - INFO: epoch 001:  25144 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4792, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=25110, lr=3.90052e-05, gnorm=0.588, clip=20, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=139308
2023-01-11 04:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:39 - progress_bar.py[line:274] - INFO: epoch 001:  25154 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3684, wps=101.1, ups=0.61, wpb=109.8, bsz=40, num_updates=25120, lr=3.9e-05, gnorm=0.379, clip=10, loss_scale=256, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=139325
2023-01-11 04:35:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:55 - progress_bar.py[line:274] - INFO: epoch 001:  25164 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3978, wps=101.6, ups=0.61, wpb=111.1, bsz=40, num_updates=25130, lr=3.89948e-05, gnorm=0.335, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139342
2023-01-11 04:35:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:35:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:12 - progress_bar.py[line:274] - INFO: epoch 001:  25174 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5149, wps=102.2, ups=0.62, wpb=109.9, bsz=40, num_updates=25140, lr=3.89896e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139358
2023-01-11 04:36:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:29 - progress_bar.py[line:274] - INFO: epoch 001:  25184 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4956, wps=98.5, ups=0.6, wpb=108.6, bsz=40, num_updates=25150, lr=3.89844e-05, gnorm=0.574, clip=30, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=139375
2023-01-11 04:36:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:45 - progress_bar.py[line:274] - INFO: epoch 001:  25194 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4757, wps=101.8, ups=0.62, wpb=110, bsz=40, num_updates=25160, lr=3.89792e-05, gnorm=0.201, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=139391
2023-01-11 04:36:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:36:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:02 - progress_bar.py[line:274] - INFO: epoch 001:  25204 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4167, wps=101.1, ups=0.61, wpb=111.3, bsz=40, num_updates=25170, lr=3.8974e-05, gnorm=0.49, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=139408
2023-01-11 04:37:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:19 - progress_bar.py[line:274] - INFO: epoch 001:  25214 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4667, wps=99, ups=0.6, wpb=109.8, bsz=40, num_updates=25180, lr=3.89688e-05, gnorm=1.542, clip=40, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=139425
2023-01-11 04:37:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:35 - progress_bar.py[line:274] - INFO: epoch 001:  25224 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3814, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=25190, lr=3.89635e-05, gnorm=0.884, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139441
2023-01-11 04:37:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:52 - progress_bar.py[line:274] - INFO: epoch 001:  25234 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4667, wps=99.3, ups=0.61, wpb=108.2, bsz=40, num_updates=25200, lr=3.89583e-05, gnorm=0.491, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=139458
2023-01-11 04:37:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:37:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:08 - progress_bar.py[line:274] - INFO: epoch 001:  25244 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3796, wps=101.5, ups=0.62, wpb=108.7, bsz=40, num_updates=25210, lr=3.89531e-05, gnorm=0.389, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=139474
2023-01-11 04:38:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:25 - progress_bar.py[line:274] - INFO: epoch 001:  25254 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3878, wps=102.5, ups=0.62, wpb=110.7, bsz=40, num_updates=25220, lr=3.89479e-05, gnorm=0.399, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139491
2023-01-11 04:38:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:41 - progress_bar.py[line:274] - INFO: epoch 001:  25264 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4717, wps=102.7, ups=0.62, wpb=109.7, bsz=40, num_updates=25230, lr=3.89427e-05, gnorm=0.309, clip=0, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=139507
2023-01-11 04:38:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:38:57 - progress_bar.py[line:274] - INFO: epoch 001:  25274 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4881, wps=103.7, ups=0.62, wpb=110.8, bsz=40, num_updates=25240, lr=3.89375e-05, gnorm=0.295, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139523
2023-01-11 04:38:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:13 - progress_bar.py[line:274] - INFO: epoch 001:  25284 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4891, wps=103.3, ups=0.63, wpb=109.5, bsz=40, num_updates=25250, lr=3.89323e-05, gnorm=0.666, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139539
2023-01-11 04:39:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:30 - progress_bar.py[line:274] - INFO: epoch 001:  25294 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3953, wps=99.8, ups=0.61, wpb=109.6, bsz=40, num_updates=25260, lr=3.89271e-05, gnorm=0.361, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=139556
2023-01-11 04:39:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:47 - progress_bar.py[line:274] - INFO: epoch 001:  25304 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.381, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=25270, lr=3.89219e-05, gnorm=0.64, clip=20, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=139573
2023-01-11 04:39:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:39:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:03 - progress_bar.py[line:274] - INFO: epoch 001:  25314 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4906, wps=100.1, ups=0.61, wpb=109.3, bsz=40, num_updates=25280, lr=3.89167e-05, gnorm=0.347, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=139589
2023-01-11 04:40:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:20 - progress_bar.py[line:274] - INFO: epoch 001:  25324 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4409, wps=101.4, ups=0.62, wpb=109.4, bsz=40, num_updates=25290, lr=3.89115e-05, gnorm=0.237, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139606
2023-01-11 04:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:37 - progress_bar.py[line:274] - INFO: epoch 001:  25334 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4598, wps=98.4, ups=0.6, wpb=109.7, bsz=40, num_updates=25300, lr=3.89063e-05, gnorm=0.78, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=139623
2023-01-11 04:40:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:53 - progress_bar.py[line:274] - INFO: epoch 001:  25344 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3868, wps=100.3, ups=0.61, wpb=109.9, bsz=40, num_updates=25310, lr=3.8901e-05, gnorm=0.359, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=139639
2023-01-11 04:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:40:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:10 - progress_bar.py[line:274] - INFO: epoch 001:  25354 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4574, wps=100.5, ups=0.61, wpb=110.5, bsz=40, num_updates=25320, lr=3.88958e-05, gnorm=0.808, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=139656
2023-01-11 04:41:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:26 - progress_bar.py[line:274] - INFO: epoch 001:  25364 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4343, wps=102.6, ups=0.62, wpb=110.1, bsz=40, num_updates=25330, lr=3.88906e-05, gnorm=0.313, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139672
2023-01-11 04:41:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:43 - progress_bar.py[line:274] - INFO: epoch 001:  25374 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4412, wps=102.1, ups=0.62, wpb=110.1, bsz=40, num_updates=25340, lr=3.88854e-05, gnorm=0.545, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=139689
2023-01-11 04:41:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:41:59 - progress_bar.py[line:274] - INFO: epoch 001:  25384 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4301, wps=102.1, ups=0.62, wpb=110.6, bsz=40, num_updates=25350, lr=3.88802e-05, gnorm=0.52, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139705
2023-01-11 04:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:16 - progress_bar.py[line:274] - INFO: epoch 001:  25394 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4198, wps=103, ups=0.61, wpb=111.9, bsz=40, num_updates=25360, lr=3.8875e-05, gnorm=0.299, clip=0, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=139722
2023-01-11 04:42:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:32 - progress_bar.py[line:274] - INFO: epoch 001:  25404 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4286, wps=100.8, ups=0.62, wpb=109, bsz=40, num_updates=25370, lr=3.88698e-05, gnorm=0.289, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139738
2023-01-11 04:42:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:48 - progress_bar.py[line:274] - INFO: epoch 001:  25414 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=102, ups=0.62, wpb=109.9, bsz=40, num_updates=25380, lr=3.88646e-05, gnorm=0.272, clip=0, loss_scale=512, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=139755
2023-01-11 04:42:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:42:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:05 - progress_bar.py[line:274] - INFO: epoch 001:  25424 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4167, wps=99.7, ups=0.61, wpb=109.9, bsz=40, num_updates=25390, lr=3.88594e-05, gnorm=0.948, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139771
2023-01-11 04:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:22 - progress_bar.py[line:274] - INFO: epoch 001:  25434 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5376, wps=101.1, ups=0.61, wpb=111.2, bsz=40, num_updates=25400, lr=3.88542e-05, gnorm=0.378, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=139788
2023-01-11 04:43:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:38 - progress_bar.py[line:274] - INFO: epoch 001:  25444 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3789, wps=103.4, ups=0.63, wpb=110.1, bsz=40, num_updates=25410, lr=3.8849e-05, gnorm=1.061, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=139804
2023-01-11 04:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:55 - progress_bar.py[line:274] - INFO: epoch 001:  25454 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.2925, wps=100.9, ups=0.62, wpb=109.2, bsz=40, num_updates=25420, lr=3.88437e-05, gnorm=0.23, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139821
2023-01-11 04:43:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:43:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:11 - progress_bar.py[line:274] - INFO: epoch 001:  25464 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4956, wps=102, ups=0.62, wpb=109.5, bsz=40, num_updates=25430, lr=3.88385e-05, gnorm=0.645, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=139837
2023-01-11 04:44:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:28 - progress_bar.py[line:274] - INFO: epoch 001:  25474 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3784, wps=98.6, ups=0.6, wpb=108.9, bsz=40, num_updates=25440, lr=3.88333e-05, gnorm=0.362, clip=10, loss_scale=512, train_wall=17, gb_free=9.8, ema_decay=0.9999, wall=139854
2023-01-11 04:44:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:44 - progress_bar.py[line:274] - INFO: epoch 001:  25484 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4257, wps=101.3, ups=0.61, wpb=110.7, bsz=40, num_updates=25450, lr=3.88281e-05, gnorm=0.265, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=139871
2023-01-11 04:44:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:44:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:00 - progress_bar.py[line:274] - INFO: epoch 001:  25494 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4545, wps=104.4, ups=0.63, wpb=110.6, bsz=40, num_updates=25460, lr=3.88229e-05, gnorm=0.486, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=139887
2023-01-11 04:45:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:17 - progress_bar.py[line:274] - INFO: epoch 001:  25504 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4198, wps=103.1, ups=0.62, wpb=111.3, bsz=40, num_updates=25470, lr=3.88177e-05, gnorm=1.641, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139903
2023-01-11 04:45:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:34 - progress_bar.py[line:274] - INFO: epoch 001:  25514 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3786, wps=100.8, ups=0.6, wpb=111.2, bsz=40, num_updates=25480, lr=3.88125e-05, gnorm=0.917, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=139920
2023-01-11 04:45:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:50 - progress_bar.py[line:274] - INFO: epoch 001:  25524 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.463, wps=99.3, ups=0.61, wpb=108.9, bsz=40, num_updates=25490, lr=3.88073e-05, gnorm=0.414, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=139937
2023-01-11 04:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:45:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:06 - progress_bar.py[line:274] - INFO: epoch 001:  25534 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4111, wps=105.8, ups=0.63, wpb=112, bsz=40, num_updates=25500, lr=3.88021e-05, gnorm=1.038, clip=20, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=139953
2023-01-11 04:46:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:23 - progress_bar.py[line:274] - INFO: epoch 001:  25544 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4369, wps=99.2, ups=0.6, wpb=110.1, bsz=40, num_updates=25510, lr=3.87969e-05, gnorm=0.361, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=139970
2023-01-11 04:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:40 - progress_bar.py[line:274] - INFO: epoch 001:  25554 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4766, wps=100.4, ups=0.61, wpb=109.8, bsz=40, num_updates=25520, lr=3.87917e-05, gnorm=0.961, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=139986
2023-01-11 04:46:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:46:57 - progress_bar.py[line:274] - INFO: epoch 001:  25564 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5673, wps=100.1, ups=0.6, wpb=110.4, bsz=40, num_updates=25530, lr=3.87865e-05, gnorm=0.814, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140003
2023-01-11 04:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:14 - progress_bar.py[line:274] - INFO: epoch 001:  25574 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4419, wps=99.5, ups=0.6, wpb=110.9, bsz=40, num_updates=25540, lr=3.87813e-05, gnorm=0.504, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=140020
2023-01-11 04:47:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:30 - progress_bar.py[line:274] - INFO: epoch 001:  25584 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.43, wps=101, ups=0.61, wpb=109.7, bsz=40, num_updates=25550, lr=3.8776e-05, gnorm=0.253, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=140036
2023-01-11 04:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:47 - progress_bar.py[line:274] - INFO: epoch 001:  25594 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4286, wps=100.6, ups=0.62, wpb=108.7, bsz=40, num_updates=25560, lr=3.87708e-05, gnorm=0.277, clip=10, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=140053
2023-01-11 04:47:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:47:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:03 - progress_bar.py[line:274] - INFO: epoch 001:  25604 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4286, wps=100.1, ups=0.61, wpb=110.1, bsz=40, num_updates=25570, lr=3.87656e-05, gnorm=0.245, clip=0, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=140070
2023-01-11 04:48:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:20 - progress_bar.py[line:274] - INFO: epoch 001:  25614 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.5, wps=99.1, ups=0.61, wpb=108.1, bsz=40, num_updates=25580, lr=3.87604e-05, gnorm=2.01, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=140086
2023-01-11 04:48:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:37 - progress_bar.py[line:274] - INFO: epoch 001:  25624 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4388, wps=100.1, ups=0.61, wpb=110, bsz=40, num_updates=25590, lr=3.87552e-05, gnorm=0.322, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140103
2023-01-11 04:48:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:53 - progress_bar.py[line:274] - INFO: epoch 001:  25634 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4393, wps=101, ups=0.62, wpb=108.4, bsz=40, num_updates=25600, lr=3.875e-05, gnorm=0.744, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=140119
2023-01-11 04:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:48:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:09 - progress_bar.py[line:274] - INFO: epoch 001:  25644 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=102.7, ups=0.63, wpb=108.1, bsz=40, num_updates=25610, lr=3.87448e-05, gnorm=0.278, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140135
2023-01-11 04:49:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:25 - progress_bar.py[line:274] - INFO: epoch 001:  25654 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4086, wps=103.8, ups=0.62, wpb=111.6, bsz=40, num_updates=25620, lr=3.87396e-05, gnorm=1.2, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140152
2023-01-11 04:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:42 - progress_bar.py[line:274] - INFO: epoch 001:  25664 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4773, wps=99.9, ups=0.6, wpb=110.7, bsz=40, num_updates=25630, lr=3.87344e-05, gnorm=0.313, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=140169
2023-01-11 04:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:49:59 - progress_bar.py[line:274] - INFO: epoch 001:  25674 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5052, wps=102.9, ups=0.62, wpb=110.2, bsz=40, num_updates=25640, lr=3.87292e-05, gnorm=1.211, clip=20, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=140185
2023-01-11 04:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:15 - progress_bar.py[line:274] - INFO: epoch 001:  25684 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4419, wps=102.3, ups=0.62, wpb=109.9, bsz=40, num_updates=25650, lr=3.8724e-05, gnorm=0.348, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140201
2023-01-11 04:50:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:32 - progress_bar.py[line:274] - INFO: epoch 001:  25694 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3431, wps=99, ups=0.6, wpb=109.3, bsz=40, num_updates=25660, lr=3.87188e-05, gnorm=0.262, clip=0, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=140218
2023-01-11 04:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:48 - progress_bar.py[line:274] - INFO: epoch 001:  25704 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4216, wps=101.7, ups=0.62, wpb=109.4, bsz=40, num_updates=25670, lr=3.87135e-05, gnorm=0.437, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140234
2023-01-11 04:50:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:50:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:05 - progress_bar.py[line:274] - INFO: epoch 001:  25714 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3925, wps=97.6, ups=0.6, wpb=108.3, bsz=40, num_updates=25680, lr=3.87083e-05, gnorm=0.516, clip=20, loss_scale=1024, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=140251
2023-01-11 04:51:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:21 - progress_bar.py[line:274] - INFO: epoch 001:  25724 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4381, wps=102.1, ups=0.62, wpb=110.3, bsz=40, num_updates=25690, lr=3.87031e-05, gnorm=0.576, clip=20, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=140268
2023-01-11 04:51:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:38 - progress_bar.py[line:274] - INFO: epoch 001:  25734 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4653, wps=99.2, ups=0.61, wpb=108.2, bsz=40, num_updates=25700, lr=3.86979e-05, gnorm=0.909, clip=10, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=140284
2023-01-11 04:51:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:55 - progress_bar.py[line:274] - INFO: epoch 001:  25744 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4038, wps=99.6, ups=0.61, wpb=109.1, bsz=40, num_updates=25710, lr=3.86927e-05, gnorm=0.601, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=140301
2023-01-11 04:51:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:51:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:11 - progress_bar.py[line:274] - INFO: epoch 001:  25754 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4674, wps=101.2, ups=0.61, wpb=110.9, bsz=40, num_updates=25720, lr=3.86875e-05, gnorm=0.245, clip=0, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=140318
2023-01-11 04:52:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:19 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 04:52:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:29 - progress_bar.py[line:274] - INFO: epoch 001:  25765 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4742, wps=94.9, ups=0.58, wpb=110, bsz=40, num_updates=25730, lr=3.86823e-05, gnorm=0.345, clip=0, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=140335
2023-01-11 04:52:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:46 - progress_bar.py[line:274] - INFO: epoch 001:  25775 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3696, wps=99.8, ups=0.6, wpb=110.9, bsz=40, num_updates=25740, lr=3.86771e-05, gnorm=0.53, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=140352
2023-01-11 04:52:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:52:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:02 - progress_bar.py[line:274] - INFO: epoch 001:  25785 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4038, wps=100.8, ups=0.61, wpb=110, bsz=40, num_updates=25750, lr=3.86719e-05, gnorm=0.307, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140369
2023-01-11 04:53:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:19 - progress_bar.py[line:274] - INFO: epoch 001:  25795 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4176, wps=104.2, ups=0.62, wpb=111.5, bsz=40, num_updates=25760, lr=3.86667e-05, gnorm=0.431, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140385
2023-01-11 04:53:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:35 - progress_bar.py[line:274] - INFO: epoch 001:  25805 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4742, wps=102.1, ups=0.62, wpb=110.2, bsz=40, num_updates=25770, lr=3.86615e-05, gnorm=0.221, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=140401
2023-01-11 04:53:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:51 - progress_bar.py[line:274] - INFO: epoch 001:  25815 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4022, wps=106, ups=0.64, wpb=109.8, bsz=40, num_updates=25780, lr=3.86563e-05, gnorm=0.469, clip=10, loss_scale=512, train_wall=15, gb_free=10.2, ema_decay=0.9999, wall=140417
2023-01-11 04:53:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:53:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:07 - progress_bar.py[line:274] - INFO: epoch 001:  25825 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4362, wps=103.1, ups=0.62, wpb=111.5, bsz=40, num_updates=25790, lr=3.8651e-05, gnorm=0.806, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=140434
2023-01-11 04:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:24 - progress_bar.py[line:274] - INFO: epoch 001:  25835 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3977, wps=102.2, ups=0.62, wpb=110.3, bsz=40, num_updates=25800, lr=3.86458e-05, gnorm=1.307, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140450
2023-01-11 04:54:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:40 - progress_bar.py[line:274] - INFO: epoch 001:  25845 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4848, wps=102.2, ups=0.62, wpb=110.1, bsz=40, num_updates=25810, lr=3.86406e-05, gnorm=0.365, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140466
2023-01-11 04:54:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:56 - progress_bar.py[line:274] - INFO: epoch 001:  25855 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4037, wps=102.4, ups=0.63, wpb=108.5, bsz=40, num_updates=25820, lr=3.86354e-05, gnorm=0.42, clip=10, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=140482
2023-01-11 04:54:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:54:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:13 - progress_bar.py[line:274] - INFO: epoch 001:  25865 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3855, wps=103.2, ups=0.62, wpb=111.7, bsz=40, num_updates=25830, lr=3.86302e-05, gnorm=0.781, clip=20, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=140499
2023-01-11 04:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:29 - progress_bar.py[line:274] - INFO: epoch 001:  25875 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4688, wps=101.2, ups=0.61, wpb=109.9, bsz=40, num_updates=25840, lr=3.8625e-05, gnorm=0.798, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=140515
2023-01-11 04:55:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:46 - progress_bar.py[line:274] - INFO: epoch 001:  25885 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.451, wps=98.4, ups=0.6, wpb=108.6, bsz=40, num_updates=25850, lr=3.86198e-05, gnorm=0.698, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=140532
2023-01-11 04:55:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:03 - progress_bar.py[line:274] - INFO: epoch 001:  25895 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4639, wps=100.8, ups=0.61, wpb=109.7, bsz=40, num_updates=25860, lr=3.86146e-05, gnorm=0.325, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140549
2023-01-11 04:56:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:20 - progress_bar.py[line:274] - INFO: epoch 001:  25905 / 100000 loss=0.323, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3936, wps=99, ups=0.6, wpb=110.9, bsz=40, num_updates=25870, lr=3.86094e-05, gnorm=1.871, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=140566
2023-01-11 04:56:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:36 - progress_bar.py[line:274] - INFO: epoch 001:  25915 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.402, wps=100.9, ups=0.61, wpb=110.6, bsz=40, num_updates=25880, lr=3.86042e-05, gnorm=0.752, clip=20, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=140582
2023-01-11 04:56:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:53 - progress_bar.py[line:274] - INFO: epoch 001:  25925 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5208, wps=100.9, ups=0.61, wpb=110.4, bsz=40, num_updates=25890, lr=3.8599e-05, gnorm=0.502, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=140599
2023-01-11 04:56:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:56:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:10 - progress_bar.py[line:274] - INFO: epoch 001:  25935 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3846, wps=99.6, ups=0.61, wpb=108.5, bsz=40, num_updates=25900, lr=3.85937e-05, gnorm=0.985, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=140616
2023-01-11 04:57:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:27 - progress_bar.py[line:274] - INFO: epoch 001:  25945 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4615, wps=99, ups=0.6, wpb=109.8, bsz=40, num_updates=25910, lr=3.85885e-05, gnorm=0.337, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=140633
2023-01-11 04:57:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:43 - progress_bar.py[line:274] - INFO: epoch 001:  25955 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5253, wps=102.2, ups=0.61, wpb=110.9, bsz=40, num_updates=25920, lr=3.85833e-05, gnorm=0.249, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=140649
2023-01-11 04:57:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:57:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:00 - progress_bar.py[line:274] - INFO: epoch 001:  25965 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5463, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=25930, lr=3.85781e-05, gnorm=0.965, clip=10, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=140666
2023-01-11 04:58:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:16 - progress_bar.py[line:274] - INFO: epoch 001:  25975 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4356, wps=102.5, ups=0.62, wpb=110.3, bsz=40, num_updates=25940, lr=3.85729e-05, gnorm=0.5, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140682
2023-01-11 04:58:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:33 - progress_bar.py[line:274] - INFO: epoch 001:  25985 / 100000 loss=0.33, loss_v1=0, loss_v2=0, nll_loss=0.181, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.3519, wps=98.8, ups=0.61, wpb=108.3, bsz=40, num_updates=25950, lr=3.85677e-05, gnorm=0.86, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=140699
2023-01-11 04:58:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:49 - progress_bar.py[line:274] - INFO: epoch 001:  25995 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4848, wps=101.6, ups=0.61, wpb=110.1, bsz=40, num_updates=25960, lr=3.85625e-05, gnorm=0.205, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=140715
2023-01-11 04:58:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:58:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:06 - progress_bar.py[line:274] - INFO: epoch 001:  26005 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5049, wps=99.4, ups=0.6, wpb=110.5, bsz=40, num_updates=25970, lr=3.85573e-05, gnorm=0.669, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=140732
2023-01-11 04:59:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:23 - progress_bar.py[line:274] - INFO: epoch 001:  26015 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3774, wps=97.9, ups=0.6, wpb=109.1, bsz=40, num_updates=25980, lr=3.85521e-05, gnorm=0.194, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=140749
2023-01-11 04:59:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:39 - progress_bar.py[line:274] - INFO: epoch 001:  26025 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4646, wps=102, ups=0.62, wpb=110.4, bsz=40, num_updates=25990, lr=3.85469e-05, gnorm=0.369, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=140766
2023-01-11 04:59:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 04:59:56 - progress_bar.py[line:274] - INFO: epoch 001:  26035 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4037, wps=101.2, ups=0.61, wpb=110.1, bsz=40, num_updates=26000, lr=3.85417e-05, gnorm=0.277, clip=0, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=140782
2023-01-11 04:59:56 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 04:59:58 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 04:59:58 - train.py[line:551] - INFO: load:1.30 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 05:00:15 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.28 GiB (GPU 0; 39.59 GiB total capacity; 9.34 GiB already allocated; 3.06 GiB free; 34.04 GiB reserved in total by PyTorch)
2023-01-11 05:00:15 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9563 MB |   15881 MB |   19844 TB |   19844 TB |
|       from large pool |    9389 MB |   15706 MB |   19838 TB |   19838 TB |
|       from small pool |     174 MB |     175 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9563 MB |   15881 MB |   19844 TB |   19844 TB |
|       from large pool |    9389 MB |   15706 MB |   19838 TB |   19838 TB |
|       from small pool |     174 MB |     175 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   34858 MB |   37192 MB |  316794 MB |  281936 MB |
|       from large pool |   34682 MB |   37010 MB |  316360 MB |  281678 MB |
|       from small pool |     176 MB |     182 MB |     434 MB |     258 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   25294 MB |   29884 MB |   20422 TB |   20422 TB |
|       from large pool |   25292 MB |   29882 MB |   20415 TB |   20415 TB |
|       from small pool |       1 MB |       3 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4634    |    4648    |     949 M  |     949 M  |
|       from large pool |     698    |     710    |     292 M  |     292 M  |
|       from small pool |    3936    |    3946    |     656 M  |     656 M  |
|---------------------------------------------------------------------------|
| Active allocs         |    4634    |    4648    |     949 M  |     949 M  |
|       from large pool |     698    |     710    |     292 M  |     292 M  |
|       from small pool |    3936    |    3946    |     656 M  |     656 M  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     184    |     194    |     909    |     725    |
|       from large pool |      96    |     103    |     692    |     596    |
|       from small pool |      88    |      91    |     217    |     129    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     133    |  692161 K  |  692161 K  |
|       from large pool |      71    |      71    |  114797 K  |  114797 K  |
|       from small pool |      55    |      67    |  577363 K  |  577363 K  |
|===========================================================================|

2023-01-11 05:00:15 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-11 05:00:15 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-01-11 05:02:30 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 05:02:30 - train.py[line:551] - INFO: load:1.33 valid_run:151.81 task_valid:147.92 collect_output:1.88
2023-01-11 05:04:57 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 05:04:57 - train.py[line:551] - INFO: load:1.36 valid_run:299.18 task_valid:290.78 collect_output:5.36
2023-01-11 05:07:28 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 05:07:28 - train.py[line:551] - INFO: load:1.38 valid_run:450.14 task_valid:433.74 collect_output:12.31
2023-01-11 05:09:56 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 05:09:56 - train.py[line:551] - INFO: load:1.41 valid_run:598.52 task_valid:578.43 collect_output:14.98
2023-01-11 05:12:28 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 05:12:28 - train.py[line:551] - INFO: load:1.43 valid_run:749.97 task_valid:725.59 collect_output:18.22
2023-01-11 05:14:59 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 05:14:59 - train.py[line:551] - INFO: load:1.46 valid_run:900.68 task_valid:870.87 collect_output:22.62
2023-01-11 05:17:31 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 05:17:31 - train.py[line:551] - INFO: load:1.48 valid_run:1052.89 task_valid:1016.83 collect_output:27.84
2023-01-11 05:20:01 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 05:20:01 - train.py[line:551] - INFO: load:1.51 valid_run:1203.10 task_valid:1157.70 collect_output:36.15
2023-01-11 05:22:30 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 05:22:30 - train.py[line:551] - INFO: load:1.54 valid_run:1351.77 task_valid:1302.45 collect_output:39.05
2023-01-11 05:24:58 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 05:24:58 - train.py[line:551] - INFO: load:1.56 valid_run:1499.25 task_valid:1445.46 collect_output:42.47
2023-01-11 05:27:27 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 05:27:27 - train.py[line:551] - INFO: load:1.59 valid_run:1648.30 task_valid:1590.37 collect_output:45.60
2023-01-11 05:29:56 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 05:29:56 - train.py[line:551] - INFO: load:1.61 valid_run:1797.46 task_valid:1735.20 collect_output:48.87
2023-01-11 05:32:25 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 05:32:25 - train.py[line:551] - INFO: load:1.64 valid_run:1946.11 task_valid:1876.89 collect_output:54.78
2023-01-11 05:34:55 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 05:34:55 - train.py[line:551] - INFO: load:1.67 valid_run:2095.85 task_valid:2022.28 collect_output:58.09
2023-01-11 05:37:24 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 05:37:24 - train.py[line:551] - INFO: load:1.69 valid_run:2245.46 task_valid:2168.87 collect_output:60.10
2023-01-11 05:39:54 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 05:39:54 - train.py[line:551] - INFO: load:1.72 valid_run:2394.81 task_valid:2312.94 collect_output:64.36
2023-01-11 05:42:24 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 05:42:24 - train.py[line:551] - INFO: load:1.74 valid_run:2545.03 task_valid:2458.26 collect_output:68.26
2023-01-11 05:44:54 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 05:44:54 - train.py[line:551] - INFO: load:1.77 valid_run:2694.85 task_valid:2605.09 collect_output:70.22
2023-01-11 05:47:21 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 05:47:21 - train.py[line:551] - INFO: load:1.79 valid_run:2842.18 task_valid:2746.76 collect_output:74.85
2023-01-11 05:49:51 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 05:49:51 - train.py[line:551] - INFO: load:1.82 valid_run:2991.67 task_valid:2891.92 collect_output:78.14
2023-01-11 05:52:22 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 05:52:22 - train.py[line:551] - INFO: load:1.84 valid_run:3142.07 task_valid:3036.49 collect_output:82.95
2023-01-11 05:54:50 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 05:54:50 - train.py[line:551] - INFO: load:1.87 valid_run:3290.52 task_valid:3181.04 collect_output:85.81
2023-01-11 05:57:20 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 05:57:20 - train.py[line:551] - INFO: load:1.90 valid_run:3440.77 task_valid:3327.45 collect_output:88.58
2023-01-11 05:59:51 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 05:59:51 - train.py[line:551] - INFO: load:1.92 valid_run:3591.38 task_valid:3473.93 collect_output:91.70

====================================================================================================
SGG eval:     R @ 50: 0.4696;     R @ 100: 0.5445;     R @ 500: 0.5807;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2842;    mR @ 100: 0.3349;    mR @ 500: 0.3759;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.5833) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3710) (lying on:0.0500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7604) (playing:0.0000) (riding:0.6801) (says:0.0000) (sitting on:0.7418) (standing on:0.2210) (using:0.6000) (walking in:0.0000) (walking on:0.3063) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4696;     R @ 100: 0.5445;     R @ 500: 0.5807;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2842;    mR @ 100: 0.3349;    mR @ 500: 0.3759;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.5833) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3710) (lying on:0.0500) (mounted on:0.0000) (painted on:0.1667) (parked on:0.7604) (playing:0.0000) (riding:0.6801) (says:0.0000) (sitting on:0.7418) (standing on:0.2210) (using:0.6000) (walking in:0.0000) (walking on:0.3063) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-11 06:02:22 - train.py[line:487] - INFO: 0.5444624649859944
2023-01-11 06:02:22 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 06:02:22 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.362 | loss_v1 0 | loss_v2 0 | nll_loss 0.208 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.544462 | ppl 1.16 | vqa_score 0.4752 | wps 119.8 | wpb 89.9 | bsz 30 | num_updates 26000 | best_R@100 0.69005
2023-01-11 06:02:22 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 26000 updates
2023-01-11 06:02:22 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_26000.pt
2023-01-11 06:03:00 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_26000.pt
2023-01-11 06:04:19 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_26000.pt (epoch 1 @ 26000 updates, score 0.5444624649859944) (writing took 116.9304612595588 seconds)
2023-01-11 06:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:36 - progress_bar.py[line:274] - INFO: epoch 001:  26045 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4505, wps=0.4, ups=0, wpb=110.6, bsz=40, num_updates=26010, lr=3.85365e-05, gnorm=0.553, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=144662
2023-01-11 06:04:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:52 - progress_bar.py[line:274] - INFO: epoch 001:  26055 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4516, wps=102.3, ups=0.61, wpb=111.1, bsz=40, num_updates=26020, lr=3.85313e-05, gnorm=0.404, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=144679
2023-01-11 06:04:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:04:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:09 - progress_bar.py[line:274] - INFO: epoch 001:  26065 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4151, wps=100.2, ups=0.61, wpb=109.4, bsz=40, num_updates=26030, lr=3.8526e-05, gnorm=0.297, clip=0, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=144695
2023-01-11 06:05:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:26 - progress_bar.py[line:274] - INFO: epoch 001:  26075 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3936, wps=100.8, ups=0.61, wpb=109.8, bsz=40, num_updates=26040, lr=3.85208e-05, gnorm=0.243, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=144712
2023-01-11 06:05:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:42 - progress_bar.py[line:274] - INFO: epoch 001:  26085 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5152, wps=100.8, ups=0.61, wpb=109.7, bsz=40, num_updates=26050, lr=3.85156e-05, gnorm=0.305, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=144728
2023-01-11 06:05:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:05:59 - progress_bar.py[line:274] - INFO: epoch 001:  26095 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4216, wps=98.3, ups=0.6, wpb=109.6, bsz=40, num_updates=26060, lr=3.85104e-05, gnorm=0.477, clip=10, loss_scale=512, train_wall=17, gb_free=10.8, ema_decay=0.9999, wall=144746
2023-01-11 06:05:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:16 - progress_bar.py[line:274] - INFO: epoch 001:  26105 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4674, wps=102, ups=0.61, wpb=111.1, bsz=40, num_updates=26070, lr=3.85052e-05, gnorm=1.219, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=144762
2023-01-11 06:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:32 - progress_bar.py[line:274] - INFO: epoch 001:  26115 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4536, wps=101.5, ups=0.62, wpb=109.5, bsz=40, num_updates=26080, lr=3.85e-05, gnorm=0.344, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=144779
2023-01-11 06:06:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:49 - progress_bar.py[line:274] - INFO: epoch 001:  26125 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4787, wps=104.1, ups=0.63, wpb=110.6, bsz=40, num_updates=26090, lr=3.84948e-05, gnorm=1.438, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=144795
2023-01-11 06:06:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:06:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:05 - progress_bar.py[line:274] - INFO: epoch 001:  26135 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4528, wps=99.5, ups=0.61, wpb=108.7, bsz=40, num_updates=26100, lr=3.84896e-05, gnorm=0.404, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=144811
2023-01-11 06:07:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:21 - progress_bar.py[line:274] - INFO: epoch 001:  26145 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4423, wps=103.2, ups=0.63, wpb=109.5, bsz=40, num_updates=26110, lr=3.84844e-05, gnorm=0.361, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=144828
2023-01-11 06:07:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:38 - progress_bar.py[line:274] - INFO: epoch 001:  26155 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5417, wps=100.5, ups=0.61, wpb=110.1, bsz=40, num_updates=26120, lr=3.84792e-05, gnorm=0.869, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=144844
2023-01-11 06:07:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:54 - progress_bar.py[line:274] - INFO: epoch 001:  26165 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.2473, wps=102.1, ups=0.62, wpb=109.9, bsz=40, num_updates=26130, lr=3.8474e-05, gnorm=0.741, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=144861
2023-01-11 06:07:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:07:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:11 - progress_bar.py[line:274] - INFO: epoch 001:  26175 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4035, wps=98.9, ups=0.61, wpb=108.1, bsz=40, num_updates=26140, lr=3.84688e-05, gnorm=1.233, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=144877
2023-01-11 06:08:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:28 - progress_bar.py[line:274] - INFO: epoch 001:  26185 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4, wps=98.9, ups=0.6, wpb=109.3, bsz=40, num_updates=26150, lr=3.84635e-05, gnorm=0.508, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=144894
2023-01-11 06:08:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:45 - progress_bar.py[line:274] - INFO: epoch 001:  26195 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4954, wps=98.5, ups=0.61, wpb=108.1, bsz=40, num_updates=26160, lr=3.84583e-05, gnorm=0.337, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=144911
2023-01-11 06:08:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:08:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:00 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 06:09:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:03 - progress_bar.py[line:274] - INFO: epoch 001:  26206 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.5, nsentences=40, sample_size=109.5, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4696, wps=94, ups=0.54, wpb=109.5, bsz=40, num_updates=26170, lr=3.84531e-05, gnorm=0.368, clip=10, loss_scale=256, train_wall=19, gb_free=10.2, ema_decay=0.9999, wall=144930
2023-01-11 06:09:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:20 - progress_bar.py[line:274] - INFO: epoch 001:  26216 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4184, wps=99.6, ups=0.61, wpb=109.5, bsz=40, num_updates=26180, lr=3.84479e-05, gnorm=0.307, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=144946
2023-01-11 06:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:37 - progress_bar.py[line:274] - INFO: epoch 001:  26226 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5278, wps=99.2, ups=0.61, wpb=108.3, bsz=40, num_updates=26190, lr=3.84427e-05, gnorm=0.819, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=144963
2023-01-11 06:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:53 - progress_bar.py[line:274] - INFO: epoch 001:  26236 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5109, wps=100, ups=0.61, wpb=110, bsz=40, num_updates=26200, lr=3.84375e-05, gnorm=0.418, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=144980
2023-01-11 06:09:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:10 - progress_bar.py[line:274] - INFO: epoch 001:  26246 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4333, wps=100.3, ups=0.61, wpb=109.9, bsz=40, num_updates=26210, lr=3.84323e-05, gnorm=0.552, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=144996
2023-01-11 06:10:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:27 - progress_bar.py[line:274] - INFO: epoch 001:  26256 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4737, wps=99.4, ups=0.61, wpb=109.2, bsz=40, num_updates=26220, lr=3.84271e-05, gnorm=0.454, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145013
2023-01-11 06:10:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:43 - progress_bar.py[line:274] - INFO: epoch 001:  26266 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4653, wps=100.6, ups=0.62, wpb=108.7, bsz=40, num_updates=26230, lr=3.84219e-05, gnorm=0.26, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=145029
2023-01-11 06:10:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:10:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:00 - progress_bar.py[line:274] - INFO: epoch 001:  26276 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4107, wps=97.1, ups=0.6, wpb=107.9, bsz=40, num_updates=26240, lr=3.84167e-05, gnorm=0.249, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=145046
2023-01-11 06:11:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:17 - progress_bar.py[line:274] - INFO: epoch 001:  26286 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3556, wps=101.5, ups=0.61, wpb=111.1, bsz=40, num_updates=26250, lr=3.84115e-05, gnorm=0.382, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=145063
2023-01-11 06:11:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:33 - progress_bar.py[line:274] - INFO: epoch 001:  26296 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.434, wps=100.8, ups=0.62, wpb=108.2, bsz=40, num_updates=26260, lr=3.84063e-05, gnorm=0.253, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=145079
2023-01-11 06:11:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:50 - progress_bar.py[line:274] - INFO: epoch 001:  26306 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4512, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=26270, lr=3.8401e-05, gnorm=0.287, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145096
2023-01-11 06:11:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:11:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:07 - progress_bar.py[line:274] - INFO: epoch 001:  26316 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4516, wps=99.9, ups=0.61, wpb=110.1, bsz=40, num_updates=26280, lr=3.83958e-05, gnorm=0.576, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145113
2023-01-11 06:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:23 - progress_bar.py[line:274] - INFO: epoch 001:  26326 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5053, wps=102.7, ups=0.63, wpb=109.1, bsz=40, num_updates=26290, lr=3.83906e-05, gnorm=0.27, clip=0, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=145129
2023-01-11 06:12:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:39 - progress_bar.py[line:274] - INFO: epoch 001:  26336 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4886, wps=104, ups=0.63, wpb=110.6, bsz=40, num_updates=26300, lr=3.83854e-05, gnorm=0.43, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145145
2023-01-11 06:12:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:56 - progress_bar.py[line:274] - INFO: epoch 001:  26346 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.48, wps=98.8, ups=0.6, wpb=109, bsz=40, num_updates=26310, lr=3.83802e-05, gnorm=0.304, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145162
2023-01-11 06:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:12:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:12 - progress_bar.py[line:274] - INFO: epoch 001:  26356 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4444, wps=101.5, ups=0.63, wpb=108.3, bsz=40, num_updates=26320, lr=3.8375e-05, gnorm=0.468, clip=10, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=145178
2023-01-11 06:13:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:29 - progress_bar.py[line:274] - INFO: epoch 001:  26366 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4574, wps=98.8, ups=0.6, wpb=110.3, bsz=40, num_updates=26330, lr=3.83698e-05, gnorm=0.553, clip=20, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=145195
2023-01-11 06:13:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:46 - progress_bar.py[line:274] - INFO: epoch 001:  26376 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.37, wps=98.6, ups=0.6, wpb=110.2, bsz=40, num_updates=26340, lr=3.83646e-05, gnorm=0.658, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=145212
2023-01-11 06:13:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:03 - progress_bar.py[line:274] - INFO: epoch 001:  26386 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3846, wps=98.7, ups=0.6, wpb=110.1, bsz=40, num_updates=26350, lr=3.83594e-05, gnorm=0.437, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=145229
2023-01-11 06:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:19 - progress_bar.py[line:274] - INFO: epoch 001:  26396 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4286, wps=101, ups=0.61, wpb=110.1, bsz=40, num_updates=26360, lr=3.83542e-05, gnorm=0.354, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145246
2023-01-11 06:14:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:36 - progress_bar.py[line:274] - INFO: epoch 001:  26406 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4182, wps=102.4, ups=0.62, wpb=110, bsz=40, num_updates=26370, lr=3.8349e-05, gnorm=0.226, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=145262
2023-01-11 06:14:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:52 - progress_bar.py[line:274] - INFO: epoch 001:  26416 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.396, wps=101.4, ups=0.61, wpb=110.6, bsz=40, num_updates=26380, lr=3.83438e-05, gnorm=0.314, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=145279
2023-01-11 06:14:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:14:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:09 - progress_bar.py[line:274] - INFO: epoch 001:  26426 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3182, wps=99.4, ups=0.6, wpb=110.1, bsz=40, num_updates=26390, lr=3.83385e-05, gnorm=0.276, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=145295
2023-01-11 06:15:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:26 - progress_bar.py[line:274] - INFO: epoch 001:  26436 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.2857, wps=101.7, ups=0.62, wpb=109.9, bsz=40, num_updates=26400, lr=3.83333e-05, gnorm=0.692, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=145312
2023-01-11 06:15:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:42 - progress_bar.py[line:274] - INFO: epoch 001:  26446 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4952, wps=101.1, ups=0.61, wpb=110.2, bsz=40, num_updates=26410, lr=3.83281e-05, gnorm=0.204, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=145328
2023-01-11 06:15:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:15:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:00 - progress_bar.py[line:274] - INFO: epoch 001:  26456 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4808, wps=100.8, ups=0.62, wpb=108.5, bsz=40, num_updates=26420, lr=3.83229e-05, gnorm=0.324, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145345
2023-01-11 06:16:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:18 - progress_bar.py[line:274] - INFO: epoch 001:  26466 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4526, wps=100.1, ups=0.61, wpb=110, bsz=40, num_updates=26430, lr=3.83177e-05, gnorm=0.5, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145363
2023-01-11 06:16:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:36 - progress_bar.py[line:274] - INFO: epoch 001:  26476 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=107.133, nsentences=40, sample_size=107.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4128, wps=98.9, ups=0.62, wpb=107.1, bsz=40, num_updates=26440, lr=3.83125e-05, gnorm=0.458, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=145381
2023-01-11 06:16:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:54 - progress_bar.py[line:274] - INFO: epoch 001:  26486 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3984, wps=99.1, ups=0.61, wpb=108.5, bsz=40, num_updates=26450, lr=3.83073e-05, gnorm=0.325, clip=10, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=145399
2023-01-11 06:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:16:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:11 - progress_bar.py[line:274] - INFO: epoch 001:  26496 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4945, wps=101.6, ups=0.61, wpb=110.2, bsz=40, num_updates=26460, lr=3.83021e-05, gnorm=0.591, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=145416
2023-01-11 06:17:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:29 - progress_bar.py[line:274] - INFO: epoch 001:  26506 / 100000 loss=0.322, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4095, wps=97.5, ups=0.6, wpb=108.7, bsz=40, num_updates=26470, lr=3.82969e-05, gnorm=0.732, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=145434
2023-01-11 06:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:48 - progress_bar.py[line:274] - INFO: epoch 001:  26516 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4434, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=26480, lr=3.82917e-05, gnorm=2.037, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145452
2023-01-11 06:17:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:17:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:06 - progress_bar.py[line:274] - INFO: epoch 001:  26526 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4194, wps=101.4, ups=0.61, wpb=110.3, bsz=40, num_updates=26490, lr=3.82865e-05, gnorm=0.338, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=145470
2023-01-11 06:18:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:24 - progress_bar.py[line:274] - INFO: epoch 001:  26536 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4245, wps=99.9, ups=0.61, wpb=110.1, bsz=40, num_updates=26500, lr=3.82812e-05, gnorm=0.352, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145489
2023-01-11 06:18:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:41 - progress_bar.py[line:274] - INFO: epoch 001:  26546 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3763, wps=101.6, ups=0.62, wpb=110.1, bsz=40, num_updates=26510, lr=3.8276e-05, gnorm=0.841, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145506
2023-01-11 06:18:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:18:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:00 - progress_bar.py[line:274] - INFO: epoch 001:  26556 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4388, wps=99.8, ups=0.61, wpb=109.9, bsz=40, num_updates=26520, lr=3.82708e-05, gnorm=0.269, clip=0, loss_scale=256, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=145524
2023-01-11 06:19:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:18 - progress_bar.py[line:274] - INFO: epoch 001:  26566 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=98.5, ups=0.6, wpb=110.3, bsz=40, num_updates=26530, lr=3.82656e-05, gnorm=0.339, clip=0, loss_scale=256, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=145543
2023-01-11 06:19:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:36 - progress_bar.py[line:274] - INFO: epoch 001:  26576 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4679, wps=100.2, ups=0.61, wpb=108.7, bsz=40, num_updates=26540, lr=3.82604e-05, gnorm=0.529, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=145561
2023-01-11 06:19:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:53 - progress_bar.py[line:274] - INFO: epoch 001:  26586 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3786, wps=102.7, ups=0.62, wpb=109.7, bsz=40, num_updates=26550, lr=3.82552e-05, gnorm=0.456, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=145578
2023-01-11 06:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:19:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:11 - progress_bar.py[line:274] - INFO: epoch 001:  26596 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4588, wps=101.8, ups=0.62, wpb=110.3, bsz=40, num_updates=26560, lr=3.825e-05, gnorm=0.363, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145596
2023-01-11 06:20:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:29 - progress_bar.py[line:274] - INFO: epoch 001:  26606 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4021, wps=100.2, ups=0.61, wpb=109.3, bsz=40, num_updates=26570, lr=3.82448e-05, gnorm=0.494, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=145613
2023-01-11 06:20:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:46 - progress_bar.py[line:274] - INFO: epoch 001:  26616 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4257, wps=100.8, ups=0.62, wpb=108.7, bsz=40, num_updates=26580, lr=3.82396e-05, gnorm=0.476, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=145631
2023-01-11 06:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:20:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:05 - progress_bar.py[line:274] - INFO: epoch 001:  26626 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4455, wps=99.5, ups=0.61, wpb=109.7, bsz=40, num_updates=26590, lr=3.82344e-05, gnorm=0.71, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145649
2023-01-11 06:21:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:23 - progress_bar.py[line:274] - INFO: epoch 001:  26636 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.48, wps=99.5, ups=0.6, wpb=110.1, bsz=40, num_updates=26600, lr=3.82292e-05, gnorm=0.317, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=145668
2023-01-11 06:21:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:41 - progress_bar.py[line:274] - INFO: epoch 001:  26646 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3208, wps=98.6, ups=0.61, wpb=108.4, bsz=40, num_updates=26610, lr=3.8224e-05, gnorm=0.429, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=145686
2023-01-11 06:21:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:21:58 - progress_bar.py[line:274] - INFO: epoch 001:  26656 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4681, wps=104.2, ups=0.63, wpb=110.6, bsz=40, num_updates=26620, lr=3.82188e-05, gnorm=0.823, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=145703
2023-01-11 06:21:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:16 - progress_bar.py[line:274] - INFO: epoch 001:  26666 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.398, wps=99.8, ups=0.6, wpb=110.5, bsz=40, num_updates=26630, lr=3.82135e-05, gnorm=0.894, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=145721
2023-01-11 06:22:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:34 - progress_bar.py[line:274] - INFO: epoch 001:  26676 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4043, wps=100.1, ups=0.6, wpb=110.3, bsz=40, num_updates=26640, lr=3.82083e-05, gnorm=0.398, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=145739
2023-01-11 06:22:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:52 - progress_bar.py[line:274] - INFO: epoch 001:  26686 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=99.5, ups=0.6, wpb=109.8, bsz=40, num_updates=26650, lr=3.82031e-05, gnorm=0.551, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=145757
2023-01-11 06:22:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:10 - progress_bar.py[line:274] - INFO: epoch 001:  26696 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.168, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3119, wps=99.2, ups=0.61, wpb=108.9, bsz=40, num_updates=26660, lr=3.81979e-05, gnorm=0.311, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=145775
2023-01-11 06:23:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:28 - progress_bar.py[line:274] - INFO: epoch 001:  26706 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.31, wps=102.7, ups=0.63, wpb=108.7, bsz=40, num_updates=26670, lr=3.81927e-05, gnorm=0.573, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145793
2023-01-11 06:23:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:45 - progress_bar.py[line:274] - INFO: epoch 001:  26716 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=99, ups=0.61, wpb=108.5, bsz=40, num_updates=26680, lr=3.81875e-05, gnorm=0.486, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145811
2023-01-11 06:23:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:23:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:03 - progress_bar.py[line:274] - INFO: epoch 001:  26726 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3368, wps=100.2, ups=0.61, wpb=108.9, bsz=40, num_updates=26690, lr=3.81823e-05, gnorm=0.289, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=145828
2023-01-11 06:24:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:21 - progress_bar.py[line:274] - INFO: epoch 001:  26736 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3761, wps=99.1, ups=0.61, wpb=108.6, bsz=40, num_updates=26700, lr=3.81771e-05, gnorm=0.416, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=145846
2023-01-11 06:24:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:39 - progress_bar.py[line:274] - INFO: epoch 001:  26746 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5051, wps=99.1, ups=0.6, wpb=110.9, bsz=40, num_updates=26710, lr=3.81719e-05, gnorm=0.245, clip=0, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=145864
2023-01-11 06:24:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:57 - progress_bar.py[line:274] - INFO: epoch 001:  26756 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3956, wps=101.6, ups=0.61, wpb=111.1, bsz=40, num_updates=26720, lr=3.81667e-05, gnorm=0.615, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=145882
2023-01-11 06:24:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:24:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:15 - progress_bar.py[line:274] - INFO: epoch 001:  26766 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4066, wps=101.9, ups=0.61, wpb=110.7, bsz=40, num_updates=26730, lr=3.81615e-05, gnorm=0.355, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145900
2023-01-11 06:25:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:33 - progress_bar.py[line:274] - INFO: epoch 001:  26776 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4421, wps=99.4, ups=0.6, wpb=110.2, bsz=40, num_updates=26740, lr=3.81563e-05, gnorm=0.966, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=145918
2023-01-11 06:25:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:51 - progress_bar.py[line:274] - INFO: epoch 001:  26786 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4255, wps=103, ups=0.62, wpb=111, bsz=40, num_updates=26750, lr=3.8151e-05, gnorm=0.232, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=145936
2023-01-11 06:25:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:25:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:10 - progress_bar.py[line:274] - INFO: epoch 001:  26796 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3725, wps=97.3, ups=0.59, wpb=109.1, bsz=40, num_updates=26760, lr=3.81458e-05, gnorm=0.233, clip=0, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=145954
2023-01-11 06:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:22 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 06:26:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:29 - progress_bar.py[line:274] - INFO: epoch 001:  26807 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=108.312, nsentences=40, sample_size=108.312, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4309, wps=94.3, ups=0.54, wpb=108.3, bsz=40, num_updates=26770, lr=3.81406e-05, gnorm=0.217, clip=0, loss_scale=256, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=145974
2023-01-11 06:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:48 - progress_bar.py[line:274] - INFO: epoch 001:  26817 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4545, wps=100.1, ups=0.61, wpb=109.8, bsz=40, num_updates=26780, lr=3.81354e-05, gnorm=0.392, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=145992
2023-01-11 06:26:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:26:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:05 - progress_bar.py[line:274] - INFO: epoch 001:  26827 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4857, wps=98.9, ups=0.61, wpb=108.7, bsz=40, num_updates=26790, lr=3.81302e-05, gnorm=0.351, clip=10, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=146010
2023-01-11 06:27:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:23 - progress_bar.py[line:274] - INFO: epoch 001:  26837 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3455, wps=104.8, ups=0.64, wpb=109.1, bsz=40, num_updates=26800, lr=3.8125e-05, gnorm=0.717, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=146028
2023-01-11 06:27:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:40 - progress_bar.py[line:274] - INFO: epoch 001:  26847 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4688, wps=104.1, ups=0.62, wpb=111.7, bsz=40, num_updates=26810, lr=3.81198e-05, gnorm=0.315, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=146045
2023-01-11 06:27:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:27:59 - progress_bar.py[line:274] - INFO: epoch 001:  26857 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4639, wps=99.4, ups=0.6, wpb=110.5, bsz=40, num_updates=26820, lr=3.81146e-05, gnorm=0.241, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=146064
2023-01-11 06:27:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:17 - progress_bar.py[line:274] - INFO: epoch 001:  26867 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5055, wps=102.2, ups=0.62, wpb=110.7, bsz=40, num_updates=26830, lr=3.81094e-05, gnorm=0.48, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=146081
2023-01-11 06:28:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:35 - progress_bar.py[line:274] - INFO: epoch 001:  26877 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4545, wps=100.4, ups=0.61, wpb=110.3, bsz=40, num_updates=26840, lr=3.81042e-05, gnorm=0.346, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=146100
2023-01-11 06:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:53 - progress_bar.py[line:274] - INFO: epoch 001:  26887 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4952, wps=99.4, ups=0.6, wpb=109.7, bsz=40, num_updates=26850, lr=3.8099e-05, gnorm=1.2, clip=40, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=146118
2023-01-11 06:28:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:28:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:10 - progress_bar.py[line:274] - INFO: epoch 001:  26897 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4712, wps=100.7, ups=0.61, wpb=109.8, bsz=40, num_updates=26860, lr=3.80938e-05, gnorm=0.292, clip=0, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=146135
2023-01-11 06:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:29 - progress_bar.py[line:274] - INFO: epoch 001:  26907 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4845, wps=102.2, ups=0.62, wpb=110.1, bsz=40, num_updates=26870, lr=3.80885e-05, gnorm=1.272, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=146153
2023-01-11 06:29:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:46 - progress_bar.py[line:274] - INFO: epoch 001:  26917 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4741, wps=100.2, ups=0.62, wpb=108, bsz=40, num_updates=26880, lr=3.80833e-05, gnorm=0.548, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=146171
2023-01-11 06:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:29:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:04 - progress_bar.py[line:274] - INFO: epoch 001:  26927 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4528, wps=99.8, ups=0.61, wpb=109.4, bsz=40, num_updates=26890, lr=3.80781e-05, gnorm=0.325, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=146189
2023-01-11 06:30:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:22 - progress_bar.py[line:274] - INFO: epoch 001:  26937 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4124, wps=103.5, ups=0.62, wpb=111.1, bsz=40, num_updates=26900, lr=3.80729e-05, gnorm=0.451, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=146207
2023-01-11 06:30:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:40 - progress_bar.py[line:274] - INFO: epoch 001:  26947 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.404, wps=100.3, ups=0.61, wpb=109.6, bsz=40, num_updates=26910, lr=3.80677e-05, gnorm=0.377, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=146225
2023-01-11 06:30:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:30:58 - progress_bar.py[line:274] - INFO: epoch 001:  26957 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5288, wps=98.1, ups=0.59, wpb=109.9, bsz=40, num_updates=26920, lr=3.80625e-05, gnorm=0.433, clip=10, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=146243
2023-01-11 06:30:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:16 - progress_bar.py[line:274] - INFO: epoch 001:  26967 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.413, wps=99.7, ups=0.6, wpb=110.7, bsz=40, num_updates=26930, lr=3.80573e-05, gnorm=0.425, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=146261
2023-01-11 06:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:34 - progress_bar.py[line:274] - INFO: epoch 001:  26977 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5534, wps=97.5, ups=0.6, wpb=108.3, bsz=40, num_updates=26940, lr=3.80521e-05, gnorm=0.39, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=146279
2023-01-11 06:31:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:52 - progress_bar.py[line:274] - INFO: epoch 001:  26987 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4231, wps=101.4, ups=0.61, wpb=110.3, bsz=40, num_updates=26950, lr=3.80469e-05, gnorm=0.28, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=146296
2023-01-11 06:31:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:09 - progress_bar.py[line:274] - INFO: epoch 001:  26997 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4086, wps=101.3, ups=0.62, wpb=109.6, bsz=40, num_updates=26960, lr=3.80417e-05, gnorm=0.654, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=146314
2023-01-11 06:32:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:27 - progress_bar.py[line:274] - INFO: epoch 001:  27007 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4771, wps=102, ups=0.62, wpb=110.3, bsz=40, num_updates=26970, lr=3.80365e-05, gnorm=0.798, clip=20, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=146332
2023-01-11 06:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:45 - progress_bar.py[line:274] - INFO: epoch 001:  27017 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.42, wps=101.3, ups=0.61, wpb=110.2, bsz=40, num_updates=26980, lr=3.80312e-05, gnorm=0.373, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=146350
2023-01-11 06:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:32:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:33:03 - progress_bar.py[line:274] - INFO: epoch 001:  27027 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4216, wps=100.2, ups=0.61, wpb=110.1, bsz=40, num_updates=26990, lr=3.8026e-05, gnorm=0.635, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=146368
2023-01-11 06:33:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:33:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:33:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:33:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:33:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 06:33:20 - progress_bar.py[line:274] - INFO: epoch 001:  27037 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4845, wps=98.7, ups=0.6, wpb=109.3, bsz=40, num_updates=27000, lr=3.80208e-05, gnorm=0.833, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=146386
2023-01-11 06:33:20 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 06:33:22 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 06:33:22 - train.py[line:551] - INFO: load:1.27 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 06:33:37 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.21 GiB (GPU 0; 39.59 GiB total capacity; 9.29 GiB already allocated; 562.19 MiB free; 36.55 GiB reserved in total by PyTorch)
2023-01-11 06:33:37 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 27        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9518 MB |   14748 MB |   20629 TB |   20629 TB |
|       from large pool |    9343 MB |   14573 MB |   20622 TB |   20622 TB |
|       from small pool |     174 MB |     175 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9518 MB |   14748 MB |   20629 TB |   20629 TB |
|       from large pool |    9343 MB |   14573 MB |   20622 TB |   20622 TB |
|       from small pool |     174 MB |     175 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   37432 MB |   37434 MB |  335288 MB |  297856 MB |
|       from large pool |   37256 MB |   37256 MB |  334834 MB |  297578 MB |
|       from small pool |     176 MB |     178 MB |     454 MB |     278 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   27913 MB |   27913 MB |   21240 TB |   21240 TB |
|       from large pool |   27912 MB |   27912 MB |   21232 TB |   21232 TB |
|       from small pool |       1 MB |       3 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4634    |    4648    |     986 M  |     986 M  |
|       from large pool |     698    |     710    |     304 M  |     304 M  |
|       from small pool |    3936    |    3946    |     682 M  |     682 M  |
|---------------------------------------------------------------------------|
| Active allocs         |    4634    |    4648    |     986 M  |     986 M  |
|       from large pool |     698    |     710    |     304 M  |     304 M  |
|       from small pool |    3936    |    3946    |     682 M  |     682 M  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     163    |     164    |     923    |     760    |
|       from large pool |      75    |      75    |     696    |     621    |
|       from small pool |      88    |      89    |     227    |     139    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     102    |     110    |  721812 K  |  721812 K  |
|       from large pool |      47    |      48    |  121463 K  |  121463 K  |
|       from small pool |      55    |      68    |  600349 K  |  600349 K  |
|===========================================================================|

2023-01-11 06:33:37 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-11 06:33:37 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-01-11 06:35:54 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 06:35:54 - train.py[line:551] - INFO: load:1.30 valid_run:152.21 task_valid:148.41 collect_output:1.99
2023-01-11 06:38:22 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 06:38:22 - train.py[line:551] - INFO: load:1.33 valid_run:300.00 task_valid:291.73 collect_output:5.39
2023-01-11 06:40:53 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 06:40:53 - train.py[line:551] - INFO: load:1.35 valid_run:451.18 task_valid:435.20 collect_output:12.02
2023-01-11 06:43:22 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 06:43:22 - train.py[line:551] - INFO: load:1.38 valid_run:599.66 task_valid:580.17 collect_output:14.46
2023-01-11 06:45:54 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 06:45:54 - train.py[line:551] - INFO: load:1.41 valid_run:751.66 task_valid:727.80 collect_output:17.77
2023-01-11 06:48:25 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 06:48:25 - train.py[line:551] - INFO: load:1.43 valid_run:902.45 task_valid:873.42 collect_output:21.88
2023-01-11 06:50:57 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 06:50:57 - train.py[line:551] - INFO: load:1.46 valid_run:1054.51 task_valid:1019.44 collect_output:26.87
2023-01-11 06:53:27 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 06:53:27 - train.py[line:551] - INFO: load:1.48 valid_run:1204.52 task_valid:1160.52 collect_output:34.76
2023-01-11 06:55:56 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 06:55:56 - train.py[line:551] - INFO: load:1.51 valid_run:1353.49 task_valid:1305.39 collect_output:37.81
2023-01-11 06:58:24 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 06:58:24 - train.py[line:551] - INFO: load:1.54 valid_run:1501.32 task_valid:1448.72 collect_output:41.23
2023-01-11 07:00:53 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 07:00:53 - train.py[line:551] - INFO: load:1.56 valid_run:1650.29 task_valid:1593.70 collect_output:44.18
2023-01-11 07:03:22 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 07:03:22 - train.py[line:551] - INFO: load:1.59 valid_run:1799.68 task_valid:1738.85 collect_output:47.35
2023-01-11 07:05:51 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 07:05:51 - train.py[line:551] - INFO: load:1.62 valid_run:1948.47 task_valid:1880.97 collect_output:52.99
2023-01-11 07:08:21 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 07:08:21 - train.py[line:551] - INFO: load:1.64 valid_run:2098.41 task_valid:2026.70 collect_output:56.15
2023-01-11 07:10:51 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 07:10:51 - train.py[line:551] - INFO: load:1.67 valid_run:2248.16 task_valid:2173.44 collect_output:58.10
2023-01-11 07:13:20 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 07:13:20 - train.py[line:551] - INFO: load:1.70 valid_run:2397.53 task_valid:2317.64 collect_output:62.19
2023-01-11 07:15:51 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 07:15:51 - train.py[line:551] - INFO: load:1.72 valid_run:2548.20 task_valid:2463.31 collect_output:66.15
2023-01-11 07:18:21 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 07:18:21 - train.py[line:551] - INFO: load:1.75 valid_run:2698.26 task_valid:2610.42 collect_output:68.03
2023-01-11 07:20:49 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 07:20:49 - train.py[line:551] - INFO: load:1.78 valid_run:2845.40 task_valid:2752.08 collect_output:72.45
2023-01-11 07:23:18 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 07:23:18 - train.py[line:551] - INFO: load:1.81 valid_run:2995.22 task_valid:2897.59 collect_output:75.70
2023-01-11 07:25:49 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 07:25:49 - train.py[line:551] - INFO: load:1.83 valid_run:3145.73 task_valid:3042.61 collect_output:80.15
2023-01-11 07:28:18 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 07:28:18 - train.py[line:551] - INFO: load:1.86 valid_run:3294.55 task_valid:3187.51 collect_output:83.00
2023-01-11 07:30:48 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 07:30:48 - train.py[line:551] - INFO: load:1.89 valid_run:3444.66 task_valid:3333.69 collect_output:85.90
2023-01-11 07:33:19 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 07:33:19 - train.py[line:551] - INFO: load:1.91 valid_run:3595.16 task_valid:3480.14 collect_output:88.91

====================================================================================================
SGG eval:     R @ 50: 0.4669;     R @ 100: 0.5446;     R @ 500: 0.5786;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2879;    mR @ 100: 0.3397;    mR @ 500: 0.3738;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.5833) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0500) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7812) (playing:0.0000) (riding:0.6879) (says:0.0000) (sitting on:0.7463) (standing on:0.2160) (using:0.6000) (walking in:0.0000) (walking on:0.3063) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4669;     R @ 100: 0.5446;     R @ 500: 0.5786;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2879;    mR @ 100: 0.3397;    mR @ 500: 0.3738;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7683) (covered in:0.5833) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0500) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7812) (playing:0.0000) (riding:0.6879) (says:0.0000) (sitting on:0.7463) (standing on:0.2160) (using:0.6000) (walking in:0.0000) (walking on:0.3063) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-11 07:35:49 - train.py[line:487] - INFO: 0.544629131652661
2023-01-11 07:35:50 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 07:35:50 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.349 | loss_v1 0 | loss_v2 0 | nll_loss 0.194 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.544629 | ppl 1.14 | vqa_score 0.4764 | wps 119.7 | wpb 89.9 | bsz 30 | num_updates 27000 | best_R@100 0.69005
2023-01-11 07:35:50 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 27000 updates
2023-01-11 07:35:50 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_27000.pt
2023-01-11 07:36:30 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_27000.pt
2023-01-11 07:37:55 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_27000.pt (epoch 1 @ 27000 updates, score 0.544629131652661) (writing took 124.05054355971515 seconds)
2023-01-11 07:37:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:37:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:37:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:12 - progress_bar.py[line:274] - INFO: epoch 001:  27047 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4175, wps=0.4, ups=0, wpb=110.7, bsz=40, num_updates=27010, lr=3.80156e-05, gnorm=0.185, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=150278
2023-01-11 07:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:28 - progress_bar.py[line:274] - INFO: epoch 001:  27057 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4286, wps=98.6, ups=0.6, wpb=109.1, bsz=40, num_updates=27020, lr=3.80104e-05, gnorm=0.937, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=150295
2023-01-11 07:38:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:45 - progress_bar.py[line:274] - INFO: epoch 001:  27067 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5053, wps=102.3, ups=0.62, wpb=109.3, bsz=40, num_updates=27030, lr=3.80052e-05, gnorm=0.91, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=150311
2023-01-11 07:38:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:38:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:01 - progress_bar.py[line:274] - INFO: epoch 001:  27077 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=112.133, nsentences=40, sample_size=112.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4783, wps=103.3, ups=0.61, wpb=112.1, bsz=40, num_updates=27040, lr=3.8e-05, gnorm=0.435, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=150327
2023-01-11 07:39:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:18 - progress_bar.py[line:274] - INFO: epoch 001:  27087 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4623, wps=97, ups=0.6, wpb=108.4, bsz=40, num_updates=27050, lr=3.79948e-05, gnorm=0.524, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=150344
2023-01-11 07:39:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:35 - progress_bar.py[line:274] - INFO: epoch 001:  27097 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4205, wps=102.6, ups=0.62, wpb=110.5, bsz=40, num_updates=27060, lr=3.79896e-05, gnorm=0.4, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=150361
2023-01-11 07:39:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:51 - progress_bar.py[line:274] - INFO: epoch 001:  27107 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4364, wps=99.9, ups=0.61, wpb=109.7, bsz=40, num_updates=27070, lr=3.79844e-05, gnorm=0.454, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=150377
2023-01-11 07:39:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:39:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:07 - progress_bar.py[line:274] - INFO: epoch 001:  27117 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4528, wps=104.8, ups=0.63, wpb=110.2, bsz=40, num_updates=27080, lr=3.79792e-05, gnorm=0.377, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=150393
2023-01-11 07:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:24 - progress_bar.py[line:274] - INFO: epoch 001:  27127 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5253, wps=102.2, ups=0.62, wpb=110.3, bsz=40, num_updates=27090, lr=3.7974e-05, gnorm=0.395, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150410
2023-01-11 07:40:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:40 - progress_bar.py[line:274] - INFO: epoch 001:  27137 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5048, wps=105.1, ups=0.64, wpb=110.1, bsz=40, num_updates=27100, lr=3.79688e-05, gnorm=0.438, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=150426
2023-01-11 07:40:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:56 - progress_bar.py[line:274] - INFO: epoch 001:  27147 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4167, wps=102.7, ups=0.63, wpb=108.9, bsz=40, num_updates=27110, lr=3.79635e-05, gnorm=0.412, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=150442
2023-01-11 07:40:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:40:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:12 - progress_bar.py[line:274] - INFO: epoch 001:  27157 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3874, wps=101.5, ups=0.62, wpb=109.1, bsz=40, num_updates=27120, lr=3.79583e-05, gnorm=0.426, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=150458
2023-01-11 07:41:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:29 - progress_bar.py[line:274] - INFO: epoch 001:  27167 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4948, wps=98.9, ups=0.61, wpb=108.9, bsz=40, num_updates=27130, lr=3.79531e-05, gnorm=0.468, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=150475
2023-01-11 07:41:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:46 - progress_bar.py[line:274] - INFO: epoch 001:  27177 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5376, wps=100.6, ups=0.6, wpb=111.2, bsz=40, num_updates=27140, lr=3.79479e-05, gnorm=0.443, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=150492
2023-01-11 07:41:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:41:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:02 - progress_bar.py[line:274] - INFO: epoch 001:  27187 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3814, wps=99, ups=0.6, wpb=110.1, bsz=40, num_updates=27150, lr=3.79427e-05, gnorm=0.568, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=150509
2023-01-11 07:42:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:19 - progress_bar.py[line:274] - INFO: epoch 001:  27197 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4019, wps=100.7, ups=0.62, wpb=109, bsz=40, num_updates=27160, lr=3.79375e-05, gnorm=1.465, clip=40, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=150525
2023-01-11 07:42:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:35 - progress_bar.py[line:274] - INFO: epoch 001:  27207 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4787, wps=102.4, ups=0.62, wpb=110.8, bsz=40, num_updates=27170, lr=3.79323e-05, gnorm=0.67, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150542
2023-01-11 07:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:52 - progress_bar.py[line:274] - INFO: epoch 001:  27217 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3125, wps=103, ups=0.62, wpb=110.1, bsz=40, num_updates=27180, lr=3.79271e-05, gnorm=0.397, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=150558
2023-01-11 07:42:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:08 - progress_bar.py[line:274] - INFO: epoch 001:  27227 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4019, wps=102.5, ups=0.62, wpb=109.4, bsz=40, num_updates=27190, lr=3.79219e-05, gnorm=0.649, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150574
2023-01-11 07:43:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:25 - progress_bar.py[line:274] - INFO: epoch 001:  27237 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.303, wps=100.4, ups=0.61, wpb=109.7, bsz=40, num_updates=27200, lr=3.79167e-05, gnorm=0.505, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150591
2023-01-11 07:43:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:41 - progress_bar.py[line:274] - INFO: epoch 001:  27247 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3776, wps=101.2, ups=0.61, wpb=110.3, bsz=40, num_updates=27210, lr=3.79115e-05, gnorm=0.26, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150607
2023-01-11 07:43:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:43:58 - progress_bar.py[line:274] - INFO: epoch 001:  27257 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4149, wps=102.9, ups=0.62, wpb=111.1, bsz=40, num_updates=27220, lr=3.79062e-05, gnorm=1.035, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150624
2023-01-11 07:43:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:14 - progress_bar.py[line:274] - INFO: epoch 001:  27267 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4124, wps=98.6, ups=0.6, wpb=108.7, bsz=40, num_updates=27230, lr=3.7901e-05, gnorm=1.301, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=150640
2023-01-11 07:44:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:31 - progress_bar.py[line:274] - INFO: epoch 001:  27277 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4752, wps=100.6, ups=0.61, wpb=109.4, bsz=40, num_updates=27240, lr=3.78958e-05, gnorm=0.251, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=150657
2023-01-11 07:44:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:48 - progress_bar.py[line:274] - INFO: epoch 001:  27287 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4123, wps=100.4, ups=0.6, wpb=110.7, bsz=40, num_updates=27250, lr=3.78906e-05, gnorm=0.196, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=150674
2023-01-11 07:44:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:44:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:04 - progress_bar.py[line:274] - INFO: epoch 001:  27297 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.44, wps=100.1, ups=0.61, wpb=110, bsz=40, num_updates=27260, lr=3.78854e-05, gnorm=0.255, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=150691
2023-01-11 07:45:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:21 - progress_bar.py[line:274] - INFO: epoch 001:  27307 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4364, wps=101.7, ups=0.62, wpb=109.4, bsz=40, num_updates=27270, lr=3.78802e-05, gnorm=0.308, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150707
2023-01-11 07:45:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:37 - progress_bar.py[line:274] - INFO: epoch 001:  27317 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.505, wps=100.4, ups=0.61, wpb=109.8, bsz=40, num_updates=27280, lr=3.7875e-05, gnorm=0.135, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=150724
2023-01-11 07:45:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:54 - progress_bar.py[line:274] - INFO: epoch 001:  27327 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3636, wps=101, ups=0.61, wpb=111.3, bsz=40, num_updates=27290, lr=3.78698e-05, gnorm=0.858, clip=20, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=150740
2023-01-11 07:45:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:45:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:10 - progress_bar.py[line:274] - INFO: epoch 001:  27337 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4457, wps=105.4, ups=0.63, wpb=111, bsz=40, num_updates=27300, lr=3.78646e-05, gnorm=0.515, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150756
2023-01-11 07:46:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:27 - progress_bar.py[line:274] - INFO: epoch 001:  27347 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4286, wps=100, ups=0.6, wpb=110.3, bsz=40, num_updates=27310, lr=3.78594e-05, gnorm=0.507, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=150773
2023-01-11 07:46:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:44 - progress_bar.py[line:274] - INFO: epoch 001:  27357 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5192, wps=101.4, ups=0.61, wpb=110.3, bsz=40, num_updates=27320, lr=3.78542e-05, gnorm=0.303, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150790
2023-01-11 07:46:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:00 - progress_bar.py[line:274] - INFO: epoch 001:  27367 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4057, wps=100.2, ups=0.61, wpb=109.3, bsz=40, num_updates=27330, lr=3.7849e-05, gnorm=0.458, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150806
2023-01-11 07:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:17 - progress_bar.py[line:274] - INFO: epoch 001:  27377 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4778, wps=101.5, ups=0.61, wpb=110.6, bsz=40, num_updates=27340, lr=3.78438e-05, gnorm=0.265, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=150823
2023-01-11 07:47:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:33 - progress_bar.py[line:274] - INFO: epoch 001:  27387 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.431, wps=100.2, ups=0.61, wpb=109.3, bsz=40, num_updates=27350, lr=3.78385e-05, gnorm=1.093, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=150840
2023-01-11 07:47:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:50 - progress_bar.py[line:274] - INFO: epoch 001:  27397 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4175, wps=101.2, ups=0.62, wpb=109.6, bsz=40, num_updates=27360, lr=3.78333e-05, gnorm=0.297, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150856
2023-01-11 07:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:47:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:06 - progress_bar.py[line:274] - INFO: epoch 001:  27407 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3585, wps=101.5, ups=0.62, wpb=109.9, bsz=40, num_updates=27370, lr=3.78281e-05, gnorm=0.378, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=150873
2023-01-11 07:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:23 - progress_bar.py[line:274] - INFO: epoch 001:  27417 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=102.2, ups=0.62, wpb=109.5, bsz=40, num_updates=27380, lr=3.78229e-05, gnorm=0.764, clip=10, loss_scale=512, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=150889
2023-01-11 07:48:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:39 - progress_bar.py[line:274] - INFO: epoch 001:  27427 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4259, wps=100.5, ups=0.61, wpb=110.6, bsz=40, num_updates=27390, lr=3.78177e-05, gnorm=0.532, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=150906
2023-01-11 07:48:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:56 - progress_bar.py[line:274] - INFO: epoch 001:  27437 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4082, wps=103.2, ups=0.62, wpb=111.1, bsz=40, num_updates=27400, lr=3.78125e-05, gnorm=0.577, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150922
2023-01-11 07:48:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:48:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:13 - progress_bar.py[line:274] - INFO: epoch 001:  27447 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4211, wps=99.2, ups=0.6, wpb=109.7, bsz=40, num_updates=27410, lr=3.78073e-05, gnorm=1.02, clip=30, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=150939
2023-01-11 07:49:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:29 - progress_bar.py[line:274] - INFO: epoch 001:  27457 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4851, wps=99.6, ups=0.61, wpb=109.7, bsz=40, num_updates=27420, lr=3.78021e-05, gnorm=0.365, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=150955
2023-01-11 07:49:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:46 - progress_bar.py[line:274] - INFO: epoch 001:  27467 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=101.2, ups=0.62, wpb=109.7, bsz=40, num_updates=27430, lr=3.77969e-05, gnorm=0.18, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=150972
2023-01-11 07:49:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:49:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:02 - progress_bar.py[line:274] - INFO: epoch 001:  27477 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4217, wps=105.1, ups=0.63, wpb=111.9, bsz=40, num_updates=27440, lr=3.77917e-05, gnorm=0.646, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=150988
2023-01-11 07:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:18 - progress_bar.py[line:274] - INFO: epoch 001:  27487 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3021, wps=101.5, ups=0.61, wpb=110.3, bsz=40, num_updates=27450, lr=3.77865e-05, gnorm=0.403, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151005
2023-01-11 07:50:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:35 - progress_bar.py[line:274] - INFO: epoch 001:  27497 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4896, wps=99.6, ups=0.61, wpb=109.6, bsz=40, num_updates=27460, lr=3.77813e-05, gnorm=0.403, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151021
2023-01-11 07:50:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:52 - progress_bar.py[line:274] - INFO: epoch 001:  27507 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4839, wps=100.7, ups=0.61, wpb=110.9, bsz=40, num_updates=27470, lr=3.7776e-05, gnorm=0.28, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151038
2023-01-11 07:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:50:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:09 - progress_bar.py[line:274] - INFO: epoch 001:  27517 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5091, wps=98.4, ups=0.6, wpb=109.9, bsz=40, num_updates=27480, lr=3.77708e-05, gnorm=1.087, clip=30, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=151055
2023-01-11 07:51:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:25 - progress_bar.py[line:274] - INFO: epoch 001:  27527 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4364, wps=99.1, ups=0.61, wpb=108.3, bsz=40, num_updates=27490, lr=3.77656e-05, gnorm=0.928, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151072
2023-01-11 07:51:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:43 - progress_bar.py[line:274] - INFO: epoch 001:  27537 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.58, wps=99.8, ups=0.6, wpb=111.7, bsz=40, num_updates=27500, lr=3.77604e-05, gnorm=2.415, clip=30, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=151089
2023-01-11 07:51:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:54 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 07:51:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:51:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:01 - progress_bar.py[line:274] - INFO: epoch 001:  27548 / 100000 loss=0.328, loss_v1=0, loss_v2=0, nll_loss=0.182, ntokens=108.75, nsentences=40, sample_size=108.75, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4132, wps=94.5, ups=0.54, wpb=108.8, bsz=40, num_updates=27510, lr=3.77552e-05, gnorm=0.805, clip=10, loss_scale=256, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=151107
2023-01-11 07:52:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:18 - progress_bar.py[line:274] - INFO: epoch 001:  27558 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4906, wps=99.1, ups=0.6, wpb=109.2, bsz=40, num_updates=27520, lr=3.775e-05, gnorm=0.529, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151124
2023-01-11 07:52:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:34 - progress_bar.py[line:274] - INFO: epoch 001:  27568 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4505, wps=104.4, ups=0.63, wpb=110.6, bsz=40, num_updates=27530, lr=3.77448e-05, gnorm=0.177, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151140
2023-01-11 07:52:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:51 - progress_bar.py[line:274] - INFO: epoch 001:  27578 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3636, wps=99.8, ups=0.61, wpb=109, bsz=40, num_updates=27540, lr=3.77396e-05, gnorm=0.542, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151157
2023-01-11 07:52:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:52:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:07 - progress_bar.py[line:274] - INFO: epoch 001:  27588 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4396, wps=100.8, ups=0.62, wpb=109.3, bsz=40, num_updates=27550, lr=3.77344e-05, gnorm=0.129, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151174
2023-01-11 07:53:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:24 - progress_bar.py[line:274] - INFO: epoch 001:  27598 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4673, wps=98.8, ups=0.6, wpb=109.1, bsz=40, num_updates=27560, lr=3.77292e-05, gnorm=0.28, clip=0, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=151190
2023-01-11 07:53:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:41 - progress_bar.py[line:274] - INFO: epoch 001:  27608 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4516, wps=100.1, ups=0.61, wpb=109.8, bsz=40, num_updates=27570, lr=3.7724e-05, gnorm=0.327, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=151207
2023-01-11 07:53:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:53:57 - progress_bar.py[line:274] - INFO: epoch 001:  27618 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.398, wps=100, ups=0.61, wpb=109.4, bsz=40, num_updates=27580, lr=3.77188e-05, gnorm=0.433, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151224
2023-01-11 07:53:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:14 - progress_bar.py[line:274] - INFO: epoch 001:  27628 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4433, wps=101.7, ups=0.62, wpb=109.6, bsz=40, num_updates=27590, lr=3.77135e-05, gnorm=0.183, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151240
2023-01-11 07:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:30 - progress_bar.py[line:274] - INFO: epoch 001:  27638 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4706, wps=101, ups=0.61, wpb=109.5, bsz=40, num_updates=27600, lr=3.77083e-05, gnorm=0.41, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=151256
2023-01-11 07:54:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:47 - progress_bar.py[line:274] - INFO: epoch 001:  27648 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4021, wps=101.5, ups=0.62, wpb=109.6, bsz=40, num_updates=27610, lr=3.77031e-05, gnorm=0.351, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=151273
2023-01-11 07:54:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:54:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:03 - progress_bar.py[line:274] - INFO: epoch 001:  27658 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4095, wps=98.7, ups=0.6, wpb=108.9, bsz=40, num_updates=27620, lr=3.76979e-05, gnorm=0.382, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151290
2023-01-11 07:55:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:20 - progress_bar.py[line:274] - INFO: epoch 001:  27668 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4889, wps=103.9, ups=0.63, wpb=110.6, bsz=40, num_updates=27630, lr=3.76927e-05, gnorm=0.726, clip=10, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=151306
2023-01-11 07:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:36 - progress_bar.py[line:274] - INFO: epoch 001:  27678 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4742, wps=100, ups=0.61, wpb=109.3, bsz=40, num_updates=27640, lr=3.76875e-05, gnorm=1.132, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=151322
2023-01-11 07:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:53 - progress_bar.py[line:274] - INFO: epoch 001:  27688 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4679, wps=98.4, ups=0.6, wpb=109.3, bsz=40, num_updates=27650, lr=3.76823e-05, gnorm=0.877, clip=20, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=151339
2023-01-11 07:55:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:10 - progress_bar.py[line:274] - INFO: epoch 001:  27698 / 100000 loss=0.325, loss_v1=0, loss_v2=0, nll_loss=0.172, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4019, wps=99.6, ups=0.61, wpb=108.5, bsz=40, num_updates=27660, lr=3.76771e-05, gnorm=2.027, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151356
2023-01-11 07:56:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:26 - progress_bar.py[line:274] - INFO: epoch 001:  27708 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=112.133, nsentences=40, sample_size=112.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4706, wps=102.3, ups=0.61, wpb=112.1, bsz=40, num_updates=27670, lr=3.76719e-05, gnorm=0.415, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151373
2023-01-11 07:56:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:43 - progress_bar.py[line:274] - INFO: epoch 001:  27718 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4667, wps=102.3, ups=0.62, wpb=109.6, bsz=40, num_updates=27680, lr=3.76667e-05, gnorm=0.493, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=151389
2023-01-11 07:56:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:56:59 - progress_bar.py[line:274] - INFO: epoch 001:  27728 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4362, wps=99.6, ups=0.61, wpb=108.3, bsz=40, num_updates=27690, lr=3.76615e-05, gnorm=0.38, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=151405
2023-01-11 07:56:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:16 - progress_bar.py[line:274] - INFO: epoch 001:  27738 / 100000 loss=0.321, loss_v1=0, loss_v2=0, nll_loss=0.167, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4505, wps=101.7, ups=0.62, wpb=109.5, bsz=40, num_updates=27700, lr=3.76563e-05, gnorm=0.469, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=151422
2023-01-11 07:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:32 - progress_bar.py[line:274] - INFO: epoch 001:  27748 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3861, wps=100.2, ups=0.6, wpb=110.5, bsz=40, num_updates=27710, lr=3.7651e-05, gnorm=0.343, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151438
2023-01-11 07:57:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:49 - progress_bar.py[line:274] - INFO: epoch 001:  27758 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3789, wps=99.4, ups=0.6, wpb=109.7, bsz=40, num_updates=27720, lr=3.76458e-05, gnorm=0.178, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=151455
2023-01-11 07:57:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:57:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:06 - progress_bar.py[line:274] - INFO: epoch 001:  27768 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.46, wps=101.1, ups=0.61, wpb=109.7, bsz=40, num_updates=27730, lr=3.76406e-05, gnorm=0.314, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151472
2023-01-11 07:58:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:23 - progress_bar.py[line:274] - INFO: epoch 001:  27778 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4286, wps=99.3, ups=0.6, wpb=110.8, bsz=40, num_updates=27740, lr=3.76354e-05, gnorm=0.266, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=151489
2023-01-11 07:58:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:39 - progress_bar.py[line:274] - INFO: epoch 001:  27788 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4211, wps=102.1, ups=0.61, wpb=111.1, bsz=40, num_updates=27750, lr=3.76302e-05, gnorm=0.315, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151505
2023-01-11 07:58:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:55 - progress_bar.py[line:274] - INFO: epoch 001:  27798 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5059, wps=102.8, ups=0.62, wpb=110.5, bsz=40, num_updates=27760, lr=3.7625e-05, gnorm=0.45, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151522
2023-01-11 07:58:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:58:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:12 - progress_bar.py[line:274] - INFO: epoch 001:  27808 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4239, wps=101.7, ups=0.61, wpb=111.3, bsz=40, num_updates=27770, lr=3.76198e-05, gnorm=0.449, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=151538
2023-01-11 07:59:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:29 - progress_bar.py[line:274] - INFO: epoch 001:  27818 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4792, wps=98.7, ups=0.6, wpb=108.9, bsz=40, num_updates=27780, lr=3.76146e-05, gnorm=0.266, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151555
2023-01-11 07:59:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:45 - progress_bar.py[line:274] - INFO: epoch 001:  27828 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4914, wps=101, ups=0.62, wpb=108.7, bsz=40, num_updates=27790, lr=3.76094e-05, gnorm=0.472, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=151571
2023-01-11 07:59:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 07:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:02 - progress_bar.py[line:274] - INFO: epoch 001:  27838 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4949, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=27800, lr=3.76042e-05, gnorm=0.316, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151588
2023-01-11 08:00:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:17 - progress_bar.py[line:274] - INFO: epoch 001:  27848 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4175, wps=107.6, ups=0.65, wpb=110.5, bsz=40, num_updates=27810, lr=3.7599e-05, gnorm=1.085, clip=20, loss_scale=256, train_wall=15, gb_free=10.1, ema_decay=0.9999, wall=151604
2023-01-11 08:00:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:34 - progress_bar.py[line:274] - INFO: epoch 001:  27858 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3505, wps=100.5, ups=0.61, wpb=109.9, bsz=40, num_updates=27820, lr=3.75938e-05, gnorm=0.361, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151620
2023-01-11 08:00:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:51 - progress_bar.py[line:274] - INFO: epoch 001:  27868 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4457, wps=99.7, ups=0.61, wpb=108.7, bsz=40, num_updates=27830, lr=3.75885e-05, gnorm=0.297, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151637
2023-01-11 08:00:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:00:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:07 - progress_bar.py[line:274] - INFO: epoch 001:  27878 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5053, wps=100.4, ups=0.61, wpb=109.6, bsz=40, num_updates=27840, lr=3.75833e-05, gnorm=0.322, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=151653
2023-01-11 08:01:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:24 - progress_bar.py[line:274] - INFO: epoch 001:  27888 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4483, wps=97.8, ups=0.59, wpb=110.9, bsz=40, num_updates=27850, lr=3.75781e-05, gnorm=0.745, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=151671
2023-01-11 08:01:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:41 - progress_bar.py[line:274] - INFO: epoch 001:  27898 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4245, wps=99.9, ups=0.61, wpb=109, bsz=40, num_updates=27860, lr=3.75729e-05, gnorm=0.703, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=151687
2023-01-11 08:01:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:01:58 - progress_bar.py[line:274] - INFO: epoch 001:  27908 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4583, wps=101.2, ups=0.61, wpb=110.5, bsz=40, num_updates=27870, lr=3.75677e-05, gnorm=0.531, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=151704
2023-01-11 08:01:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:14 - progress_bar.py[line:274] - INFO: epoch 001:  27918 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5104, wps=102.8, ups=0.62, wpb=110.5, bsz=40, num_updates=27880, lr=3.75625e-05, gnorm=0.195, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151720
2023-01-11 08:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:31 - progress_bar.py[line:274] - INFO: epoch 001:  27928 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4778, wps=102.4, ups=0.62, wpb=110.9, bsz=40, num_updates=27890, lr=3.75573e-05, gnorm=1.138, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151737
2023-01-11 08:02:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:47 - progress_bar.py[line:274] - INFO: epoch 001:  27938 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4909, wps=99.5, ups=0.6, wpb=109.7, bsz=40, num_updates=27900, lr=3.75521e-05, gnorm=0.231, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151754
2023-01-11 08:02:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:04 - progress_bar.py[line:274] - INFO: epoch 001:  27948 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4216, wps=99.9, ups=0.61, wpb=110.1, bsz=40, num_updates=27910, lr=3.75469e-05, gnorm=0.425, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151770
2023-01-11 08:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:20 - progress_bar.py[line:274] - INFO: epoch 001:  27958 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4167, wps=102, ups=0.62, wpb=109.8, bsz=40, num_updates=27920, lr=3.75417e-05, gnorm=0.769, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151787
2023-01-11 08:03:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:37 - progress_bar.py[line:274] - INFO: epoch 001:  27968 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3592, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=27930, lr=3.75365e-05, gnorm=0.299, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=151803
2023-01-11 08:03:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:53 - progress_bar.py[line:274] - INFO: epoch 001:  27978 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4731, wps=103, ups=0.62, wpb=110.9, bsz=40, num_updates=27940, lr=3.75313e-05, gnorm=0.238, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151819
2023-01-11 08:03:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:03:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:09 - progress_bar.py[line:274] - INFO: epoch 001:  27988 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4516, wps=102.7, ups=0.63, wpb=109.4, bsz=40, num_updates=27950, lr=3.7526e-05, gnorm=1.187, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=151836
2023-01-11 08:04:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:26 - progress_bar.py[line:274] - INFO: epoch 001:  27998 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4057, wps=98, ups=0.61, wpb=107.9, bsz=40, num_updates=27960, lr=3.75208e-05, gnorm=0.346, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=151852
2023-01-11 08:04:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:43 - progress_bar.py[line:274] - INFO: epoch 001:  28008 / 100000 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4891, wps=102.3, ups=0.62, wpb=110.6, bsz=40, num_updates=27970, lr=3.75156e-05, gnorm=0.216, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=151869
2023-01-11 08:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:04:59 - progress_bar.py[line:274] - INFO: epoch 001:  28018 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4304, wps=100.3, ups=0.6, wpb=110.7, bsz=40, num_updates=27980, lr=3.75104e-05, gnorm=0.303, clip=0, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=151886
2023-01-11 08:04:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:05:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:05:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:05:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:05:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:05:16 - progress_bar.py[line:274] - INFO: epoch 001:  28028 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4694, wps=99.1, ups=0.6, wpb=109.9, bsz=40, num_updates=27990, lr=3.75052e-05, gnorm=0.342, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=151902
2023-01-11 08:05:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:05:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:05:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:05:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 08:05:32 - progress_bar.py[line:274] - INFO: epoch 001:  28038 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.433, wps=104.2, ups=0.63, wpb=110.4, bsz=40, num_updates=28000, lr=3.75e-05, gnorm=0.379, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=151919
2023-01-11 08:05:32 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 08:05:34 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 08:05:34 - train.py[line:551] - INFO: load:1.66 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 08:05:50 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.21 GiB (GPU 0; 39.59 GiB total capacity; 9.29 GiB already allocated; 3.48 GiB free; 33.63 GiB reserved in total by PyTorch)
2023-01-11 08:05:50 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9509 MB |   14739 MB |   21413 TB |   21413 TB |
|       from large pool |    9335 MB |   14565 MB |   21406 TB |   21406 TB |
|       from small pool |     174 MB |     175 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9509 MB |   14739 MB |   21413 TB |   21413 TB |
|       from large pool |    9335 MB |   14565 MB |   21406 TB |   21406 TB |
|       from small pool |     174 MB |     175 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   34434 MB |   34436 MB |  341668 MB |  307234 MB |
|       from large pool |   34258 MB |   34258 MB |  341194 MB |  306936 MB |
|       from small pool |     176 MB |     178 MB |     474 MB |     298 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24924 MB |   29495 MB |   22812 TB |   22812 TB |
|       from large pool |   24922 MB |   29492 MB |   22804 TB |   22804 TB |
|       from small pool |       1 MB |       2 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4634    |    4648    |    1024 M  |    1024 M  |
|       from large pool |     698    |     710    |     315 M  |     315 M  |
|       from small pool |    3936    |    3946    |     708 M  |     708 M  |
|---------------------------------------------------------------------------|
| Active allocs         |    4634    |    4648    |    1024 M  |    1024 M  |
|       from large pool |     698    |     710    |     315 M  |     315 M  |
|       from small pool |    3936    |    3946    |     708 M  |     708 M  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     156    |     157    |     934    |     778    |
|       from large pool |      68    |      68    |     697    |     629    |
|       from small pool |      88    |      89    |     237    |     149    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      96    |     107    |     750 M  |     750 M  |
|       from large pool |      41    |      43    |     127 M  |     127 M  |
|       from small pool |      55    |      70    |     623 M  |     623 M  |
|===========================================================================|

2023-01-11 08:05:50 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-11 08:05:50 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-01-11 08:08:06 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 08:08:06 - train.py[line:551] - INFO: load:1.69 valid_run:152.14 task_valid:148.09 collect_output:1.98
2023-01-11 08:10:34 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 08:10:34 - train.py[line:551] - INFO: load:1.71 valid_run:299.74 task_valid:291.06 collect_output:5.62
2023-01-11 08:13:06 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 08:13:06 - train.py[line:551] - INFO: load:1.73 valid_run:451.04 task_valid:433.95 collect_output:12.99
2023-01-11 08:15:34 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 08:15:34 - train.py[line:551] - INFO: load:1.76 valid_run:599.62 task_valid:578.89 collect_output:15.60
2023-01-11 08:18:06 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 08:18:06 - train.py[line:551] - INFO: load:1.78 valid_run:751.25 task_valid:726.37 collect_output:18.73
2023-01-11 08:20:37 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 08:20:37 - train.py[line:551] - INFO: load:1.81 valid_run:902.70 task_valid:872.58 collect_output:22.93
2023-01-11 08:23:10 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 08:23:10 - train.py[line:551] - INFO: load:1.83 valid_run:1055.58 task_valid:1019.60 collect_output:27.67
2023-01-11 08:25:40 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 08:25:41 - train.py[line:551] - INFO: load:1.86 valid_run:1205.62 task_valid:1160.82 collect_output:35.43
2023-01-11 08:28:09 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 08:28:09 - train.py[line:551] - INFO: load:1.88 valid_run:1354.46 task_valid:1305.69 collect_output:38.36
2023-01-11 08:30:38 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 08:30:38 - train.py[line:551] - INFO: load:1.91 valid_run:1502.53 task_valid:1449.44 collect_output:41.61
2023-01-11 08:33:07 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 08:33:07 - train.py[line:551] - INFO: load:1.93 valid_run:1652.23 task_valid:1595.06 collect_output:44.61
2023-01-11 08:35:37 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 08:35:37 - train.py[line:551] - INFO: load:1.96 valid_run:1801.64 task_valid:1740.45 collect_output:47.56
2023-01-11 08:38:06 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 08:38:06 - train.py[line:551] - INFO: load:1.98 valid_run:1950.74 task_valid:1882.94 collect_output:53.09
2023-01-11 08:40:37 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 08:40:37 - train.py[line:551] - INFO: load:2.01 valid_run:2101.23 task_valid:2029.00 collect_output:56.42
2023-01-11 08:43:07 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 08:43:07 - train.py[line:551] - INFO: load:2.04 valid_run:2251.45 task_valid:2176.26 collect_output:58.28
2023-01-11 08:45:37 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 08:45:37 - train.py[line:551] - INFO: load:2.07 valid_run:2401.12 task_valid:2321.07 collect_output:62.07
2023-01-11 08:48:07 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 08:48:07 - train.py[line:551] - INFO: load:2.09 valid_run:2551.65 task_valid:2466.73 collect_output:65.87
2023-01-11 08:50:38 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 08:50:38 - train.py[line:551] - INFO: load:2.11 valid_run:2701.95 task_valid:2613.93 collect_output:67.88
2023-01-11 08:53:05 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 08:53:05 - train.py[line:551] - INFO: load:2.14 valid_run:2849.48 task_valid:2755.79 collect_output:72.52
2023-01-11 08:55:35 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 08:55:35 - train.py[line:551] - INFO: load:2.17 valid_run:2998.97 task_valid:2901.20 collect_output:75.54
2023-01-11 08:58:05 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 08:58:05 - train.py[line:551] - INFO: load:2.19 valid_run:3149.54 task_valid:3046.08 collect_output:80.17
2023-01-11 09:00:34 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 09:00:34 - train.py[line:551] - INFO: load:2.22 valid_run:3298.47 task_valid:3190.86 collect_output:83.27
2023-01-11 09:03:05 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 09:03:05 - train.py[line:551] - INFO: load:2.24 valid_run:3448.87 task_valid:3337.41 collect_output:86.08
2023-01-11 09:05:36 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 09:05:36 - train.py[line:551] - INFO: load:2.27 valid_run:3599.82 task_valid:3484.44 collect_output:88.91

====================================================================================================
SGG eval:     R @ 50: 0.4725;     R @ 100: 0.5451;     R @ 500: 0.5783;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2881;    mR @ 100: 0.3408;    mR @ 500: 0.3748;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7878) (covered in:0.5833) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0500) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7812) (playing:0.0000) (riding:0.6650) (says:0.0000) (sitting on:0.7543) (standing on:0.2160) (using:0.6000) (walking in:0.0000) (walking on:0.3243) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4725;     R @ 100: 0.5451;     R @ 500: 0.5783;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2881;    mR @ 100: 0.3408;    mR @ 500: 0.3748;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7878) (covered in:0.5833) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0500) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7812) (playing:0.0000) (riding:0.6650) (says:0.0000) (sitting on:0.7543) (standing on:0.2160) (using:0.6000) (walking in:0.0000) (walking on:0.3243) (watching:0.3056) 
--------------------------------------------------------
====================================================================================================

2023-01-11 09:08:08 - train.py[line:487] - INFO: 0.5450624649859944
2023-01-11 09:08:08 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 09:08:08 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.366 | loss_v1 0 | loss_v2 0 | nll_loss 0.213 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.545062 | ppl 1.16 | vqa_score 0.4707 | wps 119.5 | wpb 89.9 | bsz 30 | num_updates 28000 | best_R@100 0.69005
2023-01-11 09:08:08 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 28000 updates
2023-01-11 09:08:08 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_28000.pt
2023-01-11 09:08:47 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_28000.pt
2023-01-11 09:10:10 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_28000.pt (epoch 1 @ 28000 updates, score 0.5450624649859944) (writing took 122.33899872377515 seconds)
2023-01-11 09:10:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:28 - progress_bar.py[line:274] - INFO: epoch 001:  28048 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4444, wps=0.4, ups=0, wpb=109.1, bsz=40, num_updates=28010, lr=3.74948e-05, gnorm=0.279, clip=0, loss_scale=256, train_wall=17, gb_free=10.8, ema_decay=0.9999, wall=155814
2023-01-11 09:10:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:45 - progress_bar.py[line:274] - INFO: epoch 001:  28058 / 100000 loss=0.34, loss_v1=0, loss_v2=0, nll_loss=0.185, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.14, vqa_score=0.4404, wps=94.3, ups=0.58, wpb=107.6, bsz=40, num_updates=28020, lr=3.74896e-05, gnorm=1.952, clip=20, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=155832
2023-01-11 09:10:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:10:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:02 - progress_bar.py[line:274] - INFO: epoch 001:  28068 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3441, wps=98.9, ups=0.61, wpb=108.5, bsz=40, num_updates=28030, lr=3.74844e-05, gnorm=0.14, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=155848
2023-01-11 09:11:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:08 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 09:11:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:20 - progress_bar.py[line:274] - INFO: epoch 001:  28079 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.625, nsentences=40, sample_size=108.625, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5, wps=95.5, ups=0.55, wpb=108.6, bsz=40, num_updates=28040, lr=3.74792e-05, gnorm=1.364, clip=30, loss_scale=256, train_wall=18, gb_free=10.1, ema_decay=0.9999, wall=155867
2023-01-11 09:11:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:37 - progress_bar.py[line:274] - INFO: epoch 001:  28089 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4301, wps=104.2, ups=0.63, wpb=110.4, bsz=40, num_updates=28050, lr=3.7474e-05, gnorm=0.373, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=155883
2023-01-11 09:11:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:11:54 - progress_bar.py[line:274] - INFO: epoch 001:  28099 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=95.3, ups=0.58, wpb=109.2, bsz=40, num_updates=28060, lr=3.74687e-05, gnorm=1.11, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=155900
2023-01-11 09:11:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:11 - progress_bar.py[line:274] - INFO: epoch 001:  28109 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5204, wps=99.4, ups=0.6, wpb=110.9, bsz=40, num_updates=28070, lr=3.74635e-05, gnorm=0.353, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=155917
2023-01-11 09:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:28 - progress_bar.py[line:274] - INFO: epoch 001:  28119 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.47, wps=102.5, ups=0.62, wpb=110.9, bsz=40, num_updates=28080, lr=3.74583e-05, gnorm=0.996, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=155934
2023-01-11 09:12:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:44 - progress_bar.py[line:274] - INFO: epoch 001:  28129 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4818, wps=101.6, ups=0.62, wpb=109.9, bsz=40, num_updates=28090, lr=3.74531e-05, gnorm=0.516, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=155950
2023-01-11 09:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:12:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:01 - progress_bar.py[line:274] - INFO: epoch 001:  28139 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4468, wps=99.9, ups=0.61, wpb=109.1, bsz=40, num_updates=28100, lr=3.74479e-05, gnorm=0.324, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=155967
2023-01-11 09:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:17 - progress_bar.py[line:274] - INFO: epoch 001:  28149 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.396, wps=102.9, ups=0.63, wpb=109.5, bsz=40, num_updates=28110, lr=3.74427e-05, gnorm=0.212, clip=0, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=155983
2023-01-11 09:13:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:23 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-11 09:13:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:36 - progress_bar.py[line:274] - INFO: epoch 001:  28160 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.812, nsentences=40, sample_size=110.812, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4746, wps=94, ups=0.53, wpb=110.8, bsz=40, num_updates=28120, lr=3.74375e-05, gnorm=0.495, clip=10, loss_scale=128, train_wall=19, gb_free=10.3, ema_decay=0.9999, wall=156002
2023-01-11 09:13:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:13:53 - progress_bar.py[line:274] - INFO: epoch 001:  28170 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4433, wps=98.8, ups=0.61, wpb=107.9, bsz=40, num_updates=28130, lr=3.74323e-05, gnorm=0.228, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156019
2023-01-11 09:13:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:09 - progress_bar.py[line:274] - INFO: epoch 001:  28180 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3393, wps=101.6, ups=0.63, wpb=108, bsz=40, num_updates=28140, lr=3.74271e-05, gnorm=0.335, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156035
2023-01-11 09:14:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:25 - progress_bar.py[line:274] - INFO: epoch 001:  28190 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4886, wps=104.7, ups=0.62, wpb=111.8, bsz=40, num_updates=28150, lr=3.74219e-05, gnorm=0.215, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=156051
2023-01-11 09:14:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:42 - progress_bar.py[line:274] - INFO: epoch 001:  28200 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4667, wps=100.9, ups=0.61, wpb=110.3, bsz=40, num_updates=28160, lr=3.74167e-05, gnorm=0.325, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156068
2023-01-11 09:14:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:14:59 - progress_bar.py[line:274] - INFO: epoch 001:  28210 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4653, wps=100.6, ups=0.61, wpb=110.9, bsz=40, num_updates=28170, lr=3.74115e-05, gnorm=0.402, clip=10, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=156085
2023-01-11 09:15:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:15 - progress_bar.py[line:274] - INFO: epoch 001:  28220 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4038, wps=99.7, ups=0.61, wpb=109.5, bsz=40, num_updates=28180, lr=3.74062e-05, gnorm=1.784, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156101
2023-01-11 09:15:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:32 - progress_bar.py[line:274] - INFO: epoch 001:  28230 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4021, wps=102.7, ups=0.62, wpb=110.9, bsz=40, num_updates=28190, lr=3.7401e-05, gnorm=1.094, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156118
2023-01-11 09:15:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:48 - progress_bar.py[line:274] - INFO: epoch 001:  28240 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.404, wps=103.4, ups=0.62, wpb=110.9, bsz=40, num_updates=28200, lr=3.73958e-05, gnorm=0.319, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156134
2023-01-11 09:15:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:15:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:06 - progress_bar.py[line:274] - INFO: epoch 001:  28250 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5243, wps=93.5, ups=0.57, wpb=109.5, bsz=40, num_updates=28210, lr=3.73906e-05, gnorm=1.225, clip=20, loss_scale=128, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=156152
2023-01-11 09:16:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:24 - progress_bar.py[line:274] - INFO: epoch 001:  28260 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4353, wps=93.5, ups=0.56, wpb=110.5, bsz=40, num_updates=28220, lr=3.73854e-05, gnorm=0.341, clip=0, loss_scale=128, train_wall=18, gb_free=10, ema_decay=0.9999, wall=156170
2023-01-11 09:16:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:40 - progress_bar.py[line:274] - INFO: epoch 001:  28270 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4583, wps=100.8, ups=0.62, wpb=109.2, bsz=40, num_updates=28230, lr=3.73802e-05, gnorm=0.717, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156187
2023-01-11 09:16:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:16:57 - progress_bar.py[line:274] - INFO: epoch 001:  28280 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4175, wps=98.3, ups=0.6, wpb=108.3, bsz=40, num_updates=28240, lr=3.7375e-05, gnorm=0.559, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156203
2023-01-11 09:17:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:14 - progress_bar.py[line:274] - INFO: epoch 001:  28290 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3981, wps=100.1, ups=0.61, wpb=108.7, bsz=40, num_updates=28250, lr=3.73698e-05, gnorm=0.132, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156220
2023-01-11 09:17:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:31 - progress_bar.py[line:274] - INFO: epoch 001:  28300 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3789, wps=98.8, ups=0.6, wpb=109.8, bsz=40, num_updates=28260, lr=3.73646e-05, gnorm=0.172, clip=0, loss_scale=128, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=156237
2023-01-11 09:17:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:48 - progress_bar.py[line:274] - INFO: epoch 001:  28310 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.38, wps=99.9, ups=0.6, wpb=110.5, bsz=40, num_updates=28270, lr=3.73594e-05, gnorm=0.286, clip=0, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=156254
2023-01-11 09:17:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:17:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:04 - progress_bar.py[line:274] - INFO: epoch 001:  28320 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3762, wps=101.9, ups=0.61, wpb=110.6, bsz=40, num_updates=28280, lr=3.73542e-05, gnorm=0.227, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156270
2023-01-11 09:18:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:20 - progress_bar.py[line:274] - INFO: epoch 001:  28330 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5109, wps=104.3, ups=0.63, wpb=110.1, bsz=40, num_updates=28290, lr=3.7349e-05, gnorm=0.356, clip=0, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=156286
2023-01-11 09:18:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:37 - progress_bar.py[line:274] - INFO: epoch 001:  28340 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4466, wps=101.2, ups=0.62, wpb=109.5, bsz=40, num_updates=28300, lr=3.73437e-05, gnorm=0.47, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156303
2023-01-11 09:18:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:18:53 - progress_bar.py[line:274] - INFO: epoch 001:  28350 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3263, wps=101.6, ups=0.61, wpb=110.6, bsz=40, num_updates=28310, lr=3.73385e-05, gnorm=0.36, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=156319
2023-01-11 09:18:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:10 - progress_bar.py[line:274] - INFO: epoch 001:  28360 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4149, wps=102.4, ups=0.62, wpb=110.9, bsz=40, num_updates=28320, lr=3.73333e-05, gnorm=0.388, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=156336
2023-01-11 09:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:26 - progress_bar.py[line:274] - INFO: epoch 001:  28370 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3978, wps=103.6, ups=0.63, wpb=110.1, bsz=40, num_updates=28330, lr=3.73281e-05, gnorm=0.243, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156352
2023-01-11 09:19:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:42 - progress_bar.py[line:274] - INFO: epoch 001:  28380 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4057, wps=102.1, ups=0.62, wpb=109.8, bsz=40, num_updates=28340, lr=3.73229e-05, gnorm=0.501, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156369
2023-01-11 09:19:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:19:59 - progress_bar.py[line:274] - INFO: epoch 001:  28390 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4259, wps=99.8, ups=0.61, wpb=109.6, bsz=40, num_updates=28350, lr=3.73177e-05, gnorm=0.475, clip=10, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=156385
2023-01-11 09:20:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:16 - progress_bar.py[line:274] - INFO: epoch 001:  28400 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4881, wps=102, ups=0.61, wpb=111.2, bsz=40, num_updates=28360, lr=3.73125e-05, gnorm=0.219, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156402
2023-01-11 09:20:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:32 - progress_bar.py[line:274] - INFO: epoch 001:  28410 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3981, wps=102, ups=0.62, wpb=110.4, bsz=40, num_updates=28370, lr=3.73073e-05, gnorm=0.19, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=156419
2023-01-11 09:20:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:49 - progress_bar.py[line:274] - INFO: epoch 001:  28420 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4019, wps=100.4, ups=0.62, wpb=108.8, bsz=40, num_updates=28380, lr=3.73021e-05, gnorm=0.944, clip=10, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=156435
2023-01-11 09:20:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:20:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:05 - progress_bar.py[line:274] - INFO: epoch 001:  28430 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4227, wps=102.5, ups=0.62, wpb=110.1, bsz=40, num_updates=28390, lr=3.72969e-05, gnorm=0.984, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156451
2023-01-11 09:21:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:22 - progress_bar.py[line:274] - INFO: epoch 001:  28440 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3854, wps=101.1, ups=0.61, wpb=110, bsz=40, num_updates=28400, lr=3.72917e-05, gnorm=1.04, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=156468
2023-01-11 09:21:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:39 - progress_bar.py[line:274] - INFO: epoch 001:  28450 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4894, wps=100.7, ups=0.61, wpb=110.4, bsz=40, num_updates=28410, lr=3.72865e-05, gnorm=0.352, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156485
2023-01-11 09:21:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:21:55 - progress_bar.py[line:274] - INFO: epoch 001:  28460 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5051, wps=102, ups=0.62, wpb=110.1, bsz=40, num_updates=28420, lr=3.72813e-05, gnorm=0.492, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156501
2023-01-11 09:22:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:12 - progress_bar.py[line:274] - INFO: epoch 001:  28470 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4953, wps=99.7, ups=0.61, wpb=108.3, bsz=40, num_updates=28430, lr=3.7276e-05, gnorm=0.329, clip=0, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=156518
2023-01-11 09:22:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:29 - progress_bar.py[line:274] - INFO: epoch 001:  28480 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4318, wps=98.5, ups=0.6, wpb=109.9, bsz=40, num_updates=28440, lr=3.72708e-05, gnorm=0.449, clip=10, loss_scale=128, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=156535
2023-01-11 09:22:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:45 - progress_bar.py[line:274] - INFO: epoch 001:  28490 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4095, wps=100.5, ups=0.61, wpb=109.4, bsz=40, num_updates=28450, lr=3.72656e-05, gnorm=0.482, clip=10, loss_scale=128, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=156551
2023-01-11 09:22:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:02 - progress_bar.py[line:274] - INFO: epoch 001:  28500 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4388, wps=100.5, ups=0.6, wpb=111.5, bsz=40, num_updates=28460, lr=3.72604e-05, gnorm=0.85, clip=20, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=156568
2023-01-11 09:23:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:18 - progress_bar.py[line:274] - INFO: epoch 001:  28510 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4271, wps=102.7, ups=0.62, wpb=110.5, bsz=40, num_updates=28470, lr=3.72552e-05, gnorm=0.305, clip=0, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=156585
2023-01-11 09:23:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:35 - progress_bar.py[line:274] - INFO: epoch 001:  28520 / 100000 loss=0.318, loss_v1=0, loss_v2=0, nll_loss=0.164, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.459, wps=99.7, ups=0.61, wpb=108.9, bsz=40, num_updates=28480, lr=3.725e-05, gnorm=0.896, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156601
2023-01-11 09:23:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:52 - progress_bar.py[line:274] - INFO: epoch 001:  28530 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4404, wps=99.5, ups=0.62, wpb=107.9, bsz=40, num_updates=28490, lr=3.72448e-05, gnorm=0.545, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=156618
2023-01-11 09:23:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:23:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:08 - progress_bar.py[line:274] - INFO: epoch 001:  28540 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3587, wps=103.9, ups=0.62, wpb=111.7, bsz=40, num_updates=28500, lr=3.72396e-05, gnorm=0.407, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156634
2023-01-11 09:24:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:25 - progress_bar.py[line:274] - INFO: epoch 001:  28550 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4949, wps=99.3, ups=0.6, wpb=109.6, bsz=40, num_updates=28510, lr=3.72344e-05, gnorm=0.711, clip=20, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=156651
2023-01-11 09:24:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:42 - progress_bar.py[line:274] - INFO: epoch 001:  28560 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4898, wps=99.8, ups=0.6, wpb=110.4, bsz=40, num_updates=28520, lr=3.72292e-05, gnorm=0.36, clip=10, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=156668
2023-01-11 09:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:24:58 - progress_bar.py[line:274] - INFO: epoch 001:  28570 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=102.4, ups=0.61, wpb=111.3, bsz=40, num_updates=28530, lr=3.7224e-05, gnorm=0.317, clip=0, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=156685
2023-01-11 09:25:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:14 - progress_bar.py[line:274] - INFO: epoch 001:  28580 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.48, wps=104.6, ups=0.63, wpb=110.2, bsz=40, num_updates=28540, lr=3.72188e-05, gnorm=0.715, clip=40, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156701
2023-01-11 09:25:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:31 - progress_bar.py[line:274] - INFO: epoch 001:  28590 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4747, wps=101.2, ups=0.61, wpb=110.2, bsz=40, num_updates=28550, lr=3.72135e-05, gnorm=0.367, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=156717
2023-01-11 09:25:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:48 - progress_bar.py[line:274] - INFO: epoch 001:  28600 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4949, wps=100.6, ups=0.61, wpb=110.6, bsz=40, num_updates=28560, lr=3.72083e-05, gnorm=0.175, clip=0, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=156734
2023-01-11 09:25:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:25:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:05 - progress_bar.py[line:274] - INFO: epoch 001:  28610 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3824, wps=100.3, ups=0.61, wpb=110.5, bsz=40, num_updates=28570, lr=3.72031e-05, gnorm=0.434, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156751
2023-01-11 09:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:21 - progress_bar.py[line:274] - INFO: epoch 001:  28620 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5, wps=102.5, ups=0.62, wpb=109.7, bsz=40, num_updates=28580, lr=3.71979e-05, gnorm=0.398, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156767
2023-01-11 09:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:37 - progress_bar.py[line:274] - INFO: epoch 001:  28630 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4158, wps=101.4, ups=0.62, wpb=109.3, bsz=40, num_updates=28590, lr=3.71927e-05, gnorm=0.311, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=156784
2023-01-11 09:26:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:26:54 - progress_bar.py[line:274] - INFO: epoch 001:  28640 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4725, wps=99.9, ups=0.6, wpb=110.2, bsz=40, num_updates=28600, lr=3.71875e-05, gnorm=0.269, clip=0, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=156800
2023-01-11 09:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:11 - progress_bar.py[line:274] - INFO: epoch 001:  28650 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4632, wps=102.3, ups=0.62, wpb=110, bsz=40, num_updates=28610, lr=3.71823e-05, gnorm=0.375, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156817
2023-01-11 09:27:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:27 - progress_bar.py[line:274] - INFO: epoch 001:  28660 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3981, wps=101.6, ups=0.62, wpb=109.7, bsz=40, num_updates=28620, lr=3.71771e-05, gnorm=0.37, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156833
2023-01-11 09:27:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:43 - progress_bar.py[line:274] - INFO: epoch 001:  28670 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4742, wps=103.1, ups=0.63, wpb=108.3, bsz=40, num_updates=28630, lr=3.71719e-05, gnorm=0.22, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156849
2023-01-11 09:27:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:27:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:00 - progress_bar.py[line:274] - INFO: epoch 001:  28680 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4272, wps=102.4, ups=0.62, wpb=110.1, bsz=40, num_updates=28640, lr=3.71667e-05, gnorm=0.513, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156866
2023-01-11 09:28:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:16 - progress_bar.py[line:274] - INFO: epoch 001:  28690 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4851, wps=104.9, ups=0.63, wpb=110.5, bsz=40, num_updates=28650, lr=3.71615e-05, gnorm=0.464, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=156882
2023-01-11 09:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:32 - progress_bar.py[line:274] - INFO: epoch 001:  28700 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4141, wps=102.9, ups=0.62, wpb=110.1, bsz=40, num_updates=28660, lr=3.71563e-05, gnorm=0.319, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156898
2023-01-11 09:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:49 - progress_bar.py[line:274] - INFO: epoch 001:  28710 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.398, wps=101.4, ups=0.61, wpb=110, bsz=40, num_updates=28670, lr=3.7151e-05, gnorm=0.251, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156915
2023-01-11 09:28:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:28:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:05 - progress_bar.py[line:274] - INFO: epoch 001:  28720 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4423, wps=100.2, ups=0.61, wpb=109.5, bsz=40, num_updates=28680, lr=3.71458e-05, gnorm=0.385, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156931
2023-01-11 09:29:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:22 - progress_bar.py[line:274] - INFO: epoch 001:  28730 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.41, wps=101.8, ups=0.62, wpb=109.3, bsz=40, num_updates=28690, lr=3.71406e-05, gnorm=0.469, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156948
2023-01-11 09:29:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:38 - progress_bar.py[line:274] - INFO: epoch 001:  28740 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4854, wps=100.7, ups=0.61, wpb=109.4, bsz=40, num_updates=28700, lr=3.71354e-05, gnorm=0.267, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=156964
2023-01-11 09:29:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:29:55 - progress_bar.py[line:274] - INFO: epoch 001:  28750 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4078, wps=99.3, ups=0.61, wpb=109.3, bsz=40, num_updates=28710, lr=3.71302e-05, gnorm=0.18, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=156981
2023-01-11 09:30:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:12 - progress_bar.py[line:274] - INFO: epoch 001:  28760 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4951, wps=100.6, ups=0.6, wpb=110.9, bsz=40, num_updates=28720, lr=3.7125e-05, gnorm=0.317, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=156998
2023-01-11 09:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:28 - progress_bar.py[line:274] - INFO: epoch 001:  28770 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3793, wps=103, ups=0.62, wpb=111, bsz=40, num_updates=28730, lr=3.71198e-05, gnorm=0.385, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=157014
2023-01-11 09:30:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:45 - progress_bar.py[line:274] - INFO: epoch 001:  28780 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4848, wps=98.5, ups=0.6, wpb=108.8, bsz=40, num_updates=28740, lr=3.71146e-05, gnorm=0.478, clip=20, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=157031
2023-01-11 09:30:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:30:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:01 - progress_bar.py[line:274] - INFO: epoch 001:  28790 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.467, nsentences=40, sample_size=107.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4071, wps=99.5, ups=0.62, wpb=107.5, bsz=40, num_updates=28750, lr=3.71094e-05, gnorm=0.768, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=157047
2023-01-11 09:31:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:18 - progress_bar.py[line:274] - INFO: epoch 001:  28800 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4, wps=103.3, ups=0.62, wpb=111.5, bsz=40, num_updates=28760, lr=3.71042e-05, gnorm=0.599, clip=30, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=157064
2023-01-11 09:31:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:34 - progress_bar.py[line:274] - INFO: epoch 001:  28810 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4425, wps=100.7, ups=0.62, wpb=108.7, bsz=40, num_updates=28770, lr=3.7099e-05, gnorm=0.33, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=157080
2023-01-11 09:31:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:50 - progress_bar.py[line:274] - INFO: epoch 001:  28820 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4433, wps=103, ups=0.63, wpb=109.1, bsz=40, num_updates=28780, lr=3.70938e-05, gnorm=0.256, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=157096
2023-01-11 09:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:31:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:06 - progress_bar.py[line:274] - INFO: epoch 001:  28830 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4494, wps=104.9, ups=0.62, wpb=112.2, bsz=40, num_updates=28790, lr=3.70885e-05, gnorm=0.447, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=157113
2023-01-11 09:32:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:23 - progress_bar.py[line:274] - INFO: epoch 001:  28840 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5714, wps=98.4, ups=0.61, wpb=108.4, bsz=40, num_updates=28800, lr=3.70833e-05, gnorm=0.304, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=157129
2023-01-11 09:32:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:39 - progress_bar.py[line:274] - INFO: epoch 001:  28850 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4167, wps=102.8, ups=0.62, wpb=109.8, bsz=40, num_updates=28810, lr=3.70781e-05, gnorm=0.879, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=157146
2023-01-11 09:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:32:56 - progress_bar.py[line:274] - INFO: epoch 001:  28860 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.39, wps=99.3, ups=0.6, wpb=110, bsz=40, num_updates=28820, lr=3.70729e-05, gnorm=0.297, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=157162
2023-01-11 09:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:13 - progress_bar.py[line:274] - INFO: epoch 001:  28870 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3495, wps=98.2, ups=0.6, wpb=109.7, bsz=40, num_updates=28830, lr=3.70677e-05, gnorm=0.403, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=157179
2023-01-11 09:33:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:30 - progress_bar.py[line:274] - INFO: epoch 001:  28880 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4667, wps=101.4, ups=0.61, wpb=110.7, bsz=40, num_updates=28840, lr=3.70625e-05, gnorm=1.431, clip=20, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=157196
2023-01-11 09:33:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:46 - progress_bar.py[line:274] - INFO: epoch 001:  28890 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3905, wps=102.7, ups=0.62, wpb=110.9, bsz=40, num_updates=28850, lr=3.70573e-05, gnorm=0.277, clip=0, loss_scale=256, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=157212
2023-01-11 09:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:33:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:03 - progress_bar.py[line:274] - INFO: epoch 001:  28900 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4474, wps=102.9, ups=0.62, wpb=110.3, bsz=40, num_updates=28860, lr=3.70521e-05, gnorm=0.708, clip=20, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=157229
2023-01-11 09:34:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:19 - progress_bar.py[line:274] - INFO: epoch 001:  28910 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4906, wps=99.8, ups=0.61, wpb=109.9, bsz=40, num_updates=28870, lr=3.70469e-05, gnorm=0.926, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=157246
2023-01-11 09:34:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:36 - progress_bar.py[line:274] - INFO: epoch 001:  28920 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4359, wps=97.1, ups=0.6, wpb=107.8, bsz=40, num_updates=28880, lr=3.70417e-05, gnorm=0.431, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=157262
2023-01-11 09:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:34:53 - progress_bar.py[line:274] - INFO: epoch 001:  28930 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.166, ntokens=107.733, nsentences=40, sample_size=107.733, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4667, wps=97.2, ups=0.6, wpb=107.7, bsz=40, num_updates=28890, lr=3.70365e-05, gnorm=0.521, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=157279
2023-01-11 09:34:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:10 - progress_bar.py[line:274] - INFO: epoch 001:  28940 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4382, wps=101.2, ups=0.61, wpb=111.1, bsz=40, num_updates=28900, lr=3.70313e-05, gnorm=0.358, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=157296
2023-01-11 09:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:26 - progress_bar.py[line:274] - INFO: epoch 001:  28950 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5104, wps=102.1, ups=0.61, wpb=110.9, bsz=40, num_updates=28910, lr=3.7026e-05, gnorm=0.584, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=157313
2023-01-11 09:35:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:43 - progress_bar.py[line:274] - INFO: epoch 001:  28960 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4318, wps=99.8, ups=0.6, wpb=110.9, bsz=40, num_updates=28920, lr=3.70208e-05, gnorm=0.277, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=157330
2023-01-11 09:35:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:35:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:00 - progress_bar.py[line:274] - INFO: epoch 001:  28970 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4737, wps=100.1, ups=0.61, wpb=108.5, bsz=40, num_updates=28930, lr=3.70156e-05, gnorm=0.328, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=157346
2023-01-11 09:36:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:17 - progress_bar.py[line:274] - INFO: epoch 001:  28980 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3263, wps=100, ups=0.6, wpb=110.2, bsz=40, num_updates=28940, lr=3.70104e-05, gnorm=0.234, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=157363
2023-01-11 09:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:34 - progress_bar.py[line:274] - INFO: epoch 001:  28990 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4272, wps=99.5, ups=0.6, wpb=110, bsz=40, num_updates=28950, lr=3.70052e-05, gnorm=0.166, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=157380
2023-01-11 09:36:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:50 - progress_bar.py[line:274] - INFO: epoch 001:  29000 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4808, wps=100.5, ups=0.61, wpb=109.5, bsz=40, num_updates=28960, lr=3.7e-05, gnorm=0.252, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=157396
2023-01-11 09:36:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:36:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:07 - progress_bar.py[line:274] - INFO: epoch 001:  29010 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4615, wps=99.5, ups=0.61, wpb=109.1, bsz=40, num_updates=28970, lr=3.69948e-05, gnorm=0.234, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=157413
2023-01-11 09:37:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:24 - progress_bar.py[line:274] - INFO: epoch 001:  29020 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4592, wps=101.5, ups=0.61, wpb=110.1, bsz=40, num_updates=28980, lr=3.69896e-05, gnorm=0.404, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=157430
2023-01-11 09:37:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:41 - progress_bar.py[line:274] - INFO: epoch 001:  29030 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4231, wps=97.8, ups=0.6, wpb=109.3, bsz=40, num_updates=28990, lr=3.69844e-05, gnorm=0.364, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=157447
2023-01-11 09:37:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 09:37:58 - progress_bar.py[line:274] - INFO: epoch 001:  29040 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4316, wps=98.3, ups=0.6, wpb=109.8, bsz=40, num_updates=29000, lr=3.69792e-05, gnorm=0.394, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=157464
2023-01-11 09:37:58 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 09:37:59 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 09:37:59 - train.py[line:551] - INFO: load:1.33 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 09:40:32 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 09:40:32 - train.py[line:551] - INFO: load:1.36 valid_run:152.80 task_valid:149.96 collect_output:1.73
2023-01-11 09:43:01 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 09:43:01 - train.py[line:551] - INFO: load:1.38 valid_run:301.11 task_valid:294.10 collect_output:4.83
2023-01-11 09:45:32 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 09:45:32 - train.py[line:551] - INFO: load:1.41 valid_run:452.83 task_valid:437.91 collect_output:11.66
2023-01-11 09:48:02 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 09:48:02 - train.py[line:551] - INFO: load:1.44 valid_run:601.78 task_valid:583.46 collect_output:13.98
2023-01-11 09:50:34 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 09:50:34 - train.py[line:551] - INFO: load:1.46 valid_run:753.76 task_valid:731.43 collect_output:16.94
2023-01-11 09:53:06 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 09:53:06 - train.py[line:551] - INFO: load:1.49 valid_run:905.63 task_valid:878.30 collect_output:20.84
2023-01-11 09:55:38 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 09:55:39 - train.py[line:551] - INFO: load:1.52 valid_run:1058.50 task_valid:1025.18 collect_output:25.74
2023-01-11 09:58:09 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 09:58:09 - train.py[line:551] - INFO: load:1.54 valid_run:1208.60 task_valid:1166.75 collect_output:33.23
2023-01-11 10:00:38 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 10:00:38 - train.py[line:551] - INFO: load:1.57 valid_run:1357.61 task_valid:1311.79 collect_output:36.15
2023-01-11 10:03:06 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 10:03:06 - train.py[line:551] - INFO: load:1.60 valid_run:1505.84 task_valid:1455.68 collect_output:39.42
2023-01-11 10:05:36 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 10:05:36 - train.py[line:551] - INFO: load:1.62 valid_run:1655.57 task_valid:1601.35 collect_output:42.39
2023-01-11 10:08:06 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 10:08:06 - train.py[line:551] - INFO: load:1.65 valid_run:1805.36 task_valid:1747.06 collect_output:45.39
2023-01-11 10:10:35 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 10:10:35 - train.py[line:551] - INFO: load:1.68 valid_run:1954.47 task_valid:1889.33 collect_output:51.18
2023-01-11 10:13:06 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 10:13:06 - train.py[line:551] - INFO: load:1.70 valid_run:2105.26 task_valid:2035.87 collect_output:54.32
2023-01-11 10:15:36 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 10:15:36 - train.py[line:551] - INFO: load:1.73 valid_run:2255.79 task_valid:2183.35 collect_output:56.28
2023-01-11 10:18:07 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 10:18:07 - train.py[line:551] - INFO: load:1.76 valid_run:2406.08 task_valid:2329.25 collect_output:59.59
2023-01-11 10:20:38 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 10:20:38 - train.py[line:551] - INFO: load:1.79 valid_run:2557.52 task_valid:2476.16 collect_output:62.99
2023-01-11 10:23:09 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 10:23:09 - train.py[line:551] - INFO: load:1.82 valid_run:2708.47 task_valid:2624.41 collect_output:64.62
2023-01-11 10:25:37 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 10:25:37 - train.py[line:551] - INFO: load:1.85 valid_run:2856.40 task_valid:2766.87 collect_output:69.00
2023-01-11 10:28:08 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 10:28:08 - train.py[line:551] - INFO: load:1.87 valid_run:3006.44 task_valid:2912.90 collect_output:71.96
2023-01-11 10:30:39 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 10:30:39 - train.py[line:551] - INFO: load:1.90 valid_run:3157.37 task_valid:3058.54 collect_output:76.15
2023-01-11 10:33:08 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 10:33:08 - train.py[line:551] - INFO: load:1.93 valid_run:3306.78 task_valid:3204.09 collect_output:78.94
2023-01-11 10:35:39 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 10:35:39 - train.py[line:551] - INFO: load:1.95 valid_run:3457.74 task_valid:3351.41 collect_output:81.50
2023-01-11 10:38:10 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 10:38:10 - train.py[line:551] - INFO: load:1.98 valid_run:3608.97 task_valid:3498.88 collect_output:84.21

====================================================================================================
SGG eval:     R @ 50: 0.4653;     R @ 100: 0.5357;     R @ 500: 0.5734;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2872;    mR @ 100: 0.3390;    mR @ 500: 0.3735;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7634) (covered in:0.6250) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0500) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8021) (playing:0.0000) (riding:0.6408) (says:0.0000) (sitting on:0.7452) (standing on:0.2160) (using:0.6000) (walking in:0.0000) (walking on:0.3243) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4653;     R @ 100: 0.5357;     R @ 500: 0.5734;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2872;    mR @ 100: 0.3390;    mR @ 500: 0.3735;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7634) (covered in:0.6250) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0500) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8021) (playing:0.0000) (riding:0.6408) (says:0.0000) (sitting on:0.7452) (standing on:0.2160) (using:0.6000) (walking in:0.0000) (walking on:0.3243) (watching:0.2639) 
--------------------------------------------------------
====================================================================================================

2023-01-11 10:40:42 - train.py[line:487] - INFO: 0.5356957983193277
2023-01-11 10:40:42 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 10:40:42 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.369 | loss_v1 0 | loss_v2 0 | nll_loss 0.22 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.535696 | ppl 1.16 | vqa_score 0.4572 | wps 119.2 | wpb 89.9 | bsz 30 | num_updates 29000 | best_R@100 0.69005
2023-01-11 10:40:42 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 29000 updates
2023-01-11 10:40:42 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_29000.pt
2023-01-11 10:41:20 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_29000.pt
2023-01-11 10:42:41 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_29000.pt (epoch 1 @ 29000 updates, score 0.5356957983193277) (writing took 118.81124669499695 seconds)
2023-01-11 10:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:42:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:42:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:42:58 - progress_bar.py[line:274] - INFO: epoch 001:  29050 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3763, wps=0.4, ups=0, wpb=111, bsz=40, num_updates=29010, lr=3.6974e-05, gnorm=0.245, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=161364
2023-01-11 10:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:15 - progress_bar.py[line:274] - INFO: epoch 001:  29060 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4673, wps=98.8, ups=0.6, wpb=109.2, bsz=40, num_updates=29020, lr=3.69688e-05, gnorm=0.339, clip=0, loss_scale=256, train_wall=17, gb_free=9.5, ema_decay=0.9999, wall=161381
2023-01-11 10:43:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:32 - progress_bar.py[line:274] - INFO: epoch 001:  29070 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3516, wps=99.2, ups=0.6, wpb=109.5, bsz=40, num_updates=29030, lr=3.69635e-05, gnorm=0.186, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=161398
2023-01-11 10:43:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:48 - progress_bar.py[line:274] - INFO: epoch 001:  29080 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4848, wps=101.2, ups=0.62, wpb=109.4, bsz=40, num_updates=29040, lr=3.69583e-05, gnorm=1.517, clip=20, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=161415
2023-01-11 10:43:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:43:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:05 - progress_bar.py[line:274] - INFO: epoch 001:  29090 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4694, wps=100.9, ups=0.61, wpb=110.4, bsz=40, num_updates=29050, lr=3.69531e-05, gnorm=0.205, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=161431
2023-01-11 10:44:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:22 - progress_bar.py[line:274] - INFO: epoch 001:  29100 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4792, wps=98.8, ups=0.6, wpb=110.2, bsz=40, num_updates=29060, lr=3.69479e-05, gnorm=0.359, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=161448
2023-01-11 10:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:38 - progress_bar.py[line:274] - INFO: epoch 001:  29110 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4235, wps=102.6, ups=0.62, wpb=110.7, bsz=40, num_updates=29070, lr=3.69427e-05, gnorm=1.22, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=161465
2023-01-11 10:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:44:55 - progress_bar.py[line:274] - INFO: epoch 001:  29120 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4255, wps=102, ups=0.62, wpb=110.3, bsz=40, num_updates=29080, lr=3.69375e-05, gnorm=0.151, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161481
2023-01-11 10:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:12 - progress_bar.py[line:274] - INFO: epoch 001:  29130 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4815, wps=99.2, ups=0.6, wpb=109.9, bsz=40, num_updates=29090, lr=3.69323e-05, gnorm=0.363, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=161498
2023-01-11 10:45:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:28 - progress_bar.py[line:274] - INFO: epoch 001:  29140 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3187, wps=101.3, ups=0.61, wpb=110.4, bsz=40, num_updates=29100, lr=3.69271e-05, gnorm=0.294, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=161514
2023-01-11 10:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:45 - progress_bar.py[line:274] - INFO: epoch 001:  29150 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4298, wps=99.2, ups=0.61, wpb=108.7, bsz=40, num_updates=29110, lr=3.69219e-05, gnorm=0.402, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161531
2023-01-11 10:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:45:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:02 - progress_bar.py[line:274] - INFO: epoch 001:  29160 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4, wps=99.8, ups=0.6, wpb=110.3, bsz=40, num_updates=29120, lr=3.69167e-05, gnorm=0.332, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=161548
2023-01-11 10:46:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:18 - progress_bar.py[line:274] - INFO: epoch 001:  29170 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3846, wps=101.7, ups=0.62, wpb=109.8, bsz=40, num_updates=29130, lr=3.69115e-05, gnorm=0.377, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161564
2023-01-11 10:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:35 - progress_bar.py[line:274] - INFO: epoch 001:  29180 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5248, wps=99.1, ups=0.6, wpb=110, bsz=40, num_updates=29140, lr=3.69062e-05, gnorm=0.265, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=161581
2023-01-11 10:46:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:51 - progress_bar.py[line:274] - INFO: epoch 001:  29190 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.396, wps=100.7, ups=0.62, wpb=109.1, bsz=40, num_updates=29150, lr=3.6901e-05, gnorm=0.412, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=161598
2023-01-11 10:46:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:46:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:08 - progress_bar.py[line:274] - INFO: epoch 001:  29200 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4653, wps=102, ups=0.62, wpb=109.9, bsz=40, num_updates=29160, lr=3.68958e-05, gnorm=0.585, clip=20, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=161614
2023-01-11 10:47:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:24 - progress_bar.py[line:274] - INFO: epoch 001:  29210 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4381, wps=99.7, ups=0.62, wpb=108, bsz=40, num_updates=29170, lr=3.68906e-05, gnorm=0.517, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=161631
2023-01-11 10:47:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:41 - progress_bar.py[line:274] - INFO: epoch 001:  29220 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4112, wps=100.2, ups=0.61, wpb=108.7, bsz=40, num_updates=29180, lr=3.68854e-05, gnorm=0.691, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161647
2023-01-11 10:47:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:47:57 - progress_bar.py[line:274] - INFO: epoch 001:  29230 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4653, wps=101.3, ups=0.62, wpb=109.3, bsz=40, num_updates=29190, lr=3.68802e-05, gnorm=0.175, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161664
2023-01-11 10:48:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:14 - progress_bar.py[line:274] - INFO: epoch 001:  29240 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4239, wps=101.6, ups=0.62, wpb=109.9, bsz=40, num_updates=29200, lr=3.6875e-05, gnorm=0.211, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161680
2023-01-11 10:48:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:31 - progress_bar.py[line:274] - INFO: epoch 001:  29250 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4433, wps=98.8, ups=0.6, wpb=109.9, bsz=40, num_updates=29210, lr=3.68698e-05, gnorm=0.272, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=161697
2023-01-11 10:48:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:47 - progress_bar.py[line:274] - INFO: epoch 001:  29260 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4167, wps=100.1, ups=0.61, wpb=109.6, bsz=40, num_updates=29220, lr=3.68646e-05, gnorm=0.389, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=161714
2023-01-11 10:48:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:48:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:04 - progress_bar.py[line:274] - INFO: epoch 001:  29270 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4175, wps=101.6, ups=0.61, wpb=110.4, bsz=40, num_updates=29230, lr=3.68594e-05, gnorm=0.702, clip=10, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=161730
2023-01-11 10:49:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:21 - progress_bar.py[line:274] - INFO: epoch 001:  29280 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3542, wps=99.7, ups=0.6, wpb=110.9, bsz=40, num_updates=29240, lr=3.68542e-05, gnorm=0.206, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=161747
2023-01-11 10:49:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:37 - progress_bar.py[line:274] - INFO: epoch 001:  29290 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4894, wps=101.3, ups=0.61, wpb=110, bsz=40, num_updates=29250, lr=3.6849e-05, gnorm=0.6, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161763
2023-01-11 10:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:49:54 - progress_bar.py[line:274] - INFO: epoch 001:  29300 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=99.2, ups=0.61, wpb=109.1, bsz=40, num_updates=29260, lr=3.68437e-05, gnorm=0.152, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=161780
2023-01-11 10:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:11 - progress_bar.py[line:274] - INFO: epoch 001:  29310 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3372, wps=100.3, ups=0.61, wpb=110.3, bsz=40, num_updates=29270, lr=3.68385e-05, gnorm=0.948, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161797
2023-01-11 10:50:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:28 - progress_bar.py[line:274] - INFO: epoch 001:  29320 / 100000 loss=0.327, loss_v1=0, loss_v2=0, nll_loss=0.173, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4324, wps=98.7, ups=0.61, wpb=108.4, bsz=40, num_updates=29280, lr=3.68333e-05, gnorm=1.473, clip=40, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=161814
2023-01-11 10:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:43 - progress_bar.py[line:274] - INFO: epoch 001:  29330 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5413, wps=107, ups=0.65, wpb=110.2, bsz=40, num_updates=29290, lr=3.68281e-05, gnorm=0.319, clip=10, loss_scale=512, train_wall=15, gb_free=10.2, ema_decay=0.9999, wall=161829
2023-01-11 10:50:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:50:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:00 - progress_bar.py[line:274] - INFO: epoch 001:  29340 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4107, wps=99.2, ups=0.6, wpb=109.5, bsz=40, num_updates=29300, lr=3.68229e-05, gnorm=0.809, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161846
2023-01-11 10:51:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:17 - progress_bar.py[line:274] - INFO: epoch 001:  29350 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3939, wps=100.1, ups=0.61, wpb=109.9, bsz=40, num_updates=29310, lr=3.68177e-05, gnorm=0.287, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=161863
2023-01-11 10:51:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:33 - progress_bar.py[line:274] - INFO: epoch 001:  29360 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4455, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=29320, lr=3.68125e-05, gnorm=0.213, clip=0, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=161879
2023-01-11 10:51:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:50 - progress_bar.py[line:274] - INFO: epoch 001:  29370 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4074, wps=101.9, ups=0.62, wpb=109.8, bsz=40, num_updates=29330, lr=3.68073e-05, gnorm=0.197, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=161896
2023-01-11 10:51:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:51:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:06 - progress_bar.py[line:274] - INFO: epoch 001:  29380 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4343, wps=104.2, ups=0.62, wpb=111.4, bsz=40, num_updates=29340, lr=3.68021e-05, gnorm=0.386, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161912
2023-01-11 10:52:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:22 - progress_bar.py[line:274] - INFO: epoch 001:  29390 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4018, wps=100.1, ups=0.61, wpb=109.3, bsz=40, num_updates=29350, lr=3.67969e-05, gnorm=0.223, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=161929
2023-01-11 10:52:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:40 - progress_bar.py[line:274] - INFO: epoch 001:  29400 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4554, wps=98.1, ups=0.6, wpb=109.9, bsz=40, num_updates=29360, lr=3.67917e-05, gnorm=0.359, clip=0, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=161946
2023-01-11 10:52:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:52:56 - progress_bar.py[line:274] - INFO: epoch 001:  29410 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4948, wps=102.2, ups=0.62, wpb=110.1, bsz=40, num_updates=29370, lr=3.67865e-05, gnorm=0.266, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161962
2023-01-11 10:53:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:13 - progress_bar.py[line:274] - INFO: epoch 001:  29420 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4563, wps=99.1, ups=0.6, wpb=109.3, bsz=40, num_updates=29380, lr=3.67813e-05, gnorm=0.871, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=161979
2023-01-11 10:53:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:29 - progress_bar.py[line:274] - INFO: epoch 001:  29430 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4286, wps=100.8, ups=0.61, wpb=110.3, bsz=40, num_updates=29390, lr=3.6776e-05, gnorm=0.41, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=161996
2023-01-11 10:53:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:45 - progress_bar.py[line:274] - INFO: epoch 001:  29440 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4352, wps=102.8, ups=0.63, wpb=109.1, bsz=40, num_updates=29400, lr=3.67708e-05, gnorm=0.234, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162012
2023-01-11 10:53:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:53:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:02 - progress_bar.py[line:274] - INFO: epoch 001:  29450 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4592, wps=100.9, ups=0.62, wpb=108.9, bsz=40, num_updates=29410, lr=3.67656e-05, gnorm=0.758, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162028
2023-01-11 10:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:18 - progress_bar.py[line:274] - INFO: epoch 001:  29460 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4831, wps=102.2, ups=0.62, wpb=110.7, bsz=40, num_updates=29420, lr=3.67604e-05, gnorm=0.792, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162045
2023-01-11 10:54:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:35 - progress_bar.py[line:274] - INFO: epoch 001:  29470 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4947, wps=98.6, ups=0.6, wpb=109.4, bsz=40, num_updates=29430, lr=3.67552e-05, gnorm=0.34, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=162061
2023-01-11 10:54:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:54:52 - progress_bar.py[line:274] - INFO: epoch 001:  29480 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4479, wps=102.4, ups=0.61, wpb=111.7, bsz=40, num_updates=29440, lr=3.675e-05, gnorm=0.613, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=162078
2023-01-11 10:54:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:08 - progress_bar.py[line:274] - INFO: epoch 001:  29490 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4851, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=29450, lr=3.67448e-05, gnorm=0.417, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162094
2023-01-11 10:55:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:25 - progress_bar.py[line:274] - INFO: epoch 001:  29500 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.514, wps=101.8, ups=0.62, wpb=109.3, bsz=40, num_updates=29460, lr=3.67396e-05, gnorm=0.597, clip=30, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=162111
2023-01-11 10:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:42 - progress_bar.py[line:274] - INFO: epoch 001:  29510 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4433, wps=98.6, ups=0.6, wpb=110.1, bsz=40, num_updates=29470, lr=3.67344e-05, gnorm=0.545, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=162128
2023-01-11 10:55:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:55:58 - progress_bar.py[line:274] - INFO: epoch 001:  29520 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4476, wps=99.4, ups=0.61, wpb=109.2, bsz=40, num_updates=29480, lr=3.67292e-05, gnorm=0.843, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162144
2023-01-11 10:56:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:15 - progress_bar.py[line:274] - INFO: epoch 001:  29530 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4141, wps=100.5, ups=0.61, wpb=109.7, bsz=40, num_updates=29490, lr=3.6724e-05, gnorm=0.256, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=162161
2023-01-11 10:56:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:32 - progress_bar.py[line:274] - INFO: epoch 001:  29540 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4419, wps=100.8, ups=0.6, wpb=111.7, bsz=40, num_updates=29500, lr=3.67188e-05, gnorm=0.425, clip=10, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=162178
2023-01-11 10:56:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:48 - progress_bar.py[line:274] - INFO: epoch 001:  29550 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4255, wps=103.3, ups=0.63, wpb=109.8, bsz=40, num_updates=29510, lr=3.67135e-05, gnorm=0.437, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162194
2023-01-11 10:56:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:56:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:05 - progress_bar.py[line:274] - INFO: epoch 001:  29560 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4615, wps=100.5, ups=0.61, wpb=109.7, bsz=40, num_updates=29520, lr=3.67083e-05, gnorm=0.165, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=162211
2023-01-11 10:57:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:22 - progress_bar.py[line:274] - INFO: epoch 001:  29570 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4608, wps=99.6, ups=0.6, wpb=110.1, bsz=40, num_updates=29530, lr=3.67031e-05, gnorm=0.406, clip=0, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=162228
2023-01-11 10:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:38 - progress_bar.py[line:274] - INFO: epoch 001:  29580 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4186, wps=103, ups=0.62, wpb=110.5, bsz=40, num_updates=29540, lr=3.66979e-05, gnorm=0.36, clip=0, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=162245
2023-01-11 10:57:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:57:55 - progress_bar.py[line:274] - INFO: epoch 001:  29590 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=112.067, nsentences=40, sample_size=112.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4138, wps=102.1, ups=0.61, wpb=112.1, bsz=40, num_updates=29550, lr=3.66927e-05, gnorm=0.444, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162261
2023-01-11 10:58:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:12 - progress_bar.py[line:274] - INFO: epoch 001:  29600 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4444, wps=99.9, ups=0.61, wpb=110, bsz=40, num_updates=29560, lr=3.66875e-05, gnorm=0.471, clip=10, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=162278
2023-01-11 10:58:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:28 - progress_bar.py[line:274] - INFO: epoch 001:  29610 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4608, wps=100.5, ups=0.62, wpb=108.7, bsz=40, num_updates=29570, lr=3.66823e-05, gnorm=0.321, clip=0, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=162294
2023-01-11 10:58:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:45 - progress_bar.py[line:274] - INFO: epoch 001:  29620 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4545, wps=96.8, ups=0.6, wpb=108.3, bsz=40, num_updates=29580, lr=3.66771e-05, gnorm=0.433, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=162312
2023-01-11 10:58:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:58:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:02 - progress_bar.py[line:274] - INFO: epoch 001:  29630 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.419, wps=101.6, ups=0.62, wpb=109.3, bsz=40, num_updates=29590, lr=3.66719e-05, gnorm=0.326, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=162328
2023-01-11 10:59:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:19 - progress_bar.py[line:274] - INFO: epoch 001:  29640 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4554, wps=98.3, ups=0.6, wpb=109.6, bsz=40, num_updates=29600, lr=3.66667e-05, gnorm=0.485, clip=20, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=162345
2023-01-11 10:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:35 - progress_bar.py[line:274] - INFO: epoch 001:  29650 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4167, wps=101.9, ups=0.62, wpb=109.1, bsz=40, num_updates=29610, lr=3.66615e-05, gnorm=0.276, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162361
2023-01-11 10:59:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:51 - progress_bar.py[line:274] - INFO: epoch 001:  29660 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4907, wps=102, ups=0.62, wpb=109.3, bsz=40, num_updates=29620, lr=3.66562e-05, gnorm=1.706, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=162377
2023-01-11 10:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 10:59:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:08 - progress_bar.py[line:274] - INFO: epoch 001:  29670 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4211, wps=100.3, ups=0.6, wpb=110.6, bsz=40, num_updates=29630, lr=3.6651e-05, gnorm=0.367, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162394
2023-01-11 11:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:24 - progress_bar.py[line:274] - INFO: epoch 001:  29680 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5, wps=103.1, ups=0.62, wpb=110.2, bsz=40, num_updates=29640, lr=3.66458e-05, gnorm=0.233, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162410
2023-01-11 11:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:40 - progress_bar.py[line:274] - INFO: epoch 001:  29690 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4182, wps=101.6, ups=0.62, wpb=108.8, bsz=40, num_updates=29650, lr=3.66406e-05, gnorm=0.196, clip=0, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162427
2023-01-11 11:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:00:57 - progress_bar.py[line:274] - INFO: epoch 001:  29700 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3846, wps=100.3, ups=0.61, wpb=110.1, bsz=40, num_updates=29660, lr=3.66354e-05, gnorm=0.257, clip=0, loss_scale=1024, train_wall=16, gb_free=10, ema_decay=0.9999, wall=162443
2023-01-11 11:01:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:14 - progress_bar.py[line:274] - INFO: epoch 001:  29710 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5158, wps=101.7, ups=0.62, wpb=109.9, bsz=40, num_updates=29670, lr=3.66302e-05, gnorm=0.513, clip=10, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162460
2023-01-11 11:01:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:30 - progress_bar.py[line:274] - INFO: epoch 001:  29720 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4862, wps=100.5, ups=0.61, wpb=109.5, bsz=40, num_updates=29680, lr=3.6625e-05, gnorm=0.615, clip=10, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162476
2023-01-11 11:01:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:47 - progress_bar.py[line:274] - INFO: epoch 001:  29730 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4468, wps=103, ups=0.62, wpb=111.3, bsz=40, num_updates=29690, lr=3.66198e-05, gnorm=0.224, clip=0, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162493
2023-01-11 11:01:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:01:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:04 - progress_bar.py[line:274] - INFO: epoch 001:  29740 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4752, wps=98.8, ups=0.6, wpb=110.4, bsz=40, num_updates=29700, lr=3.66146e-05, gnorm=0.35, clip=0, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=162510
2023-01-11 11:02:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:20 - progress_bar.py[line:274] - INFO: epoch 001:  29750 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4691, wps=100, ups=0.61, wpb=109.9, bsz=40, num_updates=29710, lr=3.66094e-05, gnorm=0.387, clip=10, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162527
2023-01-11 11:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:37 - progress_bar.py[line:274] - INFO: epoch 001:  29760 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3608, wps=100.5, ups=0.61, wpb=110.3, bsz=40, num_updates=29720, lr=3.66042e-05, gnorm=0.122, clip=0, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162543
2023-01-11 11:02:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:02:54 - progress_bar.py[line:274] - INFO: epoch 001:  29770 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4286, wps=100, ups=0.61, wpb=108.7, bsz=40, num_updates=29730, lr=3.6599e-05, gnorm=0.292, clip=10, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=162560
2023-01-11 11:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:10 - progress_bar.py[line:274] - INFO: epoch 001:  29780 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4242, wps=100.9, ups=0.62, wpb=108.5, bsz=40, num_updates=29740, lr=3.65938e-05, gnorm=0.32, clip=0, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162576
2023-01-11 11:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:27 - progress_bar.py[line:274] - INFO: epoch 001:  29790 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4725, wps=100.6, ups=0.61, wpb=109.7, bsz=40, num_updates=29750, lr=3.65885e-05, gnorm=0.34, clip=10, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=162593
2023-01-11 11:03:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:43 - progress_bar.py[line:274] - INFO: epoch 001:  29800 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.404, wps=100.2, ups=0.61, wpb=108.9, bsz=40, num_updates=29760, lr=3.65833e-05, gnorm=0.322, clip=0, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162609
2023-01-11 11:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:03:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:00 - progress_bar.py[line:274] - INFO: epoch 001:  29810 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5545, wps=99.7, ups=0.61, wpb=109.5, bsz=40, num_updates=29770, lr=3.65781e-05, gnorm=0.347, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=162626
2023-01-11 11:04:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:17 - progress_bar.py[line:274] - INFO: epoch 001:  29820 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4516, wps=100.8, ups=0.61, wpb=110.9, bsz=40, num_updates=29780, lr=3.65729e-05, gnorm=0.282, clip=0, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162643
2023-01-11 11:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:33 - progress_bar.py[line:274] - INFO: epoch 001:  29830 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.44, wps=100.7, ups=0.61, wpb=109.3, bsz=40, num_updates=29790, lr=3.65677e-05, gnorm=0.52, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162659
2023-01-11 11:04:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:50 - progress_bar.py[line:274] - INFO: epoch 001:  29840 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4272, wps=97.8, ups=0.59, wpb=109.7, bsz=40, num_updates=29800, lr=3.65625e-05, gnorm=0.36, clip=10, loss_scale=1024, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=162677
2023-01-11 11:04:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:04:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:07 - progress_bar.py[line:274] - INFO: epoch 001:  29850 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4725, wps=100.1, ups=0.61, wpb=110.3, bsz=40, num_updates=29810, lr=3.65573e-05, gnorm=0.394, clip=20, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162693
2023-01-11 11:05:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:24 - progress_bar.py[line:274] - INFO: epoch 001:  29860 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3786, wps=99.5, ups=0.61, wpb=109.1, bsz=40, num_updates=29820, lr=3.65521e-05, gnorm=0.452, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=162710
2023-01-11 11:05:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:40 - progress_bar.py[line:274] - INFO: epoch 001:  29870 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4286, wps=103.1, ups=0.62, wpb=110.3, bsz=40, num_updates=29830, lr=3.65469e-05, gnorm=0.673, clip=20, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=162726
2023-01-11 11:05:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:05:57 - progress_bar.py[line:274] - INFO: epoch 001:  29880 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4526, wps=100.7, ups=0.61, wpb=109.8, bsz=40, num_updates=29840, lr=3.65417e-05, gnorm=0.252, clip=0, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=162743
2023-01-11 11:06:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:13 - progress_bar.py[line:274] - INFO: epoch 001:  29890 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5114, wps=101.4, ups=0.61, wpb=110.9, bsz=40, num_updates=29850, lr=3.65365e-05, gnorm=0.349, clip=10, loss_scale=1024, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=162760
2023-01-11 11:06:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:30 - progress_bar.py[line:274] - INFO: epoch 001:  29900 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.443, wps=101.6, ups=0.61, wpb=111.5, bsz=40, num_updates=29860, lr=3.65313e-05, gnorm=0.373, clip=0, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162776
2023-01-11 11:06:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:46 - progress_bar.py[line:274] - INFO: epoch 001:  29910 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4455, wps=102.9, ups=0.62, wpb=110.1, bsz=40, num_updates=29870, lr=3.6526e-05, gnorm=0.262, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=162793
2023-01-11 11:06:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:06:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:03 - progress_bar.py[line:274] - INFO: epoch 001:  29920 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5283, wps=103.2, ups=0.62, wpb=110.5, bsz=40, num_updates=29880, lr=3.65208e-05, gnorm=0.251, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=162809
2023-01-11 11:07:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:19 - progress_bar.py[line:274] - INFO: epoch 001:  29930 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4646, wps=102.7, ups=0.62, wpb=110.6, bsz=40, num_updates=29890, lr=3.65156e-05, gnorm=0.261, clip=0, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162825
2023-01-11 11:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:35 - progress_bar.py[line:274] - INFO: epoch 001:  29940 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3878, wps=106.5, ups=0.64, wpb=110.8, bsz=40, num_updates=29900, lr=3.65104e-05, gnorm=0.332, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=162841
2023-01-11 11:07:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:51 - progress_bar.py[line:274] - INFO: epoch 001:  29950 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5222, wps=104.6, ups=0.63, wpb=111.2, bsz=40, num_updates=29910, lr=3.65052e-05, gnorm=0.285, clip=0, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=162857
2023-01-11 11:07:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:07:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:08 - progress_bar.py[line:274] - INFO: epoch 001:  29960 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4519, wps=100.1, ups=0.6, wpb=110.6, bsz=40, num_updates=29920, lr=3.65e-05, gnorm=0.493, clip=10, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=162874
2023-01-11 11:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:25 - progress_bar.py[line:274] - INFO: epoch 001:  29970 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3725, wps=99.5, ups=0.61, wpb=109.6, bsz=40, num_updates=29930, lr=3.64948e-05, gnorm=0.537, clip=10, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162891
2023-01-11 11:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:41 - progress_bar.py[line:274] - INFO: epoch 001:  29980 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4045, wps=102, ups=0.61, wpb=111, bsz=40, num_updates=29940, lr=3.64896e-05, gnorm=0.336, clip=10, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=162907
2023-01-11 11:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:08:58 - progress_bar.py[line:274] - INFO: epoch 001:  29990 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.495, wps=99.4, ups=0.61, wpb=108.9, bsz=40, num_updates=29950, lr=3.64844e-05, gnorm=0.246, clip=0, loss_scale=1024, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=162924
2023-01-11 11:09:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:14 - progress_bar.py[line:274] - INFO: epoch 001:  30000 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4245, wps=101.9, ups=0.62, wpb=109.5, bsz=40, num_updates=29960, lr=3.64792e-05, gnorm=0.292, clip=0, loss_scale=1024, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=162940
2023-01-11 11:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:31 - progress_bar.py[line:274] - INFO: epoch 001:  30010 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3824, wps=101.6, ups=0.61, wpb=110.1, bsz=40, num_updates=29970, lr=3.6474e-05, gnorm=0.357, clip=0, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=162957
2023-01-11 11:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:48 - progress_bar.py[line:274] - INFO: epoch 001:  30020 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4348, wps=101.5, ups=0.62, wpb=109.1, bsz=40, num_updates=29980, lr=3.64688e-05, gnorm=0.419, clip=20, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=162974
2023-01-11 11:09:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:10:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:10:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:10:04 - progress_bar.py[line:274] - INFO: epoch 001:  30030 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4381, wps=98.9, ups=0.6, wpb=109.1, bsz=40, num_updates=29990, lr=3.64635e-05, gnorm=0.23, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=162990
2023-01-11 11:10:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:10:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:10:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:10:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 11:10:21 - progress_bar.py[line:274] - INFO: epoch 001:  30040 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4167, wps=101.5, ups=0.61, wpb=111.3, bsz=40, num_updates=30000, lr=3.64583e-05, gnorm=0.351, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=163007
2023-01-11 11:10:21 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 11:10:22 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 11:10:23 - train.py[line:551] - INFO: load:1.25 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 11:12:55 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 11:12:55 - train.py[line:551] - INFO: load:1.28 valid_run:152.25 task_valid:149.60 collect_output:1.50
2023-01-11 11:15:24 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 11:15:24 - train.py[line:551] - INFO: load:1.30 valid_run:300.93 task_valid:293.97 collect_output:4.71
2023-01-11 11:17:56 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 11:17:56 - train.py[line:551] - INFO: load:1.33 valid_run:452.75 task_valid:438.15 collect_output:11.28
2023-01-11 11:20:25 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 11:20:25 - train.py[line:551] - INFO: load:1.36 valid_run:602.20 task_valid:584.23 collect_output:13.58
2023-01-11 11:22:58 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 11:22:58 - train.py[line:551] - INFO: load:1.38 valid_run:754.83 task_valid:732.76 collect_output:16.58
2023-01-11 11:25:30 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 11:25:30 - train.py[line:551] - INFO: load:1.41 valid_run:906.79 task_valid:879.66 collect_output:20.53
2023-01-11 11:28:03 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 11:28:03 - train.py[line:551] - INFO: load:1.44 valid_run:1059.44 task_valid:1026.62 collect_output:25.18
2023-01-11 11:30:33 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 11:30:33 - train.py[line:551] - INFO: load:1.46 valid_run:1210.10 task_valid:1169.05 collect_output:32.31
2023-01-11 11:33:03 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 11:33:03 - train.py[line:551] - INFO: load:1.49 valid_run:1359.88 task_valid:1315.15 collect_output:34.87
2023-01-11 11:35:32 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 11:35:32 - train.py[line:551] - INFO: load:1.51 valid_run:1508.73 task_valid:1459.88 collect_output:37.89
2023-01-11 11:38:02 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 11:38:02 - train.py[line:551] - INFO: load:1.54 valid_run:1658.88 task_valid:1606.09 collect_output:40.74
2023-01-11 11:40:33 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 11:40:33 - train.py[line:551] - INFO: load:1.57 valid_run:1808.94 task_valid:1752.17 collect_output:43.58
2023-01-11 11:43:02 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 11:43:02 - train.py[line:551] - INFO: load:1.59 valid_run:1957.92 task_valid:1894.47 collect_output:49.19
2023-01-11 11:45:33 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 11:45:33 - train.py[line:551] - INFO: load:1.62 valid_run:2108.93 task_valid:2041.33 collect_output:52.24
2023-01-11 11:48:04 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 11:48:04 - train.py[line:551] - INFO: load:1.64 valid_run:2259.75 task_valid:2188.95 collect_output:54.38
2023-01-11 11:50:34 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 11:50:34 - train.py[line:551] - INFO: load:1.67 valid_run:2409.64 task_valid:2333.94 collect_output:58.16
2023-01-11 11:53:05 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 11:53:05 - train.py[line:551] - INFO: load:1.70 valid_run:2560.68 task_valid:2480.09 collect_output:61.92
2023-01-11 11:55:35 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 11:55:35 - train.py[line:551] - INFO: load:1.72 valid_run:2711.12 task_valid:2627.73 collect_output:63.69
2023-01-11 11:58:03 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 11:58:03 - train.py[line:551] - INFO: load:1.75 valid_run:2859.08 task_valid:2770.02 collect_output:68.23
2023-01-11 12:00:33 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 12:00:33 - train.py[line:551] - INFO: load:1.78 valid_run:3009.09 task_valid:2915.92 collect_output:71.23
2023-01-11 12:03:05 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 12:03:05 - train.py[line:551] - INFO: load:1.80 valid_run:3160.33 task_valid:3061.53 collect_output:75.74
2023-01-11 12:05:34 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 12:05:34 - train.py[line:551] - INFO: load:1.83 valid_run:3309.60 task_valid:3206.97 collect_output:78.51
2023-01-11 12:08:05 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 12:08:05 - train.py[line:551] - INFO: load:1.85 valid_run:3460.21 task_valid:3353.75 collect_output:81.28
2023-01-11 12:10:36 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 12:10:36 - train.py[line:551] - INFO: load:1.88 valid_run:3611.74 task_valid:3501.42 collect_output:84.04

====================================================================================================
SGG eval:     R @ 50: 0.4611;     R @ 100: 0.5266;     R @ 500: 0.5592;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2809;    mR @ 100: 0.3348;    mR @ 500: 0.3639;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7390) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8021) (playing:0.0000) (riding:0.6343) (says:0.0000) (sitting on:0.7384) (standing on:0.2260) (using:0.6000) (walking in:0.0000) (walking on:0.2973) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-01-11 12:13:08 - train.py[line:487] - INFO: 0.5266481792717087

====================================================================================================
SGG eval:     R @ 50: 0.4611;     R @ 100: 0.5266;     R @ 500: 0.5592;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2809;    mR @ 100: 0.3348;    mR @ 500: 0.3639;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7390) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8021) (playing:0.0000) (riding:0.6343) (says:0.0000) (sitting on:0.7384) (standing on:0.2260) (using:0.6000) (walking in:0.0000) (walking on:0.2973) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-01-11 12:13:08 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 12:13:08 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.362 | loss_v1 0 | loss_v2 0 | nll_loss 0.208 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.526648 | ppl 1.16 | vqa_score 0.4448 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 30000 | best_R@100 0.69005
2023-01-11 12:13:08 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 30000 updates
2023-01-11 12:13:08 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_30000.pt
2023-01-11 12:13:52 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_30000.pt
2023-01-11 12:15:21 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_30000.pt (epoch 1 @ 30000 updates, score 0.5266481792717087) (writing took 132.42674304917455 seconds)
2023-01-11 12:15:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:15:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:15:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:15:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:15:38 - progress_bar.py[line:274] - INFO: epoch 001:  30050 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4434, wps=0.4, ups=0, wpb=108.8, bsz=40, num_updates=30010, lr=3.64531e-05, gnorm=0.451, clip=10, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=166924
2023-01-11 12:15:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:15:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:15:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:15:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:15:54 - progress_bar.py[line:274] - INFO: epoch 001:  30060 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.449, wps=102.2, ups=0.62, wpb=109.7, bsz=40, num_updates=30020, lr=3.64479e-05, gnorm=1.287, clip=50, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=166940
2023-01-11 12:15:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:11 - progress_bar.py[line:274] - INFO: epoch 001:  30070 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3627, wps=99.1, ups=0.61, wpb=108.4, bsz=40, num_updates=30030, lr=3.64427e-05, gnorm=0.941, clip=20, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=166957
2023-01-11 12:16:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:27 - progress_bar.py[line:274] - INFO: epoch 001:  30080 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4, wps=102.5, ups=0.63, wpb=108.8, bsz=40, num_updates=30040, lr=3.64375e-05, gnorm=0.313, clip=10, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=166973
2023-01-11 12:16:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:43 - progress_bar.py[line:274] - INFO: epoch 001:  30090 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4536, wps=103.8, ups=0.63, wpb=110.6, bsz=40, num_updates=30050, lr=3.64323e-05, gnorm=0.744, clip=10, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=166989
2023-01-11 12:16:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:16:57 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 12:16:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:01 - progress_bar.py[line:274] - INFO: epoch 001:  30101 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5098, wps=96.1, ups=0.58, wpb=111.3, bsz=40, num_updates=30060, lr=3.64271e-05, gnorm=0.823, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=167007
2023-01-11 12:17:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:17 - progress_bar.py[line:274] - INFO: epoch 001:  30111 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5213, wps=100.3, ups=0.61, wpb=109.9, bsz=40, num_updates=30070, lr=3.64219e-05, gnorm=0.724, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=167023
2023-01-11 12:17:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:34 - progress_bar.py[line:274] - INFO: epoch 001:  30121 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4175, wps=100.7, ups=0.61, wpb=110.3, bsz=40, num_updates=30080, lr=3.64167e-05, gnorm=0.792, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=167040
2023-01-11 12:17:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:50 - progress_bar.py[line:274] - INFO: epoch 001:  30131 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4688, wps=104.9, ups=0.63, wpb=111.3, bsz=40, num_updates=30090, lr=3.64115e-05, gnorm=0.322, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=167056
2023-01-11 12:17:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:17:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:07 - progress_bar.py[line:274] - INFO: epoch 001:  30141 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3853, wps=101.1, ups=0.61, wpb=109.9, bsz=40, num_updates=30100, lr=3.64063e-05, gnorm=0.477, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=167073
2023-01-11 12:18:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:23 - progress_bar.py[line:274] - INFO: epoch 001:  30151 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5122, wps=103, ups=0.61, wpb=111.8, bsz=40, num_updates=30110, lr=3.6401e-05, gnorm=1.074, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=167090
2023-01-11 12:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:40 - progress_bar.py[line:274] - INFO: epoch 001:  30161 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4857, wps=99, ups=0.6, wpb=109.9, bsz=40, num_updates=30120, lr=3.63958e-05, gnorm=0.484, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=167107
2023-01-11 12:18:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:18:57 - progress_bar.py[line:274] - INFO: epoch 001:  30171 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4455, wps=100.2, ups=0.61, wpb=108.7, bsz=40, num_updates=30130, lr=3.63906e-05, gnorm=0.422, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167123
2023-01-11 12:19:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:13 - progress_bar.py[line:274] - INFO: epoch 001:  30181 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5393, wps=103.2, ups=0.62, wpb=111.3, bsz=40, num_updates=30140, lr=3.63854e-05, gnorm=0.275, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167140
2023-01-11 12:19:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:30 - progress_bar.py[line:274] - INFO: epoch 001:  30191 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5152, wps=102.5, ups=0.62, wpb=110.9, bsz=40, num_updates=30150, lr=3.63802e-05, gnorm=0.253, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=167156
2023-01-11 12:19:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:47 - progress_bar.py[line:274] - INFO: epoch 001:  30201 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4141, wps=101.9, ups=0.62, wpb=109.5, bsz=40, num_updates=30160, lr=3.6375e-05, gnorm=0.222, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167173
2023-01-11 12:19:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:19:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:03 - progress_bar.py[line:274] - INFO: epoch 001:  30211 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4286, wps=100.2, ups=0.61, wpb=110.1, bsz=40, num_updates=30170, lr=3.63698e-05, gnorm=0.387, clip=10, loss_scale=512, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=167189
2023-01-11 12:20:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:19 - progress_bar.py[line:274] - INFO: epoch 001:  30221 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4184, wps=104.7, ups=0.64, wpb=109.5, bsz=40, num_updates=30180, lr=3.63646e-05, gnorm=0.484, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=167205
2023-01-11 12:20:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:36 - progress_bar.py[line:274] - INFO: epoch 001:  30231 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5116, wps=101.4, ups=0.61, wpb=110.1, bsz=40, num_updates=30190, lr=3.63594e-05, gnorm=0.708, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167222
2023-01-11 12:20:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:52 - progress_bar.py[line:274] - INFO: epoch 001:  30241 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4554, wps=100.9, ups=0.61, wpb=110.2, bsz=40, num_updates=30200, lr=3.63542e-05, gnorm=0.206, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167239
2023-01-11 12:20:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:20:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:09 - progress_bar.py[line:274] - INFO: epoch 001:  30251 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4653, wps=98.7, ups=0.6, wpb=110, bsz=40, num_updates=30210, lr=3.6349e-05, gnorm=0.224, clip=0, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=167256
2023-01-11 12:21:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:26 - progress_bar.py[line:274] - INFO: epoch 001:  30261 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3267, wps=99.6, ups=0.6, wpb=110.4, bsz=40, num_updates=30220, lr=3.63437e-05, gnorm=0.288, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=167273
2023-01-11 12:21:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:43 - progress_bar.py[line:274] - INFO: epoch 001:  30271 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5196, wps=100.3, ups=0.61, wpb=109.8, bsz=40, num_updates=30230, lr=3.63385e-05, gnorm=1.309, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=167289
2023-01-11 12:21:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:21:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:00 - progress_bar.py[line:274] - INFO: epoch 001:  30281 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.466, wps=97.8, ups=0.6, wpb=108.8, bsz=40, num_updates=30240, lr=3.63333e-05, gnorm=0.464, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=167306
2023-01-11 12:22:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:17 - progress_bar.py[line:274] - INFO: epoch 001:  30291 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4653, wps=98.7, ups=0.6, wpb=109.5, bsz=40, num_updates=30250, lr=3.63281e-05, gnorm=0.371, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=167323
2023-01-11 12:22:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:34 - progress_bar.py[line:274] - INFO: epoch 001:  30301 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4563, wps=99.9, ups=0.61, wpb=109.1, bsz=40, num_updates=30260, lr=3.63229e-05, gnorm=0.457, clip=20, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=167340
2023-01-11 12:22:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:51 - progress_bar.py[line:274] - INFO: epoch 001:  30311 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4149, wps=99, ups=0.6, wpb=110.8, bsz=40, num_updates=30270, lr=3.63177e-05, gnorm=0.542, clip=10, loss_scale=512, train_wall=17, gb_free=9.5, ema_decay=0.9999, wall=167357
2023-01-11 12:22:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:22:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:08 - progress_bar.py[line:274] - INFO: epoch 001:  30321 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5053, wps=100.2, ups=0.6, wpb=111.1, bsz=40, num_updates=30280, lr=3.63125e-05, gnorm=0.883, clip=10, loss_scale=512, train_wall=17, gb_free=9.5, ema_decay=0.9999, wall=167374
2023-01-11 12:23:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:24 - progress_bar.py[line:274] - INFO: epoch 001:  30331 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=102.3, ups=0.62, wpb=109.7, bsz=40, num_updates=30290, lr=3.63073e-05, gnorm=0.714, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=167390
2023-01-11 12:23:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:41 - progress_bar.py[line:274] - INFO: epoch 001:  30341 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4184, wps=99.1, ups=0.6, wpb=110.1, bsz=40, num_updates=30300, lr=3.63021e-05, gnorm=0.342, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=167407
2023-01-11 12:23:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:23:58 - progress_bar.py[line:274] - INFO: epoch 001:  30351 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4943, wps=102, ups=0.61, wpb=112.2, bsz=40, num_updates=30310, lr=3.62969e-05, gnorm=0.411, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=167424
2023-01-11 12:24:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:14 - progress_bar.py[line:274] - INFO: epoch 001:  30361 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4375, wps=101.6, ups=0.62, wpb=109.2, bsz=40, num_updates=30320, lr=3.62917e-05, gnorm=1.489, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167440
2023-01-11 12:24:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:31 - progress_bar.py[line:274] - INFO: epoch 001:  30371 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3736, wps=99.5, ups=0.61, wpb=109.5, bsz=40, num_updates=30330, lr=3.62865e-05, gnorm=0.857, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=167457
2023-01-11 12:24:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:48 - progress_bar.py[line:274] - INFO: epoch 001:  30381 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.383, wps=101.4, ups=0.61, wpb=110.5, bsz=40, num_updates=30340, lr=3.62812e-05, gnorm=0.275, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=167474
2023-01-11 12:24:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:24:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:04 - progress_bar.py[line:274] - INFO: epoch 001:  30391 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5052, wps=103, ups=0.62, wpb=110.9, bsz=40, num_updates=30350, lr=3.6276e-05, gnorm=0.841, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=167490
2023-01-11 12:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:21 - progress_bar.py[line:274] - INFO: epoch 001:  30401 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4444, wps=97.9, ups=0.59, wpb=110.4, bsz=40, num_updates=30360, lr=3.62708e-05, gnorm=0.292, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=167507
2023-01-11 12:25:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:37 - progress_bar.py[line:274] - INFO: epoch 001:  30411 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4375, wps=102.1, ups=0.63, wpb=108.8, bsz=40, num_updates=30370, lr=3.62656e-05, gnorm=0.362, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167524
2023-01-11 12:25:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:25:54 - progress_bar.py[line:274] - INFO: epoch 001:  30421 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.42, wps=99.7, ups=0.6, wpb=109.9, bsz=40, num_updates=30380, lr=3.62604e-05, gnorm=0.411, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=167540
2023-01-11 12:25:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:11 - progress_bar.py[line:274] - INFO: epoch 001:  30431 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4571, wps=98.9, ups=0.61, wpb=108.7, bsz=40, num_updates=30390, lr=3.62552e-05, gnorm=0.298, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167557
2023-01-11 12:26:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:27 - progress_bar.py[line:274] - INFO: epoch 001:  30441 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.159, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4818, wps=101.2, ups=0.62, wpb=109.3, bsz=40, num_updates=30400, lr=3.625e-05, gnorm=1.298, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=167574
2023-01-11 12:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:44 - progress_bar.py[line:274] - INFO: epoch 001:  30451 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3861, wps=101.9, ups=0.62, wpb=110.2, bsz=40, num_updates=30410, lr=3.62448e-05, gnorm=0.194, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=167590
2023-01-11 12:26:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:26:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:00 - progress_bar.py[line:274] - INFO: epoch 001:  30461 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4674, wps=102.7, ups=0.62, wpb=109.7, bsz=40, num_updates=30420, lr=3.62396e-05, gnorm=0.188, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167606
2023-01-11 12:27:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:17 - progress_bar.py[line:274] - INFO: epoch 001:  30471 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5306, wps=99.2, ups=0.6, wpb=109.5, bsz=40, num_updates=30430, lr=3.62344e-05, gnorm=0.213, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=167623
2023-01-11 12:27:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:34 - progress_bar.py[line:274] - INFO: epoch 001:  30481 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4947, wps=101.6, ups=0.62, wpb=109.4, bsz=40, num_updates=30440, lr=3.62292e-05, gnorm=0.168, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=167640
2023-01-11 12:27:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:50 - progress_bar.py[line:274] - INFO: epoch 001:  30491 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5158, wps=102.1, ups=0.62, wpb=110.7, bsz=40, num_updates=30450, lr=3.6224e-05, gnorm=0.433, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167656
2023-01-11 12:27:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:27:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:07 - progress_bar.py[line:274] - INFO: epoch 001:  30501 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4639, wps=98.2, ups=0.59, wpb=110.3, bsz=40, num_updates=30460, lr=3.62188e-05, gnorm=0.292, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=167674
2023-01-11 12:28:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:24 - progress_bar.py[line:274] - INFO: epoch 001:  30511 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5521, wps=98.5, ups=0.6, wpb=110.1, bsz=40, num_updates=30470, lr=3.62135e-05, gnorm=0.394, clip=0, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=167691
2023-01-11 12:28:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:41 - progress_bar.py[line:274] - INFO: epoch 001:  30521 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4946, wps=99.5, ups=0.61, wpb=109.7, bsz=40, num_updates=30480, lr=3.62083e-05, gnorm=0.523, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=167707
2023-01-11 12:28:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:28:58 - progress_bar.py[line:274] - INFO: epoch 001:  30531 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3736, wps=100.3, ups=0.61, wpb=109.7, bsz=40, num_updates=30490, lr=3.62031e-05, gnorm=0.162, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167724
2023-01-11 12:29:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:15 - progress_bar.py[line:274] - INFO: epoch 001:  30541 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5306, wps=100.6, ups=0.61, wpb=110.1, bsz=40, num_updates=30500, lr=3.61979e-05, gnorm=0.32, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=167741
2023-01-11 12:29:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:32 - progress_bar.py[line:274] - INFO: epoch 001:  30551 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3491, wps=99.1, ups=0.61, wpb=108.5, bsz=40, num_updates=30510, lr=3.61927e-05, gnorm=0.634, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=167758
2023-01-11 12:29:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:48 - progress_bar.py[line:274] - INFO: epoch 001:  30561 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4286, wps=100.9, ups=0.61, wpb=110.3, bsz=40, num_updates=30520, lr=3.61875e-05, gnorm=0.443, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=167774
2023-01-11 12:29:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:29:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:05 - progress_bar.py[line:274] - INFO: epoch 001:  30571 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4742, wps=102.6, ups=0.62, wpb=110.2, bsz=40, num_updates=30530, lr=3.61823e-05, gnorm=0.224, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167791
2023-01-11 12:30:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:21 - progress_bar.py[line:274] - INFO: epoch 001:  30581 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.43, wps=102.9, ups=0.62, wpb=109.9, bsz=40, num_updates=30540, lr=3.61771e-05, gnorm=0.224, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=167807
2023-01-11 12:30:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:37 - progress_bar.py[line:274] - INFO: epoch 001:  30591 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4951, wps=103, ups=0.63, wpb=109.7, bsz=40, num_updates=30550, lr=3.61719e-05, gnorm=1.289, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167823
2023-01-11 12:30:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:30:54 - progress_bar.py[line:274] - INFO: epoch 001:  30601 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5158, wps=101.1, ups=0.61, wpb=110.5, bsz=40, num_updates=30560, lr=3.61667e-05, gnorm=0.249, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167840
2023-01-11 12:30:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:11 - progress_bar.py[line:274] - INFO: epoch 001:  30611 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=98.9, ups=0.6, wpb=110.2, bsz=40, num_updates=30570, lr=3.61615e-05, gnorm=0.307, clip=10, loss_scale=1024, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=167857
2023-01-11 12:31:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:28 - progress_bar.py[line:274] - INFO: epoch 001:  30621 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4455, wps=98.1, ups=0.6, wpb=109.6, bsz=40, num_updates=30580, lr=3.61563e-05, gnorm=0.172, clip=0, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=167874
2023-01-11 12:31:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:44 - progress_bar.py[line:274] - INFO: epoch 001:  30631 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4857, wps=99.9, ups=0.61, wpb=109.8, bsz=40, num_updates=30590, lr=3.6151e-05, gnorm=0.281, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167891
2023-01-11 12:31:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:31:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:01 - progress_bar.py[line:274] - INFO: epoch 001:  30641 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4674, wps=101.2, ups=0.61, wpb=109.8, bsz=40, num_updates=30600, lr=3.61458e-05, gnorm=0.312, clip=10, loss_scale=1024, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=167907
2023-01-11 12:32:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:18 - progress_bar.py[line:274] - INFO: epoch 001:  30651 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4388, wps=100.7, ups=0.62, wpb=109.1, bsz=40, num_updates=30610, lr=3.61406e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=167924
2023-01-11 12:32:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:36 - progress_bar.py[line:274] - INFO: epoch 001:  30661 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4184, wps=99.4, ups=0.6, wpb=111.1, bsz=40, num_updates=30620, lr=3.61354e-05, gnorm=0.415, clip=10, loss_scale=1024, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=167941
2023-01-11 12:32:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:32:53 - progress_bar.py[line:274] - INFO: epoch 001:  30671 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.48, wps=101.2, ups=0.61, wpb=110.8, bsz=40, num_updates=30630, lr=3.61302e-05, gnorm=0.292, clip=0, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=167959
2023-01-11 12:32:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:11 - progress_bar.py[line:274] - INFO: epoch 001:  30681 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4434, wps=95.7, ups=0.57, wpb=111.3, bsz=40, num_updates=30640, lr=3.6125e-05, gnorm=0.31, clip=0, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=167977
2023-01-11 12:33:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:20 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 12:33:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:30 - progress_bar.py[line:274] - INFO: epoch 001:  30692 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5429, wps=92.1, ups=0.55, wpb=110.9, bsz=40, num_updates=30650, lr=3.61198e-05, gnorm=0.463, clip=10, loss_scale=512, train_wall=18, gb_free=10.4, ema_decay=0.9999, wall=167996
2023-01-11 12:33:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:47 - progress_bar.py[line:274] - INFO: epoch 001:  30702 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.404, wps=98.5, ups=0.61, wpb=108.5, bsz=40, num_updates=30660, lr=3.61146e-05, gnorm=0.352, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=168013
2023-01-11 12:33:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:33:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:04 - progress_bar.py[line:274] - INFO: epoch 001:  30712 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4434, wps=93.3, ups=0.57, wpb=109.2, bsz=40, num_updates=30670, lr=3.61094e-05, gnorm=0.408, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=168031
2023-01-11 12:34:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:21 - progress_bar.py[line:274] - INFO: epoch 001:  30722 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4434, wps=100.1, ups=0.6, wpb=110.6, bsz=40, num_updates=30680, lr=3.61042e-05, gnorm=0.254, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=168048
2023-01-11 12:34:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:38 - progress_bar.py[line:274] - INFO: epoch 001:  30732 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4457, wps=98.9, ups=0.6, wpb=110.5, bsz=40, num_updates=30690, lr=3.6099e-05, gnorm=0.411, clip=10, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=168065
2023-01-11 12:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:34:55 - progress_bar.py[line:274] - INFO: epoch 001:  30742 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.434, wps=102.1, ups=0.62, wpb=109, bsz=40, num_updates=30700, lr=3.60938e-05, gnorm=0.299, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=168081
2023-01-11 12:34:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:12 - progress_bar.py[line:274] - INFO: epoch 001:  30752 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3679, wps=98.3, ups=0.6, wpb=109.1, bsz=40, num_updates=30710, lr=3.60885e-05, gnorm=0.19, clip=0, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=168098
2023-01-11 12:35:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:29 - progress_bar.py[line:274] - INFO: epoch 001:  30762 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4124, wps=98.6, ups=0.6, wpb=110, bsz=40, num_updates=30720, lr=3.60833e-05, gnorm=0.347, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=168115
2023-01-11 12:35:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:46 - progress_bar.py[line:274] - INFO: epoch 001:  30772 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.47, wps=102.7, ups=0.62, wpb=110.6, bsz=40, num_updates=30730, lr=3.60781e-05, gnorm=0.898, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=168132
2023-01-11 12:35:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:35:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:02 - progress_bar.py[line:274] - INFO: epoch 001:  30782 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4479, wps=99.6, ups=0.61, wpb=109.7, bsz=40, num_updates=30740, lr=3.60729e-05, gnorm=0.684, clip=30, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=168149
2023-01-11 12:36:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:19 - progress_bar.py[line:274] - INFO: epoch 001:  30792 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4434, wps=101.4, ups=0.62, wpb=109, bsz=40, num_updates=30750, lr=3.60677e-05, gnorm=0.317, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=168165
2023-01-11 12:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:35 - progress_bar.py[line:274] - INFO: epoch 001:  30802 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5102, wps=101.2, ups=0.61, wpb=110.5, bsz=40, num_updates=30760, lr=3.60625e-05, gnorm=0.208, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=168182
2023-01-11 12:36:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:52 - progress_bar.py[line:274] - INFO: epoch 001:  30812 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4, wps=101, ups=0.61, wpb=111, bsz=40, num_updates=30770, lr=3.60573e-05, gnorm=0.21, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=168198
2023-01-11 12:36:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:36:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:09 - progress_bar.py[line:274] - INFO: epoch 001:  30822 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.514, wps=102.4, ups=0.62, wpb=109.9, bsz=40, num_updates=30780, lr=3.60521e-05, gnorm=0.484, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=168215
2023-01-11 12:37:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:25 - progress_bar.py[line:274] - INFO: epoch 001:  30832 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4393, wps=100.2, ups=0.61, wpb=108.9, bsz=40, num_updates=30790, lr=3.60469e-05, gnorm=0.348, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=168231
2023-01-11 12:37:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:42 - progress_bar.py[line:274] - INFO: epoch 001:  30842 / 100000 loss=0.319, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3333, wps=97.2, ups=0.6, wpb=108.3, bsz=40, num_updates=30800, lr=3.60417e-05, gnorm=0.491, clip=10, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=168248
2023-01-11 12:37:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:37:58 - progress_bar.py[line:274] - INFO: epoch 001:  30852 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4444, wps=101, ups=0.62, wpb=108, bsz=40, num_updates=30810, lr=3.60365e-05, gnorm=1.003, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=168265
2023-01-11 12:38:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:15 - progress_bar.py[line:274] - INFO: epoch 001:  30862 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4348, wps=101, ups=0.61, wpb=110.2, bsz=40, num_updates=30820, lr=3.60313e-05, gnorm=1.08, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=168281
2023-01-11 12:38:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:32 - progress_bar.py[line:274] - INFO: epoch 001:  30872 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3298, wps=100.6, ups=0.6, wpb=111.4, bsz=40, num_updates=30830, lr=3.6026e-05, gnorm=1.5, clip=20, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=168298
2023-01-11 12:38:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:49 - progress_bar.py[line:274] - INFO: epoch 001:  30882 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4747, wps=99.4, ups=0.61, wpb=109.3, bsz=40, num_updates=30840, lr=3.60208e-05, gnorm=0.369, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=168315
2023-01-11 12:38:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:38:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:05 - progress_bar.py[line:274] - INFO: epoch 001:  30892 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4434, wps=100.3, ups=0.62, wpb=108.7, bsz=40, num_updates=30850, lr=3.60156e-05, gnorm=0.273, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=168331
2023-01-11 12:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:22 - progress_bar.py[line:274] - INFO: epoch 001:  30902 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5051, wps=99.9, ups=0.61, wpb=109.9, bsz=40, num_updates=30860, lr=3.60104e-05, gnorm=0.583, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=168348
2023-01-11 12:39:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:39 - progress_bar.py[line:274] - INFO: epoch 001:  30912 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3736, wps=101.7, ups=0.61, wpb=111.3, bsz=40, num_updates=30870, lr=3.60052e-05, gnorm=0.444, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=168365
2023-01-11 12:39:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:39:56 - progress_bar.py[line:274] - INFO: epoch 001:  30922 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4182, wps=94.7, ups=0.58, wpb=109, bsz=40, num_updates=30880, lr=3.6e-05, gnorm=0.553, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=168382
2023-01-11 12:40:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:14 - progress_bar.py[line:274] - INFO: epoch 001:  30932 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.495, wps=96.7, ups=0.59, wpb=109.7, bsz=40, num_updates=30890, lr=3.59948e-05, gnorm=0.563, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=168400
2023-01-11 12:40:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:32 - progress_bar.py[line:274] - INFO: epoch 001:  30942 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.51, wps=97.1, ups=0.59, wpb=109.3, bsz=40, num_updates=30900, lr=3.59896e-05, gnorm=0.446, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=168418
2023-01-11 12:40:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:48 - progress_bar.py[line:274] - INFO: epoch 001:  30952 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5185, wps=102.4, ups=0.62, wpb=109.4, bsz=40, num_updates=30910, lr=3.59844e-05, gnorm=1.071, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=168434
2023-01-11 12:40:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:40:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:05 - progress_bar.py[line:274] - INFO: epoch 001:  30962 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4245, wps=97.7, ups=0.59, wpb=109.5, bsz=40, num_updates=30920, lr=3.59792e-05, gnorm=0.204, clip=0, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=168451
2023-01-11 12:41:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:22 - progress_bar.py[line:274] - INFO: epoch 001:  30972 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4242, wps=98.4, ups=0.6, wpb=109.5, bsz=40, num_updates=30930, lr=3.5974e-05, gnorm=0.328, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=168468
2023-01-11 12:41:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:39 - progress_bar.py[line:274] - INFO: epoch 001:  30982 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5098, wps=98.9, ups=0.61, wpb=107.9, bsz=40, num_updates=30940, lr=3.59688e-05, gnorm=0.405, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=168485
2023-01-11 12:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:41:56 - progress_bar.py[line:274] - INFO: epoch 001:  30992 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4815, wps=99.5, ups=0.6, wpb=111.3, bsz=40, num_updates=30950, lr=3.59635e-05, gnorm=0.486, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=168502
2023-01-11 12:41:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:13 - progress_bar.py[line:274] - INFO: epoch 001:  31002 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3723, wps=102.4, ups=0.61, wpb=111.1, bsz=40, num_updates=30960, lr=3.59583e-05, gnorm=0.547, clip=10, loss_scale=512, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=168519
2023-01-11 12:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:29 - progress_bar.py[line:274] - INFO: epoch 001:  31012 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4286, wps=99.7, ups=0.61, wpb=109.5, bsz=40, num_updates=30970, lr=3.59531e-05, gnorm=0.491, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=168535
2023-01-11 12:42:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:46 - progress_bar.py[line:274] - INFO: epoch 001:  31022 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5044, wps=101.1, ups=0.62, wpb=109.4, bsz=40, num_updates=30980, lr=3.59479e-05, gnorm=0.206, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=168552
2023-01-11 12:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:42:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:43:04 - progress_bar.py[line:274] - INFO: epoch 001:  31032 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4434, wps=95.1, ups=0.57, wpb=110.4, bsz=40, num_updates=30990, lr=3.59427e-05, gnorm=0.237, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=168570
2023-01-11 12:43:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:43:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:43:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:43:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:43:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 12:43:20 - progress_bar.py[line:274] - INFO: epoch 001:  31042 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3854, wps=102, ups=0.62, wpb=110.1, bsz=40, num_updates=31000, lr=3.59375e-05, gnorm=0.494, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=168586
2023-01-11 12:43:20 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 12:43:23 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 12:43:23 - train.py[line:551] - INFO: load:1.57 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 12:45:55 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 12:45:55 - train.py[line:551] - INFO: load:1.60 valid_run:152.23 task_valid:149.38 collect_output:1.70
2023-01-11 12:48:23 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 12:48:23 - train.py[line:551] - INFO: load:1.62 valid_run:300.40 task_valid:293.35 collect_output:4.78
2023-01-11 12:50:56 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 12:50:56 - train.py[line:551] - INFO: load:1.65 valid_run:452.45 task_valid:437.91 collect_output:11.10
2023-01-11 12:53:25 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 12:53:25 - train.py[line:551] - INFO: load:1.68 valid_run:601.96 task_valid:583.95 collect_output:13.36
2023-01-11 12:55:57 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 12:55:57 - train.py[line:551] - INFO: load:1.71 valid_run:754.05 task_valid:732.19 collect_output:16.14
2023-01-11 12:58:29 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 12:58:29 - train.py[line:551] - INFO: load:1.73 valid_run:905.76 task_valid:878.90 collect_output:20.00
2023-01-11 13:01:02 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 13:01:02 - train.py[line:551] - INFO: load:1.76 valid_run:1058.54 task_valid:1025.93 collect_output:24.60
2023-01-11 13:03:33 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 13:03:33 - train.py[line:551] - INFO: load:1.79 valid_run:1209.07 task_valid:1167.90 collect_output:32.03
2023-01-11 13:06:02 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 13:06:02 - train.py[line:551] - INFO: load:1.81 valid_run:1358.59 task_valid:1313.62 collect_output:34.71
2023-01-11 13:08:31 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 13:08:31 - train.py[line:551] - INFO: load:1.84 valid_run:1507.33 task_valid:1458.42 collect_output:37.53
2023-01-11 13:11:01 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 13:11:01 - train.py[line:551] - INFO: load:1.87 valid_run:1657.11 task_valid:1604.34 collect_output:40.29
2023-01-11 13:13:31 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 13:13:31 - train.py[line:551] - INFO: load:1.90 valid_run:1807.31 task_valid:1750.55 collect_output:43.16
2023-01-11 13:16:01 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 13:16:01 - train.py[line:551] - INFO: load:1.92 valid_run:1956.91 task_valid:1893.56 collect_output:48.63
2023-01-11 13:18:32 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 13:18:32 - train.py[line:551] - INFO: load:1.95 valid_run:2107.75 task_valid:2040.08 collect_output:51.83
2023-01-11 13:21:03 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 13:21:03 - train.py[line:551] - INFO: load:1.98 valid_run:2258.35 task_valid:2187.73 collect_output:53.67
2023-01-11 13:23:33 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 13:23:33 - train.py[line:551] - INFO: load:2.01 valid_run:2408.52 task_valid:2333.06 collect_output:57.35
2023-01-11 13:26:04 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 13:26:04 - train.py[line:551] - INFO: load:2.05 valid_run:2559.53 task_valid:2479.43 collect_output:60.88
2023-01-11 13:28:35 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 13:28:35 - train.py[line:551] - INFO: load:2.08 valid_run:2710.06 task_valid:2627.13 collect_output:62.62
2023-01-11 13:31:02 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 13:31:02 - train.py[line:551] - INFO: load:2.10 valid_run:2857.94 task_valid:2769.93 collect_output:66.62
2023-01-11 13:33:32 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 13:33:32 - train.py[line:551] - INFO: load:2.13 valid_run:3007.83 task_valid:2915.63 collect_output:69.74
2023-01-11 13:36:04 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 13:36:04 - train.py[line:551] - INFO: load:2.16 valid_run:3159.00 task_valid:3061.41 collect_output:73.97
2023-01-11 13:38:34 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 13:38:34 - train.py[line:551] - INFO: load:2.19 valid_run:3308.77 task_valid:3207.33 collect_output:76.72
2023-01-11 13:41:05 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 13:41:05 - train.py[line:551] - INFO: load:2.21 valid_run:3459.87 task_valid:3354.71 collect_output:79.30
2023-01-11 13:43:36 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 13:43:36 - train.py[line:551] - INFO: load:2.24 valid_run:3611.17 task_valid:3502.01 collect_output:82.18

====================================================================================================
SGG eval:     R @ 50: 0.4567;     R @ 100: 0.5217;     R @ 500: 0.5535;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2735;    mR @ 100: 0.3312;    mR @ 500: 0.3605;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7268) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7812) (playing:0.0000) (riding:0.6127) (says:0.0000) (sitting on:0.7418) (standing on:0.2060) (using:0.6000) (walking in:0.0000) (walking on:0.2973) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

====================================================================================================
SGG eval:     R @ 50: 0.4567;     R @ 100: 0.5217;     R @ 500: 0.5535;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2735;    mR @ 100: 0.3312;    mR @ 500: 0.3605;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7268) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7812) (playing:0.0000) (riding:0.6127) (says:0.0000) (sitting on:0.7418) (standing on:0.2060) (using:0.6000) (walking in:0.0000) (walking on:0.2973) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================


2023-01-11 13:46:08 - train.py[line:487] - INFO: 0.5217148459383755
2023-01-11 13:46:08 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 13:46:09 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.362 | loss_v1 0 | loss_v2 0 | nll_loss 0.206 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.521715 | ppl 1.15 | vqa_score 0.4381 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 31000 | best_R@100 0.69005
2023-01-11 13:46:09 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 31000 updates
2023-01-11 13:46:09 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_31000.pt
2023-01-11 13:46:49 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_31000.pt
2023-01-11 13:48:19 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_31000.pt (epoch 1 @ 31000 updates, score 0.5217148459383755) (writing took 129.9138602744788 seconds)
2023-01-11 13:48:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:48:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:48:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:48:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:48:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:48:36 - progress_bar.py[line:274] - INFO: epoch 001:  31052 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=0.4, ups=0, wpb=109.1, bsz=40, num_updates=31010, lr=3.59323e-05, gnorm=0.197, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=172502
2023-01-11 13:48:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:48:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:48:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:48:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:48:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:48:54 - progress_bar.py[line:274] - INFO: epoch 001:  31062 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3977, wps=100.3, ups=0.6, wpb=111.5, bsz=40, num_updates=31020, lr=3.59271e-05, gnorm=0.337, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=172520
2023-01-11 13:48:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:11 - progress_bar.py[line:274] - INFO: epoch 001:  31072 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4409, wps=101.6, ups=0.61, wpb=111.5, bsz=40, num_updates=31030, lr=3.59219e-05, gnorm=0.307, clip=0, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=172537
2023-01-11 13:49:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:29 - progress_bar.py[line:274] - INFO: epoch 001:  31082 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3846, wps=99.8, ups=0.6, wpb=110.1, bsz=40, num_updates=31040, lr=3.59167e-05, gnorm=0.213, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=172554
2023-01-11 13:49:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:46 - progress_bar.py[line:274] - INFO: epoch 001:  31092 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4444, wps=100.2, ups=0.61, wpb=110.1, bsz=40, num_updates=31050, lr=3.59115e-05, gnorm=0.257, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=172572
2023-01-11 13:49:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:49:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:04 - progress_bar.py[line:274] - INFO: epoch 001:  31102 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4592, wps=97.6, ups=0.59, wpb=110.4, bsz=40, num_updates=31060, lr=3.59063e-05, gnorm=0.27, clip=0, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=172589
2023-01-11 13:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:22 - progress_bar.py[line:274] - INFO: epoch 001:  31112 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3524, wps=98, ups=0.59, wpb=110, bsz=40, num_updates=31070, lr=3.5901e-05, gnorm=0.233, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=172607
2023-01-11 13:50:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:39 - progress_bar.py[line:274] - INFO: epoch 001:  31122 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.475, wps=98.1, ups=0.6, wpb=109.1, bsz=40, num_updates=31080, lr=3.58958e-05, gnorm=0.379, clip=10, loss_scale=512, train_wall=17, gb_free=10.8, ema_decay=0.9999, wall=172625
2023-01-11 13:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:50:57 - progress_bar.py[line:274] - INFO: epoch 001:  31132 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.413, wps=98.3, ups=0.59, wpb=111.7, bsz=40, num_updates=31090, lr=3.58906e-05, gnorm=0.301, clip=0, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=172643
2023-01-11 13:51:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:14 - progress_bar.py[line:274] - INFO: epoch 001:  31142 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4222, wps=99.3, ups=0.6, wpb=110.1, bsz=40, num_updates=31100, lr=3.58854e-05, gnorm=0.299, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=172660
2023-01-11 13:51:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:32 - progress_bar.py[line:274] - INFO: epoch 001:  31152 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4712, wps=95.7, ups=0.58, wpb=109.3, bsz=40, num_updates=31110, lr=3.58802e-05, gnorm=0.463, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=172678
2023-01-11 13:51:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:49 - progress_bar.py[line:274] - INFO: epoch 001:  31162 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4045, wps=103.1, ups=0.62, wpb=110.8, bsz=40, num_updates=31120, lr=3.5875e-05, gnorm=0.224, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=172695
2023-01-11 13:51:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:51:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:07 - progress_bar.py[line:274] - INFO: epoch 001:  31172 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4717, wps=94.9, ups=0.58, wpb=108.5, bsz=40, num_updates=31130, lr=3.58698e-05, gnorm=0.413, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=172713
2023-01-11 13:52:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:25 - progress_bar.py[line:274] - INFO: epoch 001:  31182 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4951, wps=100.4, ups=0.6, wpb=110.7, bsz=40, num_updates=31140, lr=3.58646e-05, gnorm=0.271, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=172730
2023-01-11 13:52:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:43 - progress_bar.py[line:274] - INFO: epoch 001:  31192 / 100000 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4125, wps=98.3, ups=0.59, wpb=110.9, bsz=40, num_updates=31150, lr=3.58594e-05, gnorm=0.442, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=172748
2023-01-11 13:52:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:52:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:01 - progress_bar.py[line:274] - INFO: epoch 001:  31202 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.36, wps=95.7, ups=0.59, wpb=108.6, bsz=40, num_updates=31160, lr=3.58542e-05, gnorm=0.321, clip=0, loss_scale=1024, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=172766
2023-01-11 13:53:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:18 - progress_bar.py[line:274] - INFO: epoch 001:  31212 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4615, wps=97.6, ups=0.59, wpb=109.8, bsz=40, num_updates=31170, lr=3.5849e-05, gnorm=0.243, clip=0, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=172784
2023-01-11 13:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:36 - progress_bar.py[line:274] - INFO: epoch 001:  31222 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4327, wps=98.6, ups=0.6, wpb=110.3, bsz=40, num_updates=31180, lr=3.58438e-05, gnorm=0.376, clip=20, loss_scale=1024, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=172801
2023-01-11 13:53:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:41 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-11 13:53:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:54 - progress_bar.py[line:274] - INFO: epoch 001:  31233 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4257, wps=92.6, ups=0.56, wpb=109.4, bsz=40, num_updates=31190, lr=3.58385e-05, gnorm=0.409, clip=10, loss_scale=512, train_wall=18, gb_free=10.6, ema_decay=0.9999, wall=172820
2023-01-11 13:53:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:53:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:12 - progress_bar.py[line:274] - INFO: epoch 001:  31243 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5155, wps=102.3, ups=0.62, wpb=110.7, bsz=40, num_updates=31200, lr=3.58333e-05, gnorm=0.415, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=172837
2023-01-11 13:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:29 - progress_bar.py[line:274] - INFO: epoch 001:  31253 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4845, wps=101.5, ups=0.62, wpb=109.3, bsz=40, num_updates=31210, lr=3.58281e-05, gnorm=0.4, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=172854
2023-01-11 13:54:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:46 - progress_bar.py[line:274] - INFO: epoch 001:  31263 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3958, wps=101.7, ups=0.62, wpb=110.1, bsz=40, num_updates=31220, lr=3.58229e-05, gnorm=0.395, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=172871
2023-01-11 13:54:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:54:59 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 13:55:05 - progress_bar.py[line:274] - INFO: epoch 001:  31274 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5248, wps=92, ups=0.55, wpb=110.8, bsz=40, num_updates=31230, lr=3.58177e-05, gnorm=0.123, clip=0, loss_scale=256, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=172890
2023-01-11 13:55:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:22 - progress_bar.py[line:274] - INFO: epoch 001:  31284 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4681, wps=97.2, ups=0.59, wpb=109.3, bsz=40, num_updates=31240, lr=3.58125e-05, gnorm=0.469, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=172908
2023-01-11 13:55:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:40 - progress_bar.py[line:274] - INFO: epoch 001:  31294 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3368, wps=98.1, ups=0.59, wpb=110.6, bsz=40, num_updates=31250, lr=3.58073e-05, gnorm=0.287, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=172926
2023-01-11 13:55:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:55:58 - progress_bar.py[line:274] - INFO: epoch 001:  31304 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5321, wps=97.5, ups=0.59, wpb=110.6, bsz=40, num_updates=31260, lr=3.58021e-05, gnorm=0.388, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=172943
2023-01-11 13:55:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:15 - progress_bar.py[line:274] - INFO: epoch 001:  31314 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5155, wps=100.7, ups=0.61, wpb=110.4, bsz=40, num_updates=31270, lr=3.57969e-05, gnorm=0.894, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=172961
2023-01-11 13:56:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:33 - progress_bar.py[line:274] - INFO: epoch 001:  31324 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4466, wps=97.5, ups=0.59, wpb=110.5, bsz=40, num_updates=31280, lr=3.57917e-05, gnorm=0.704, clip=40, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=172978
2023-01-11 13:56:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:45 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-11 13:56:51 - progress_bar.py[line:274] - INFO: epoch 001:  31335 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4688, wps=91.5, ups=0.55, wpb=110.1, bsz=40, num_updates=31290, lr=3.57865e-05, gnorm=0.529, clip=10, loss_scale=128, train_wall=18, gb_free=10.6, ema_decay=0.9999, wall=172997
2023-01-11 13:56:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:56:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:09 - progress_bar.py[line:274] - INFO: epoch 001:  31345 / 100000 loss=0.312, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4444, wps=99.3, ups=0.6, wpb=109.5, bsz=40, num_updates=31300, lr=3.57812e-05, gnorm=1.315, clip=20, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=173015
2023-01-11 13:57:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:27 - progress_bar.py[line:274] - INFO: epoch 001:  31355 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4579, wps=95.7, ups=0.58, wpb=109.1, bsz=40, num_updates=31310, lr=3.5776e-05, gnorm=0.434, clip=10, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=173032
2023-01-11 13:57:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:44 - progress_bar.py[line:274] - INFO: epoch 001:  31365 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.505, wps=100.1, ups=0.61, wpb=109.2, bsz=40, num_updates=31320, lr=3.57708e-05, gnorm=0.379, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=173050
2023-01-11 13:57:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:57:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:01 - progress_bar.py[line:274] - INFO: epoch 001:  31375 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4537, wps=99, ups=0.6, wpb=110.1, bsz=40, num_updates=31330, lr=3.57656e-05, gnorm=0.708, clip=20, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=173067
2023-01-11 13:58:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:20 - progress_bar.py[line:274] - INFO: epoch 001:  31385 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4536, wps=95.8, ups=0.58, wpb=111.1, bsz=40, num_updates=31340, lr=3.57604e-05, gnorm=0.194, clip=0, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=173085
2023-01-11 13:58:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:37 - progress_bar.py[line:274] - INFO: epoch 001:  31395 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4019, wps=100.2, ups=0.61, wpb=110.4, bsz=40, num_updates=31350, lr=3.57552e-05, gnorm=0.216, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=173103
2023-01-11 13:58:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:55 - progress_bar.py[line:274] - INFO: epoch 001:  31405 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.52, wps=98, ups=0.59, wpb=110.7, bsz=40, num_updates=31360, lr=3.575e-05, gnorm=0.251, clip=0, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=173121
2023-01-11 13:58:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:14 - progress_bar.py[line:274] - INFO: epoch 001:  31415 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.505, wps=94.5, ups=0.57, wpb=109.9, bsz=40, num_updates=31370, lr=3.57448e-05, gnorm=0.302, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173139
2023-01-11 13:59:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:32 - progress_bar.py[line:274] - INFO: epoch 001:  31425 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4348, wps=99.9, ups=0.6, wpb=110.7, bsz=40, num_updates=31380, lr=3.57396e-05, gnorm=0.384, clip=10, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=173157
2023-01-11 13:59:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:50 - progress_bar.py[line:274] - INFO: epoch 001:  31435 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5047, wps=95.1, ups=0.58, wpb=109.8, bsz=40, num_updates=31390, lr=3.57344e-05, gnorm=0.183, clip=0, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=173175
2023-01-11 13:59:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 13:59:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:08 - progress_bar.py[line:274] - INFO: epoch 001:  31445 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4952, wps=92.8, ups=0.56, wpb=109.5, bsz=40, num_updates=31400, lr=3.57292e-05, gnorm=0.177, clip=0, loss_scale=128, train_wall=18, gb_free=10.1, ema_decay=0.9999, wall=173194
2023-01-11 14:00:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:26 - progress_bar.py[line:274] - INFO: epoch 001:  31455 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4952, wps=95.1, ups=0.58, wpb=108.7, bsz=40, num_updates=31410, lr=3.5724e-05, gnorm=0.967, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=173212
2023-01-11 14:00:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:44 - progress_bar.py[line:274] - INFO: epoch 001:  31465 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.533, nsentences=40, sample_size=107.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4057, wps=96.4, ups=0.6, wpb=107.5, bsz=40, num_updates=31420, lr=3.57187e-05, gnorm=0.331, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173229
2023-01-11 14:00:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:00:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:01 - progress_bar.py[line:274] - INFO: epoch 001:  31475 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5657, wps=99.2, ups=0.6, wpb=110.4, bsz=40, num_updates=31430, lr=3.57135e-05, gnorm=0.25, clip=0, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=173247
2023-01-11 14:01:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:18 - progress_bar.py[line:274] - INFO: epoch 001:  31485 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4348, wps=101.4, ups=0.61, wpb=110.8, bsz=40, num_updates=31440, lr=3.57083e-05, gnorm=1.133, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=173264
2023-01-11 14:01:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:36 - progress_bar.py[line:274] - INFO: epoch 001:  31495 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5524, wps=97.6, ups=0.59, wpb=109.9, bsz=40, num_updates=31450, lr=3.57031e-05, gnorm=0.508, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173282
2023-01-11 14:01:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:54 - progress_bar.py[line:274] - INFO: epoch 001:  31505 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4057, wps=96.8, ups=0.59, wpb=109.1, bsz=40, num_updates=31460, lr=3.56979e-05, gnorm=0.244, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173300
2023-01-11 14:01:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:01:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:12 - progress_bar.py[line:274] - INFO: epoch 001:  31515 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4018, wps=96.4, ups=0.59, wpb=108.7, bsz=40, num_updates=31470, lr=3.56927e-05, gnorm=0.222, clip=0, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=173318
2023-01-11 14:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:30 - progress_bar.py[line:274] - INFO: epoch 001:  31525 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4444, wps=98.4, ups=0.59, wpb=110.9, bsz=40, num_updates=31480, lr=3.56875e-05, gnorm=0.419, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=173335
2023-01-11 14:02:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:47 - progress_bar.py[line:274] - INFO: epoch 001:  31535 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3868, wps=97.2, ups=0.59, wpb=109.4, bsz=40, num_updates=31490, lr=3.56823e-05, gnorm=0.249, clip=0, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=173353
2023-01-11 14:02:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:02:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:05 - progress_bar.py[line:274] - INFO: epoch 001:  31545 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4946, wps=98.3, ups=0.59, wpb=110.2, bsz=40, num_updates=31500, lr=3.56771e-05, gnorm=0.286, clip=0, loss_scale=128, train_wall=17, gb_free=10, ema_decay=0.9999, wall=173371
2023-01-11 14:03:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:23 - progress_bar.py[line:274] - INFO: epoch 001:  31555 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3939, wps=96.3, ups=0.58, wpb=110.5, bsz=40, num_updates=31510, lr=3.56719e-05, gnorm=0.523, clip=10, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=173389
2023-01-11 14:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:40 - progress_bar.py[line:274] - INFO: epoch 001:  31565 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4286, wps=100, ups=0.61, wpb=109.9, bsz=40, num_updates=31520, lr=3.56667e-05, gnorm=0.322, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=173406
2023-01-11 14:03:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:03:58 - progress_bar.py[line:274] - INFO: epoch 001:  31575 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4457, wps=97.7, ups=0.59, wpb=111, bsz=40, num_updates=31530, lr=3.56615e-05, gnorm=0.563, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173424
2023-01-11 14:03:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:15 - progress_bar.py[line:274] - INFO: epoch 001:  31585 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4037, wps=100.6, ups=0.61, wpb=109.6, bsz=40, num_updates=31540, lr=3.56563e-05, gnorm=0.173, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=173441
2023-01-11 14:04:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:33 - progress_bar.py[line:274] - INFO: epoch 001:  31595 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4839, wps=100, ups=0.61, wpb=110.1, bsz=40, num_updates=31550, lr=3.5651e-05, gnorm=0.511, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=173458
2023-01-11 14:04:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:50 - progress_bar.py[line:274] - INFO: epoch 001:  31605 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=96.9, ups=0.59, wpb=108.9, bsz=40, num_updates=31560, lr=3.56458e-05, gnorm=0.237, clip=0, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=173476
2023-01-11 14:04:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:04:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:07 - progress_bar.py[line:274] - INFO: epoch 001:  31615 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4455, wps=101.6, ups=0.61, wpb=110.9, bsz=40, num_updates=31570, lr=3.56406e-05, gnorm=0.949, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=173493
2023-01-11 14:05:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:25 - progress_bar.py[line:274] - INFO: epoch 001:  31625 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.396, wps=99.9, ups=0.61, wpb=109.7, bsz=40, num_updates=31580, lr=3.56354e-05, gnorm=0.421, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=173510
2023-01-11 14:05:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:43 - progress_bar.py[line:274] - INFO: epoch 001:  31635 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.6145, wps=98.9, ups=0.59, wpb=111.9, bsz=40, num_updates=31590, lr=3.56302e-05, gnorm=0.364, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173528
2023-01-11 14:05:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:05:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:00 - progress_bar.py[line:274] - INFO: epoch 001:  31645 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3945, wps=98.6, ups=0.6, wpb=109.1, bsz=40, num_updates=31600, lr=3.5625e-05, gnorm=0.913, clip=20, loss_scale=128, train_wall=17, gb_free=10, ema_decay=0.9999, wall=173546
2023-01-11 14:06:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:18 - progress_bar.py[line:274] - INFO: epoch 001:  31655 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4146, wps=101.9, ups=0.61, wpb=112, bsz=40, num_updates=31610, lr=3.56198e-05, gnorm=0.354, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=173563
2023-01-11 14:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:36 - progress_bar.py[line:274] - INFO: epoch 001:  31665 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4519, wps=92.6, ups=0.57, wpb=107.9, bsz=40, num_updates=31620, lr=3.56146e-05, gnorm=0.411, clip=10, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=173582
2023-01-11 14:06:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:54 - progress_bar.py[line:274] - INFO: epoch 001:  31675 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4639, wps=97, ups=0.59, wpb=110, bsz=40, num_updates=31630, lr=3.56094e-05, gnorm=0.514, clip=10, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=173599
2023-01-11 14:06:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:06:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:12 - progress_bar.py[line:274] - INFO: epoch 001:  31685 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4778, wps=96.4, ups=0.58, wpb=111.4, bsz=40, num_updates=31640, lr=3.56042e-05, gnorm=0.868, clip=10, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=173617
2023-01-11 14:07:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:29 - progress_bar.py[line:274] - INFO: epoch 001:  31695 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3645, wps=100.9, ups=0.61, wpb=110.6, bsz=40, num_updates=31650, lr=3.5599e-05, gnorm=0.288, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=173635
2023-01-11 14:07:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:47 - progress_bar.py[line:274] - INFO: epoch 001:  31705 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4455, wps=96.6, ups=0.59, wpb=109.9, bsz=40, num_updates=31660, lr=3.55938e-05, gnorm=0.399, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173653
2023-01-11 14:07:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:07:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:04 - progress_bar.py[line:274] - INFO: epoch 001:  31715 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4894, wps=101.3, ups=0.61, wpb=110.9, bsz=40, num_updates=31670, lr=3.55885e-05, gnorm=0.243, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=173670
2023-01-11 14:08:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:22 - progress_bar.py[line:274] - INFO: epoch 001:  31725 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3656, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=31680, lr=3.55833e-05, gnorm=0.867, clip=30, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=173687
2023-01-11 14:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:39 - progress_bar.py[line:274] - INFO: epoch 001:  31735 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5169, wps=99.4, ups=0.6, wpb=110.5, bsz=40, num_updates=31690, lr=3.55781e-05, gnorm=0.263, clip=0, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=173705
2023-01-11 14:08:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:56 - progress_bar.py[line:274] - INFO: epoch 001:  31745 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4948, wps=98.2, ups=0.6, wpb=109.8, bsz=40, num_updates=31700, lr=3.55729e-05, gnorm=1.975, clip=20, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=173722
2023-01-11 14:08:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:08:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:14 - progress_bar.py[line:274] - INFO: epoch 001:  31755 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3918, wps=99.7, ups=0.6, wpb=111.3, bsz=40, num_updates=31710, lr=3.55677e-05, gnorm=0.536, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173740
2023-01-11 14:09:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:31 - progress_bar.py[line:274] - INFO: epoch 001:  31765 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4184, wps=100.8, ups=0.61, wpb=109.9, bsz=40, num_updates=31720, lr=3.55625e-05, gnorm=0.362, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=173757
2023-01-11 14:09:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:49 - progress_bar.py[line:274] - INFO: epoch 001:  31775 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3519, wps=94.7, ups=0.58, wpb=108.9, bsz=40, num_updates=31730, lr=3.55573e-05, gnorm=0.122, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173775
2023-01-11 14:09:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:06 - progress_bar.py[line:274] - INFO: epoch 001:  31785 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5161, wps=102.2, ups=0.62, wpb=110.3, bsz=40, num_updates=31740, lr=3.55521e-05, gnorm=0.389, clip=10, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=173792
2023-01-11 14:10:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:23 - progress_bar.py[line:274] - INFO: epoch 001:  31795 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4643, wps=98, ups=0.6, wpb=108.8, bsz=40, num_updates=31750, lr=3.55469e-05, gnorm=0.347, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173809
2023-01-11 14:10:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:42 - progress_bar.py[line:274] - INFO: epoch 001:  31805 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5408, wps=96.4, ups=0.59, wpb=109.9, bsz=40, num_updates=31760, lr=3.55417e-05, gnorm=1.032, clip=20, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173827
2023-01-11 14:10:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:10:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:00 - progress_bar.py[line:274] - INFO: epoch 001:  31815 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4854, wps=96.5, ups=0.59, wpb=108.7, bsz=40, num_updates=31770, lr=3.55365e-05, gnorm=0.23, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173845
2023-01-11 14:11:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:17 - progress_bar.py[line:274] - INFO: epoch 001:  31825 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4417, wps=95, ups=0.59, wpb=108.3, bsz=40, num_updates=31780, lr=3.55313e-05, gnorm=2.818, clip=30, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173863
2023-01-11 14:11:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:34 - progress_bar.py[line:274] - INFO: epoch 001:  31835 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4299, wps=97.7, ups=0.6, wpb=108.5, bsz=40, num_updates=31790, lr=3.5526e-05, gnorm=0.443, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=173881
2023-01-11 14:11:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:51 - progress_bar.py[line:274] - INFO: epoch 001:  31845 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4257, wps=99.4, ups=0.61, wpb=109.1, bsz=40, num_updates=31800, lr=3.55208e-05, gnorm=0.184, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=173897
2023-01-11 14:11:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:11:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:07 - progress_bar.py[line:274] - INFO: epoch 001:  31855 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4057, wps=105.2, ups=0.64, wpb=109.5, bsz=40, num_updates=31810, lr=3.55156e-05, gnorm=0.299, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=173913
2023-01-11 14:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:24 - progress_bar.py[line:274] - INFO: epoch 001:  31865 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3846, wps=101.5, ups=0.62, wpb=109.9, bsz=40, num_updates=31820, lr=3.55104e-05, gnorm=0.277, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=173930
2023-01-11 14:12:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:40 - progress_bar.py[line:274] - INFO: epoch 001:  31875 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5333, wps=101.4, ups=0.61, wpb=111.1, bsz=40, num_updates=31830, lr=3.55052e-05, gnorm=0.261, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=173946
2023-01-11 14:12:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:57 - progress_bar.py[line:274] - INFO: epoch 001:  31885 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.6042, wps=101, ups=0.61, wpb=110.2, bsz=40, num_updates=31840, lr=3.55e-05, gnorm=0.416, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=173963
2023-01-11 14:12:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:12:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:14 - progress_bar.py[line:274] - INFO: epoch 001:  31895 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4444, wps=99.8, ups=0.61, wpb=109.6, bsz=40, num_updates=31850, lr=3.54948e-05, gnorm=0.395, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=173980
2023-01-11 14:13:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:31 - progress_bar.py[line:274] - INFO: epoch 001:  31905 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4583, wps=99.8, ups=0.61, wpb=109.9, bsz=40, num_updates=31860, lr=3.54896e-05, gnorm=0.486, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=173997
2023-01-11 14:13:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:48 - progress_bar.py[line:274] - INFO: epoch 001:  31915 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3558, wps=99.5, ups=0.6, wpb=109.9, bsz=40, num_updates=31870, lr=3.54844e-05, gnorm=0.344, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=174014
2023-01-11 14:13:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:13:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:04 - progress_bar.py[line:274] - INFO: epoch 001:  31925 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4571, wps=100.7, ups=0.62, wpb=108.7, bsz=40, num_updates=31880, lr=3.54792e-05, gnorm=1.069, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=174031
2023-01-11 14:14:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:21 - progress_bar.py[line:274] - INFO: epoch 001:  31935 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4184, wps=102.1, ups=0.62, wpb=110.5, bsz=40, num_updates=31890, lr=3.5474e-05, gnorm=0.363, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=174047
2023-01-11 14:14:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:37 - progress_bar.py[line:274] - INFO: epoch 001:  31945 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4528, wps=104.5, ups=0.63, wpb=110.7, bsz=40, num_updates=31900, lr=3.54687e-05, gnorm=1.744, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=174063
2023-01-11 14:14:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:54 - progress_bar.py[line:274] - INFO: epoch 001:  31955 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5429, wps=101.8, ups=0.62, wpb=109.7, bsz=40, num_updates=31910, lr=3.54635e-05, gnorm=0.406, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=174080
2023-01-11 14:14:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:14:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:11 - progress_bar.py[line:274] - INFO: epoch 001:  31965 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4019, wps=101.2, ups=0.62, wpb=109.5, bsz=40, num_updates=31920, lr=3.54583e-05, gnorm=0.147, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=174096
2023-01-11 14:15:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:27 - progress_bar.py[line:274] - INFO: epoch 001:  31975 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4598, wps=103.1, ups=0.62, wpb=111.2, bsz=40, num_updates=31930, lr=3.54531e-05, gnorm=0.239, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=174113
2023-01-11 14:15:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:43 - progress_bar.py[line:274] - INFO: epoch 001:  31985 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4747, wps=105.2, ups=0.63, wpb=110.5, bsz=40, num_updates=31940, lr=3.54479e-05, gnorm=0.373, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=174129
2023-01-11 14:15:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:15:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:00 - progress_bar.py[line:274] - INFO: epoch 001:  31995 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3535, wps=100.8, ups=0.61, wpb=109.8, bsz=40, num_updates=31950, lr=3.54427e-05, gnorm=0.283, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=174146
2023-01-11 14:16:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:17 - progress_bar.py[line:274] - INFO: epoch 001:  32005 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4752, wps=99.1, ups=0.61, wpb=109.2, bsz=40, num_updates=31960, lr=3.54375e-05, gnorm=0.329, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=174163
2023-01-11 14:16:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:33 - progress_bar.py[line:274] - INFO: epoch 001:  32015 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4952, wps=101, ups=0.62, wpb=109.1, bsz=40, num_updates=31970, lr=3.54323e-05, gnorm=0.547, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=174179
2023-01-11 14:16:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:50 - progress_bar.py[line:274] - INFO: epoch 001:  32025 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4854, wps=99.9, ups=0.6, wpb=110.1, bsz=40, num_updates=31980, lr=3.54271e-05, gnorm=0.437, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=174196
2023-01-11 14:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:16:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:17:07 - progress_bar.py[line:274] - INFO: epoch 001:  32035 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4653, wps=100.6, ups=0.61, wpb=110.2, bsz=40, num_updates=31990, lr=3.54219e-05, gnorm=0.495, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=174213
2023-01-11 14:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:17:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:17:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:17:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:17:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 14:17:24 - progress_bar.py[line:274] - INFO: epoch 001:  32045 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4839, wps=101.3, ups=0.61, wpb=111.2, bsz=40, num_updates=32000, lr=3.54167e-05, gnorm=0.353, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=174230
2023-01-11 14:17:24 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 14:17:26 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 14:17:26 - train.py[line:551] - INFO: load:1.53 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 14:19:58 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 14:19:58 - train.py[line:551] - INFO: load:1.56 valid_run:152.07 task_valid:149.31 collect_output:1.60
2023-01-11 14:22:26 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 14:22:26 - train.py[line:551] - INFO: load:1.58 valid_run:300.46 task_valid:293.14 collect_output:5.04
2023-01-11 14:24:58 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 14:24:58 - train.py[line:551] - INFO: load:1.61 valid_run:451.92 task_valid:436.80 collect_output:11.76
2023-01-11 14:27:27 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 14:27:27 - train.py[line:551] - INFO: load:1.63 valid_run:600.88 task_valid:582.41 collect_output:14.03
2023-01-11 14:29:59 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 14:29:59 - train.py[line:551] - INFO: load:1.66 valid_run:753.40 task_valid:730.98 collect_output:16.87
2023-01-11 14:32:31 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 14:32:31 - train.py[line:551] - INFO: load:1.69 valid_run:904.92 task_valid:877.13 collect_output:21.16
2023-01-11 14:35:04 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 14:35:04 - train.py[line:551] - INFO: load:1.71 valid_run:1057.64 task_valid:1023.92 collect_output:25.95
2023-01-11 14:37:35 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 14:37:35 - train.py[line:551] - INFO: load:1.74 valid_run:1208.26 task_valid:1166.09 collect_output:33.23
2023-01-11 14:40:04 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 14:40:04 - train.py[line:551] - INFO: load:1.77 valid_run:1358.01 task_valid:1311.90 collect_output:36.06
2023-01-11 14:42:33 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 14:42:33 - train.py[line:551] - INFO: load:1.79 valid_run:1506.45 task_valid:1456.09 collect_output:39.18
2023-01-11 14:45:03 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 14:45:03 - train.py[line:551] - INFO: load:1.82 valid_run:1655.96 task_valid:1601.52 collect_output:42.18
2023-01-11 14:47:32 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 14:47:32 - train.py[line:551] - INFO: load:1.85 valid_run:1805.54 task_valid:1747.10 collect_output:45.10
2023-01-11 14:50:02 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 14:50:02 - train.py[line:551] - INFO: load:1.88 valid_run:1955.25 task_valid:1889.85 collect_output:50.90
2023-01-11 14:52:33 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 14:52:33 - train.py[line:551] - INFO: load:1.90 valid_run:2106.03 task_valid:2036.42 collect_output:53.99
2023-01-11 14:55:04 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 14:55:04 - train.py[line:551] - INFO: load:1.94 valid_run:2256.77 task_valid:2184.30 collect_output:55.69
2023-01-11 14:57:34 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 14:57:34 - train.py[line:551] - INFO: load:1.96 valid_run:2406.84 task_valid:2329.66 collect_output:59.29
2023-01-11 15:00:05 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 15:00:05 - train.py[line:551] - INFO: load:1.99 valid_run:2558.31 task_valid:2476.38 collect_output:62.94
2023-01-11 15:02:36 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 15:02:36 - train.py[line:551] - INFO: load:2.02 valid_run:2709.05 task_valid:2624.30 collect_output:64.66
2023-01-11 15:05:04 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 15:05:04 - train.py[line:551] - INFO: load:2.04 valid_run:2857.06 task_valid:2766.86 collect_output:69.00
2023-01-11 15:07:35 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 15:07:35 - train.py[line:551] - INFO: load:2.07 valid_run:3007.24 task_valid:2913.18 collect_output:71.76
2023-01-11 15:10:06 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 15:10:06 - train.py[line:551] - INFO: load:2.10 valid_run:3158.54 task_valid:3058.98 collect_output:76.14
2023-01-11 15:12:35 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 15:12:35 - train.py[line:551] - INFO: load:2.12 valid_run:3307.69 task_valid:3204.36 collect_output:78.82
2023-01-11 15:15:07 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 15:15:07 - train.py[line:551] - INFO: load:2.15 valid_run:3459.12 task_valid:3351.97 collect_output:81.45
2023-01-11 15:17:38 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 15:17:38 - train.py[line:551] - INFO: load:2.18 valid_run:3610.46 task_valid:3499.50 collect_output:84.11

====================================================================================================
SGG eval:     R @ 50: 0.4494;     R @ 100: 0.5145;     R @ 500: 0.5395;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2751;    mR @ 100: 0.3311;    mR @ 500: 0.3564;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7146) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8229) (playing:0.0000) (riding:0.5882) (says:0.0000) (sitting on:0.7350) (standing on:0.2060) (using:0.6000) (walking in:0.0000) (walking on:0.2973) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4494;     R @ 100: 0.5145;     R @ 500: 0.5395;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2751;    mR @ 100: 0.3311;    mR @ 500: 0.3564;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7146) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8229) (playing:0.0000) (riding:0.5882) (says:0.0000) (sitting on:0.7350) (standing on:0.2060) (using:0.6000) (walking in:0.0000) (walking on:0.2973) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-01-11 15:20:11 - train.py[line:487] - INFO: 0.5145148459383754
2023-01-11 15:20:11 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 15:20:11 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.353 | loss_v1 0 | loss_v2 0 | nll_loss 0.2 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.514515 | ppl 1.15 | vqa_score 0.4302 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 32000 | best_R@100 0.69005
2023-01-11 15:20:11 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 32000 updates
2023-01-11 15:20:11 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_32000.pt
2023-01-11 15:21:01 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_32000.pt
2023-01-11 15:22:37 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_32000.pt (epoch 1 @ 32000 updates, score 0.5145148459383754) (writing took 145.8415815476328 seconds)
2023-01-11 15:22:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:22:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:22:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:22:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:22:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:22:54 - progress_bar.py[line:274] - INFO: epoch 001:  32055 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4632, wps=0.4, ups=0, wpb=110.9, bsz=40, num_updates=32010, lr=3.54115e-05, gnorm=0.193, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=178160
2023-01-11 15:22:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:12 - progress_bar.py[line:274] - INFO: epoch 001:  32065 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5341, wps=98.1, ups=0.6, wpb=109.6, bsz=40, num_updates=32020, lr=3.54063e-05, gnorm=0.323, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=178178
2023-01-11 15:23:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:30 - progress_bar.py[line:274] - INFO: epoch 001:  32075 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4388, wps=97.5, ups=0.59, wpb=109.9, bsz=40, num_updates=32030, lr=3.5401e-05, gnorm=0.648, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=178196
2023-01-11 15:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:48 - progress_bar.py[line:274] - INFO: epoch 001:  32085 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4149, wps=98.1, ups=0.59, wpb=110.2, bsz=40, num_updates=32040, lr=3.53958e-05, gnorm=0.174, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=178213
2023-01-11 15:23:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:23:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:05 - progress_bar.py[line:274] - INFO: epoch 001:  32095 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.534, wps=99.8, ups=0.61, wpb=109.3, bsz=40, num_updates=32050, lr=3.53906e-05, gnorm=1.387, clip=40, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=178231
2023-01-11 15:24:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:23 - progress_bar.py[line:274] - INFO: epoch 001:  32105 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5093, wps=99.9, ups=0.6, wpb=110.5, bsz=40, num_updates=32060, lr=3.53854e-05, gnorm=0.174, clip=0, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=178248
2023-01-11 15:24:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:41 - progress_bar.py[line:274] - INFO: epoch 001:  32115 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4388, wps=95.9, ups=0.59, wpb=109.2, bsz=40, num_updates=32070, lr=3.53802e-05, gnorm=0.237, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=178267
2023-01-11 15:24:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:24:58 - progress_bar.py[line:274] - INFO: epoch 001:  32125 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4222, wps=103, ups=0.62, wpb=111.2, bsz=40, num_updates=32080, lr=3.5375e-05, gnorm=0.987, clip=10, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=178284
2023-01-11 15:24:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:16 - progress_bar.py[line:274] - INFO: epoch 001:  32135 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4712, wps=99.4, ups=0.6, wpb=109.7, bsz=40, num_updates=32090, lr=3.53698e-05, gnorm=0.263, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=178301
2023-01-11 15:25:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:34 - progress_bar.py[line:274] - INFO: epoch 001:  32145 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4235, wps=98.8, ups=0.6, wpb=110.7, bsz=40, num_updates=32100, lr=3.53646e-05, gnorm=0.356, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=178320
2023-01-11 15:25:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:39 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-11 15:25:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:54 - progress_bar.py[line:274] - INFO: epoch 001:  32156 / 100000 loss=0.334, loss_v1=0, loss_v2=0, nll_loss=0.18, ntokens=108.375, nsentences=40, sample_size=108.375, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4, wps=92.5, ups=0.53, wpb=108.4, bsz=40, num_updates=32110, lr=3.53594e-05, gnorm=1.222, clip=30, loss_scale=128, train_wall=19, gb_free=10.2, ema_decay=0.9999, wall=178339
2023-01-11 15:25:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:25:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:12 - progress_bar.py[line:274] - INFO: epoch 001:  32166 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4242, wps=99.3, ups=0.6, wpb=110, bsz=40, num_updates=32120, lr=3.53542e-05, gnorm=0.581, clip=10, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=178358
2023-01-11 15:26:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:30 - progress_bar.py[line:274] - INFO: epoch 001:  32176 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4762, wps=98.5, ups=0.6, wpb=110.3, bsz=40, num_updates=32130, lr=3.5349e-05, gnorm=0.611, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=178375
2023-01-11 15:26:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:48 - progress_bar.py[line:274] - INFO: epoch 001:  32186 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3763, wps=98.3, ups=0.6, wpb=109.7, bsz=40, num_updates=32140, lr=3.53437e-05, gnorm=0.348, clip=0, loss_scale=128, train_wall=17, gb_free=10.7, ema_decay=0.9999, wall=178393
2023-01-11 15:26:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:26:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:05 - progress_bar.py[line:274] - INFO: epoch 001:  32196 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5227, wps=99.4, ups=0.61, wpb=108.3, bsz=40, num_updates=32150, lr=3.53385e-05, gnorm=0.246, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=178411
2023-01-11 15:27:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:23 - progress_bar.py[line:274] - INFO: epoch 001:  32206 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4536, wps=98.3, ups=0.6, wpb=109.3, bsz=40, num_updates=32160, lr=3.53333e-05, gnorm=0.592, clip=20, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=178428
2023-01-11 15:27:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:40 - progress_bar.py[line:274] - INFO: epoch 001:  32216 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=102.4, ups=0.61, wpb=111.2, bsz=40, num_updates=32170, lr=3.53281e-05, gnorm=2.576, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=178445
2023-01-11 15:27:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:27:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:00 - progress_bar.py[line:274] - INFO: epoch 001:  32226 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4019, wps=96.4, ups=0.59, wpb=108.8, bsz=40, num_updates=32180, lr=3.53229e-05, gnorm=0.566, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=178463
2023-01-11 15:28:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:17 - progress_bar.py[line:274] - INFO: epoch 001:  32236 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4556, wps=101.2, ups=0.61, wpb=110.2, bsz=40, num_updates=32190, lr=3.53177e-05, gnorm=0.249, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=178482
2023-01-11 15:28:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:34 - progress_bar.py[line:274] - INFO: epoch 001:  32246 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4537, wps=98.7, ups=0.61, wpb=108.6, bsz=40, num_updates=32200, lr=3.53125e-05, gnorm=0.215, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=178500
2023-01-11 15:28:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:52 - progress_bar.py[line:274] - INFO: epoch 001:  32256 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4688, wps=100.9, ups=0.61, wpb=110.2, bsz=40, num_updates=32210, lr=3.53073e-05, gnorm=0.253, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=178517
2023-01-11 15:28:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:28:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:10 - progress_bar.py[line:274] - INFO: epoch 001:  32266 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4022, wps=97.1, ups=0.59, wpb=109.7, bsz=40, num_updates=32220, lr=3.53021e-05, gnorm=0.284, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=178535
2023-01-11 15:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:28 - progress_bar.py[line:274] - INFO: epoch 001:  32276 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.434, wps=99, ups=0.6, wpb=109.3, bsz=40, num_updates=32230, lr=3.52969e-05, gnorm=0.352, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=178553
2023-01-11 15:29:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:46 - progress_bar.py[line:274] - INFO: epoch 001:  32286 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.469, wps=99.3, ups=0.61, wpb=109.3, bsz=40, num_updates=32240, lr=3.52917e-05, gnorm=1.04, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=178571
2023-01-11 15:29:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:29:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:03 - progress_bar.py[line:274] - INFO: epoch 001:  32296 / 100000 loss=0.317, loss_v1=0, loss_v2=0, nll_loss=0.162, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3093, wps=101.8, ups=0.62, wpb=109.7, bsz=40, num_updates=32250, lr=3.52865e-05, gnorm=0.523, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=178589
2023-01-11 15:30:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:20 - progress_bar.py[line:274] - INFO: epoch 001:  32306 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3708, wps=102.9, ups=0.62, wpb=110.7, bsz=40, num_updates=32260, lr=3.52813e-05, gnorm=0.221, clip=0, loss_scale=128, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=178606
2023-01-11 15:30:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:39 - progress_bar.py[line:274] - INFO: epoch 001:  32316 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.383, wps=99.7, ups=0.6, wpb=110.4, bsz=40, num_updates=32270, lr=3.5276e-05, gnorm=0.352, clip=0, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=178623
2023-01-11 15:30:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:56 - progress_bar.py[line:274] - INFO: epoch 001:  32326 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4848, wps=98, ups=0.6, wpb=109.8, bsz=40, num_updates=32280, lr=3.52708e-05, gnorm=0.354, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=178642
2023-01-11 15:30:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:30:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:14 - progress_bar.py[line:274] - INFO: epoch 001:  32336 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4752, wps=99.1, ups=0.61, wpb=108.5, bsz=40, num_updates=32290, lr=3.52656e-05, gnorm=0.317, clip=10, loss_scale=128, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=178659
2023-01-11 15:31:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:31 - progress_bar.py[line:274] - INFO: epoch 001:  32346 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.537, wps=100.8, ups=0.61, wpb=110, bsz=40, num_updates=32300, lr=3.52604e-05, gnorm=0.175, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=178677
2023-01-11 15:31:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:49 - progress_bar.py[line:274] - INFO: epoch 001:  32356 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4231, wps=97.6, ups=0.59, wpb=109.5, bsz=40, num_updates=32310, lr=3.52552e-05, gnorm=3.372, clip=20, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=178694
2023-01-11 15:31:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:31:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:06 - progress_bar.py[line:274] - INFO: epoch 001:  32366 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4135, wps=102.4, ups=0.62, wpb=109.7, bsz=40, num_updates=32320, lr=3.525e-05, gnorm=0.259, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=178712
2023-01-11 15:32:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:23 - progress_bar.py[line:274] - INFO: epoch 001:  32376 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5161, wps=103, ups=0.62, wpb=110.9, bsz=40, num_updates=32330, lr=3.52448e-05, gnorm=0.225, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=178729
2023-01-11 15:32:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:41 - progress_bar.py[line:274] - INFO: epoch 001:  32386 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.398, wps=98, ups=0.59, wpb=110.9, bsz=40, num_updates=32340, lr=3.52396e-05, gnorm=0.892, clip=20, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=178747
2023-01-11 15:32:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:32:59 - progress_bar.py[line:274] - INFO: epoch 001:  32396 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4848, wps=98.3, ups=0.6, wpb=108.5, bsz=40, num_updates=32350, lr=3.52344e-05, gnorm=0.187, clip=0, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=178764
2023-01-11 15:32:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:16 - progress_bar.py[line:274] - INFO: epoch 001:  32406 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4468, wps=100.4, ups=0.61, wpb=109.5, bsz=40, num_updates=32360, lr=3.52292e-05, gnorm=1.541, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=178782
2023-01-11 15:33:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:34 - progress_bar.py[line:274] - INFO: epoch 001:  32416 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.155, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3714, wps=97.4, ups=0.59, wpb=109.3, bsz=40, num_updates=32370, lr=3.5224e-05, gnorm=2.253, clip=30, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=178800
2023-01-11 15:33:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:52 - progress_bar.py[line:274] - INFO: epoch 001:  32426 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4057, wps=97.7, ups=0.6, wpb=108.7, bsz=40, num_updates=32380, lr=3.52187e-05, gnorm=0.383, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=178817
2023-01-11 15:33:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:33:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:10 - progress_bar.py[line:274] - INFO: epoch 001:  32436 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4157, wps=99.5, ups=0.6, wpb=111.4, bsz=40, num_updates=32390, lr=3.52135e-05, gnorm=0.79, clip=20, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=178835
2023-01-11 15:34:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:27 - progress_bar.py[line:274] - INFO: epoch 001:  32446 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4949, wps=100.7, ups=0.61, wpb=109.6, bsz=40, num_updates=32400, lr=3.52083e-05, gnorm=0.341, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=178852
2023-01-11 15:34:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:44 - progress_bar.py[line:274] - INFO: epoch 001:  32456 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.39, wps=99.2, ups=0.6, wpb=109.9, bsz=40, num_updates=32410, lr=3.52031e-05, gnorm=0.523, clip=20, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=178870
2023-01-11 15:34:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:34:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:02 - progress_bar.py[line:274] - INFO: epoch 001:  32466 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4486, wps=100.9, ups=0.62, wpb=109.2, bsz=40, num_updates=32420, lr=3.51979e-05, gnorm=0.555, clip=20, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=178887
2023-01-11 15:35:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:19 - progress_bar.py[line:274] - INFO: epoch 001:  32476 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4954, wps=99, ups=0.61, wpb=108.4, bsz=40, num_updates=32430, lr=3.51927e-05, gnorm=0.311, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=178905
2023-01-11 15:35:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:37 - progress_bar.py[line:274] - INFO: epoch 001:  32486 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.567, wps=99, ups=0.61, wpb=108.8, bsz=40, num_updates=32440, lr=3.51875e-05, gnorm=0.325, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=178922
2023-01-11 15:35:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:54 - progress_bar.py[line:274] - INFO: epoch 001:  32496 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4111, wps=101.6, ups=0.61, wpb=110.3, bsz=40, num_updates=32450, lr=3.51823e-05, gnorm=0.346, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=178939
2023-01-11 15:35:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:35:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:10 - progress_bar.py[line:274] - INFO: epoch 001:  32506 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=107.667, nsentences=40, sample_size=107.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5345, wps=103.7, ups=0.64, wpb=107.7, bsz=40, num_updates=32460, lr=3.51771e-05, gnorm=0.26, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=178956
2023-01-11 15:36:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:27 - progress_bar.py[line:274] - INFO: epoch 001:  32516 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4706, wps=102.9, ups=0.62, wpb=111.1, bsz=40, num_updates=32470, lr=3.51719e-05, gnorm=0.3, clip=0, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=178973
2023-01-11 15:36:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:45 - progress_bar.py[line:274] - INFO: epoch 001:  32526 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4949, wps=100.2, ups=0.61, wpb=109.5, bsz=40, num_updates=32480, lr=3.51667e-05, gnorm=0.426, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=178990
2023-01-11 15:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:36:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:03 - progress_bar.py[line:274] - INFO: epoch 001:  32536 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3511, wps=99.2, ups=0.6, wpb=109.7, bsz=40, num_updates=32490, lr=3.51615e-05, gnorm=0.56, clip=20, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=179008
2023-01-11 15:37:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:20 - progress_bar.py[line:274] - INFO: epoch 001:  32546 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4455, wps=101.6, ups=0.61, wpb=110.3, bsz=40, num_updates=32500, lr=3.51563e-05, gnorm=0.264, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=179025
2023-01-11 15:37:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:38 - progress_bar.py[line:274] - INFO: epoch 001:  32556 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5644, wps=98.3, ups=0.6, wpb=109.9, bsz=40, num_updates=32510, lr=3.5151e-05, gnorm=0.363, clip=10, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=179043
2023-01-11 15:37:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:55 - progress_bar.py[line:274] - INFO: epoch 001:  32566 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4757, wps=102.2, ups=0.62, wpb=109.4, bsz=40, num_updates=32520, lr=3.51458e-05, gnorm=0.341, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=179060
2023-01-11 15:37:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:37:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:12 - progress_bar.py[line:274] - INFO: epoch 001:  32576 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4327, wps=98.4, ups=0.6, wpb=109, bsz=40, num_updates=32530, lr=3.51406e-05, gnorm=0.409, clip=10, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=179078
2023-01-11 15:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:30 - progress_bar.py[line:274] - INFO: epoch 001:  32586 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4554, wps=98.7, ups=0.6, wpb=109.9, bsz=40, num_updates=32540, lr=3.51354e-05, gnorm=0.248, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=179096
2023-01-11 15:38:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:48 - progress_bar.py[line:274] - INFO: epoch 001:  32596 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4412, wps=99.1, ups=0.61, wpb=108.9, bsz=40, num_updates=32550, lr=3.51302e-05, gnorm=0.254, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=179113
2023-01-11 15:38:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:38:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:05 - progress_bar.py[line:274] - INFO: epoch 001:  32606 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4505, wps=103.4, ups=0.64, wpb=108.5, bsz=40, num_updates=32560, lr=3.5125e-05, gnorm=0.454, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=179131
2023-01-11 15:39:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:23 - progress_bar.py[line:274] - INFO: epoch 001:  32616 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4845, wps=98.4, ups=0.6, wpb=109.6, bsz=40, num_updates=32570, lr=3.51198e-05, gnorm=0.446, clip=20, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=179148
2023-01-11 15:39:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:41 - progress_bar.py[line:274] - INFO: epoch 001:  32626 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4021, wps=100.5, ups=0.6, wpb=111, bsz=40, num_updates=32580, lr=3.51146e-05, gnorm=0.258, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=179166
2023-01-11 15:39:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:39:59 - progress_bar.py[line:274] - INFO: epoch 001:  32636 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4476, wps=96.9, ups=0.59, wpb=109.4, bsz=40, num_updates=32590, lr=3.51094e-05, gnorm=0.176, clip=0, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=179184
2023-01-11 15:39:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:17 - progress_bar.py[line:274] - INFO: epoch 001:  32646 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4592, wps=96.8, ups=0.59, wpb=109.9, bsz=40, num_updates=32600, lr=3.51042e-05, gnorm=0.25, clip=0, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=179202
2023-01-11 15:40:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:35 - progress_bar.py[line:274] - INFO: epoch 001:  32656 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.404, wps=97.6, ups=0.6, wpb=109.3, bsz=40, num_updates=32610, lr=3.5099e-05, gnorm=0.349, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=179220
2023-01-11 15:40:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:53 - progress_bar.py[line:274] - INFO: epoch 001:  32666 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5143, wps=95.6, ups=0.59, wpb=108.1, bsz=40, num_updates=32620, lr=3.50938e-05, gnorm=0.224, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=179238
2023-01-11 15:40:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:40:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:11 - progress_bar.py[line:274] - INFO: epoch 001:  32676 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=98.3, ups=0.6, wpb=109.6, bsz=40, num_updates=32630, lr=3.50885e-05, gnorm=0.296, clip=0, loss_scale=256, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=179256
2023-01-11 15:41:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:28 - progress_bar.py[line:274] - INFO: epoch 001:  32686 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4393, wps=100.4, ups=0.61, wpb=110, bsz=40, num_updates=32640, lr=3.50833e-05, gnorm=0.528, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=179274
2023-01-11 15:41:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:46 - progress_bar.py[line:274] - INFO: epoch 001:  32696 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.47, wps=99.6, ups=0.6, wpb=110.3, bsz=40, num_updates=32650, lr=3.50781e-05, gnorm=0.339, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=179291
2023-01-11 15:41:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:41:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:03 - progress_bar.py[line:274] - INFO: epoch 001:  32706 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4787, wps=103.2, ups=0.62, wpb=110.7, bsz=40, num_updates=32660, lr=3.50729e-05, gnorm=0.229, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=179309
2023-01-11 15:42:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:21 - progress_bar.py[line:274] - INFO: epoch 001:  32716 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3838, wps=98.9, ups=0.61, wpb=108.9, bsz=40, num_updates=32670, lr=3.50677e-05, gnorm=0.326, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=179326
2023-01-11 15:42:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:38 - progress_bar.py[line:274] - INFO: epoch 001:  32726 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4602, wps=98, ups=0.6, wpb=108.7, bsz=40, num_updates=32680, lr=3.50625e-05, gnorm=0.176, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=179344
2023-01-11 15:42:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:56 - progress_bar.py[line:274] - INFO: epoch 001:  32736 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=100.3, ups=0.6, wpb=111.4, bsz=40, num_updates=32690, lr=3.50573e-05, gnorm=0.317, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=179361
2023-01-11 15:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:42:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:14 - progress_bar.py[line:274] - INFO: epoch 001:  32746 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3763, wps=98.7, ups=0.6, wpb=109.2, bsz=40, num_updates=32700, lr=3.50521e-05, gnorm=0.262, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=179379
2023-01-11 15:43:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:31 - progress_bar.py[line:274] - INFO: epoch 001:  32756 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4725, wps=102.9, ups=0.62, wpb=110.7, bsz=40, num_updates=32710, lr=3.50469e-05, gnorm=0.245, clip=0, loss_scale=256, train_wall=16, gb_free=10.9, ema_decay=0.9999, wall=179396
2023-01-11 15:43:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:48 - progress_bar.py[line:274] - INFO: epoch 001:  32766 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.46, wps=102.9, ups=0.62, wpb=110.7, bsz=40, num_updates=32720, lr=3.50417e-05, gnorm=0.305, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=179413
2023-01-11 15:43:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:43:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:05 - progress_bar.py[line:274] - INFO: epoch 001:  32776 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5254, wps=101.4, ups=0.61, wpb=110.5, bsz=40, num_updates=32730, lr=3.50365e-05, gnorm=0.251, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=179431
2023-01-11 15:44:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:23 - progress_bar.py[line:274] - INFO: epoch 001:  32786 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4412, wps=99.2, ups=0.61, wpb=109.2, bsz=40, num_updates=32740, lr=3.50313e-05, gnorm=2.587, clip=30, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=179448
2023-01-11 15:44:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:41 - progress_bar.py[line:274] - INFO: epoch 001:  32796 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4095, wps=98.8, ups=0.6, wpb=109.9, bsz=40, num_updates=32750, lr=3.5026e-05, gnorm=0.181, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=179466
2023-01-11 15:44:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:44:59 - progress_bar.py[line:274] - INFO: epoch 001:  32806 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4953, wps=97.6, ups=0.6, wpb=108.9, bsz=40, num_updates=32760, lr=3.50208e-05, gnorm=0.532, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=179484
2023-01-11 15:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:16 - progress_bar.py[line:274] - INFO: epoch 001:  32816 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4576, wps=98.7, ups=0.61, wpb=108.5, bsz=40, num_updates=32770, lr=3.50156e-05, gnorm=0.182, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=179502
2023-01-11 15:45:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:34 - progress_bar.py[line:274] - INFO: epoch 001:  32826 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3636, wps=98.9, ups=0.6, wpb=109.9, bsz=40, num_updates=32780, lr=3.50104e-05, gnorm=0.35, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=179519
2023-01-11 15:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:51 - progress_bar.py[line:274] - INFO: epoch 001:  32836 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4141, wps=100.1, ups=0.61, wpb=109.1, bsz=40, num_updates=32790, lr=3.50052e-05, gnorm=1.913, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=179537
2023-01-11 15:45:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:45:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:09 - progress_bar.py[line:274] - INFO: epoch 001:  32846 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4727, wps=98.1, ups=0.6, wpb=109.5, bsz=40, num_updates=32800, lr=3.5e-05, gnorm=0.569, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=179555
2023-01-11 15:46:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:27 - progress_bar.py[line:274] - INFO: epoch 001:  32856 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=100, ups=0.61, wpb=110, bsz=40, num_updates=32810, lr=3.49948e-05, gnorm=0.255, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=179572
2023-01-11 15:46:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:45 - progress_bar.py[line:274] - INFO: epoch 001:  32866 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=112.267, nsentences=40, sample_size=112.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4706, wps=98.4, ups=0.58, wpb=112.3, bsz=40, num_updates=32820, lr=3.49896e-05, gnorm=0.357, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=179590
2023-01-11 15:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:46:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:03 - progress_bar.py[line:274] - INFO: epoch 001:  32876 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3656, wps=100.1, ups=0.6, wpb=111.7, bsz=40, num_updates=32830, lr=3.49844e-05, gnorm=0.4, clip=10, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=179608
2023-01-11 15:47:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:20 - progress_bar.py[line:274] - INFO: epoch 001:  32886 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4902, wps=103.7, ups=0.63, wpb=110.3, bsz=40, num_updates=32840, lr=3.49792e-05, gnorm=0.17, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=179625
2023-01-11 15:47:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:38 - progress_bar.py[line:274] - INFO: epoch 001:  32896 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3838, wps=98.2, ups=0.59, wpb=110.3, bsz=40, num_updates=32850, lr=3.4974e-05, gnorm=0.274, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=179643
2023-01-11 15:47:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:55 - progress_bar.py[line:274] - INFO: epoch 001:  32906 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4651, wps=102.3, ups=0.62, wpb=110.9, bsz=40, num_updates=32860, lr=3.49688e-05, gnorm=0.597, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=179660
2023-01-11 15:47:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:47:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:13 - progress_bar.py[line:274] - INFO: epoch 001:  32916 / 100000 loss=0.316, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4158, wps=98.4, ups=0.6, wpb=109.1, bsz=40, num_updates=32870, lr=3.49635e-05, gnorm=1.991, clip=30, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=179678
2023-01-11 15:48:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:29 - progress_bar.py[line:274] - INFO: epoch 001:  32926 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4375, wps=101.7, ups=0.62, wpb=110.1, bsz=40, num_updates=32880, lr=3.49583e-05, gnorm=0.352, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=179695
2023-01-11 15:48:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:46 - progress_bar.py[line:274] - INFO: epoch 001:  32936 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4419, wps=99.5, ups=0.6, wpb=109.8, bsz=40, num_updates=32890, lr=3.49531e-05, gnorm=0.259, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=179712
2023-01-11 15:48:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:48:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:03 - progress_bar.py[line:274] - INFO: epoch 001:  32946 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4052, wps=101.3, ups=0.62, wpb=109.3, bsz=40, num_updates=32900, lr=3.49479e-05, gnorm=0.348, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=179729
2023-01-11 15:49:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:19 - progress_bar.py[line:274] - INFO: epoch 001:  32956 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3918, wps=100.8, ups=0.61, wpb=109.7, bsz=40, num_updates=32910, lr=3.49427e-05, gnorm=0.401, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=179745
2023-01-11 15:49:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:36 - progress_bar.py[line:274] - INFO: epoch 001:  32966 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4455, wps=103.7, ups=0.63, wpb=109.5, bsz=40, num_updates=32920, lr=3.49375e-05, gnorm=0.417, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=179762
2023-01-11 15:49:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:52 - progress_bar.py[line:274] - INFO: epoch 001:  32976 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.495, wps=101.4, ups=0.61, wpb=110.1, bsz=40, num_updates=32930, lr=3.49323e-05, gnorm=0.335, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=179778
2023-01-11 15:49:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:49:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:09 - progress_bar.py[line:274] - INFO: epoch 001:  32986 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.434, wps=98.7, ups=0.6, wpb=109.5, bsz=40, num_updates=32940, lr=3.49271e-05, gnorm=0.328, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=179795
2023-01-11 15:50:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:26 - progress_bar.py[line:274] - INFO: epoch 001:  32996 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3878, wps=99.6, ups=0.6, wpb=110.3, bsz=40, num_updates=32950, lr=3.49219e-05, gnorm=0.244, clip=0, loss_scale=256, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=179812
2023-01-11 15:50:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:43 - progress_bar.py[line:274] - INFO: epoch 001:  33006 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4235, wps=101.8, ups=0.61, wpb=112, bsz=40, num_updates=32960, lr=3.49167e-05, gnorm=0.32, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=179829
2023-01-11 15:50:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:50:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:00 - progress_bar.py[line:274] - INFO: epoch 001:  33016 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4526, wps=101.1, ups=0.62, wpb=109, bsz=40, num_updates=32970, lr=3.49115e-05, gnorm=0.472, clip=10, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=179846
2023-01-11 15:51:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:16 - progress_bar.py[line:274] - INFO: epoch 001:  33026 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.475, wps=105.3, ups=0.63, wpb=110.7, bsz=40, num_updates=32980, lr=3.49063e-05, gnorm=0.568, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=179862
2023-01-11 15:51:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:32 - progress_bar.py[line:274] - INFO: epoch 001:  33036 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.38, wps=102.7, ups=0.61, wpb=111.3, bsz=40, num_updates=32990, lr=3.4901e-05, gnorm=0.7, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=179878
2023-01-11 15:51:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 15:51:49 - progress_bar.py[line:274] - INFO: epoch 001:  33046 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5474, wps=102.5, ups=0.62, wpb=110.3, bsz=40, num_updates=33000, lr=3.48958e-05, gnorm=0.972, clip=20, loss_scale=256, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=179895
2023-01-11 15:51:49 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 15:51:51 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 15:51:51 - train.py[line:551] - INFO: load:1.32 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 15:54:23 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 15:54:23 - train.py[line:551] - INFO: load:1.34 valid_run:151.82 task_valid:149.22 collect_output:1.44
2023-01-11 15:56:51 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 15:56:51 - train.py[line:551] - INFO: load:1.37 valid_run:300.40 task_valid:293.73 collect_output:4.36
2023-01-11 15:59:23 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 15:59:23 - train.py[line:551] - INFO: load:1.39 valid_run:451.97 task_valid:437.76 collect_output:10.82
2023-01-11 16:01:53 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 16:01:53 - train.py[line:551] - INFO: load:1.42 valid_run:601.33 task_valid:583.64 collect_output:13.19
2023-01-11 16:04:25 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 16:04:25 - train.py[line:551] - INFO: load:1.44 valid_run:753.60 task_valid:731.95 collect_output:16.01
2023-01-11 16:06:57 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 16:06:57 - train.py[line:551] - INFO: load:1.47 valid_run:905.33 task_valid:878.66 collect_output:19.86
2023-01-11 16:09:29 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 16:09:29 - train.py[line:551] - INFO: load:1.49 valid_run:1057.68 task_valid:1026.10 collect_output:23.61
2023-01-11 16:12:00 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 16:12:00 - train.py[line:551] - INFO: load:1.52 valid_run:1208.18 task_valid:1168.34 collect_output:30.79
2023-01-11 16:14:29 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 16:14:29 - train.py[line:551] - INFO: load:1.54 valid_run:1357.46 task_valid:1313.80 collect_output:33.53
2023-01-11 16:16:58 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 16:16:58 - train.py[line:551] - INFO: load:1.57 valid_run:1506.23 task_valid:1458.32 collect_output:36.64
2023-01-11 16:19:28 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 16:19:28 - train.py[line:551] - INFO: load:1.60 valid_run:1656.27 task_valid:1604.35 collect_output:39.52
2023-01-11 16:21:58 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 16:21:58 - train.py[line:551] - INFO: load:1.62 valid_run:1806.20 task_valid:1750.08 collect_output:42.63
2023-01-11 16:24:28 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 16:24:28 - train.py[line:551] - INFO: load:1.65 valid_run:1955.46 task_valid:1892.65 collect_output:48.26
2023-01-11 16:26:59 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 16:26:59 - train.py[line:551] - INFO: load:1.67 valid_run:2106.36 task_valid:2039.54 collect_output:51.18
2023-01-11 16:29:29 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 16:29:29 - train.py[line:551] - INFO: load:1.70 valid_run:2256.60 task_valid:2186.69 collect_output:53.21
2023-01-11 16:31:59 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 16:31:59 - train.py[line:551] - INFO: load:1.73 valid_run:2406.87 task_valid:2332.05 collect_output:56.98
2023-01-11 16:34:31 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 16:34:31 - train.py[line:551] - INFO: load:1.75 valid_run:2558.39 task_valid:2478.99 collect_output:60.47
2023-01-11 16:37:02 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 16:37:02 - train.py[line:551] - INFO: load:1.78 valid_run:2709.02 task_valid:2626.76 collect_output:62.26
2023-01-11 16:39:30 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 16:39:30 - train.py[line:551] - INFO: load:1.80 valid_run:2857.22 task_valid:2769.52 collect_output:66.59
2023-01-11 16:42:01 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 16:42:01 - train.py[line:551] - INFO: load:1.83 valid_run:3008.16 task_valid:2916.44 collect_output:69.51
2023-01-11 16:44:33 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 16:44:33 - train.py[line:551] - INFO: load:1.86 valid_run:3159.55 task_valid:3062.41 collect_output:73.80
2023-01-11 16:47:02 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 16:47:02 - train.py[line:551] - INFO: load:1.88 valid_run:3308.72 task_valid:3207.82 collect_output:76.48
2023-01-11 16:49:33 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 16:49:33 - train.py[line:551] - INFO: load:1.91 valid_run:3459.54 task_valid:3354.86 collect_output:79.15
2023-01-11 16:52:06 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 16:52:06 - train.py[line:551] - INFO: load:1.94 valid_run:3612.18 task_valid:3503.42 collect_output:82.08

====================================================================================================
SGG eval:     R @ 50: 0.4428;     R @ 100: 0.5119;     R @ 500: 0.5390;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2724;    mR @ 100: 0.3311;    mR @ 500: 0.3572;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7146) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.5735) (says:0.0000) (sitting on:0.7290) (standing on:0.2060) (using:0.6000) (walking in:0.0000) (walking on:0.2973) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4428;     R @ 100: 0.5119;     R @ 500: 0.5390;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2724;    mR @ 100: 0.3311;    mR @ 500: 0.3572;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.7146) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3548) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.5735) (says:0.0000) (sitting on:0.7290) (standing on:0.2060) (using:0.6000) (walking in:0.0000) (walking on:0.2973) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-01-11 16:54:37 - train.py[line:487] - INFO: 0.5119148459383753
2023-01-11 16:54:37 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 16:54:37 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.373 | loss_v1 0 | loss_v2 0 | nll_loss 0.218 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.511915 | ppl 1.16 | vqa_score 0.4223 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 33000 | best_R@100 0.69005
2023-01-11 16:54:37 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 33000 updates
2023-01-11 16:54:37 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_33000.pt
2023-01-11 16:55:15 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_33000.pt
2023-01-11 16:56:40 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_33000.pt (epoch 1 @ 33000 updates, score 0.5119148459383753) (writing took 122.83961611427367 seconds)
2023-01-11 16:56:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:56:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:56:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:56:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:56:57 - progress_bar.py[line:274] - INFO: epoch 001:  33056 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4787, wps=0.4, ups=0, wpb=110.4, bsz=40, num_updates=33010, lr=3.48906e-05, gnorm=0.222, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=183803
2023-01-11 16:56:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:56:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:14 - progress_bar.py[line:274] - INFO: epoch 001:  33066 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4175, wps=100.2, ups=0.61, wpb=109.5, bsz=40, num_updates=33020, lr=3.48854e-05, gnorm=0.879, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=183820
2023-01-11 16:57:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:31 - progress_bar.py[line:274] - INFO: epoch 001:  33076 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4818, wps=100.3, ups=0.61, wpb=109.9, bsz=40, num_updates=33030, lr=3.48802e-05, gnorm=0.364, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=183837
2023-01-11 16:57:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:47 - progress_bar.py[line:274] - INFO: epoch 001:  33086 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5196, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=33040, lr=3.4875e-05, gnorm=0.366, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=183853
2023-01-11 16:57:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:57:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:04 - progress_bar.py[line:274] - INFO: epoch 001:  33096 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4038, wps=98.6, ups=0.6, wpb=109.5, bsz=40, num_updates=33050, lr=3.48698e-05, gnorm=0.254, clip=0, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=183870
2023-01-11 16:58:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:21 - progress_bar.py[line:274] - INFO: epoch 001:  33106 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4184, wps=102, ups=0.61, wpb=110.9, bsz=40, num_updates=33060, lr=3.48646e-05, gnorm=0.356, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=183887
2023-01-11 16:58:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:36 - progress_bar.py[line:274] - INFO: epoch 001:  33116 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.41, wps=106.4, ups=0.64, wpb=110.4, bsz=40, num_updates=33070, lr=3.48594e-05, gnorm=0.293, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=183903
2023-01-11 16:58:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:53 - progress_bar.py[line:274] - INFO: epoch 001:  33126 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3846, wps=100.1, ups=0.6, wpb=110.3, bsz=40, num_updates=33080, lr=3.48542e-05, gnorm=0.259, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=183919
2023-01-11 16:58:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:58:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:10 - progress_bar.py[line:274] - INFO: epoch 001:  33136 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3617, wps=99.7, ups=0.6, wpb=110.4, bsz=40, num_updates=33090, lr=3.4849e-05, gnorm=0.445, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=183936
2023-01-11 16:59:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:26 - progress_bar.py[line:274] - INFO: epoch 001:  33146 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4911, wps=100.9, ups=0.62, wpb=109.1, bsz=40, num_updates=33100, lr=3.48438e-05, gnorm=0.257, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=183953
2023-01-11 16:59:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:31 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-11 16:59:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:45 - progress_bar.py[line:274] - INFO: epoch 001:  33157 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.062, nsentences=40, sample_size=111.062, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4505, wps=97.2, ups=0.55, wpb=111.1, bsz=40, num_updates=33110, lr=3.48385e-05, gnorm=0.273, clip=10, loss_scale=128, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=183971
2023-01-11 16:59:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 16:59:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:02 - progress_bar.py[line:274] - INFO: epoch 001:  33167 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5347, wps=100.9, ups=0.61, wpb=110.8, bsz=40, num_updates=33120, lr=3.48333e-05, gnorm=0.281, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=183988
2023-01-11 17:00:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:18 - progress_bar.py[line:274] - INFO: epoch 001:  33177 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4457, wps=105.1, ups=0.63, wpb=110.5, bsz=40, num_updates=33130, lr=3.48281e-05, gnorm=1.311, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184004
2023-01-11 17:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:34 - progress_bar.py[line:274] - INFO: epoch 001:  33187 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5347, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=33140, lr=3.48229e-05, gnorm=0.356, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184020
2023-01-11 17:00:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:51 - progress_bar.py[line:274] - INFO: epoch 001:  33197 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5408, wps=101.1, ups=0.62, wpb=109.5, bsz=40, num_updates=33150, lr=3.48177e-05, gnorm=0.316, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184037
2023-01-11 17:00:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:00:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:08 - progress_bar.py[line:274] - INFO: epoch 001:  33207 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3922, wps=100.7, ups=0.61, wpb=110.6, bsz=40, num_updates=33160, lr=3.48125e-05, gnorm=0.536, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184054
2023-01-11 17:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:24 - progress_bar.py[line:274] - INFO: epoch 001:  33217 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.383, wps=100.3, ups=0.61, wpb=109.4, bsz=40, num_updates=33170, lr=3.48073e-05, gnorm=0.272, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=184071
2023-01-11 17:01:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:41 - progress_bar.py[line:274] - INFO: epoch 001:  33227 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3981, wps=101.1, ups=0.62, wpb=108.8, bsz=40, num_updates=33180, lr=3.48021e-05, gnorm=1.536, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184087
2023-01-11 17:01:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:01:57 - progress_bar.py[line:274] - INFO: epoch 001:  33237 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.43, wps=100.2, ups=0.61, wpb=110.1, bsz=40, num_updates=33190, lr=3.47969e-05, gnorm=0.262, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=184104
2023-01-11 17:01:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:14 - progress_bar.py[line:274] - INFO: epoch 001:  33247 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5152, wps=102.7, ups=0.62, wpb=110.3, bsz=40, num_updates=33200, lr=3.47917e-05, gnorm=0.245, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=184120
2023-01-11 17:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:31 - progress_bar.py[line:274] - INFO: epoch 001:  33257 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4158, wps=98.6, ups=0.61, wpb=108.2, bsz=40, num_updates=33210, lr=3.47865e-05, gnorm=0.157, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=184137
2023-01-11 17:02:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:47 - progress_bar.py[line:274] - INFO: epoch 001:  33267 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4951, wps=100.9, ups=0.61, wpb=110.7, bsz=40, num_updates=33220, lr=3.47813e-05, gnorm=0.265, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184153
2023-01-11 17:02:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:04 - progress_bar.py[line:274] - INFO: epoch 001:  33277 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4583, wps=101.9, ups=0.61, wpb=110.5, bsz=40, num_updates=33230, lr=3.4776e-05, gnorm=0.17, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184170
2023-01-11 17:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:21 - progress_bar.py[line:274] - INFO: epoch 001:  33287 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5567, wps=96.8, ups=0.59, wpb=108.7, bsz=40, num_updates=33240, lr=3.47708e-05, gnorm=0.282, clip=0, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=184187
2023-01-11 17:03:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:38 - progress_bar.py[line:274] - INFO: epoch 001:  33297 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4857, wps=101, ups=0.62, wpb=109.4, bsz=40, num_updates=33250, lr=3.47656e-05, gnorm=0.32, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184204
2023-01-11 17:03:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:54 - progress_bar.py[line:274] - INFO: epoch 001:  33307 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4479, wps=102.4, ups=0.62, wpb=109.7, bsz=40, num_updates=33260, lr=3.47604e-05, gnorm=0.322, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=184220
2023-01-11 17:03:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:03:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:10 - progress_bar.py[line:274] - INFO: epoch 001:  33317 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4382, wps=103.1, ups=0.62, wpb=110.9, bsz=40, num_updates=33270, lr=3.47552e-05, gnorm=0.375, clip=10, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=184236
2023-01-11 17:04:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:27 - progress_bar.py[line:274] - INFO: epoch 001:  33327 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4216, wps=100.3, ups=0.61, wpb=109.1, bsz=40, num_updates=33280, lr=3.475e-05, gnorm=0.249, clip=10, loss_scale=128, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=184253
2023-01-11 17:04:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:43 - progress_bar.py[line:274] - INFO: epoch 001:  33337 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4608, wps=102.9, ups=0.62, wpb=110.7, bsz=40, num_updates=33290, lr=3.47448e-05, gnorm=1.57, clip=10, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=184269
2023-01-11 17:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:04:59 - progress_bar.py[line:274] - INFO: epoch 001:  33347 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4737, wps=104.4, ups=0.62, wpb=111.5, bsz=40, num_updates=33300, lr=3.47396e-05, gnorm=1.499, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184286
2023-01-11 17:04:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:16 - progress_bar.py[line:274] - INFO: epoch 001:  33357 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4314, wps=100.5, ups=0.61, wpb=109.2, bsz=40, num_updates=33310, lr=3.47344e-05, gnorm=0.275, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184302
2023-01-11 17:05:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:32 - progress_bar.py[line:274] - INFO: epoch 001:  33367 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4286, wps=99, ups=0.61, wpb=108.3, bsz=40, num_updates=33320, lr=3.47292e-05, gnorm=0.305, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184319
2023-01-11 17:05:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:49 - progress_bar.py[line:274] - INFO: epoch 001:  33377 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4286, wps=100.7, ups=0.61, wpb=110.5, bsz=40, num_updates=33330, lr=3.4724e-05, gnorm=0.383, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184335
2023-01-11 17:05:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:05:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:06 - progress_bar.py[line:274] - INFO: epoch 001:  33387 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5429, wps=101.3, ups=0.62, wpb=109.7, bsz=40, num_updates=33340, lr=3.47188e-05, gnorm=0.259, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=184352
2023-01-11 17:06:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:22 - progress_bar.py[line:274] - INFO: epoch 001:  33397 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4124, wps=102.6, ups=0.62, wpb=111.1, bsz=40, num_updates=33350, lr=3.47135e-05, gnorm=0.254, clip=0, loss_scale=128, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=184368
2023-01-11 17:06:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:39 - progress_bar.py[line:274] - INFO: epoch 001:  33407 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5234, wps=99.8, ups=0.61, wpb=109.1, bsz=40, num_updates=33360, lr=3.47083e-05, gnorm=0.271, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184385
2023-01-11 17:06:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:55 - progress_bar.py[line:274] - INFO: epoch 001:  33417 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4674, wps=101, ups=0.61, wpb=110.3, bsz=40, num_updates=33370, lr=3.47031e-05, gnorm=0.344, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184401
2023-01-11 17:06:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:06:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:12 - progress_bar.py[line:274] - INFO: epoch 001:  33427 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4022, wps=101.7, ups=0.62, wpb=110.1, bsz=40, num_updates=33380, lr=3.46979e-05, gnorm=0.282, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=184418
2023-01-11 17:07:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:29 - progress_bar.py[line:274] - INFO: epoch 001:  33437 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5106, wps=99.7, ups=0.6, wpb=109.9, bsz=40, num_updates=33390, lr=3.46927e-05, gnorm=0.186, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=184435
2023-01-11 17:07:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:45 - progress_bar.py[line:274] - INFO: epoch 001:  33447 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4486, wps=99.8, ups=0.61, wpb=109, bsz=40, num_updates=33400, lr=3.46875e-05, gnorm=0.281, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=184451
2023-01-11 17:07:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:07:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:02 - progress_bar.py[line:274] - INFO: epoch 001:  33457 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4842, wps=101.4, ups=0.61, wpb=110.9, bsz=40, num_updates=33410, lr=3.46823e-05, gnorm=0.156, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184468
2023-01-11 17:08:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:18 - progress_bar.py[line:274] - INFO: epoch 001:  33467 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4091, wps=103.6, ups=0.63, wpb=110.2, bsz=40, num_updates=33420, lr=3.46771e-05, gnorm=0.275, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=184484
2023-01-11 17:08:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:35 - progress_bar.py[line:274] - INFO: epoch 001:  33477 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4742, wps=100.1, ups=0.6, wpb=110.5, bsz=40, num_updates=33430, lr=3.46719e-05, gnorm=0.209, clip=0, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=184501
2023-01-11 17:08:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:52 - progress_bar.py[line:274] - INFO: epoch 001:  33487 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.514, wps=100.8, ups=0.61, wpb=110.7, bsz=40, num_updates=33440, lr=3.46667e-05, gnorm=0.361, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184518
2023-01-11 17:08:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:08:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:08 - progress_bar.py[line:274] - INFO: epoch 001:  33497 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5062, wps=100.9, ups=0.6, wpb=111.5, bsz=40, num_updates=33450, lr=3.46615e-05, gnorm=0.329, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=184535
2023-01-11 17:09:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:25 - progress_bar.py[line:274] - INFO: epoch 001:  33507 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4694, wps=100.5, ups=0.61, wpb=109.2, bsz=40, num_updates=33460, lr=3.46562e-05, gnorm=0.208, clip=0, loss_scale=128, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=184551
2023-01-11 17:09:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:41 - progress_bar.py[line:274] - INFO: epoch 001:  33517 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4375, wps=104.1, ups=0.63, wpb=110.5, bsz=40, num_updates=33470, lr=3.4651e-05, gnorm=0.426, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184567
2023-01-11 17:09:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:09:58 - progress_bar.py[line:274] - INFO: epoch 001:  33527 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4343, wps=100.7, ups=0.61, wpb=109.4, bsz=40, num_updates=33480, lr=3.46458e-05, gnorm=0.269, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184584
2023-01-11 17:09:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:14 - progress_bar.py[line:274] - INFO: epoch 001:  33537 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4563, wps=100.2, ups=0.61, wpb=109.5, bsz=40, num_updates=33490, lr=3.46406e-05, gnorm=0.556, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=184601
2023-01-11 17:10:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:31 - progress_bar.py[line:274] - INFO: epoch 001:  33547 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4906, wps=98.6, ups=0.6, wpb=110.3, bsz=40, num_updates=33500, lr=3.46354e-05, gnorm=0.848, clip=30, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=184618
2023-01-11 17:10:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:48 - progress_bar.py[line:274] - INFO: epoch 001:  33557 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4571, wps=100.7, ups=0.61, wpb=109.6, bsz=40, num_updates=33510, lr=3.46302e-05, gnorm=1.046, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184634
2023-01-11 17:10:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:10:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:04 - progress_bar.py[line:274] - INFO: epoch 001:  33567 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5149, wps=100.2, ups=0.61, wpb=109.8, bsz=40, num_updates=33520, lr=3.4625e-05, gnorm=0.278, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184651
2023-01-11 17:11:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:21 - progress_bar.py[line:274] - INFO: epoch 001:  33577 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.383, wps=99.1, ups=0.6, wpb=110.5, bsz=40, num_updates=33530, lr=3.46198e-05, gnorm=0.375, clip=10, loss_scale=128, train_wall=17, gb_free=10, ema_decay=0.9999, wall=184668
2023-01-11 17:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:38 - progress_bar.py[line:274] - INFO: epoch 001:  33587 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4639, wps=98.4, ups=0.6, wpb=109.3, bsz=40, num_updates=33540, lr=3.46146e-05, gnorm=0.193, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=184685
2023-01-11 17:11:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:55 - progress_bar.py[line:274] - INFO: epoch 001:  33597 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4362, wps=101.8, ups=0.61, wpb=110.9, bsz=40, num_updates=33550, lr=3.46094e-05, gnorm=0.621, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184701
2023-01-11 17:11:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:11:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:11 - progress_bar.py[line:274] - INFO: epoch 001:  33607 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4536, wps=102.7, ups=0.62, wpb=109.9, bsz=40, num_updates=33560, lr=3.46042e-05, gnorm=0.329, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184717
2023-01-11 17:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:27 - progress_bar.py[line:274] - INFO: epoch 001:  33617 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5052, wps=103.2, ups=0.63, wpb=109.8, bsz=40, num_updates=33570, lr=3.4599e-05, gnorm=0.341, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184734
2023-01-11 17:12:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:44 - progress_bar.py[line:274] - INFO: epoch 001:  33627 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4425, wps=98.4, ups=0.6, wpb=108.9, bsz=40, num_updates=33580, lr=3.45938e-05, gnorm=0.261, clip=0, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=184751
2023-01-11 17:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:12:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:01 - progress_bar.py[line:274] - INFO: epoch 001:  33637 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4902, wps=99.7, ups=0.61, wpb=109.5, bsz=40, num_updates=33590, lr=3.45885e-05, gnorm=1.047, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184767
2023-01-11 17:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:18 - progress_bar.py[line:274] - INFO: epoch 001:  33647 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4952, wps=100.2, ups=0.61, wpb=109.1, bsz=40, num_updates=33600, lr=3.45833e-05, gnorm=0.294, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=184784
2023-01-11 17:13:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:34 - progress_bar.py[line:274] - INFO: epoch 001:  33657 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4194, wps=100.2, ups=0.61, wpb=110.3, bsz=40, num_updates=33610, lr=3.45781e-05, gnorm=0.816, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184801
2023-01-11 17:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:51 - progress_bar.py[line:274] - INFO: epoch 001:  33667 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.161, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.31, wps=100.4, ups=0.6, wpb=110.9, bsz=40, num_updates=33620, lr=3.45729e-05, gnorm=0.323, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=184818
2023-01-11 17:13:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:13:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:08 - progress_bar.py[line:274] - INFO: epoch 001:  33677 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4587, wps=100.1, ups=0.62, wpb=108.3, bsz=40, num_updates=33630, lr=3.45677e-05, gnorm=0.287, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=184834
2023-01-11 17:14:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:25 - progress_bar.py[line:274] - INFO: epoch 001:  33687 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=99.2, ups=0.6, wpb=109.7, bsz=40, num_updates=33640, lr=3.45625e-05, gnorm=0.235, clip=0, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=184851
2023-01-11 17:14:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:41 - progress_bar.py[line:274] - INFO: epoch 001:  33697 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4286, wps=100.8, ups=0.61, wpb=110, bsz=40, num_updates=33650, lr=3.45573e-05, gnorm=0.256, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184867
2023-01-11 17:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:14:58 - progress_bar.py[line:274] - INFO: epoch 001:  33707 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4505, wps=100.1, ups=0.61, wpb=109.5, bsz=40, num_updates=33660, lr=3.45521e-05, gnorm=0.635, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184884
2023-01-11 17:14:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:14 - progress_bar.py[line:274] - INFO: epoch 001:  33717 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3837, wps=103.1, ups=0.62, wpb=111.5, bsz=40, num_updates=33670, lr=3.45469e-05, gnorm=0.462, clip=10, loss_scale=256, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=184901
2023-01-11 17:15:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:31 - progress_bar.py[line:274] - INFO: epoch 001:  33727 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4388, wps=98.3, ups=0.6, wpb=109, bsz=40, num_updates=33680, lr=3.45417e-05, gnorm=0.372, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=184917
2023-01-11 17:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:48 - progress_bar.py[line:274] - INFO: epoch 001:  33737 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3776, wps=102.3, ups=0.62, wpb=110.6, bsz=40, num_updates=33690, lr=3.45365e-05, gnorm=0.743, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=184934
2023-01-11 17:15:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:15:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:04 - progress_bar.py[line:274] - INFO: epoch 001:  33747 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4956, wps=100.1, ups=0.61, wpb=109.3, bsz=40, num_updates=33700, lr=3.45313e-05, gnorm=0.471, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=184951
2023-01-11 17:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:21 - progress_bar.py[line:274] - INFO: epoch 001:  33757 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.36, wps=99.1, ups=0.6, wpb=110.1, bsz=40, num_updates=33710, lr=3.4526e-05, gnorm=0.484, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=184968
2023-01-11 17:16:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:38 - progress_bar.py[line:274] - INFO: epoch 001:  33767 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4273, wps=102.9, ups=0.62, wpb=109.9, bsz=40, num_updates=33720, lr=3.45208e-05, gnorm=0.339, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=184984
2023-01-11 17:16:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:54 - progress_bar.py[line:274] - INFO: epoch 001:  33777 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4694, wps=100.3, ups=0.61, wpb=109.5, bsz=40, num_updates=33730, lr=3.45156e-05, gnorm=0.289, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=185000
2023-01-11 17:16:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:16:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:11 - progress_bar.py[line:274] - INFO: epoch 001:  33787 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4455, wps=102.5, ups=0.62, wpb=110.7, bsz=40, num_updates=33740, lr=3.45104e-05, gnorm=0.267, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=185017
2023-01-11 17:17:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:28 - progress_bar.py[line:274] - INFO: epoch 001:  33797 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4757, wps=100, ups=0.61, wpb=109.6, bsz=40, num_updates=33750, lr=3.45052e-05, gnorm=0.32, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=185034
2023-01-11 17:17:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:44 - progress_bar.py[line:274] - INFO: epoch 001:  33807 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4519, wps=98.6, ups=0.6, wpb=108.9, bsz=40, num_updates=33760, lr=3.45e-05, gnorm=0.224, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=185051
2023-01-11 17:17:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:17:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:01 - progress_bar.py[line:274] - INFO: epoch 001:  33817 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4639, wps=99, ups=0.6, wpb=110.1, bsz=40, num_updates=33770, lr=3.44948e-05, gnorm=0.346, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=185068
2023-01-11 17:18:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:18 - progress_bar.py[line:274] - INFO: epoch 001:  33827 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3786, wps=102.7, ups=0.62, wpb=109.8, bsz=40, num_updates=33780, lr=3.44896e-05, gnorm=0.247, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=185084
2023-01-11 17:18:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:34 - progress_bar.py[line:274] - INFO: epoch 001:  33837 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3636, wps=102.1, ups=0.62, wpb=108.9, bsz=40, num_updates=33790, lr=3.44844e-05, gnorm=0.51, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=185100
2023-01-11 17:18:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:51 - progress_bar.py[line:274] - INFO: epoch 001:  33847 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4, wps=99.6, ups=0.6, wpb=109.9, bsz=40, num_updates=33800, lr=3.44792e-05, gnorm=0.384, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=185117
2023-01-11 17:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:18:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:07 - progress_bar.py[line:274] - INFO: epoch 001:  33857 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5196, wps=102.6, ups=0.62, wpb=110.3, bsz=40, num_updates=33810, lr=3.4474e-05, gnorm=0.525, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=185133
2023-01-11 17:19:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:24 - progress_bar.py[line:274] - INFO: epoch 001:  33867 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3663, wps=99.9, ups=0.6, wpb=110.5, bsz=40, num_updates=33820, lr=3.44688e-05, gnorm=0.987, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=185150
2023-01-11 17:19:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:41 - progress_bar.py[line:274] - INFO: epoch 001:  33877 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.47, wps=99.6, ups=0.6, wpb=110.2, bsz=40, num_updates=33830, lr=3.44635e-05, gnorm=0.178, clip=0, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=185167
2023-01-11 17:19:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:57 - progress_bar.py[line:274] - INFO: epoch 001:  33887 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4483, wps=104.7, ups=0.63, wpb=110.8, bsz=40, num_updates=33840, lr=3.44583e-05, gnorm=0.857, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=185183
2023-01-11 17:19:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:19:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:14 - progress_bar.py[line:274] - INFO: epoch 001:  33897 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4598, wps=100.6, ups=0.61, wpb=109.7, bsz=40, num_updates=33850, lr=3.44531e-05, gnorm=1.164, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=185200
2023-01-11 17:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:31 - progress_bar.py[line:274] - INFO: epoch 001:  33907 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3918, wps=98.6, ups=0.6, wpb=109.5, bsz=40, num_updates=33860, lr=3.44479e-05, gnorm=0.285, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=185217
2023-01-11 17:20:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:47 - progress_bar.py[line:274] - INFO: epoch 001:  33917 / 100000 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5667, wps=103.6, ups=0.62, wpb=111.2, bsz=40, num_updates=33870, lr=3.44427e-05, gnorm=0.206, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=185233
2023-01-11 17:20:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:20:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:03 - progress_bar.py[line:274] - INFO: epoch 001:  33927 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4579, wps=103.9, ups=0.63, wpb=109.3, bsz=40, num_updates=33880, lr=3.44375e-05, gnorm=0.218, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=185249
2023-01-11 17:21:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:20 - progress_bar.py[line:274] - INFO: epoch 001:  33937 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=107.333, nsentences=40, sample_size=107.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5289, wps=97.1, ups=0.6, wpb=107.3, bsz=40, num_updates=33890, lr=3.44323e-05, gnorm=0.459, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=185266
2023-01-11 17:21:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:36 - progress_bar.py[line:274] - INFO: epoch 001:  33947 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3571, wps=105, ups=0.63, wpb=110.7, bsz=40, num_updates=33900, lr=3.44271e-05, gnorm=0.361, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=185282
2023-01-11 17:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:53 - progress_bar.py[line:274] - INFO: epoch 001:  33957 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4382, wps=100.4, ups=0.61, wpb=109.7, bsz=40, num_updates=33910, lr=3.44219e-05, gnorm=0.301, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=185299
2023-01-11 17:21:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:21:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:09 - progress_bar.py[line:274] - INFO: epoch 001:  33967 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5149, wps=101.9, ups=0.62, wpb=110.3, bsz=40, num_updates=33920, lr=3.44167e-05, gnorm=0.212, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=185315
2023-01-11 17:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:26 - progress_bar.py[line:274] - INFO: epoch 001:  33977 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3933, wps=101.1, ups=0.61, wpb=110.4, bsz=40, num_updates=33930, lr=3.44115e-05, gnorm=0.366, clip=10, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=185332
2023-01-11 17:22:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:43 - progress_bar.py[line:274] - INFO: epoch 001:  33987 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4574, wps=100.4, ups=0.61, wpb=109.7, bsz=40, num_updates=33940, lr=3.44062e-05, gnorm=0.252, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=185349
2023-01-11 17:22:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:22:59 - progress_bar.py[line:274] - INFO: epoch 001:  33997 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4766, wps=99.3, ups=0.6, wpb=109.6, bsz=40, num_updates=33950, lr=3.4401e-05, gnorm=0.354, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=185366
2023-01-11 17:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:16 - progress_bar.py[line:274] - INFO: epoch 001:  34007 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4857, wps=102.1, ups=0.61, wpb=111.1, bsz=40, num_updates=33960, lr=3.43958e-05, gnorm=0.176, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=185382
2023-01-11 17:23:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:33 - progress_bar.py[line:274] - INFO: epoch 001:  34017 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4732, wps=98.8, ups=0.6, wpb=109, bsz=40, num_updates=33970, lr=3.43906e-05, gnorm=0.163, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=185399
2023-01-11 17:23:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:50 - progress_bar.py[line:274] - INFO: epoch 001:  34027 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3838, wps=98.7, ups=0.59, wpb=110.7, bsz=40, num_updates=33980, lr=3.43854e-05, gnorm=0.298, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=185416
2023-01-11 17:23:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:23:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:24:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:24:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:24:06 - progress_bar.py[line:274] - INFO: epoch 001:  34037 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4886, wps=101.6, ups=0.62, wpb=109.7, bsz=40, num_updates=33990, lr=3.43802e-05, gnorm=0.155, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=185433
2023-01-11 17:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:24:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:24:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:24:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 17:24:23 - progress_bar.py[line:274] - INFO: epoch 001:  34047 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5133, wps=100.8, ups=0.61, wpb=109.6, bsz=40, num_updates=34000, lr=3.4375e-05, gnorm=0.342, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=185449
2023-01-11 17:24:23 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 17:24:25 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 17:24:25 - train.py[line:551] - INFO: load:1.57 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 17:26:57 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 17:26:57 - train.py[line:551] - INFO: load:1.59 valid_run:151.69 task_valid:148.84 collect_output:1.74
2023-01-11 17:29:24 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 17:29:24 - train.py[line:551] - INFO: load:1.62 valid_run:299.35 task_valid:292.23 collect_output:4.97
2023-01-11 17:31:56 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 17:31:56 - train.py[line:551] - INFO: load:1.64 valid_run:451.34 task_valid:436.47 collect_output:11.56
2023-01-11 17:34:25 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 17:34:25 - train.py[line:551] - INFO: load:1.66 valid_run:600.23 task_valid:581.93 collect_output:13.90
2023-01-11 17:36:58 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 17:36:58 - train.py[line:551] - INFO: load:1.69 valid_run:752.49 task_valid:729.96 collect_output:17.04
2023-01-11 17:39:29 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 17:39:29 - train.py[line:551] - INFO: load:1.72 valid_run:903.86 task_valid:876.07 collect_output:21.17
2023-01-11 17:42:01 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 17:42:01 - train.py[line:551] - INFO: load:1.74 valid_run:1056.03 task_valid:1022.59 collect_output:25.80
2023-01-11 17:44:32 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 17:44:32 - train.py[line:551] - INFO: load:1.77 valid_run:1206.41 task_valid:1164.33 collect_output:33.38
2023-01-11 17:47:01 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 17:47:01 - train.py[line:551] - INFO: load:1.80 valid_run:1355.55 task_valid:1309.34 collect_output:36.48
2023-01-11 17:49:29 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 17:49:29 - train.py[line:551] - INFO: load:1.83 valid_run:1503.29 task_valid:1452.83 collect_output:39.71
2023-01-11 17:51:58 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 17:51:58 - train.py[line:551] - INFO: load:1.85 valid_run:1652.57 task_valid:1598.17 collect_output:42.63
2023-01-11 17:54:28 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 17:54:28 - train.py[line:551] - INFO: load:1.88 valid_run:1802.02 task_valid:1743.65 collect_output:45.54
2023-01-11 17:56:57 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 17:56:57 - train.py[line:551] - INFO: load:1.90 valid_run:1951.37 task_valid:1886.33 collect_output:51.12
2023-01-11 17:59:28 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 17:59:28 - train.py[line:551] - INFO: load:1.93 valid_run:2101.97 task_valid:2032.58 collect_output:54.39
2023-01-11 18:01:58 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 18:01:58 - train.py[line:551] - INFO: load:1.95 valid_run:2252.36 task_valid:2179.82 collect_output:56.48
2023-01-11 18:04:29 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 18:04:29 - train.py[line:551] - INFO: load:1.98 valid_run:2402.59 task_valid:2325.22 collect_output:60.16
2023-01-11 18:07:00 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 18:07:00 - train.py[line:551] - INFO: load:2.01 valid_run:2553.66 task_valid:2471.38 collect_output:63.98
2023-01-11 18:09:31 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 18:09:31 - train.py[line:551] - INFO: load:2.03 valid_run:2704.44 task_valid:2619.27 collect_output:65.76
2023-01-11 18:11:59 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 18:11:59 - train.py[line:551] - INFO: load:2.06 valid_run:2852.28 task_valid:2761.54 collect_output:70.28
2023-01-11 18:14:29 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 18:14:29 - train.py[line:551] - INFO: load:2.08 valid_run:3002.34 task_valid:2907.48 collect_output:73.34
2023-01-11 18:17:00 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 18:17:00 - train.py[line:551] - INFO: load:2.11 valid_run:3153.52 task_valid:3052.84 collect_output:78.07
2023-01-11 18:19:30 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 18:19:30 - train.py[line:551] - INFO: load:2.14 valid_run:3302.89 task_valid:3198.15 collect_output:81.05
2023-01-11 18:22:01 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 18:22:01 - train.py[line:551] - INFO: load:2.17 valid_run:3454.05 task_valid:3345.71 collect_output:83.55
2023-01-11 18:24:32 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 18:24:32 - train.py[line:551] - INFO: load:2.19 valid_run:3605.37 task_valid:3493.07 collect_output:86.45

====================================================================================================
SGG eval:     R @ 50: 0.4395;     R @ 100: 0.5076;     R @ 500: 0.5337;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2716;    mR @ 100: 0.3297;    mR @ 500: 0.3554;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6902) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3065) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.5637) (says:0.0000) (sitting on:0.7324) (standing on:0.2060) (using:0.6500) (walking in:0.0000) (walking on:0.2973) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4395;     R @ 100: 0.5076;     R @ 500: 0.5337;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2716;    mR @ 100: 0.3297;    mR @ 500: 0.3554;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6902) (covered in:0.6875) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3065) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.5637) (says:0.0000) (sitting on:0.7324) (standing on:0.2060) (using:0.6500) (walking in:0.0000) (walking on:0.2973) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-01-11 18:27:04 - train.py[line:487] - INFO: 0.507581512605042
2023-01-11 18:27:04 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 18:27:04 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.403 | loss_v1 0 | loss_v2 0 | nll_loss 0.254 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.507582 | ppl 1.19 | vqa_score 0.4167 | wps 119.3 | wpb 89.9 | bsz 30 | num_updates 34000 | best_R@100 0.69005
2023-01-11 18:27:04 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 34000 updates
2023-01-11 18:27:04 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_34000.pt
2023-01-11 18:27:45 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_34000.pt
2023-01-11 18:29:20 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_34000.pt (epoch 1 @ 34000 updates, score 0.507581512605042) (writing took 136.0786249730736 seconds)
2023-01-11 18:29:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:29:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:29:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:29:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:29:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:29:37 - progress_bar.py[line:274] - INFO: epoch 001:  34057 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4105, wps=0.4, ups=0, wpb=110, bsz=40, num_updates=34010, lr=3.43698e-05, gnorm=0.169, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=189364
2023-01-11 18:29:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:29:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:29:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:29:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:29:54 - progress_bar.py[line:274] - INFO: epoch 001:  34067 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=100.4, ups=0.61, wpb=109.5, bsz=40, num_updates=34020, lr=3.43646e-05, gnorm=0.295, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=189380
2023-01-11 18:29:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:29:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:12 - progress_bar.py[line:274] - INFO: epoch 001:  34077 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4257, wps=95.6, ups=0.58, wpb=109.2, bsz=40, num_updates=34030, lr=3.43594e-05, gnorm=1.083, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=189399
2023-01-11 18:30:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:29 - progress_bar.py[line:274] - INFO: epoch 001:  34087 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3654, wps=98.1, ups=0.6, wpb=109.3, bsz=40, num_updates=34040, lr=3.43542e-05, gnorm=0.336, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=189415
2023-01-11 18:30:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:46 - progress_bar.py[line:274] - INFO: epoch 001:  34097 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4904, wps=98, ups=0.59, wpb=109.9, bsz=40, num_updates=34050, lr=3.4349e-05, gnorm=0.38, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=189433
2023-01-11 18:30:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:30:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:03 - progress_bar.py[line:274] - INFO: epoch 001:  34107 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=102, ups=0.61, wpb=111.7, bsz=40, num_updates=34060, lr=3.43438e-05, gnorm=0.475, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=189449
2023-01-11 18:31:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:21 - progress_bar.py[line:274] - INFO: epoch 001:  34117 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4766, wps=97.7, ups=0.6, wpb=109, bsz=40, num_updates=34070, lr=3.43385e-05, gnorm=0.367, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=189467
2023-01-11 18:31:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:38 - progress_bar.py[line:274] - INFO: epoch 001:  34127 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4175, wps=97.1, ups=0.6, wpb=108.5, bsz=40, num_updates=34080, lr=3.43333e-05, gnorm=0.793, clip=10, loss_scale=256, train_wall=17, gb_free=10.7, ema_decay=0.9999, wall=189484
2023-01-11 18:31:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:55 - progress_bar.py[line:274] - INFO: epoch 001:  34137 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4423, wps=99.5, ups=0.61, wpb=109.4, bsz=40, num_updates=34090, lr=3.43281e-05, gnorm=0.288, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=189501
2023-01-11 18:31:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:31:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:11 - progress_bar.py[line:274] - INFO: epoch 001:  34147 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4455, wps=101.1, ups=0.61, wpb=111.2, bsz=40, num_updates=34100, lr=3.43229e-05, gnorm=3.025, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=189518
2023-01-11 18:32:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:28 - progress_bar.py[line:274] - INFO: epoch 001:  34157 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4535, wps=99.3, ups=0.6, wpb=109.9, bsz=40, num_updates=34110, lr=3.43177e-05, gnorm=0.435, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=189534
2023-01-11 18:32:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:45 - progress_bar.py[line:274] - INFO: epoch 001:  34167 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.49, wps=100.8, ups=0.62, wpb=109.2, bsz=40, num_updates=34120, lr=3.43125e-05, gnorm=0.419, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=189551
2023-01-11 18:32:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:32:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:02 - progress_bar.py[line:274] - INFO: epoch 001:  34177 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5196, wps=100.3, ups=0.6, wpb=111.3, bsz=40, num_updates=34130, lr=3.43073e-05, gnorm=0.322, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=189568
2023-01-11 18:33:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:18 - progress_bar.py[line:274] - INFO: epoch 001:  34187 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5047, wps=101.3, ups=0.62, wpb=109.4, bsz=40, num_updates=34140, lr=3.43021e-05, gnorm=0.568, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=189584
2023-01-11 18:33:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:35 - progress_bar.py[line:274] - INFO: epoch 001:  34197 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.396, wps=99.9, ups=0.62, wpb=108.2, bsz=40, num_updates=34150, lr=3.42969e-05, gnorm=0.158, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=189601
2023-01-11 18:33:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:51 - progress_bar.py[line:274] - INFO: epoch 001:  34207 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5049, wps=102.9, ups=0.63, wpb=109.4, bsz=40, num_updates=34160, lr=3.42917e-05, gnorm=0.175, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=189617
2023-01-11 18:33:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:07 - progress_bar.py[line:274] - INFO: epoch 001:  34217 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4563, wps=100.3, ups=0.61, wpb=108.9, bsz=40, num_updates=34170, lr=3.42865e-05, gnorm=0.219, clip=0, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=189634
2023-01-11 18:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:24 - progress_bar.py[line:274] - INFO: epoch 001:  34227 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4356, wps=99.6, ups=0.6, wpb=109.9, bsz=40, num_updates=34180, lr=3.42812e-05, gnorm=0.38, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=189650
2023-01-11 18:34:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:41 - progress_bar.py[line:274] - INFO: epoch 001:  34237 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5102, wps=101.2, ups=0.61, wpb=111.2, bsz=40, num_updates=34190, lr=3.4276e-05, gnorm=0.277, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=189667
2023-01-11 18:34:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:47 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 18:34:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:34:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:00 - progress_bar.py[line:274] - INFO: epoch 001:  34248 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.312, nsentences=40, sample_size=110.312, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4113, wps=94.6, ups=0.54, wpb=110.3, bsz=40, num_updates=34200, lr=3.42708e-05, gnorm=0.244, clip=0, loss_scale=256, train_wall=19, gb_free=10.1, ema_decay=0.9999, wall=189686
2023-01-11 18:35:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:16 - progress_bar.py[line:274] - INFO: epoch 001:  34258 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3796, wps=99.6, ups=0.61, wpb=108.7, bsz=40, num_updates=34210, lr=3.42656e-05, gnorm=0.438, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=189703
2023-01-11 18:35:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:33 - progress_bar.py[line:274] - INFO: epoch 001:  34268 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3465, wps=100.8, ups=0.6, wpb=111.1, bsz=40, num_updates=34220, lr=3.42604e-05, gnorm=0.232, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=189719
2023-01-11 18:35:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:50 - progress_bar.py[line:274] - INFO: epoch 001:  34278 / 100000 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5, wps=97.3, ups=0.58, wpb=110.9, bsz=40, num_updates=34230, lr=3.42552e-05, gnorm=0.254, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=189737
2023-01-11 18:35:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:35:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:07 - progress_bar.py[line:274] - INFO: epoch 001:  34288 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4245, wps=97.4, ups=0.6, wpb=108.7, bsz=40, num_updates=34240, lr=3.425e-05, gnorm=0.591, clip=10, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=189754
2023-01-11 18:36:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:24 - progress_bar.py[line:274] - INFO: epoch 001:  34298 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4848, wps=100.9, ups=0.61, wpb=110.7, bsz=40, num_updates=34250, lr=3.42448e-05, gnorm=0.27, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=189770
2023-01-11 18:36:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:40 - progress_bar.py[line:274] - INFO: epoch 001:  34308 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4712, wps=103.5, ups=0.62, wpb=110.5, bsz=40, num_updates=34260, lr=3.42396e-05, gnorm=0.289, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=189786
2023-01-11 18:36:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:57 - progress_bar.py[line:274] - INFO: epoch 001:  34318 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3696, wps=100.1, ups=0.61, wpb=109.8, bsz=40, num_updates=34270, lr=3.42344e-05, gnorm=0.295, clip=0, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=189803
2023-01-11 18:36:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:36:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:14 - progress_bar.py[line:274] - INFO: epoch 001:  34328 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3434, wps=99.1, ups=0.61, wpb=109.1, bsz=40, num_updates=34280, lr=3.42292e-05, gnorm=0.337, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=189820
2023-01-11 18:37:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:32 - progress_bar.py[line:274] - INFO: epoch 001:  34338 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4948, wps=95.5, ups=0.57, wpb=110.8, bsz=40, num_updates=34290, lr=3.4224e-05, gnorm=0.571, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=189838
2023-01-11 18:37:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:49 - progress_bar.py[line:274] - INFO: epoch 001:  34348 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4632, wps=101.3, ups=0.61, wpb=110.6, bsz=40, num_updates=34300, lr=3.42188e-05, gnorm=0.134, clip=0, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=189855
2023-01-11 18:37:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:37:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:06 - progress_bar.py[line:274] - INFO: epoch 001:  34358 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5288, wps=101.5, ups=0.61, wpb=110.7, bsz=40, num_updates=34310, lr=3.42135e-05, gnorm=0.666, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=189872
2023-01-11 18:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:23 - progress_bar.py[line:274] - INFO: epoch 001:  34368 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.451, wps=101.3, ups=0.61, wpb=110.6, bsz=40, num_updates=34320, lr=3.42083e-05, gnorm=0.187, clip=0, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=189889
2023-01-11 18:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:40 - progress_bar.py[line:274] - INFO: epoch 001:  34378 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4804, wps=97.4, ups=0.59, wpb=109.5, bsz=40, num_updates=34330, lr=3.42031e-05, gnorm=0.544, clip=10, loss_scale=256, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=189907
2023-01-11 18:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:57 - progress_bar.py[line:274] - INFO: epoch 001:  34388 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.383, wps=103.6, ups=0.62, wpb=112, bsz=40, num_updates=34340, lr=3.41979e-05, gnorm=0.231, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=189923
2023-01-11 18:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:38:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:14 - progress_bar.py[line:274] - INFO: epoch 001:  34398 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4667, wps=98.8, ups=0.6, wpb=110.3, bsz=40, num_updates=34350, lr=3.41927e-05, gnorm=0.382, clip=10, loss_scale=256, train_wall=17, gb_free=9.8, ema_decay=0.9999, wall=189940
2023-01-11 18:39:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:31 - progress_bar.py[line:274] - INFO: epoch 001:  34408 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4433, wps=99.7, ups=0.61, wpb=109.3, bsz=40, num_updates=34360, lr=3.41875e-05, gnorm=0.2, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=189957
2023-01-11 18:39:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:48 - progress_bar.py[line:274] - INFO: epoch 001:  34418 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4951, wps=98.7, ups=0.6, wpb=109.1, bsz=40, num_updates=34370, lr=3.41823e-05, gnorm=0.295, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=189974
2023-01-11 18:39:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:39:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:04 - progress_bar.py[line:274] - INFO: epoch 001:  34428 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4375, wps=100.8, ups=0.6, wpb=111.2, bsz=40, num_updates=34380, lr=3.41771e-05, gnorm=0.415, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=189991
2023-01-11 18:40:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:21 - progress_bar.py[line:274] - INFO: epoch 001:  34438 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4667, wps=98.3, ups=0.6, wpb=108.3, bsz=40, num_updates=34390, lr=3.41719e-05, gnorm=0.317, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190007
2023-01-11 18:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:38 - progress_bar.py[line:274] - INFO: epoch 001:  34448 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4845, wps=99.2, ups=0.6, wpb=109.8, bsz=40, num_updates=34400, lr=3.41667e-05, gnorm=0.5, clip=20, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=190024
2023-01-11 18:40:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:55 - progress_bar.py[line:274] - INFO: epoch 001:  34458 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4476, wps=101, ups=0.61, wpb=110.7, bsz=40, num_updates=34410, lr=3.41615e-05, gnorm=0.247, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=190041
2023-01-11 18:40:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:40:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:11 - progress_bar.py[line:274] - INFO: epoch 001:  34468 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5484, wps=103.6, ups=0.62, wpb=110.8, bsz=40, num_updates=34420, lr=3.41563e-05, gnorm=0.347, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=190057
2023-01-11 18:41:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:28 - progress_bar.py[line:274] - INFO: epoch 001:  34478 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4673, wps=100.1, ups=0.61, wpb=109.4, bsz=40, num_updates=34430, lr=3.4151e-05, gnorm=0.225, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190074
2023-01-11 18:41:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:45 - progress_bar.py[line:274] - INFO: epoch 001:  34488 / 100000 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5281, wps=99.2, ups=0.6, wpb=110.7, bsz=40, num_updates=34440, lr=3.41458e-05, gnorm=0.211, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=190091
2023-01-11 18:41:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:41:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:02 - progress_bar.py[line:274] - INFO: epoch 001:  34498 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4118, wps=99.6, ups=0.61, wpb=108.3, bsz=40, num_updates=34450, lr=3.41406e-05, gnorm=0.861, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190108
2023-01-11 18:42:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:18 - progress_bar.py[line:274] - INFO: epoch 001:  34508 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4, wps=99.7, ups=0.61, wpb=108.6, bsz=40, num_updates=34460, lr=3.41354e-05, gnorm=0.233, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=190124
2023-01-11 18:42:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:35 - progress_bar.py[line:274] - INFO: epoch 001:  34518 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4167, wps=97.2, ups=0.59, wpb=109.5, bsz=40, num_updates=34470, lr=3.41302e-05, gnorm=0.493, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=190142
2023-01-11 18:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:52 - progress_bar.py[line:274] - INFO: epoch 001:  34528 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4655, wps=101.6, ups=0.62, wpb=109.5, bsz=40, num_updates=34480, lr=3.4125e-05, gnorm=0.623, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=190158
2023-01-11 18:42:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:42:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:08 - progress_bar.py[line:274] - INFO: epoch 001:  34538 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.44, wps=102.7, ups=0.62, wpb=109.9, bsz=40, num_updates=34490, lr=3.41198e-05, gnorm=0.272, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=190174
2023-01-11 18:43:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:25 - progress_bar.py[line:274] - INFO: epoch 001:  34548 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4476, wps=100.2, ups=0.61, wpb=109.2, bsz=40, num_updates=34500, lr=3.41146e-05, gnorm=0.624, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=190191
2023-01-11 18:43:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:42 - progress_bar.py[line:274] - INFO: epoch 001:  34558 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.43, wps=101.4, ups=0.61, wpb=110.1, bsz=40, num_updates=34510, lr=3.41094e-05, gnorm=0.461, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190208
2023-01-11 18:43:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:43:58 - progress_bar.py[line:274] - INFO: epoch 001:  34568 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.37, wps=103.3, ups=0.63, wpb=109.7, bsz=40, num_updates=34520, lr=3.41042e-05, gnorm=0.237, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=190224
2023-01-11 18:43:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:14 - progress_bar.py[line:274] - INFO: epoch 001:  34578 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4592, wps=102.3, ups=0.62, wpb=110, bsz=40, num_updates=34530, lr=3.4099e-05, gnorm=0.308, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190240
2023-01-11 18:44:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:31 - progress_bar.py[line:274] - INFO: epoch 001:  34588 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4949, wps=103.4, ups=0.62, wpb=111.3, bsz=40, num_updates=34540, lr=3.40938e-05, gnorm=0.944, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=190257
2023-01-11 18:44:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:47 - progress_bar.py[line:274] - INFO: epoch 001:  34598 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5446, wps=103.8, ups=0.64, wpb=108.5, bsz=40, num_updates=34550, lr=3.40885e-05, gnorm=0.312, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190273
2023-01-11 18:44:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:44:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:03 - progress_bar.py[line:274] - INFO: epoch 001:  34608 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4151, wps=101.4, ups=0.62, wpb=109.5, bsz=40, num_updates=34560, lr=3.40833e-05, gnorm=0.521, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190289
2023-01-11 18:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:20 - progress_bar.py[line:274] - INFO: epoch 001:  34618 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4519, wps=98, ups=0.61, wpb=107.9, bsz=40, num_updates=34570, lr=3.40781e-05, gnorm=0.22, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=190306
2023-01-11 18:45:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:37 - progress_bar.py[line:274] - INFO: epoch 001:  34628 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5283, wps=97.3, ups=0.6, wpb=109, bsz=40, num_updates=34580, lr=3.40729e-05, gnorm=0.451, clip=10, loss_scale=256, train_wall=17, gb_free=9.5, ema_decay=0.9999, wall=190323
2023-01-11 18:45:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:54 - progress_bar.py[line:274] - INFO: epoch 001:  34638 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3725, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=34590, lr=3.40677e-05, gnorm=0.184, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190340
2023-01-11 18:45:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:11 - progress_bar.py[line:274] - INFO: epoch 001:  34648 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4862, wps=99.3, ups=0.61, wpb=109.4, bsz=40, num_updates=34600, lr=3.40625e-05, gnorm=0.18, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=190357
2023-01-11 18:46:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:27 - progress_bar.py[line:274] - INFO: epoch 001:  34658 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3981, wps=102, ups=0.62, wpb=110, bsz=40, num_updates=34610, lr=3.40573e-05, gnorm=0.959, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190373
2023-01-11 18:46:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:44 - progress_bar.py[line:274] - INFO: epoch 001:  34668 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4369, wps=99.3, ups=0.61, wpb=109.1, bsz=40, num_updates=34620, lr=3.40521e-05, gnorm=0.281, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190390
2023-01-11 18:46:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:46:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:00 - progress_bar.py[line:274] - INFO: epoch 001:  34678 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.466, wps=102.5, ups=0.63, wpb=108.9, bsz=40, num_updates=34630, lr=3.40469e-05, gnorm=0.197, clip=0, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=190406
2023-01-11 18:47:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:17 - progress_bar.py[line:274] - INFO: epoch 001:  34688 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4894, wps=100.8, ups=0.61, wpb=109.9, bsz=40, num_updates=34640, lr=3.40417e-05, gnorm=0.273, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=190423
2023-01-11 18:47:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:34 - progress_bar.py[line:274] - INFO: epoch 001:  34698 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.449, wps=100.5, ups=0.6, wpb=111, bsz=40, num_updates=34650, lr=3.40365e-05, gnorm=0.703, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=190440
2023-01-11 18:47:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:50 - progress_bar.py[line:274] - INFO: epoch 001:  34708 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4643, wps=100.7, ups=0.62, wpb=107.9, bsz=40, num_updates=34660, lr=3.40313e-05, gnorm=0.213, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=190456
2023-01-11 18:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:07 - progress_bar.py[line:274] - INFO: epoch 001:  34718 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4112, wps=98.4, ups=0.6, wpb=110, bsz=40, num_updates=34670, lr=3.4026e-05, gnorm=0.345, clip=0, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=190473
2023-01-11 18:48:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:24 - progress_bar.py[line:274] - INFO: epoch 001:  34728 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4352, wps=101.3, ups=0.62, wpb=109.3, bsz=40, num_updates=34680, lr=3.40208e-05, gnorm=0.27, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=190490
2023-01-11 18:48:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:40 - progress_bar.py[line:274] - INFO: epoch 001:  34738 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5413, wps=98.8, ups=0.61, wpb=108.9, bsz=40, num_updates=34690, lr=3.40156e-05, gnorm=0.221, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=190507
2023-01-11 18:48:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:48:58 - progress_bar.py[line:274] - INFO: epoch 001:  34748 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4078, wps=98.8, ups=0.6, wpb=109.4, bsz=40, num_updates=34700, lr=3.40104e-05, gnorm=0.231, clip=0, loss_scale=256, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=190524
2023-01-11 18:48:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:15 - progress_bar.py[line:274] - INFO: epoch 001:  34758 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4845, wps=99.8, ups=0.6, wpb=110.5, bsz=40, num_updates=34710, lr=3.40052e-05, gnorm=0.5, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=190541
2023-01-11 18:49:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:31 - progress_bar.py[line:274] - INFO: epoch 001:  34768 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4757, wps=101.5, ups=0.61, wpb=110.5, bsz=40, num_updates=34720, lr=3.4e-05, gnorm=0.593, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=190557
2023-01-11 18:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:48 - progress_bar.py[line:274] - INFO: epoch 001:  34778 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4434, wps=99.5, ups=0.61, wpb=109.1, bsz=40, num_updates=34730, lr=3.39948e-05, gnorm=0.273, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190574
2023-01-11 18:49:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:49:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:04 - progress_bar.py[line:274] - INFO: epoch 001:  34788 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4175, wps=104.1, ups=0.62, wpb=111.8, bsz=40, num_updates=34740, lr=3.39896e-05, gnorm=0.201, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190590
2023-01-11 18:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:21 - progress_bar.py[line:274] - INFO: epoch 001:  34798 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4513, wps=99.8, ups=0.61, wpb=109.6, bsz=40, num_updates=34750, lr=3.39844e-05, gnorm=0.449, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190607
2023-01-11 18:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:38 - progress_bar.py[line:274] - INFO: epoch 001:  34808 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5204, wps=99.7, ups=0.6, wpb=110.4, bsz=40, num_updates=34760, lr=3.39792e-05, gnorm=1.352, clip=20, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=190624
2023-01-11 18:50:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:55 - progress_bar.py[line:274] - INFO: epoch 001:  34818 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5049, wps=100.5, ups=0.61, wpb=109.9, bsz=40, num_updates=34770, lr=3.3974e-05, gnorm=0.307, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190641
2023-01-11 18:50:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:50:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:12 - progress_bar.py[line:274] - INFO: epoch 001:  34828 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4205, wps=98.6, ups=0.6, wpb=110.1, bsz=40, num_updates=34780, lr=3.39688e-05, gnorm=0.718, clip=30, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=190658
2023-01-11 18:51:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:28 - progress_bar.py[line:274] - INFO: epoch 001:  34838 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5625, wps=100, ups=0.61, wpb=109.8, bsz=40, num_updates=34790, lr=3.39635e-05, gnorm=0.36, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=190675
2023-01-11 18:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:45 - progress_bar.py[line:274] - INFO: epoch 001:  34848 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4592, wps=104.3, ups=0.63, wpb=109.7, bsz=40, num_updates=34800, lr=3.39583e-05, gnorm=0.23, clip=0, loss_scale=512, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=190691
2023-01-11 18:51:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:51:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:02 - progress_bar.py[line:274] - INFO: epoch 001:  34858 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4038, wps=97.8, ups=0.59, wpb=109.7, bsz=40, num_updates=34810, lr=3.39531e-05, gnorm=0.444, clip=10, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=190708
2023-01-11 18:52:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:19 - progress_bar.py[line:274] - INFO: epoch 001:  34868 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4688, wps=100.2, ups=0.61, wpb=109.1, bsz=40, num_updates=34820, lr=3.39479e-05, gnorm=0.563, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=190725
2023-01-11 18:52:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:36 - progress_bar.py[line:274] - INFO: epoch 001:  34878 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.36, wps=99.4, ups=0.61, wpb=109.4, bsz=40, num_updates=34830, lr=3.39427e-05, gnorm=0.354, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=190742
2023-01-11 18:52:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:52 - progress_bar.py[line:274] - INFO: epoch 001:  34888 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5161, wps=102, ups=0.61, wpb=111.5, bsz=40, num_updates=34840, lr=3.39375e-05, gnorm=0.389, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=190758
2023-01-11 18:52:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:52:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:08 - progress_bar.py[line:274] - INFO: epoch 001:  34898 / 100000 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4235, wps=104.5, ups=0.62, wpb=111.5, bsz=40, num_updates=34850, lr=3.39323e-05, gnorm=0.234, clip=10, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=190775
2023-01-11 18:53:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:25 - progress_bar.py[line:274] - INFO: epoch 001:  34908 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4388, wps=99.7, ups=0.6, wpb=110.3, bsz=40, num_updates=34860, lr=3.39271e-05, gnorm=0.664, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=190791
2023-01-11 18:53:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:41 - progress_bar.py[line:274] - INFO: epoch 001:  34918 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4066, wps=104.4, ups=0.63, wpb=110.9, bsz=40, num_updates=34870, lr=3.39219e-05, gnorm=0.192, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190808
2023-01-11 18:53:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:53:58 - progress_bar.py[line:274] - INFO: epoch 001:  34928 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4904, wps=101.8, ups=0.62, wpb=110.1, bsz=40, num_updates=34880, lr=3.39167e-05, gnorm=0.244, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190824
2023-01-11 18:53:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:14 - progress_bar.py[line:274] - INFO: epoch 001:  34938 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4536, wps=102.4, ups=0.62, wpb=110.6, bsz=40, num_updates=34890, lr=3.39115e-05, gnorm=0.257, clip=10, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=190841
2023-01-11 18:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:31 - progress_bar.py[line:274] - INFO: epoch 001:  34948 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4615, wps=102.8, ups=0.62, wpb=111.1, bsz=40, num_updates=34900, lr=3.39063e-05, gnorm=0.316, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190857
2023-01-11 18:54:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:48 - progress_bar.py[line:274] - INFO: epoch 001:  34958 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.45, wps=99.8, ups=0.61, wpb=108.7, bsz=40, num_updates=34910, lr=3.3901e-05, gnorm=0.617, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=190874
2023-01-11 18:54:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:54:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:04 - progress_bar.py[line:274] - INFO: epoch 001:  34968 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4118, wps=99.7, ups=0.61, wpb=109.4, bsz=40, num_updates=34920, lr=3.38958e-05, gnorm=0.527, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190890
2023-01-11 18:55:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:12 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 18:55:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:23 - progress_bar.py[line:274] - INFO: epoch 001:  34979 / 100000 loss=0.326, loss_v1=0, loss_v2=0, nll_loss=0.174, ntokens=108.312, nsentences=40, sample_size=108.312, sample_size_v1=0, sample_size_v2=0, ppl=1.13, vqa_score=0.4485, wps=94.8, ups=0.55, wpb=108.3, bsz=40, num_updates=34930, lr=3.38906e-05, gnorm=3.595, clip=30, loss_scale=256, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=190909
2023-01-11 18:55:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:40 - progress_bar.py[line:274] - INFO: epoch 001:  34989 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4135, wps=100.6, ups=0.61, wpb=109.3, bsz=40, num_updates=34940, lr=3.38854e-05, gnorm=0.503, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=190926
2023-01-11 18:55:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:55:56 - progress_bar.py[line:274] - INFO: epoch 001:  34999 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5051, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=34950, lr=3.38802e-05, gnorm=0.244, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=190942
2023-01-11 18:55:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:12 - progress_bar.py[line:274] - INFO: epoch 001:  35009 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3978, wps=101.8, ups=0.62, wpb=108.9, bsz=40, num_updates=34960, lr=3.3875e-05, gnorm=0.241, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=190959
2023-01-11 18:56:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:29 - progress_bar.py[line:274] - INFO: epoch 001:  35019 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4457, wps=99, ups=0.6, wpb=109.2, bsz=40, num_updates=34970, lr=3.38698e-05, gnorm=0.434, clip=20, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=190975
2023-01-11 18:56:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:46 - progress_bar.py[line:274] - INFO: epoch 001:  35029 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5152, wps=98.8, ups=0.6, wpb=109, bsz=40, num_updates=34980, lr=3.38646e-05, gnorm=0.374, clip=0, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=190992
2023-01-11 18:56:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:56:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:57:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:57:03 - progress_bar.py[line:274] - INFO: epoch 001:  35039 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4245, wps=101, ups=0.61, wpb=110, bsz=40, num_updates=34990, lr=3.38594e-05, gnorm=0.502, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=191009
2023-01-11 18:57:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:57:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:57:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:57:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:57:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 18:57:19 - progress_bar.py[line:274] - INFO: epoch 001:  35049 / 100000 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=112.2, nsentences=40, sample_size=112.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5233, wps=104, ups=0.62, wpb=112.2, bsz=40, num_updates=35000, lr=3.38542e-05, gnorm=0.217, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=191025
2023-01-11 18:57:19 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 18:57:21 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 18:57:21 - train.py[line:551] - INFO: load:1.25 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 18:59:53 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 18:59:53 - train.py[line:551] - INFO: load:1.28 valid_run:152.02 task_valid:149.26 collect_output:1.59
2023-01-11 19:02:21 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 19:02:21 - train.py[line:551] - INFO: load:1.30 valid_run:300.24 task_valid:293.25 collect_output:4.69
2023-01-11 19:04:52 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 19:04:52 - train.py[line:551] - INFO: load:1.33 valid_run:451.66 task_valid:436.55 collect_output:11.77
2023-01-11 19:07:21 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 19:07:21 - train.py[line:551] - INFO: load:1.36 valid_run:600.59 task_valid:582.10 collect_output:14.04
2023-01-11 19:09:53 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 19:09:53 - train.py[line:551] - INFO: load:1.38 valid_run:752.49 task_valid:730.07 collect_output:16.89
2023-01-11 19:12:25 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 19:12:25 - train.py[line:551] - INFO: load:1.41 valid_run:903.91 task_valid:876.31 collect_output:21.01
2023-01-11 19:14:58 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 19:14:58 - train.py[line:551] - INFO: load:1.44 valid_run:1056.69 task_valid:1023.08 collect_output:25.89
2023-01-11 19:17:28 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 19:17:28 - train.py[line:551] - INFO: load:1.47 valid_run:1207.23 task_valid:1165.11 collect_output:33.30
2023-01-11 19:19:58 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 19:19:58 - train.py[line:551] - INFO: load:1.49 valid_run:1356.25 task_valid:1310.30 collect_output:36.07
2023-01-11 19:22:26 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 19:22:26 - train.py[line:551] - INFO: load:1.52 valid_run:1505.03 task_valid:1454.88 collect_output:39.15
2023-01-11 19:24:56 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 19:24:56 - train.py[line:551] - INFO: load:1.55 valid_run:1654.73 task_valid:1600.54 collect_output:42.09
2023-01-11 19:27:26 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 19:27:26 - train.py[line:551] - INFO: load:1.58 valid_run:1804.22 task_valid:1745.94 collect_output:45.08
2023-01-11 19:29:55 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 19:29:55 - train.py[line:551] - INFO: load:1.60 valid_run:1953.80 task_valid:1888.78 collect_output:50.72
2023-01-11 19:32:26 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 19:32:26 - train.py[line:551] - INFO: load:1.63 valid_run:2104.26 task_valid:2035.01 collect_output:53.89
2023-01-11 19:34:57 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 19:34:57 - train.py[line:551] - INFO: load:1.66 valid_run:2255.15 task_valid:2182.75 collect_output:55.94
2023-01-11 19:37:27 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 19:37:27 - train.py[line:551] - INFO: load:1.69 valid_run:2404.88 task_valid:2327.45 collect_output:59.83
2023-01-11 19:39:58 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 19:39:58 - train.py[line:551] - INFO: load:1.72 valid_run:2555.60 task_valid:2473.38 collect_output:63.55
2023-01-11 19:42:28 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 19:42:28 - train.py[line:551] - INFO: load:1.74 valid_run:2706.40 task_valid:2621.20 collect_output:65.41
2023-01-11 19:44:57 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 19:44:57 - train.py[line:551] - INFO: load:1.77 valid_run:2854.61 task_valid:2763.62 collect_output:70.10
2023-01-11 19:47:27 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 19:47:27 - train.py[line:551] - INFO: load:1.80 valid_run:3004.87 task_valid:2909.95 collect_output:72.94
2023-01-11 19:49:58 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 19:49:58 - train.py[line:551] - INFO: load:1.83 valid_run:3156.01 task_valid:3055.81 collect_output:77.09
2023-01-11 19:52:28 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 19:52:28 - train.py[line:551] - INFO: load:1.85 valid_run:3305.12 task_valid:3201.03 collect_output:79.91
2023-01-11 19:54:59 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 19:54:59 - train.py[line:551] - INFO: load:1.88 valid_run:3456.18 task_valid:3348.10 collect_output:82.77
2023-01-11 19:57:30 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 19:57:30 - train.py[line:551] - INFO: load:1.91 valid_run:3607.70 task_valid:3495.69 collect_output:85.59

====================================================================================================
SGG eval:     R @ 50: 0.4322;     R @ 100: 0.4994;     R @ 500: 0.5242;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2681;    mR @ 100: 0.3213;    mR @ 500: 0.3467;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6659) (covered in:0.6458) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3065) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.5392) (says:0.0000) (sitting on:0.7324) (standing on:0.2127) (using:0.6500) (walking in:0.0000) (walking on:0.2973) (watching:0.1389) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4322;     R @ 100: 0.4994;     R @ 500: 0.5242;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2681;    mR @ 100: 0.3213;    mR @ 500: 0.3467;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6659) (covered in:0.6458) (covering:0.3714) (eating:0.6471) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3065) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.5392) (says:0.0000) (sitting on:0.7324) (standing on:0.2127) (using:0.6500) (walking in:0.0000) (walking on:0.2973) (watching:0.1389) 
--------------------------------------------------------
====================================================================================================

2023-01-11 20:00:01 - train.py[line:487] - INFO: 0.4994148459383753
2023-01-11 20:00:02 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 20:00:02 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.366 | loss_v1 0 | loss_v2 0 | nll_loss 0.213 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.499415 | ppl 1.16 | vqa_score 0.4122 | wps 119.3 | wpb 89.9 | bsz 30 | num_updates 35000 | best_R@100 0.69005
2023-01-11 20:00:02 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 35000 updates
2023-01-11 20:00:02 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_35000.pt
2023-01-11 20:00:48 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_35000.pt
2023-01-11 20:02:18 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_35000.pt (epoch 1 @ 35000 updates, score 0.4994148459383753) (writing took 136.01536014303565 seconds)
2023-01-11 20:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:02:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:02:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:02:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:02:35 - progress_bar.py[line:274] - INFO: epoch 001:  35059 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3871, wps=0.4, ups=0, wpb=110.3, bsz=40, num_updates=35010, lr=3.3849e-05, gnorm=0.504, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=194941
2023-01-11 20:02:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:02:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:02:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:02:52 - progress_bar.py[line:274] - INFO: epoch 001:  35069 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.449, wps=100.9, ups=0.61, wpb=110, bsz=40, num_updates=35020, lr=3.38437e-05, gnorm=0.271, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=194958
2023-01-11 20:02:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:08 - progress_bar.py[line:274] - INFO: epoch 001:  35079 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4086, wps=100.4, ups=0.61, wpb=109.9, bsz=40, num_updates=35030, lr=3.38385e-05, gnorm=0.243, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=194975
2023-01-11 20:03:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:26 - progress_bar.py[line:274] - INFO: epoch 001:  35089 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4118, wps=96.5, ups=0.59, wpb=108.7, bsz=40, num_updates=35040, lr=3.38333e-05, gnorm=0.325, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=194992
2023-01-11 20:03:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:42 - progress_bar.py[line:274] - INFO: epoch 001:  35099 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4182, wps=99.6, ups=0.6, wpb=110, bsz=40, num_updates=35050, lr=3.38281e-05, gnorm=0.225, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=195009
2023-01-11 20:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:03:59 - progress_bar.py[line:274] - INFO: epoch 001:  35109 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4118, wps=100.3, ups=0.61, wpb=110.3, bsz=40, num_updates=35060, lr=3.38229e-05, gnorm=0.29, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=195025
2023-01-11 20:03:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:16 - progress_bar.py[line:274] - INFO: epoch 001:  35119 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5149, wps=100.7, ups=0.61, wpb=110.8, bsz=40, num_updates=35070, lr=3.38177e-05, gnorm=0.195, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195042
2023-01-11 20:04:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:33 - progress_bar.py[line:274] - INFO: epoch 001:  35129 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.383, wps=98.8, ups=0.59, wpb=110.7, bsz=40, num_updates=35080, lr=3.38125e-05, gnorm=0.267, clip=0, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=195059
2023-01-11 20:04:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:50 - progress_bar.py[line:274] - INFO: epoch 001:  35139 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4421, wps=100.5, ups=0.61, wpb=110.5, bsz=40, num_updates=35090, lr=3.38073e-05, gnorm=0.193, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195076
2023-01-11 20:04:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:06 - progress_bar.py[line:274] - INFO: epoch 001:  35149 / 100000 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5248, wps=104.8, ups=0.63, wpb=110.5, bsz=40, num_updates=35100, lr=3.38021e-05, gnorm=0.331, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=195092
2023-01-11 20:05:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:22 - progress_bar.py[line:274] - INFO: epoch 001:  35159 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4022, wps=101.2, ups=0.61, wpb=110.1, bsz=40, num_updates=35110, lr=3.37969e-05, gnorm=0.131, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=195109
2023-01-11 20:05:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:39 - progress_bar.py[line:274] - INFO: epoch 001:  35169 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4545, wps=99, ups=0.61, wpb=108.4, bsz=40, num_updates=35120, lr=3.37917e-05, gnorm=0.18, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195125
2023-01-11 20:05:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:05:56 - progress_bar.py[line:274] - INFO: epoch 001:  35179 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4495, wps=102.2, ups=0.62, wpb=109.5, bsz=40, num_updates=35130, lr=3.37865e-05, gnorm=0.191, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=195142
2023-01-11 20:05:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:11 - progress_bar.py[line:274] - INFO: epoch 001:  35189 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4851, wps=105.8, ups=0.64, wpb=109.5, bsz=40, num_updates=35140, lr=3.37812e-05, gnorm=0.212, clip=0, loss_scale=256, train_wall=15, gb_free=10.6, ema_decay=0.9999, wall=195157
2023-01-11 20:06:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:28 - progress_bar.py[line:274] - INFO: epoch 001:  35199 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4516, wps=100, ups=0.6, wpb=111.7, bsz=40, num_updates=35150, lr=3.3776e-05, gnorm=0.303, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=195175
2023-01-11 20:06:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:45 - progress_bar.py[line:274] - INFO: epoch 001:  35209 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5208, wps=99.7, ups=0.6, wpb=110.4, bsz=40, num_updates=35160, lr=3.37708e-05, gnorm=0.171, clip=0, loss_scale=256, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=195191
2023-01-11 20:06:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:06:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:02 - progress_bar.py[line:274] - INFO: epoch 001:  35219 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4519, wps=99.6, ups=0.61, wpb=109.5, bsz=40, num_updates=35170, lr=3.37656e-05, gnorm=0.432, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=195208
2023-01-11 20:07:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:19 - progress_bar.py[line:274] - INFO: epoch 001:  35229 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4, wps=97.6, ups=0.59, wpb=109.6, bsz=40, num_updates=35180, lr=3.37604e-05, gnorm=0.167, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=195225
2023-01-11 20:07:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:36 - progress_bar.py[line:274] - INFO: epoch 001:  35239 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5204, wps=101.6, ups=0.62, wpb=110.1, bsz=40, num_updates=35190, lr=3.37552e-05, gnorm=0.235, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=195242
2023-01-11 20:07:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:07:52 - progress_bar.py[line:274] - INFO: epoch 001:  35249 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4747, wps=102, ups=0.62, wpb=110.1, bsz=40, num_updates=35200, lr=3.375e-05, gnorm=0.28, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=195258
2023-01-11 20:07:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:09 - progress_bar.py[line:274] - INFO: epoch 001:  35259 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4435, wps=101.2, ups=0.61, wpb=109.9, bsz=40, num_updates=35210, lr=3.37448e-05, gnorm=0.378, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=195275
2023-01-11 20:08:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:26 - progress_bar.py[line:274] - INFO: epoch 001:  35269 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.486, wps=98.2, ups=0.6, wpb=108.7, bsz=40, num_updates=35220, lr=3.37396e-05, gnorm=0.322, clip=0, loss_scale=256, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=195292
2023-01-11 20:08:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:42 - progress_bar.py[line:274] - INFO: epoch 001:  35279 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.37, wps=101, ups=0.61, wpb=109.7, bsz=40, num_updates=35230, lr=3.37344e-05, gnorm=0.326, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=195308
2023-01-11 20:08:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:08:59 - progress_bar.py[line:274] - INFO: epoch 001:  35289 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4955, wps=98.1, ups=0.6, wpb=108.4, bsz=40, num_updates=35240, lr=3.37292e-05, gnorm=0.26, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=195325
2023-01-11 20:08:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:16 - progress_bar.py[line:274] - INFO: epoch 001:  35299 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4038, wps=100.7, ups=0.61, wpb=111, bsz=40, num_updates=35250, lr=3.3724e-05, gnorm=0.565, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195342
2023-01-11 20:09:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:33 - progress_bar.py[line:274] - INFO: epoch 001:  35309 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4902, wps=98.2, ups=0.61, wpb=107.9, bsz=40, num_updates=35260, lr=3.37187e-05, gnorm=0.158, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195359
2023-01-11 20:09:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:49 - progress_bar.py[line:274] - INFO: epoch 001:  35319 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=108, nsentences=40, sample_size=108, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.449, wps=100.3, ups=0.62, wpb=108, bsz=40, num_updates=35270, lr=3.37135e-05, gnorm=0.166, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=195375
2023-01-11 20:09:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:09:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:06 - progress_bar.py[line:274] - INFO: epoch 001:  35329 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.533, nsentences=40, sample_size=107.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4706, wps=98.8, ups=0.61, wpb=107.5, bsz=40, num_updates=35280, lr=3.37083e-05, gnorm=0.323, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=195392
2023-01-11 20:10:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:22 - progress_bar.py[line:274] - INFO: epoch 001:  35339 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3645, wps=103, ups=0.63, wpb=109, bsz=40, num_updates=35290, lr=3.37031e-05, gnorm=0.183, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=195408
2023-01-11 20:10:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:39 - progress_bar.py[line:274] - INFO: epoch 001:  35349 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4536, wps=101.8, ups=0.61, wpb=111.1, bsz=40, num_updates=35300, lr=3.36979e-05, gnorm=0.368, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=195425
2023-01-11 20:10:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:10:55 - progress_bar.py[line:274] - INFO: epoch 001:  35359 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=101.5, ups=0.61, wpb=111.8, bsz=40, num_updates=35310, lr=3.36927e-05, gnorm=0.349, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=195441
2023-01-11 20:10:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:12 - progress_bar.py[line:274] - INFO: epoch 001:  35369 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4444, wps=101.1, ups=0.61, wpb=110.6, bsz=40, num_updates=35320, lr=3.36875e-05, gnorm=0.396, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195458
2023-01-11 20:11:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:29 - progress_bar.py[line:274] - INFO: epoch 001:  35379 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4362, wps=100.9, ups=0.62, wpb=109.1, bsz=40, num_updates=35330, lr=3.36823e-05, gnorm=0.198, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=195475
2023-01-11 20:11:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:45 - progress_bar.py[line:274] - INFO: epoch 001:  35389 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4951, wps=100.9, ups=0.62, wpb=109.2, bsz=40, num_updates=35340, lr=3.36771e-05, gnorm=0.404, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=195491
2023-01-11 20:11:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:11:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:01 - progress_bar.py[line:274] - INFO: epoch 001:  35399 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4587, wps=104, ups=0.63, wpb=109.9, bsz=40, num_updates=35350, lr=3.36719e-05, gnorm=1.591, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=195507
2023-01-11 20:12:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:18 - progress_bar.py[line:274] - INFO: epoch 001:  35409 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4842, wps=101.8, ups=0.61, wpb=111.5, bsz=40, num_updates=35360, lr=3.36667e-05, gnorm=0.378, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195524
2023-01-11 20:12:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:35 - progress_bar.py[line:274] - INFO: epoch 001:  35419 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4455, wps=98.1, ups=0.6, wpb=109.7, bsz=40, num_updates=35370, lr=3.36615e-05, gnorm=0.472, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=195541
2023-01-11 20:12:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:39 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-11 20:12:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:12:54 - progress_bar.py[line:274] - INFO: epoch 001:  35430 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4017, wps=97.4, ups=0.55, wpb=111, bsz=40, num_updates=35380, lr=3.36563e-05, gnorm=5.45, clip=20, loss_scale=128, train_wall=18, gb_free=10.4, ema_decay=0.9999, wall=195560
2023-01-11 20:13:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:11 - progress_bar.py[line:274] - INFO: epoch 001:  35440 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3191, wps=102.5, ups=0.62, wpb=110, bsz=40, num_updates=35390, lr=3.3651e-05, gnorm=1.625, clip=30, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195577
2023-01-11 20:13:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:27 - progress_bar.py[line:274] - INFO: epoch 001:  35450 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4314, wps=102.2, ups=0.62, wpb=109.7, bsz=40, num_updates=35400, lr=3.36458e-05, gnorm=0.608, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=195593
2023-01-11 20:13:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:44 - progress_bar.py[line:274] - INFO: epoch 001:  35460 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3776, wps=100.9, ups=0.61, wpb=110.2, bsz=40, num_updates=35410, lr=3.36406e-05, gnorm=0.132, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=195610
2023-01-11 20:13:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:13:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:00 - progress_bar.py[line:274] - INFO: epoch 001:  35470 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4433, wps=101.8, ups=0.62, wpb=109.9, bsz=40, num_updates=35420, lr=3.36354e-05, gnorm=0.302, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=195626
2023-01-11 20:14:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:17 - progress_bar.py[line:274] - INFO: epoch 001:  35480 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4421, wps=101, ups=0.61, wpb=110.5, bsz=40, num_updates=35430, lr=3.36302e-05, gnorm=0.344, clip=0, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=195643
2023-01-11 20:14:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:33 - progress_bar.py[line:274] - INFO: epoch 001:  35490 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4272, wps=103.3, ups=0.62, wpb=110.5, bsz=40, num_updates=35440, lr=3.3625e-05, gnorm=0.437, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=195659
2023-01-11 20:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:50 - progress_bar.py[line:274] - INFO: epoch 001:  35500 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5051, wps=99.5, ups=0.6, wpb=110.5, bsz=40, num_updates=35450, lr=3.36198e-05, gnorm=0.437, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=195676
2023-01-11 20:14:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:14:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:07 - progress_bar.py[line:274] - INFO: epoch 001:  35510 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5172, wps=100.3, ups=0.6, wpb=111.3, bsz=40, num_updates=35460, lr=3.36146e-05, gnorm=0.478, clip=10, loss_scale=128, train_wall=17, gb_free=10, ema_decay=0.9999, wall=195693
2023-01-11 20:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:23 - progress_bar.py[line:274] - INFO: epoch 001:  35520 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4245, wps=99.7, ups=0.61, wpb=109, bsz=40, num_updates=35470, lr=3.36094e-05, gnorm=0.67, clip=20, loss_scale=128, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=195710
2023-01-11 20:15:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:40 - progress_bar.py[line:274] - INFO: epoch 001:  35530 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=101.3, ups=0.62, wpb=109.5, bsz=40, num_updates=35480, lr=3.36042e-05, gnorm=0.603, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195726
2023-01-11 20:15:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:15:57 - progress_bar.py[line:274] - INFO: epoch 001:  35540 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4545, wps=97.4, ups=0.59, wpb=110.9, bsz=40, num_updates=35490, lr=3.3599e-05, gnorm=0.733, clip=20, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=195743
2023-01-11 20:16:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:14 - progress_bar.py[line:274] - INFO: epoch 001:  35550 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.451, wps=99.3, ups=0.6, wpb=109.9, bsz=40, num_updates=35500, lr=3.35938e-05, gnorm=1.727, clip=10, loss_scale=128, train_wall=17, gb_free=9.8, ema_decay=0.9999, wall=195760
2023-01-11 20:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:31 - progress_bar.py[line:274] - INFO: epoch 001:  35560 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5567, wps=98.6, ups=0.61, wpb=108.1, bsz=40, num_updates=35510, lr=3.35885e-05, gnorm=0.252, clip=0, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=195777
2023-01-11 20:16:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:47 - progress_bar.py[line:274] - INFO: epoch 001:  35570 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3725, wps=98.2, ups=0.6, wpb=108.3, bsz=40, num_updates=35520, lr=3.35833e-05, gnorm=0.249, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=195794
2023-01-11 20:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:16:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:04 - progress_bar.py[line:274] - INFO: epoch 001:  35580 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4388, wps=101.1, ups=0.61, wpb=110.2, bsz=40, num_updates=35530, lr=3.35781e-05, gnorm=0.159, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195810
2023-01-11 20:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:21 - progress_bar.py[line:274] - INFO: epoch 001:  35590 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5109, wps=100.9, ups=0.61, wpb=110.3, bsz=40, num_updates=35540, lr=3.35729e-05, gnorm=0.589, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195827
2023-01-11 20:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:37 - progress_bar.py[line:274] - INFO: epoch 001:  35600 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4595, wps=99.8, ups=0.6, wpb=110.2, bsz=40, num_updates=35550, lr=3.35677e-05, gnorm=0.227, clip=0, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=195844
2023-01-11 20:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:17:54 - progress_bar.py[line:274] - INFO: epoch 001:  35610 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4211, wps=102.8, ups=0.62, wpb=111, bsz=40, num_updates=35560, lr=3.35625e-05, gnorm=1.596, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195860
2023-01-11 20:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:11 - progress_bar.py[line:274] - INFO: epoch 001:  35620 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.419, wps=100.4, ups=0.61, wpb=110.1, bsz=40, num_updates=35570, lr=3.35573e-05, gnorm=0.506, clip=10, loss_scale=128, train_wall=16, gb_free=10, ema_decay=0.9999, wall=195877
2023-01-11 20:18:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:28 - progress_bar.py[line:274] - INFO: epoch 001:  35630 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4316, wps=97.8, ups=0.59, wpb=110, bsz=40, num_updates=35580, lr=3.35521e-05, gnorm=0.207, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=195894
2023-01-11 20:18:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:44 - progress_bar.py[line:274] - INFO: epoch 001:  35640 / 100000 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=112.267, nsentences=40, sample_size=112.267, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4938, wps=101.5, ups=0.6, wpb=112.3, bsz=40, num_updates=35590, lr=3.35469e-05, gnorm=0.288, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=195911
2023-01-11 20:18:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:18:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:01 - progress_bar.py[line:274] - INFO: epoch 001:  35650 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3725, wps=100.9, ups=0.61, wpb=109.4, bsz=40, num_updates=35600, lr=3.35417e-05, gnorm=0.478, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195927
2023-01-11 20:19:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:17 - progress_bar.py[line:274] - INFO: epoch 001:  35660 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4, wps=101.1, ups=0.62, wpb=109.3, bsz=40, num_updates=35610, lr=3.35365e-05, gnorm=0.227, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=195944
2023-01-11 20:19:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:34 - progress_bar.py[line:274] - INFO: epoch 001:  35670 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4151, wps=100.9, ups=0.61, wpb=109.7, bsz=40, num_updates=35620, lr=3.35313e-05, gnorm=0.972, clip=20, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=195960
2023-01-11 20:19:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:51 - progress_bar.py[line:274] - INFO: epoch 001:  35680 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3936, wps=99.5, ups=0.6, wpb=110.2, bsz=40, num_updates=35630, lr=3.3526e-05, gnorm=0.605, clip=20, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=195977
2023-01-11 20:19:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:19:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:07 - progress_bar.py[line:274] - INFO: epoch 001:  35690 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=100.7, ups=0.62, wpb=109.1, bsz=40, num_updates=35640, lr=3.35208e-05, gnorm=0.2, clip=0, loss_scale=128, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=195994
2023-01-11 20:20:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:24 - progress_bar.py[line:274] - INFO: epoch 001:  35700 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3218, wps=100.9, ups=0.61, wpb=111, bsz=40, num_updates=35650, lr=3.35156e-05, gnorm=0.38, clip=0, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=196010
2023-01-11 20:20:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:41 - progress_bar.py[line:274] - INFO: epoch 001:  35710 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4719, wps=100.1, ups=0.6, wpb=111.7, bsz=40, num_updates=35660, lr=3.35104e-05, gnorm=0.366, clip=10, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=196027
2023-01-11 20:20:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:20:58 - progress_bar.py[line:274] - INFO: epoch 001:  35720 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5288, wps=101.1, ups=0.61, wpb=109.8, bsz=40, num_updates=35670, lr=3.35052e-05, gnorm=0.238, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=196044
2023-01-11 20:21:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:14 - progress_bar.py[line:274] - INFO: epoch 001:  35730 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4579, wps=100.9, ups=0.62, wpb=109.4, bsz=40, num_updates=35680, lr=3.35e-05, gnorm=0.276, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=196060
2023-01-11 20:21:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:31 - progress_bar.py[line:274] - INFO: epoch 001:  35740 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5421, wps=102.6, ups=0.62, wpb=110.2, bsz=40, num_updates=35690, lr=3.34948e-05, gnorm=0.14, clip=0, loss_scale=128, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=196077
2023-01-11 20:21:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:47 - progress_bar.py[line:274] - INFO: epoch 001:  35750 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4381, wps=99.4, ups=0.61, wpb=108.5, bsz=40, num_updates=35700, lr=3.34896e-05, gnorm=0.818, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=196093
2023-01-11 20:21:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:21:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:04 - progress_bar.py[line:274] - INFO: epoch 001:  35760 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.375, wps=97.7, ups=0.59, wpb=110.3, bsz=40, num_updates=35710, lr=3.34844e-05, gnorm=0.214, clip=0, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=196111
2023-01-11 20:22:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:21 - progress_bar.py[line:274] - INFO: epoch 001:  35770 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.451, wps=97.3, ups=0.6, wpb=108.3, bsz=40, num_updates=35720, lr=3.34792e-05, gnorm=0.166, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=196127
2023-01-11 20:22:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:38 - progress_bar.py[line:274] - INFO: epoch 001:  35780 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3895, wps=101.4, ups=0.62, wpb=109.9, bsz=40, num_updates=35730, lr=3.3474e-05, gnorm=0.457, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=196144
2023-01-11 20:22:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:22:54 - progress_bar.py[line:274] - INFO: epoch 001:  35790 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=107.133, nsentences=40, sample_size=107.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4701, wps=98.8, ups=0.62, wpb=107.1, bsz=40, num_updates=35740, lr=3.34688e-05, gnorm=0.351, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=196161
2023-01-11 20:23:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:11 - progress_bar.py[line:274] - INFO: epoch 001:  35800 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4694, wps=104.3, ups=0.63, wpb=110.1, bsz=40, num_updates=35750, lr=3.34635e-05, gnorm=0.411, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=196177
2023-01-11 20:23:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:27 - progress_bar.py[line:274] - INFO: epoch 001:  35810 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4433, wps=101.7, ups=0.61, wpb=110.9, bsz=40, num_updates=35760, lr=3.34583e-05, gnorm=0.222, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=196194
2023-01-11 20:23:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:44 - progress_bar.py[line:274] - INFO: epoch 001:  35820 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4651, wps=99.9, ups=0.61, wpb=109.5, bsz=40, num_updates=35770, lr=3.34531e-05, gnorm=0.455, clip=20, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=196210
2023-01-11 20:23:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:23:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:01 - progress_bar.py[line:274] - INFO: epoch 001:  35830 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3605, wps=100.4, ups=0.6, wpb=110.7, bsz=40, num_updates=35780, lr=3.34479e-05, gnorm=0.882, clip=30, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=196227
2023-01-11 20:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:17 - progress_bar.py[line:274] - INFO: epoch 001:  35840 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.402, wps=103.3, ups=0.63, wpb=110.1, bsz=40, num_updates=35790, lr=3.34427e-05, gnorm=0.412, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=196243
2023-01-11 20:24:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:34 - progress_bar.py[line:274] - INFO: epoch 001:  35850 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.449, wps=102.4, ups=0.62, wpb=110.7, bsz=40, num_updates=35800, lr=3.34375e-05, gnorm=1.057, clip=40, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=196260
2023-01-11 20:24:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:50 - progress_bar.py[line:274] - INFO: epoch 001:  35860 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3918, wps=97.6, ups=0.6, wpb=108.2, bsz=40, num_updates=35810, lr=3.34323e-05, gnorm=0.476, clip=10, loss_scale=128, train_wall=17, gb_free=10, ema_decay=0.9999, wall=196277
2023-01-11 20:24:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:24:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:07 - progress_bar.py[line:274] - INFO: epoch 001:  35870 / 100000 loss=0.32, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3922, wps=101, ups=0.61, wpb=110, bsz=40, num_updates=35820, lr=3.34271e-05, gnorm=0.802, clip=30, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=196293
2023-01-11 20:25:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:24 - progress_bar.py[line:274] - INFO: epoch 001:  35880 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4848, wps=98.8, ups=0.6, wpb=109.3, bsz=40, num_updates=35830, lr=3.34219e-05, gnorm=0.3, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=196310
2023-01-11 20:25:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:41 - progress_bar.py[line:274] - INFO: epoch 001:  35890 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4141, wps=100.3, ups=0.61, wpb=110.3, bsz=40, num_updates=35840, lr=3.34167e-05, gnorm=0.562, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=196327
2023-01-11 20:25:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:25:58 - progress_bar.py[line:274] - INFO: epoch 001:  35900 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4479, wps=98.5, ups=0.6, wpb=109.4, bsz=40, num_updates=35850, lr=3.34115e-05, gnorm=0.215, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=196344
2023-01-11 20:26:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:14 - progress_bar.py[line:274] - INFO: epoch 001:  35910 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4078, wps=98.7, ups=0.61, wpb=108.6, bsz=40, num_updates=35860, lr=3.34063e-05, gnorm=0.228, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=196361
2023-01-11 20:26:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:31 - progress_bar.py[line:274] - INFO: epoch 001:  35920 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.381, wps=98.4, ups=0.6, wpb=109.4, bsz=40, num_updates=35870, lr=3.3401e-05, gnorm=0.231, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=196378
2023-01-11 20:26:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:48 - progress_bar.py[line:274] - INFO: epoch 001:  35930 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4091, wps=98.8, ups=0.6, wpb=109.1, bsz=40, num_updates=35880, lr=3.33958e-05, gnorm=0.255, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=196394
2023-01-11 20:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:26:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:05 - progress_bar.py[line:274] - INFO: epoch 001:  35940 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4536, wps=101.1, ups=0.62, wpb=109.5, bsz=40, num_updates=35890, lr=3.33906e-05, gnorm=0.382, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=196411
2023-01-11 20:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:22 - progress_bar.py[line:274] - INFO: epoch 001:  35950 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4333, wps=101.4, ups=0.61, wpb=110.5, bsz=40, num_updates=35900, lr=3.33854e-05, gnorm=0.476, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=196427
2023-01-11 20:27:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:38 - progress_bar.py[line:274] - INFO: epoch 001:  35960 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5047, wps=99.4, ups=0.61, wpb=108.3, bsz=40, num_updates=35910, lr=3.33802e-05, gnorm=0.223, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=196444
2023-01-11 20:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:27:54 - progress_bar.py[line:274] - INFO: epoch 001:  35970 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3956, wps=103.7, ups=0.62, wpb=111.5, bsz=40, num_updates=35920, lr=3.3375e-05, gnorm=0.375, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=196461
2023-01-11 20:28:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:11 - progress_bar.py[line:274] - INFO: epoch 001:  35980 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5208, wps=100.6, ups=0.61, wpb=109.9, bsz=40, num_updates=35930, lr=3.33698e-05, gnorm=0.381, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=196477
2023-01-11 20:28:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:28 - progress_bar.py[line:274] - INFO: epoch 001:  35990 / 100000 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4894, wps=103.2, ups=0.62, wpb=111.2, bsz=40, num_updates=35940, lr=3.33646e-05, gnorm=0.441, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=196494
2023-01-11 20:28:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:44 - progress_bar.py[line:274] - INFO: epoch 001:  36000 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.47, wps=99.5, ups=0.6, wpb=110.3, bsz=40, num_updates=35950, lr=3.33594e-05, gnorm=0.251, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=196511
2023-01-11 20:28:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:28:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:01 - progress_bar.py[line:274] - INFO: epoch 001:  36010 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3846, wps=99.8, ups=0.61, wpb=109.5, bsz=40, num_updates=35960, lr=3.33542e-05, gnorm=0.373, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=196527
2023-01-11 20:29:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:17 - progress_bar.py[line:274] - INFO: epoch 001:  36020 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4624, wps=102.7, ups=0.62, wpb=109.9, bsz=40, num_updates=35970, lr=3.3349e-05, gnorm=0.38, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=196544
2023-01-11 20:29:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:34 - progress_bar.py[line:274] - INFO: epoch 001:  36030 / 100000 loss=0.313, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4196, wps=98.7, ups=0.6, wpb=109.3, bsz=40, num_updates=35980, lr=3.33438e-05, gnorm=0.555, clip=20, loss_scale=256, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=196561
2023-01-11 20:29:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:51 - progress_bar.py[line:274] - INFO: epoch 001:  36040 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4554, wps=98.7, ups=0.6, wpb=110.5, bsz=40, num_updates=35990, lr=3.33385e-05, gnorm=0.253, clip=0, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=196578
2023-01-11 20:29:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:29:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:30:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:30:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:30:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 20:30:08 - progress_bar.py[line:274] - INFO: epoch 001:  36050 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4624, wps=101.8, ups=0.62, wpb=109.5, bsz=40, num_updates=36000, lr=3.33333e-05, gnorm=0.235, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=196594
2023-01-11 20:30:08 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 20:30:09 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 20:30:09 - train.py[line:551] - INFO: load:1.31 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 20:32:41 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 20:32:41 - train.py[line:551] - INFO: load:1.34 valid_run:151.86 task_valid:149.26 collect_output:1.48
2023-01-11 20:35:10 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 20:35:10 - train.py[line:551] - INFO: load:1.36 valid_run:300.08 task_valid:293.59 collect_output:4.27
2023-01-11 20:37:42 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 20:37:42 - train.py[line:551] - INFO: load:1.39 valid_run:452.25 task_valid:438.07 collect_output:10.82
2023-01-11 20:40:12 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 20:40:12 - train.py[line:551] - INFO: load:1.42 valid_run:602.52 task_valid:584.97 collect_output:13.04
2023-01-11 20:42:46 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 20:42:46 - train.py[line:551] - INFO: load:1.45 valid_run:755.98 task_valid:735.06 collect_output:15.21
2023-01-11 20:45:17 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 20:45:17 - train.py[line:551] - INFO: load:1.47 valid_run:907.40 task_valid:881.20 collect_output:19.40
2023-01-11 20:47:50 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 20:47:50 - train.py[line:551] - INFO: load:1.50 valid_run:1060.08 task_valid:1028.34 collect_output:23.80
2023-01-11 20:50:21 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 20:50:21 - train.py[line:551] - INFO: load:1.53 valid_run:1210.28 task_valid:1170.60 collect_output:30.62
2023-01-11 20:52:50 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 20:52:50 - train.py[line:551] - INFO: load:1.55 valid_run:1359.89 task_valid:1316.47 collect_output:33.23
2023-01-11 20:55:19 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 20:55:19 - train.py[line:551] - INFO: load:1.58 valid_run:1508.76 task_valid:1461.04 collect_output:36.43
2023-01-11 20:57:49 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 20:57:49 - train.py[line:551] - INFO: load:1.60 valid_run:1658.79 task_valid:1606.95 collect_output:39.47
2023-01-11 21:00:20 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 21:00:20 - train.py[line:551] - INFO: load:1.63 valid_run:1809.34 task_valid:1753.56 collect_output:42.25
2023-01-11 21:02:49 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 21:02:49 - train.py[line:551] - INFO: load:1.65 valid_run:1958.77 task_valid:1896.17 collect_output:47.98
2023-01-11 21:05:20 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 21:05:20 - train.py[line:551] - INFO: load:1.68 valid_run:2109.19 task_valid:2042.49 collect_output:51.02
2023-01-11 21:07:51 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 21:07:51 - train.py[line:551] - INFO: load:1.71 valid_run:2259.96 task_valid:2190.32 collect_output:52.84
2023-01-11 21:10:21 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 21:10:21 - train.py[line:551] - INFO: load:1.73 valid_run:2409.81 task_valid:2335.40 collect_output:56.56
2023-01-11 21:12:52 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 21:12:52 - train.py[line:551] - INFO: load:1.76 valid_run:2560.88 task_valid:2481.92 collect_output:60.00
2023-01-11 21:15:23 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 21:15:23 - train.py[line:551] - INFO: load:1.79 valid_run:2711.64 task_valid:2629.95 collect_output:61.64
2023-01-11 21:17:51 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 21:17:51 - train.py[line:551] - INFO: load:1.81 valid_run:2859.76 task_valid:2772.75 collect_output:65.83
2023-01-11 21:20:21 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 21:20:21 - train.py[line:551] - INFO: load:1.84 valid_run:3009.78 task_valid:2918.87 collect_output:68.63
2023-01-11 21:22:52 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 21:22:52 - train.py[line:551] - INFO: load:1.86 valid_run:3160.86 task_valid:3064.62 collect_output:72.86
2023-01-11 21:25:21 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 21:25:21 - train.py[line:551] - INFO: load:1.89 valid_run:3309.87 task_valid:3209.93 collect_output:75.52
2023-01-11 21:27:53 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 21:27:53 - train.py[line:551] - INFO: load:1.92 valid_run:3461.11 task_valid:3357.12 collect_output:78.48
2023-01-11 21:30:24 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 21:30:24 - train.py[line:551] - INFO: load:1.94 valid_run:3612.14 task_valid:3504.29 collect_output:81.28

====================================================================================================
SGG eval:     R @ 50: 0.4280;     R @ 100: 0.4952;     R @ 500: 0.5218;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2650;    mR @ 100: 0.3203;    mR @ 500: 0.3528;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6659) (covered in:0.6458) (covering:0.3714) (eating:0.5882) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3065) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.5245) (says:0.0000) (sitting on:0.7324) (standing on:0.1927) (using:0.6500) (walking in:0.0000) (walking on:0.2883) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4280;     R @ 100: 0.4952;     R @ 500: 0.5218;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2650;    mR @ 100: 0.3203;    mR @ 500: 0.3528;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6659) (covered in:0.6458) (covering:0.3714) (eating:0.5882) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3065) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.5245) (says:0.0000) (sitting on:0.7324) (standing on:0.1927) (using:0.6500) (walking in:0.0000) (walking on:0.2883) (watching:0.2222) 
--------------------------------------------------------
====================================================================================================

2023-01-11 21:32:55 - train.py[line:487] - INFO: 0.49524817927170867
2023-01-11 21:32:55 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 21:32:56 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.369 | loss_v1 0 | loss_v2 0 | nll_loss 0.217 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.495248 | ppl 1.16 | vqa_score 0.4088 | wps 119.1 | wpb 89.9 | bsz 30 | num_updates 36000 | best_R@100 0.69005
2023-01-11 21:32:56 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 36000 updates
2023-01-11 21:32:56 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_36000.pt
2023-01-11 21:33:47 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_36000.pt
2023-01-11 21:35:39 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_36000.pt (epoch 1 @ 36000 updates, score 0.49524817927170867) (writing took 163.5733721666038 seconds)
2023-01-11 21:35:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:35:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:35:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:35:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:35:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:35:56 - progress_bar.py[line:274] - INFO: epoch 001:  36060 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4141, wps=0.4, ups=0, wpb=110.2, bsz=40, num_updates=36010, lr=3.33281e-05, gnorm=0.747, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=200542
2023-01-11 21:36:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:13 - progress_bar.py[line:274] - INFO: epoch 001:  36070 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4466, wps=100.1, ups=0.61, wpb=109, bsz=40, num_updates=36020, lr=3.33229e-05, gnorm=0.269, clip=0, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=200559
2023-01-11 21:36:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:29 - progress_bar.py[line:274] - INFO: epoch 001:  36080 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4845, wps=102, ups=0.62, wpb=110, bsz=40, num_updates=36030, lr=3.33177e-05, gnorm=0.247, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=200575
2023-01-11 21:36:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:45 - progress_bar.py[line:274] - INFO: epoch 001:  36090 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4804, wps=100.3, ups=0.62, wpb=108.5, bsz=40, num_updates=36040, lr=3.33125e-05, gnorm=0.175, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=200592
2023-01-11 21:36:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:36:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:02 - progress_bar.py[line:274] - INFO: epoch 001:  36100 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4737, wps=103.5, ups=0.63, wpb=109.9, bsz=40, num_updates=36050, lr=3.33073e-05, gnorm=0.814, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=200608
2023-01-11 21:37:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:18 - progress_bar.py[line:274] - INFO: epoch 001:  36110 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.43, wps=100.8, ups=0.61, wpb=110.5, bsz=40, num_updates=36060, lr=3.33021e-05, gnorm=0.178, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=200625
2023-01-11 21:37:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:35 - progress_bar.py[line:274] - INFO: epoch 001:  36120 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4167, wps=97.9, ups=0.59, wpb=109.7, bsz=40, num_updates=36070, lr=3.32969e-05, gnorm=0.181, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=200642
2023-01-11 21:37:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:37:52 - progress_bar.py[line:274] - INFO: epoch 001:  36130 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4455, wps=99.8, ups=0.6, wpb=110.7, bsz=40, num_updates=36080, lr=3.32917e-05, gnorm=0.491, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=200659
2023-01-11 21:37:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:09 - progress_bar.py[line:274] - INFO: epoch 001:  36140 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4468, wps=100.2, ups=0.61, wpb=109.8, bsz=40, num_updates=36090, lr=3.32865e-05, gnorm=1.965, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=200675
2023-01-11 21:38:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:26 - progress_bar.py[line:274] - INFO: epoch 001:  36150 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4388, wps=98.5, ups=0.6, wpb=110, bsz=40, num_updates=36100, lr=3.32813e-05, gnorm=1.102, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=200692
2023-01-11 21:38:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:42 - progress_bar.py[line:274] - INFO: epoch 001:  36160 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4222, wps=103.3, ups=0.62, wpb=111, bsz=40, num_updates=36110, lr=3.3276e-05, gnorm=0.287, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=200709
2023-01-11 21:38:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:38:59 - progress_bar.py[line:274] - INFO: epoch 001:  36170 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3878, wps=101.3, ups=0.61, wpb=109.9, bsz=40, num_updates=36120, lr=3.32708e-05, gnorm=0.377, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=200725
2023-01-11 21:39:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:15 - progress_bar.py[line:274] - INFO: epoch 001:  36180 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=112.067, nsentences=40, sample_size=112.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4412, wps=104, ups=0.62, wpb=112.1, bsz=40, num_updates=36130, lr=3.32656e-05, gnorm=1.707, clip=30, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=200741
2023-01-11 21:39:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:32 - progress_bar.py[line:274] - INFO: epoch 001:  36190 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.45, wps=102.1, ups=0.62, wpb=110.1, bsz=40, num_updates=36140, lr=3.32604e-05, gnorm=0.251, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=200758
2023-01-11 21:39:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:48 - progress_bar.py[line:274] - INFO: epoch 001:  36200 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4778, wps=102.6, ups=0.62, wpb=110.3, bsz=40, num_updates=36150, lr=3.32552e-05, gnorm=0.25, clip=0, loss_scale=256, train_wall=16, gb_free=9.8, ema_decay=0.9999, wall=200774
2023-01-11 21:39:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:39:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:05 - progress_bar.py[line:274] - INFO: epoch 001:  36210 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4388, wps=101.7, ups=0.61, wpb=111.6, bsz=40, num_updates=36160, lr=3.325e-05, gnorm=0.316, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=200791
2023-01-11 21:40:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:21 - progress_bar.py[line:274] - INFO: epoch 001:  36220 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4522, wps=100.4, ups=0.62, wpb=108.5, bsz=40, num_updates=36170, lr=3.32448e-05, gnorm=0.188, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=200807
2023-01-11 21:40:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:38 - progress_bar.py[line:274] - INFO: epoch 001:  36230 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4352, wps=97.1, ups=0.6, wpb=108.2, bsz=40, num_updates=36180, lr=3.32396e-05, gnorm=0.331, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=200824
2023-01-11 21:40:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:40:55 - progress_bar.py[line:274] - INFO: epoch 001:  36240 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5435, wps=100.1, ups=0.61, wpb=109.4, bsz=40, num_updates=36190, lr=3.32344e-05, gnorm=0.383, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=200841
2023-01-11 21:41:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:11 - progress_bar.py[line:274] - INFO: epoch 001:  36250 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3922, wps=101.9, ups=0.62, wpb=109.7, bsz=40, num_updates=36200, lr=3.32292e-05, gnorm=0.483, clip=10, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=200858
2023-01-11 21:41:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:28 - progress_bar.py[line:274] - INFO: epoch 001:  36260 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4242, wps=100.4, ups=0.61, wpb=109.7, bsz=40, num_updates=36210, lr=3.3224e-05, gnorm=0.669, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=200874
2023-01-11 21:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:45 - progress_bar.py[line:274] - INFO: epoch 001:  36270 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4434, wps=100, ups=0.61, wpb=108.7, bsz=40, num_updates=36220, lr=3.32187e-05, gnorm=0.205, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=200891
2023-01-11 21:41:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:41:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:01 - progress_bar.py[line:274] - INFO: epoch 001:  36280 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5234, wps=101.2, ups=0.61, wpb=109.8, bsz=40, num_updates=36230, lr=3.32135e-05, gnorm=0.345, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=200907
2023-01-11 21:42:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:18 - progress_bar.py[line:274] - INFO: epoch 001:  36290 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5625, wps=98.3, ups=0.6, wpb=109.3, bsz=40, num_updates=36240, lr=3.32083e-05, gnorm=1.921, clip=30, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=200924
2023-01-11 21:42:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:35 - progress_bar.py[line:274] - INFO: epoch 001:  36300 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4141, wps=101.3, ups=0.61, wpb=110.2, bsz=40, num_updates=36250, lr=3.32031e-05, gnorm=0.756, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=200941
2023-01-11 21:42:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:51 - progress_bar.py[line:274] - INFO: epoch 001:  36310 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4211, wps=103.1, ups=0.62, wpb=110.5, bsz=40, num_updates=36260, lr=3.31979e-05, gnorm=0.192, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=200957
2023-01-11 21:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:42:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:07 - progress_bar.py[line:274] - INFO: epoch 001:  36320 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4271, wps=101.3, ups=0.62, wpb=109, bsz=40, num_updates=36270, lr=3.31927e-05, gnorm=0.431, clip=10, loss_scale=256, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=200973
2023-01-11 21:43:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:24 - progress_bar.py[line:274] - INFO: epoch 001:  36330 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4231, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=36280, lr=3.31875e-05, gnorm=0.242, clip=0, loss_scale=256, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=200990
2023-01-11 21:43:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:40 - progress_bar.py[line:274] - INFO: epoch 001:  36340 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4842, wps=102.9, ups=0.62, wpb=110.2, bsz=40, num_updates=36290, lr=3.31823e-05, gnorm=0.62, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201006
2023-01-11 21:43:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:43:57 - progress_bar.py[line:274] - INFO: epoch 001:  36350 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4639, wps=101.5, ups=0.61, wpb=110.3, bsz=40, num_updates=36300, lr=3.31771e-05, gnorm=0.616, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201023
2023-01-11 21:44:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:13 - progress_bar.py[line:274] - INFO: epoch 001:  36360 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4557, wps=101.7, ups=0.61, wpb=111, bsz=40, num_updates=36310, lr=3.31719e-05, gnorm=0.344, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=201039
2023-01-11 21:44:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:30 - progress_bar.py[line:274] - INFO: epoch 001:  36370 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4369, wps=102.1, ups=0.62, wpb=110.7, bsz=40, num_updates=36320, lr=3.31667e-05, gnorm=0.272, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201056
2023-01-11 21:44:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:46 - progress_bar.py[line:274] - INFO: epoch 001:  36380 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4851, wps=99.2, ups=0.61, wpb=108.8, bsz=40, num_updates=36330, lr=3.31615e-05, gnorm=0.476, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201073
2023-01-11 21:44:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:44:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:03 - progress_bar.py[line:274] - INFO: epoch 001:  36390 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4862, wps=99.7, ups=0.61, wpb=109.1, bsz=40, num_updates=36340, lr=3.31563e-05, gnorm=0.304, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201089
2023-01-11 21:45:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:20 - progress_bar.py[line:274] - INFO: epoch 001:  36400 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4082, wps=101.3, ups=0.61, wpb=109.9, bsz=40, num_updates=36350, lr=3.3151e-05, gnorm=0.496, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=201106
2023-01-11 21:45:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:36 - progress_bar.py[line:274] - INFO: epoch 001:  36410 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.42, wps=101.3, ups=0.61, wpb=110.3, bsz=40, num_updates=36360, lr=3.31458e-05, gnorm=0.759, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201122
2023-01-11 21:45:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:45:52 - progress_bar.py[line:274] - INFO: epoch 001:  36420 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4742, wps=102.4, ups=0.62, wpb=109.8, bsz=40, num_updates=36370, lr=3.31406e-05, gnorm=0.477, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=201139
2023-01-11 21:45:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:09 - progress_bar.py[line:274] - INFO: epoch 001:  36430 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4688, wps=101.4, ups=0.61, wpb=110.3, bsz=40, num_updates=36380, lr=3.31354e-05, gnorm=0.231, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201155
2023-01-11 21:46:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:26 - progress_bar.py[line:274] - INFO: epoch 001:  36440 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4386, wps=98.8, ups=0.6, wpb=109.2, bsz=40, num_updates=36390, lr=3.31302e-05, gnorm=0.232, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=201172
2023-01-11 21:46:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:43 - progress_bar.py[line:274] - INFO: epoch 001:  36450 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4947, wps=99.1, ups=0.61, wpb=109.1, bsz=40, num_updates=36400, lr=3.3125e-05, gnorm=0.4, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=201189
2023-01-11 21:46:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:46:59 - progress_bar.py[line:274] - INFO: epoch 001:  36460 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5106, wps=100.8, ups=0.61, wpb=109.4, bsz=40, num_updates=36410, lr=3.31198e-05, gnorm=0.44, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201205
2023-01-11 21:47:00 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 21:47:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:18 - progress_bar.py[line:274] - INFO: epoch 001:  36471 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4944, wps=90.9, ups=0.54, wpb=111.3, bsz=40, num_updates=36420, lr=3.31146e-05, gnorm=0.341, clip=10, loss_scale=256, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=201224
2023-01-11 21:47:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:35 - progress_bar.py[line:274] - INFO: epoch 001:  36481 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4257, wps=101, ups=0.61, wpb=109.6, bsz=40, num_updates=36430, lr=3.31094e-05, gnorm=0.315, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=201241
2023-01-11 21:47:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:51 - progress_bar.py[line:274] - INFO: epoch 001:  36491 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3962, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=36440, lr=3.31042e-05, gnorm=0.347, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201257
2023-01-11 21:47:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:47:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:08 - progress_bar.py[line:274] - INFO: epoch 001:  36501 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4, wps=102.7, ups=0.62, wpb=110.4, bsz=40, num_updates=36450, lr=3.3099e-05, gnorm=0.146, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=201274
2023-01-11 21:48:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:24 - progress_bar.py[line:274] - INFO: epoch 001:  36511 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.494, wps=101.7, ups=0.62, wpb=110, bsz=40, num_updates=36460, lr=3.30938e-05, gnorm=0.545, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201290
2023-01-11 21:48:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:40 - progress_bar.py[line:274] - INFO: epoch 001:  36521 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3611, wps=104.4, ups=0.63, wpb=110.1, bsz=40, num_updates=36470, lr=3.30885e-05, gnorm=0.458, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201306
2023-01-11 21:48:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:48:56 - progress_bar.py[line:274] - INFO: epoch 001:  36531 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4681, wps=103.2, ups=0.62, wpb=110.6, bsz=40, num_updates=36480, lr=3.30833e-05, gnorm=0.377, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=201323
2023-01-11 21:49:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:13 - progress_bar.py[line:274] - INFO: epoch 001:  36541 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4762, wps=100.3, ups=0.61, wpb=109.9, bsz=40, num_updates=36490, lr=3.30781e-05, gnorm=0.279, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201339
2023-01-11 21:49:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:30 - progress_bar.py[line:274] - INFO: epoch 001:  36551 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4082, wps=97.4, ups=0.6, wpb=108.7, bsz=40, num_updates=36500, lr=3.30729e-05, gnorm=0.38, clip=10, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=201356
2023-01-11 21:49:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:47 - progress_bar.py[line:274] - INFO: epoch 001:  36561 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4468, wps=99.8, ups=0.6, wpb=110.2, bsz=40, num_updates=36510, lr=3.30677e-05, gnorm=0.266, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=201373
2023-01-11 21:49:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:49:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:04 - progress_bar.py[line:274] - INFO: epoch 001:  36571 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3617, wps=99.8, ups=0.6, wpb=110.5, bsz=40, num_updates=36520, lr=3.30625e-05, gnorm=0.23, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=201390
2023-01-11 21:50:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:20 - progress_bar.py[line:274] - INFO: epoch 001:  36581 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4381, wps=100.7, ups=0.61, wpb=109.5, bsz=40, num_updates=36530, lr=3.30573e-05, gnorm=0.67, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=201406
2023-01-11 21:50:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:37 - progress_bar.py[line:274] - INFO: epoch 001:  36591 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4902, wps=100.6, ups=0.61, wpb=110.2, bsz=40, num_updates=36540, lr=3.30521e-05, gnorm=0.499, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201423
2023-01-11 21:50:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:50:53 - progress_bar.py[line:274] - INFO: epoch 001:  36601 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5098, wps=102.6, ups=0.62, wpb=110.1, bsz=40, num_updates=36550, lr=3.30469e-05, gnorm=0.224, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201439
2023-01-11 21:50:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:10 - progress_bar.py[line:274] - INFO: epoch 001:  36611 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.396, wps=102, ups=0.62, wpb=110.1, bsz=40, num_updates=36560, lr=3.30417e-05, gnorm=0.848, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=201456
2023-01-11 21:51:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:26 - progress_bar.py[line:274] - INFO: epoch 001:  36621 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4954, wps=99.6, ups=0.61, wpb=109.6, bsz=40, num_updates=36570, lr=3.30365e-05, gnorm=0.528, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201473
2023-01-11 21:51:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:43 - progress_bar.py[line:274] - INFO: epoch 001:  36631 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4565, wps=103.7, ups=0.62, wpb=111, bsz=40, num_updates=36580, lr=3.30312e-05, gnorm=0.288, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201489
2023-01-11 21:51:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:51:59 - progress_bar.py[line:274] - INFO: epoch 001:  36641 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5877, wps=98.5, ups=0.6, wpb=108.9, bsz=40, num_updates=36590, lr=3.3026e-05, gnorm=0.202, clip=0, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=201506
2023-01-11 21:52:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:16 - progress_bar.py[line:274] - INFO: epoch 001:  36651 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.413, wps=102.1, ups=0.61, wpb=111.1, bsz=40, num_updates=36600, lr=3.30208e-05, gnorm=0.231, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201522
2023-01-11 21:52:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:32 - progress_bar.py[line:274] - INFO: epoch 001:  36661 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4737, wps=103.4, ups=0.62, wpb=110.9, bsz=40, num_updates=36610, lr=3.30156e-05, gnorm=0.392, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201539
2023-01-11 21:52:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:49 - progress_bar.py[line:274] - INFO: epoch 001:  36671 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3874, wps=98.2, ups=0.6, wpb=109.8, bsz=40, num_updates=36620, lr=3.30104e-05, gnorm=0.464, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=201556
2023-01-11 21:52:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:52:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:06 - progress_bar.py[line:274] - INFO: epoch 001:  36681 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4519, wps=99.6, ups=0.61, wpb=109.2, bsz=40, num_updates=36630, lr=3.30052e-05, gnorm=0.75, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201572
2023-01-11 21:53:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:23 - progress_bar.py[line:274] - INFO: epoch 001:  36691 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4457, wps=96.9, ups=0.59, wpb=110.1, bsz=40, num_updates=36640, lr=3.3e-05, gnorm=0.656, clip=10, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=201589
2023-01-11 21:53:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:39 - progress_bar.py[line:274] - INFO: epoch 001:  36701 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4176, wps=104.5, ups=0.63, wpb=111.4, bsz=40, num_updates=36650, lr=3.29948e-05, gnorm=0.399, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201606
2023-01-11 21:53:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:53:56 - progress_bar.py[line:274] - INFO: epoch 001:  36711 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4742, wps=99.4, ups=0.6, wpb=109.9, bsz=40, num_updates=36660, lr=3.29896e-05, gnorm=0.285, clip=0, loss_scale=256, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=201622
2023-01-11 21:54:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:13 - progress_bar.py[line:274] - INFO: epoch 001:  36721 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4747, wps=101.1, ups=0.61, wpb=110.3, bsz=40, num_updates=36670, lr=3.29844e-05, gnorm=0.487, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201639
2023-01-11 21:54:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:30 - progress_bar.py[line:274] - INFO: epoch 001:  36731 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3725, wps=99.7, ups=0.61, wpb=109.3, bsz=40, num_updates=36680, lr=3.29792e-05, gnorm=0.682, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201656
2023-01-11 21:54:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:46 - progress_bar.py[line:274] - INFO: epoch 001:  36741 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4476, wps=102.8, ups=0.62, wpb=109.9, bsz=40, num_updates=36690, lr=3.2974e-05, gnorm=0.172, clip=0, loss_scale=256, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=201672
2023-01-11 21:54:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:54:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:02 - progress_bar.py[line:274] - INFO: epoch 001:  36751 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.39, wps=100.4, ups=0.61, wpb=109, bsz=40, num_updates=36700, lr=3.29688e-05, gnorm=0.339, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201689
2023-01-11 21:55:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:19 - progress_bar.py[line:274] - INFO: epoch 001:  36761 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4176, wps=102.1, ups=0.61, wpb=110.7, bsz=40, num_updates=36710, lr=3.29635e-05, gnorm=0.461, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=201705
2023-01-11 21:55:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:36 - progress_bar.py[line:274] - INFO: epoch 001:  36771 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4468, wps=101.2, ups=0.61, wpb=110.5, bsz=40, num_updates=36720, lr=3.29583e-05, gnorm=0.221, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=201722
2023-01-11 21:55:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:52 - progress_bar.py[line:274] - INFO: epoch 001:  36781 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4479, wps=99.8, ups=0.61, wpb=108.4, bsz=40, num_updates=36730, lr=3.29531e-05, gnorm=0.373, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201738
2023-01-11 21:55:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:55:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:08 - progress_bar.py[line:274] - INFO: epoch 001:  36791 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.402, wps=101.4, ups=0.62, wpb=108.8, bsz=40, num_updates=36740, lr=3.29479e-05, gnorm=0.265, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=201755
2023-01-11 21:56:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:25 - progress_bar.py[line:274] - INFO: epoch 001:  36801 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4898, wps=101.6, ups=0.62, wpb=109.9, bsz=40, num_updates=36750, lr=3.29427e-05, gnorm=0.374, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201771
2023-01-11 21:56:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:42 - progress_bar.py[line:274] - INFO: epoch 001:  36811 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4421, wps=98.5, ups=0.6, wpb=109.5, bsz=40, num_updates=36760, lr=3.29375e-05, gnorm=0.384, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=201788
2023-01-11 21:56:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:56:59 - progress_bar.py[line:274] - INFO: epoch 001:  36821 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.41, wps=101.3, ups=0.61, wpb=111.4, bsz=40, num_updates=36770, lr=3.29323e-05, gnorm=0.24, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201805
2023-01-11 21:57:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:15 - progress_bar.py[line:274] - INFO: epoch 001:  36831 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.37, wps=100.9, ups=0.61, wpb=110.5, bsz=40, num_updates=36780, lr=3.29271e-05, gnorm=0.409, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201822
2023-01-11 21:57:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:32 - progress_bar.py[line:274] - INFO: epoch 001:  36841 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3434, wps=103.6, ups=0.62, wpb=111.1, bsz=40, num_updates=36790, lr=3.29219e-05, gnorm=0.324, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=201838
2023-01-11 21:57:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:49 - progress_bar.py[line:274] - INFO: epoch 001:  36851 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3431, wps=97.9, ups=0.6, wpb=108.6, bsz=40, num_updates=36800, lr=3.29167e-05, gnorm=0.558, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=201855
2023-01-11 21:57:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:57:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:05 - progress_bar.py[line:274] - INFO: epoch 001:  36861 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4486, wps=101.5, ups=0.62, wpb=109.2, bsz=40, num_updates=36810, lr=3.29115e-05, gnorm=0.302, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=201871
2023-01-11 21:58:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:22 - progress_bar.py[line:274] - INFO: epoch 001:  36871 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4483, wps=103.1, ups=0.62, wpb=110.6, bsz=40, num_updates=36820, lr=3.29063e-05, gnorm=1.685, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201888
2023-01-11 21:58:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:38 - progress_bar.py[line:274] - INFO: epoch 001:  36881 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3978, wps=101.1, ups=0.61, wpb=110.3, bsz=40, num_updates=36830, lr=3.2901e-05, gnorm=0.407, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201904
2023-01-11 21:58:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:58:55 - progress_bar.py[line:274] - INFO: epoch 001:  36891 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3761, wps=100.3, ups=0.61, wpb=109.5, bsz=40, num_updates=36840, lr=3.28958e-05, gnorm=0.348, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201921
2023-01-11 21:58:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:12 - progress_bar.py[line:274] - INFO: epoch 001:  36901 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4667, wps=98.5, ups=0.6, wpb=109.3, bsz=40, num_updates=36850, lr=3.28906e-05, gnorm=0.39, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=201938
2023-01-11 21:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:28 - progress_bar.py[line:274] - INFO: epoch 001:  36911 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.52, wps=103, ups=0.63, wpb=109.7, bsz=40, num_updates=36860, lr=3.28854e-05, gnorm=0.179, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=201954
2023-01-11 21:59:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:45 - progress_bar.py[line:274] - INFO: epoch 001:  36921 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4483, wps=102.9, ups=0.62, wpb=111.2, bsz=40, num_updates=36870, lr=3.28802e-05, gnorm=0.338, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=201971
2023-01-11 21:59:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 21:59:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:01 - progress_bar.py[line:274] - INFO: epoch 001:  36931 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4118, wps=99.8, ups=0.61, wpb=109.5, bsz=40, num_updates=36880, lr=3.2875e-05, gnorm=0.246, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=201987
2023-01-11 22:00:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:18 - progress_bar.py[line:274] - INFO: epoch 001:  36941 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4896, wps=98.1, ups=0.59, wpb=110.2, bsz=40, num_updates=36890, lr=3.28698e-05, gnorm=0.325, clip=10, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=202004
2023-01-11 22:00:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:35 - progress_bar.py[line:274] - INFO: epoch 001:  36951 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3263, wps=99.8, ups=0.6, wpb=110.1, bsz=40, num_updates=36900, lr=3.28646e-05, gnorm=0.166, clip=0, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=202021
2023-01-11 22:00:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:51 - progress_bar.py[line:274] - INFO: epoch 001:  36961 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4679, wps=103.1, ups=0.63, wpb=109.9, bsz=40, num_updates=36910, lr=3.28594e-05, gnorm=0.393, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=202037
2023-01-11 22:00:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:00:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:08 - progress_bar.py[line:274] - INFO: epoch 001:  36971 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4583, wps=99.8, ups=0.6, wpb=110.5, bsz=40, num_updates=36920, lr=3.28542e-05, gnorm=0.261, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=202054
2023-01-11 22:01:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:25 - progress_bar.py[line:274] - INFO: epoch 001:  36981 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4184, wps=99.6, ups=0.6, wpb=110.3, bsz=40, num_updates=36930, lr=3.2849e-05, gnorm=0.145, clip=0, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=202071
2023-01-11 22:01:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:42 - progress_bar.py[line:274] - INFO: epoch 001:  36991 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=98.6, ups=0.6, wpb=109.8, bsz=40, num_updates=36940, lr=3.28438e-05, gnorm=0.283, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=202088
2023-01-11 22:01:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:01:58 - progress_bar.py[line:274] - INFO: epoch 001:  37001 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4184, wps=103.7, ups=0.62, wpb=112, bsz=40, num_updates=36950, lr=3.28385e-05, gnorm=0.147, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=202105
2023-01-11 22:02:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:15 - progress_bar.py[line:274] - INFO: epoch 001:  37011 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4945, wps=99.4, ups=0.61, wpb=108.7, bsz=40, num_updates=36960, lr=3.28333e-05, gnorm=0.422, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=202121
2023-01-11 22:02:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:32 - progress_bar.py[line:274] - INFO: epoch 001:  37021 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3365, wps=100.5, ups=0.61, wpb=109.5, bsz=40, num_updates=36970, lr=3.28281e-05, gnorm=0.318, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=202138
2023-01-11 22:02:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:48 - progress_bar.py[line:274] - INFO: epoch 001:  37031 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4211, wps=101.3, ups=0.61, wpb=109.9, bsz=40, num_updates=36980, lr=3.28229e-05, gnorm=0.431, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=202154
2023-01-11 22:02:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:02:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:03:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:03:05 - progress_bar.py[line:274] - INFO: epoch 001:  37041 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4245, wps=99.9, ups=0.61, wpb=109.7, bsz=40, num_updates=36990, lr=3.28177e-05, gnorm=0.319, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=202171
2023-01-11 22:03:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:03:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:03:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:03:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 22:03:21 - progress_bar.py[line:274] - INFO: epoch 001:  37051 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5106, wps=102.7, ups=0.62, wpb=111.1, bsz=40, num_updates=37000, lr=3.28125e-05, gnorm=0.196, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=202187
2023-01-11 22:03:21 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 22:03:23 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 22:03:23 - train.py[line:551] - INFO: load:1.36 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 22:05:55 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 22:05:55 - train.py[line:551] - INFO: load:1.38 valid_run:152.01 task_valid:149.17 collect_output:1.74
2023-01-11 22:08:23 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 22:08:23 - train.py[line:551] - INFO: load:1.41 valid_run:300.09 task_valid:292.99 collect_output:4.93
2023-01-11 22:10:55 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 22:10:55 - train.py[line:551] - INFO: load:1.44 valid_run:451.45 task_valid:436.82 collect_output:11.39
2023-01-11 22:13:23 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 22:13:23 - train.py[line:551] - INFO: load:1.46 valid_run:600.26 task_valid:582.27 collect_output:13.67
2023-01-11 22:15:56 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 22:15:56 - train.py[line:551] - INFO: load:1.49 valid_run:752.28 task_valid:730.17 collect_output:16.76
2023-01-11 22:18:27 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 22:18:27 - train.py[line:551] - INFO: load:1.51 valid_run:903.80 task_valid:876.19 collect_output:21.17
2023-01-11 22:20:59 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 22:20:59 - train.py[line:551] - INFO: load:1.54 valid_run:1055.70 task_valid:1022.42 collect_output:25.81
2023-01-11 22:23:29 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 22:23:29 - train.py[line:551] - INFO: load:1.57 valid_run:1205.71 task_valid:1164.24 collect_output:32.85
2023-01-11 22:25:59 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 22:25:59 - train.py[line:551] - INFO: load:1.60 valid_run:1355.15 task_valid:1309.72 collect_output:35.73
2023-01-11 22:28:27 - train.py[line:549] - INFO: 2000 / 4988
2023-01-11 22:28:27 - train.py[line:551] - INFO: load:1.63 valid_run:1503.05 task_valid:1453.33 collect_output:38.98
2023-01-11 22:30:56 - train.py[line:549] - INFO: 2200 / 4988
2023-01-11 22:30:56 - train.py[line:551] - INFO: load:1.65 valid_run:1652.55 task_valid:1598.80 collect_output:41.95
2023-01-11 22:33:26 - train.py[line:549] - INFO: 2400 / 4988
2023-01-11 22:33:26 - train.py[line:551] - INFO: load:1.68 valid_run:1801.91 task_valid:1744.17 collect_output:44.89
2023-01-11 22:35:56 - train.py[line:549] - INFO: 2600 / 4988
2023-01-11 22:35:56 - train.py[line:551] - INFO: load:1.71 valid_run:1951.62 task_valid:1887.18 collect_output:50.44
2023-01-11 22:38:27 - train.py[line:549] - INFO: 2800 / 4988
2023-01-11 22:38:27 - train.py[line:551] - INFO: load:1.73 valid_run:2102.23 task_valid:2033.59 collect_output:53.49
2023-01-11 22:40:57 - train.py[line:549] - INFO: 3000 / 4988
2023-01-11 22:40:57 - train.py[line:551] - INFO: load:1.76 valid_run:2252.24 task_valid:2180.61 collect_output:55.39
2023-01-11 22:43:27 - train.py[line:549] - INFO: 3200 / 4988
2023-01-11 22:43:27 - train.py[line:551] - INFO: load:1.79 valid_run:2401.82 task_valid:2325.33 collect_output:59.17
2023-01-11 22:45:58 - train.py[line:549] - INFO: 3400 / 4988
2023-01-11 22:45:58 - train.py[line:551] - INFO: load:1.81 valid_run:2553.16 task_valid:2471.75 collect_output:62.99
2023-01-11 22:48:29 - train.py[line:549] - INFO: 3600 / 4988
2023-01-11 22:48:29 - train.py[line:551] - INFO: load:1.84 valid_run:2703.33 task_valid:2619.02 collect_output:64.84
2023-01-11 22:50:56 - train.py[line:549] - INFO: 3800 / 4988
2023-01-11 22:50:57 - train.py[line:551] - INFO: load:1.86 valid_run:2851.20 task_valid:2761.28 collect_output:69.30
2023-01-11 22:53:26 - train.py[line:549] - INFO: 4000 / 4988
2023-01-11 22:53:26 - train.py[line:551] - INFO: load:1.89 valid_run:3000.99 task_valid:2906.91 collect_output:72.37
2023-01-11 22:55:57 - train.py[line:549] - INFO: 4200 / 4988
2023-01-11 22:55:57 - train.py[line:551] - INFO: load:1.92 valid_run:3151.92 task_valid:3052.23 collect_output:76.93
2023-01-11 22:58:26 - train.py[line:549] - INFO: 4400 / 4988
2023-01-11 22:58:26 - train.py[line:551] - INFO: load:1.94 valid_run:3300.70 task_valid:3197.32 collect_output:79.56
2023-01-11 23:00:57 - train.py[line:549] - INFO: 4600 / 4988
2023-01-11 23:00:57 - train.py[line:551] - INFO: load:1.97 valid_run:3450.98 task_valid:3343.93 collect_output:82.19
2023-01-11 23:03:28 - train.py[line:549] - INFO: 4800 / 4988
2023-01-11 23:03:28 - train.py[line:551] - INFO: load:2.00 valid_run:3601.83 task_valid:3490.96 collect_output:84.97

====================================================================================================
SGG eval:     R @ 50: 0.4099;     R @ 100: 0.4769;     R @ 500: 0.5055;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2542;    mR @ 100: 0.3089;    mR @ 500: 0.3415;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6415) (covered in:0.6458) (covering:0.3714) (eating:0.5294) (flying in:0.0000) (growing on:0.1250) (hanging from:0.2742) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.4837) (says:0.0000) (sitting on:0.7290) (standing on:0.1927) (using:0.6500) (walking in:0.0000) (walking on:0.2613) (watching:0.1806) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4099;     R @ 100: 0.4769;     R @ 500: 0.5055;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2542;    mR @ 100: 0.3089;    mR @ 500: 0.3415;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6415) (covered in:0.6458) (covering:0.3714) (eating:0.5294) (flying in:0.0000) (growing on:0.1250) (hanging from:0.2742) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.4837) (says:0.0000) (sitting on:0.7290) (standing on:0.1927) (using:0.6500) (walking in:0.0000) (walking on:0.2613) (watching:0.1806) 
--------------------------------------------------------
====================================================================================================

2023-01-11 23:05:59 - train.py[line:487] - INFO: 0.47691484593837535
2023-01-11 23:05:59 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-11 23:05:59 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.377 | loss_v1 0 | loss_v2 0 | nll_loss 0.225 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.476915 | ppl 1.17 | vqa_score 0.3953 | wps 119.4 | wpb 89.9 | bsz 30 | num_updates 37000 | best_R@100 0.69005
2023-01-11 23:05:59 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 37000 updates
2023-01-11 23:05:59 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_37000.pt
2023-01-11 23:06:38 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_37000.pt
2023-01-11 23:08:03 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_37000.pt (epoch 1 @ 37000 updates, score 0.47691484593837535) (writing took 123.760591737926 seconds)
2023-01-11 23:08:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:20 - progress_bar.py[line:274] - INFO: epoch 001:  37061 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4587, wps=0.4, ups=0, wpb=109.9, bsz=40, num_updates=37010, lr=3.28073e-05, gnorm=0.284, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=206086
2023-01-11 23:08:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:38 - progress_bar.py[line:274] - INFO: epoch 001:  37071 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3939, wps=100.4, ups=0.61, wpb=110.5, bsz=40, num_updates=37020, lr=3.28021e-05, gnorm=0.242, clip=0, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=206103
2023-01-11 23:08:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:08:54 - progress_bar.py[line:274] - INFO: epoch 001:  37081 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4216, wps=101.7, ups=0.61, wpb=110.5, bsz=40, num_updates=37030, lr=3.27969e-05, gnorm=0.124, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206120
2023-01-11 23:08:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:11 - progress_bar.py[line:274] - INFO: epoch 001:  37091 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4615, wps=104.1, ups=0.63, wpb=110.7, bsz=40, num_updates=37040, lr=3.27917e-05, gnorm=0.222, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206137
2023-01-11 23:09:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:28 - progress_bar.py[line:274] - INFO: epoch 001:  37101 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3619, wps=98, ups=0.61, wpb=107.8, bsz=40, num_updates=37050, lr=3.27865e-05, gnorm=0.389, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=206154
2023-01-11 23:09:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:46 - progress_bar.py[line:274] - INFO: epoch 001:  37111 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5657, wps=98.6, ups=0.6, wpb=109.1, bsz=40, num_updates=37060, lr=3.27813e-05, gnorm=0.303, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=206171
2023-01-11 23:09:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:09:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:03 - progress_bar.py[line:274] - INFO: epoch 001:  37121 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4217, wps=100.9, ups=0.61, wpb=110.9, bsz=40, num_updates=37070, lr=3.2776e-05, gnorm=0.263, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=206188
2023-01-11 23:10:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:20 - progress_bar.py[line:274] - INFO: epoch 001:  37131 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5102, wps=101.1, ups=0.61, wpb=110, bsz=40, num_updates=37080, lr=3.27708e-05, gnorm=1.759, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=206206
2023-01-11 23:10:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:37 - progress_bar.py[line:274] - INFO: epoch 001:  37141 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4158, wps=98.4, ups=0.6, wpb=109.1, bsz=40, num_updates=37090, lr=3.27656e-05, gnorm=0.248, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=206223
2023-01-11 23:10:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:10:54 - progress_bar.py[line:274] - INFO: epoch 001:  37151 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3535, wps=101.7, ups=0.61, wpb=110.5, bsz=40, num_updates=37100, lr=3.27604e-05, gnorm=0.361, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206240
2023-01-11 23:10:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:11 - progress_bar.py[line:274] - INFO: epoch 001:  37161 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.47, wps=102, ups=0.63, wpb=108.6, bsz=40, num_updates=37110, lr=3.27552e-05, gnorm=0.221, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=206257
2023-01-11 23:11:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:27 - progress_bar.py[line:274] - INFO: epoch 001:  37171 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4615, wps=105.3, ups=0.64, wpb=110.5, bsz=40, num_updates=37120, lr=3.275e-05, gnorm=0.282, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=206273
2023-01-11 23:11:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:44 - progress_bar.py[line:274] - INFO: epoch 001:  37181 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4615, wps=100.9, ups=0.62, wpb=109.4, bsz=40, num_updates=37130, lr=3.27448e-05, gnorm=0.298, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=206290
2023-01-11 23:11:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:11:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:01 - progress_bar.py[line:274] - INFO: epoch 001:  37191 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5567, wps=100.7, ups=0.6, wpb=111.1, bsz=40, num_updates=37140, lr=3.27396e-05, gnorm=0.219, clip=0, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=206307
2023-01-11 23:12:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:19 - progress_bar.py[line:274] - INFO: epoch 001:  37201 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4545, wps=100.4, ups=0.61, wpb=109.2, bsz=40, num_updates=37150, lr=3.27344e-05, gnorm=0.236, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=206324
2023-01-11 23:12:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:35 - progress_bar.py[line:274] - INFO: epoch 001:  37211 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4286, wps=101.5, ups=0.61, wpb=110.8, bsz=40, num_updates=37160, lr=3.27292e-05, gnorm=0.317, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206341
2023-01-11 23:12:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:53 - progress_bar.py[line:274] - INFO: epoch 001:  37221 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4579, wps=101, ups=0.62, wpb=108.7, bsz=40, num_updates=37170, lr=3.2724e-05, gnorm=0.275, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=206358
2023-01-11 23:12:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:12:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:10 - progress_bar.py[line:274] - INFO: epoch 001:  37231 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4205, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=37180, lr=3.27187e-05, gnorm=0.359, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206375
2023-01-11 23:13:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:26 - progress_bar.py[line:274] - INFO: epoch 001:  37241 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4583, wps=102.7, ups=0.62, wpb=110.4, bsz=40, num_updates=37190, lr=3.27135e-05, gnorm=0.185, clip=0, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=206392
2023-01-11 23:13:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:44 - progress_bar.py[line:274] - INFO: epoch 001:  37251 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4796, wps=101.7, ups=0.61, wpb=110.6, bsz=40, num_updates=37200, lr=3.27083e-05, gnorm=0.284, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206409
2023-01-11 23:13:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:13:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:00 - progress_bar.py[line:274] - INFO: epoch 001:  37261 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4574, wps=101.7, ups=0.62, wpb=109.9, bsz=40, num_updates=37210, lr=3.27031e-05, gnorm=0.489, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=206426
2023-01-11 23:14:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:17 - progress_bar.py[line:274] - INFO: epoch 001:  37271 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4458, wps=99.4, ups=0.59, wpb=111.5, bsz=40, num_updates=37220, lr=3.26979e-05, gnorm=0.315, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=206443
2023-01-11 23:14:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:34 - progress_bar.py[line:274] - INFO: epoch 001:  37281 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.39, wps=100.8, ups=0.61, wpb=110.1, bsz=40, num_updates=37230, lr=3.26927e-05, gnorm=0.211, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206460
2023-01-11 23:14:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:51 - progress_bar.py[line:274] - INFO: epoch 001:  37291 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3805, wps=99.4, ups=0.61, wpb=108.4, bsz=40, num_updates=37240, lr=3.26875e-05, gnorm=0.328, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206477
2023-01-11 23:14:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:14:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:07 - progress_bar.py[line:274] - INFO: epoch 001:  37301 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3774, wps=99.4, ups=0.61, wpb=109, bsz=40, num_updates=37250, lr=3.26823e-05, gnorm=0.357, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=206493
2023-01-11 23:15:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:24 - progress_bar.py[line:274] - INFO: epoch 001:  37311 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3981, wps=100.7, ups=0.62, wpb=108.7, bsz=40, num_updates=37260, lr=3.26771e-05, gnorm=0.273, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206510
2023-01-11 23:15:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:40 - progress_bar.py[line:274] - INFO: epoch 001:  37321 / 100000 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.6, wps=102, ups=0.62, wpb=109.9, bsz=40, num_updates=37270, lr=3.26719e-05, gnorm=0.187, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=206526
2023-01-11 23:15:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:15:57 - progress_bar.py[line:274] - INFO: epoch 001:  37331 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4433, wps=99.4, ups=0.6, wpb=109.7, bsz=40, num_updates=37280, lr=3.26667e-05, gnorm=0.314, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206543
2023-01-11 23:16:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:14 - progress_bar.py[line:274] - INFO: epoch 001:  37341 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4381, wps=100.3, ups=0.61, wpb=108.7, bsz=40, num_updates=37290, lr=3.26615e-05, gnorm=0.223, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=206560
2023-01-11 23:16:15 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-11 23:16:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:31 - progress_bar.py[line:274] - INFO: epoch 001:  37352 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4706, wps=93.1, ups=0.57, wpb=109.1, bsz=40, num_updates=37300, lr=3.26562e-05, gnorm=0.206, clip=0, loss_scale=256, train_wall=18, gb_free=10.1, ema_decay=0.9999, wall=206578
2023-01-11 23:16:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:48 - progress_bar.py[line:274] - INFO: epoch 001:  37362 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4397, wps=102, ups=0.62, wpb=109.1, bsz=40, num_updates=37310, lr=3.2651e-05, gnorm=0.211, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=206594
2023-01-11 23:16:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:16:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:04 - progress_bar.py[line:274] - INFO: epoch 001:  37372 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4059, wps=101.1, ups=0.61, wpb=110.9, bsz=40, num_updates=37320, lr=3.26458e-05, gnorm=0.378, clip=10, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=206611
2023-01-11 23:17:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:21 - progress_bar.py[line:274] - INFO: epoch 001:  37382 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.38, wps=99.4, ups=0.61, wpb=109.3, bsz=40, num_updates=37330, lr=3.26406e-05, gnorm=0.221, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206627
2023-01-11 23:17:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:38 - progress_bar.py[line:274] - INFO: epoch 001:  37392 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4255, wps=102.7, ups=0.62, wpb=111.1, bsz=40, num_updates=37340, lr=3.26354e-05, gnorm=0.136, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206644
2023-01-11 23:17:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:17:54 - progress_bar.py[line:274] - INFO: epoch 001:  37402 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4787, wps=100.9, ups=0.61, wpb=109.5, bsz=40, num_updates=37350, lr=3.26302e-05, gnorm=0.161, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=206660
2023-01-11 23:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:11 - progress_bar.py[line:274] - INFO: epoch 001:  37412 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4234, wps=99.8, ups=0.62, wpb=108.1, bsz=40, num_updates=37360, lr=3.2625e-05, gnorm=0.13, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=206677
2023-01-11 23:18:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:27 - progress_bar.py[line:274] - INFO: epoch 001:  37422 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5, wps=101.4, ups=0.61, wpb=110.4, bsz=40, num_updates=37370, lr=3.26198e-05, gnorm=0.141, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=206693
2023-01-11 23:18:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:44 - progress_bar.py[line:274] - INFO: epoch 001:  37432 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4216, wps=102.3, ups=0.62, wpb=110.3, bsz=40, num_updates=37380, lr=3.26146e-05, gnorm=0.244, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206710
2023-01-11 23:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:18:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:00 - progress_bar.py[line:274] - INFO: epoch 001:  37442 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5111, wps=101.3, ups=0.61, wpb=110.5, bsz=40, num_updates=37390, lr=3.26094e-05, gnorm=0.121, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206726
2023-01-11 23:19:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:17 - progress_bar.py[line:274] - INFO: epoch 001:  37452 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3763, wps=100.4, ups=0.6, wpb=110.7, bsz=40, num_updates=37400, lr=3.26042e-05, gnorm=0.306, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=206743
2023-01-11 23:19:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:34 - progress_bar.py[line:274] - INFO: epoch 001:  37462 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4343, wps=101, ups=0.61, wpb=110, bsz=40, num_updates=37410, lr=3.2599e-05, gnorm=0.229, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=206760
2023-01-11 23:19:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:50 - progress_bar.py[line:274] - INFO: epoch 001:  37472 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4725, wps=102.3, ups=0.61, wpb=111.6, bsz=40, num_updates=37420, lr=3.25938e-05, gnorm=0.353, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=206776
2023-01-11 23:19:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:19:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:07 - progress_bar.py[line:274] - INFO: epoch 001:  37482 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3265, wps=100.3, ups=0.61, wpb=110.1, bsz=40, num_updates=37430, lr=3.25885e-05, gnorm=0.144, clip=0, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=206793
2023-01-11 23:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:24 - progress_bar.py[line:274] - INFO: epoch 001:  37492 / 100000 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4947, wps=100.4, ups=0.6, wpb=111.4, bsz=40, num_updates=37440, lr=3.25833e-05, gnorm=0.22, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=206810
2023-01-11 23:20:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:40 - progress_bar.py[line:274] - INFO: epoch 001:  37502 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4381, wps=102.6, ups=0.62, wpb=110.2, bsz=40, num_updates=37450, lr=3.25781e-05, gnorm=0.154, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206826
2023-01-11 23:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:20:57 - progress_bar.py[line:274] - INFO: epoch 001:  37512 / 100000 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4118, wps=102.2, ups=0.61, wpb=111.7, bsz=40, num_updates=37460, lr=3.25729e-05, gnorm=0.143, clip=0, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=206843
2023-01-11 23:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:13 - progress_bar.py[line:274] - INFO: epoch 001:  37522 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4286, wps=102, ups=0.62, wpb=110.3, bsz=40, num_updates=37470, lr=3.25677e-05, gnorm=0.22, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206859
2023-01-11 23:21:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:30 - progress_bar.py[line:274] - INFO: epoch 001:  37532 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5204, wps=101.9, ups=0.62, wpb=110.2, bsz=40, num_updates=37480, lr=3.25625e-05, gnorm=0.342, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206876
2023-01-11 23:21:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:46 - progress_bar.py[line:274] - INFO: epoch 001:  37542 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.449, wps=101.9, ups=0.62, wpb=109.9, bsz=40, num_updates=37490, lr=3.25573e-05, gnorm=0.332, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206892
2023-01-11 23:21:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:21:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:03 - progress_bar.py[line:274] - INFO: epoch 001:  37552 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3874, wps=99.5, ups=0.61, wpb=108.5, bsz=40, num_updates=37500, lr=3.25521e-05, gnorm=0.248, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206909
2023-01-11 23:22:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:19 - progress_bar.py[line:274] - INFO: epoch 001:  37562 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4851, wps=101.3, ups=0.61, wpb=109.8, bsz=40, num_updates=37510, lr=3.25469e-05, gnorm=0.417, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=206925
2023-01-11 23:22:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:35 - progress_bar.py[line:274] - INFO: epoch 001:  37572 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4, wps=103.9, ups=0.63, wpb=109.9, bsz=40, num_updates=37520, lr=3.25417e-05, gnorm=0.231, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206941
2023-01-11 23:22:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:52 - progress_bar.py[line:274] - INFO: epoch 001:  37582 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4583, wps=100.8, ups=0.61, wpb=110, bsz=40, num_updates=37530, lr=3.25365e-05, gnorm=0.178, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=206958
2023-01-11 23:22:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:22:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:09 - progress_bar.py[line:274] - INFO: epoch 001:  37592 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4912, wps=98.6, ups=0.61, wpb=108.6, bsz=40, num_updates=37540, lr=3.25313e-05, gnorm=0.292, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=206975
2023-01-11 23:23:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:25 - progress_bar.py[line:274] - INFO: epoch 001:  37602 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4038, wps=100.8, ups=0.62, wpb=108.7, bsz=40, num_updates=37550, lr=3.2526e-05, gnorm=0.11, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=206991
2023-01-11 23:23:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:41 - progress_bar.py[line:274] - INFO: epoch 001:  37612 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4098, wps=101.4, ups=0.62, wpb=108.9, bsz=40, num_updates=37560, lr=3.25208e-05, gnorm=0.333, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207008
2023-01-11 23:23:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:23:58 - progress_bar.py[line:274] - INFO: epoch 001:  37622 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4536, wps=100.3, ups=0.61, wpb=109.6, bsz=40, num_updates=37570, lr=3.25156e-05, gnorm=0.172, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207024
2023-01-11 23:24:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:15 - progress_bar.py[line:274] - INFO: epoch 001:  37632 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4455, wps=101.3, ups=0.61, wpb=110.7, bsz=40, num_updates=37580, lr=3.25104e-05, gnorm=0.2, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=207041
2023-01-11 23:24:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:31 - progress_bar.py[line:274] - INFO: epoch 001:  37642 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=101.6, ups=0.61, wpb=110.1, bsz=40, num_updates=37590, lr=3.25052e-05, gnorm=0.533, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=207057
2023-01-11 23:24:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:48 - progress_bar.py[line:274] - INFO: epoch 001:  37652 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3958, wps=99.6, ups=0.6, wpb=110.9, bsz=40, num_updates=37600, lr=3.25e-05, gnorm=0.228, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=207074
2023-01-11 23:24:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:24:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:05 - progress_bar.py[line:274] - INFO: epoch 001:  37662 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3796, wps=99.4, ups=0.6, wpb=109.7, bsz=40, num_updates=37610, lr=3.24948e-05, gnorm=0.195, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=207091
2023-01-11 23:25:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:21 - progress_bar.py[line:274] - INFO: epoch 001:  37672 / 100000 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.494, wps=100.6, ups=0.61, wpb=110, bsz=40, num_updates=37620, lr=3.24896e-05, gnorm=0.206, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207108
2023-01-11 23:25:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:38 - progress_bar.py[line:274] - INFO: epoch 001:  37682 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5049, wps=100.7, ups=0.62, wpb=108.9, bsz=40, num_updates=37630, lr=3.24844e-05, gnorm=0.371, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=207124
2023-01-11 23:25:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:25:54 - progress_bar.py[line:274] - INFO: epoch 001:  37692 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4286, wps=101.1, ups=0.62, wpb=109.1, bsz=40, num_updates=37640, lr=3.24792e-05, gnorm=0.156, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207140
2023-01-11 23:25:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:11 - progress_bar.py[line:274] - INFO: epoch 001:  37702 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4569, wps=100.3, ups=0.61, wpb=109.3, bsz=40, num_updates=37650, lr=3.2474e-05, gnorm=0.356, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=207157
2023-01-11 23:26:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:27 - progress_bar.py[line:274] - INFO: epoch 001:  37712 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4135, wps=101.4, ups=0.62, wpb=109.7, bsz=40, num_updates=37660, lr=3.24688e-05, gnorm=0.207, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=207173
2023-01-11 23:26:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:44 - progress_bar.py[line:274] - INFO: epoch 001:  37722 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4783, wps=101, ups=0.62, wpb=108.8, bsz=40, num_updates=37670, lr=3.24635e-05, gnorm=0.44, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207190
2023-01-11 23:26:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:26:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:00 - progress_bar.py[line:274] - INFO: epoch 001:  37732 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4105, wps=100.9, ups=0.61, wpb=110.7, bsz=40, num_updates=37680, lr=3.24583e-05, gnorm=0.159, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207206
2023-01-11 23:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:17 - progress_bar.py[line:274] - INFO: epoch 001:  37742 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5043, wps=99.2, ups=0.61, wpb=109.3, bsz=40, num_updates=37690, lr=3.24531e-05, gnorm=0.214, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=207223
2023-01-11 23:27:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:33 - progress_bar.py[line:274] - INFO: epoch 001:  37752 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4196, wps=101.8, ups=0.62, wpb=109.5, bsz=40, num_updates=37700, lr=3.24479e-05, gnorm=0.439, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207240
2023-01-11 23:27:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:50 - progress_bar.py[line:274] - INFO: epoch 001:  37762 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4787, wps=100.1, ups=0.6, wpb=110.3, bsz=40, num_updates=37710, lr=3.24427e-05, gnorm=0.284, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=207256
2023-01-11 23:27:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:27:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:07 - progress_bar.py[line:274] - INFO: epoch 001:  37772 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4495, wps=99.6, ups=0.6, wpb=109.9, bsz=40, num_updates=37720, lr=3.24375e-05, gnorm=0.176, clip=0, loss_scale=256, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=207273
2023-01-11 23:28:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:24 - progress_bar.py[line:274] - INFO: epoch 001:  37782 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2604, wps=99.3, ups=0.6, wpb=110.4, bsz=40, num_updates=37730, lr=3.24323e-05, gnorm=0.125, clip=0, loss_scale=256, train_wall=17, gb_free=10.8, ema_decay=0.9999, wall=207290
2023-01-11 23:28:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:40 - progress_bar.py[line:274] - INFO: epoch 001:  37792 / 100000 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=113.067, nsentences=40, sample_size=113.067, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4762, wps=106.6, ups=0.63, wpb=113.1, bsz=40, num_updates=37740, lr=3.24271e-05, gnorm=0.298, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=207306
2023-01-11 23:28:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:28:56 - progress_bar.py[line:274] - INFO: epoch 001:  37802 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4891, wps=103.4, ups=0.62, wpb=110.9, bsz=40, num_updates=37750, lr=3.24219e-05, gnorm=0.407, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=207323
2023-01-11 23:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:13 - progress_bar.py[line:274] - INFO: epoch 001:  37812 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4679, wps=97.7, ups=0.6, wpb=109.3, bsz=40, num_updates=37760, lr=3.24167e-05, gnorm=0.241, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=207340
2023-01-11 23:29:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:30 - progress_bar.py[line:274] - INFO: epoch 001:  37822 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4157, wps=101.1, ups=0.61, wpb=110.7, bsz=40, num_updates=37770, lr=3.24115e-05, gnorm=0.804, clip=20, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=207356
2023-01-11 23:29:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:47 - progress_bar.py[line:274] - INFO: epoch 001:  37832 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4725, wps=101.1, ups=0.61, wpb=110.5, bsz=40, num_updates=37780, lr=3.24063e-05, gnorm=0.323, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=207373
2023-01-11 23:29:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:29:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:03 - progress_bar.py[line:274] - INFO: epoch 001:  37842 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3861, wps=100.5, ups=0.61, wpb=109.6, bsz=40, num_updates=37790, lr=3.2401e-05, gnorm=0.171, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=207389
2023-01-11 23:30:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:20 - progress_bar.py[line:274] - INFO: epoch 001:  37852 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.152, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4352, wps=101.7, ups=0.61, wpb=111.3, bsz=40, num_updates=37800, lr=3.23958e-05, gnorm=0.438, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207406
2023-01-11 23:30:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:37 - progress_bar.py[line:274] - INFO: epoch 001:  37862 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5053, wps=99.5, ups=0.6, wpb=109.8, bsz=40, num_updates=37810, lr=3.23906e-05, gnorm=0.448, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=207423
2023-01-11 23:30:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:53 - progress_bar.py[line:274] - INFO: epoch 001:  37872 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4188, wps=100, ups=0.61, wpb=108.7, bsz=40, num_updates=37820, lr=3.23854e-05, gnorm=0.236, clip=0, loss_scale=512, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=207439
2023-01-11 23:30:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:30:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:10 - progress_bar.py[line:274] - INFO: epoch 001:  37882 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4158, wps=100.3, ups=0.61, wpb=109.7, bsz=40, num_updates=37830, lr=3.23802e-05, gnorm=1.329, clip=20, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=207456
2023-01-11 23:31:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:26 - progress_bar.py[line:274] - INFO: epoch 001:  37892 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.383, wps=101.4, ups=0.62, wpb=109.7, bsz=40, num_updates=37840, lr=3.2375e-05, gnorm=0.302, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207473
2023-01-11 23:31:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:43 - progress_bar.py[line:274] - INFO: epoch 001:  37902 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4526, wps=103.4, ups=0.62, wpb=110.5, bsz=40, num_updates=37850, lr=3.23698e-05, gnorm=0.39, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=207489
2023-01-11 23:31:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:31:59 - progress_bar.py[line:274] - INFO: epoch 001:  37912 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4808, wps=101.6, ups=0.62, wpb=110.1, bsz=40, num_updates=37860, lr=3.23646e-05, gnorm=0.711, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=207505
2023-01-11 23:32:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:16 - progress_bar.py[line:274] - INFO: epoch 001:  37922 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4095, wps=98.9, ups=0.6, wpb=109.7, bsz=40, num_updates=37870, lr=3.23594e-05, gnorm=0.926, clip=20, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=207522
2023-01-11 23:32:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:32 - progress_bar.py[line:274] - INFO: epoch 001:  37932 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4742, wps=103.2, ups=0.62, wpb=110.5, bsz=40, num_updates=37880, lr=3.23542e-05, gnorm=0.56, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=207539
2023-01-11 23:32:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:49 - progress_bar.py[line:274] - INFO: epoch 001:  37942 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4608, wps=104.4, ups=0.63, wpb=109.7, bsz=40, num_updates=37890, lr=3.2349e-05, gnorm=0.245, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207555
2023-01-11 23:32:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:32:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:05 - progress_bar.py[line:274] - INFO: epoch 001:  37952 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.404, wps=101, ups=0.61, wpb=110.3, bsz=40, num_updates=37900, lr=3.23438e-05, gnorm=0.246, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=207571
2023-01-11 23:33:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:22 - progress_bar.py[line:274] - INFO: epoch 001:  37962 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3913, wps=101.6, ups=0.61, wpb=110.7, bsz=40, num_updates=37910, lr=3.23385e-05, gnorm=0.271, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207588
2023-01-11 23:33:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:39 - progress_bar.py[line:274] - INFO: epoch 001:  37972 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4851, wps=99.4, ups=0.6, wpb=110.9, bsz=40, num_updates=37920, lr=3.23333e-05, gnorm=0.178, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=207605
2023-01-11 23:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:33:55 - progress_bar.py[line:274] - INFO: epoch 001:  37982 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3694, wps=103.1, ups=0.62, wpb=110.3, bsz=40, num_updates=37930, lr=3.23281e-05, gnorm=0.263, clip=0, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=207622
2023-01-11 23:33:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:11 - progress_bar.py[line:274] - INFO: epoch 001:  37992 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4444, wps=102.4, ups=0.63, wpb=108.3, bsz=40, num_updates=37940, lr=3.23229e-05, gnorm=0.184, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=207638
2023-01-11 23:34:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:28 - progress_bar.py[line:274] - INFO: epoch 001:  38002 / 100000 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5, wps=100.6, ups=0.6, wpb=110.9, bsz=40, num_updates=37950, lr=3.23177e-05, gnorm=0.162, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207654
2023-01-11 23:34:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:44 - progress_bar.py[line:274] - INFO: epoch 001:  38012 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4815, wps=102.9, ups=0.63, wpb=108.9, bsz=40, num_updates=37960, lr=3.23125e-05, gnorm=0.394, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207670
2023-01-11 23:34:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:34:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:01 - progress_bar.py[line:274] - INFO: epoch 001:  38022 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4667, wps=98.9, ups=0.6, wpb=110.5, bsz=40, num_updates=37970, lr=3.23073e-05, gnorm=0.358, clip=0, loss_scale=512, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=207687
2023-01-11 23:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:17 - progress_bar.py[line:274] - INFO: epoch 001:  38032 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3909, wps=103.6, ups=0.63, wpb=110.1, bsz=40, num_updates=37980, lr=3.23021e-05, gnorm=0.299, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=207704
2023-01-11 23:35:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:34 - progress_bar.py[line:274] - INFO: epoch 001:  38042 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5357, wps=99.1, ups=0.61, wpb=108.4, bsz=40, num_updates=37990, lr=3.22969e-05, gnorm=0.259, clip=0, loss_scale=512, train_wall=16, gb_free=10.8, ema_decay=0.9999, wall=207720
2023-01-11 23:35:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-11 23:35:51 - progress_bar.py[line:274] - INFO: epoch 001:  38052 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4479, wps=100.6, ups=0.61, wpb=109.5, bsz=40, num_updates=38000, lr=3.22917e-05, gnorm=0.233, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=207737
2023-01-11 23:35:51 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-11 23:35:52 - train.py[line:549] - INFO: 0 / 4988
2023-01-11 23:35:52 - train.py[line:551] - INFO: load:1.24 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-11 23:36:08 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.21 GiB (GPU 0; 39.59 GiB total capacity; 9.30 GiB already allocated; 2.87 GiB free; 34.23 GiB reserved in total by PyTorch)
2023-01-11 23:36:08 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |    9519 MB |   14749 MB |   29251 TB |   29251 TB |
|       from large pool |    9345 MB |   14574 MB |   29242 TB |   29242 TB |
|       from small pool |     174 MB |     175 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| Active memory         |    9519 MB |   14749 MB |   29251 TB |   29251 TB |
|       from large pool |    9345 MB |   14574 MB |   29242 TB |   29242 TB |
|       from small pool |     174 MB |     175 MB |       9 TB |       9 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   35054 MB |   36008 MB |  490974 MB |  455920 MB |
|       from large pool |   34878 MB |   35826 MB |  490374 MB |  455496 MB |
|       from small pool |     176 MB |     182 MB |     600 MB |     424 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   25534 MB |   30106 MB |   31189 TB |   31189 TB |
|       from large pool |   25532 MB |   30104 MB |   31178 TB |   31178 TB |
|       from small pool |       1 MB |       3 MB |      10 TB |      10 TB |
|---------------------------------------------------------------------------|
| Allocations           |    4634    |    4648    |    1400 M  |    1400 M  |
|       from large pool |     698    |     710    |     431 M  |     431 M  |
|       from small pool |    3936    |    3946    |     969 M  |     969 M  |
|---------------------------------------------------------------------------|
| Active allocs         |    4634    |    4648    |    1400 M  |    1400 M  |
|       from large pool |     698    |     710    |     431 M  |     431 M  |
|       from small pool |    3936    |    3946    |     969 M  |     969 M  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     189    |     194    |    1250    |    1061    |
|       from large pool |     101    |     103    |     950    |     849    |
|       from small pool |      88    |      91    |     300    |     212    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     127    |     134    |    1017 M  |    1017 M  |
|       from large pool |      73    |      74    |     168 M  |     168 M  |
|       from small pool |      54    |      65    |     849 M  |     849 M  |
|===========================================================================|

2023-01-11 23:36:08 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-01-11 23:36:08 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-01-11 23:38:25 - train.py[line:549] - INFO: 200 / 4988
2023-01-11 23:38:25 - train.py[line:551] - INFO: load:1.26 valid_run:152.71 task_valid:148.68 collect_output:1.86
2023-01-11 23:40:53 - train.py[line:549] - INFO: 400 / 4988
2023-01-11 23:40:53 - train.py[line:551] - INFO: load:1.29 valid_run:300.99 task_valid:292.67 collect_output:5.07
2023-01-11 23:43:24 - train.py[line:549] - INFO: 600 / 4988
2023-01-11 23:43:24 - train.py[line:551] - INFO: load:1.31 valid_run:451.93 task_valid:435.87 collect_output:11.79
2023-01-11 23:45:53 - train.py[line:549] - INFO: 800 / 4988
2023-01-11 23:45:53 - train.py[line:551] - INFO: load:1.34 valid_run:600.37 task_valid:580.90 collect_output:14.16
2023-01-11 23:48:25 - train.py[line:549] - INFO: 1000 / 4988
2023-01-11 23:48:25 - train.py[line:551] - INFO: load:1.36 valid_run:751.90 task_valid:728.37 collect_output:17.20
2023-01-11 23:50:56 - train.py[line:549] - INFO: 1200 / 4988
2023-01-11 23:50:56 - train.py[line:551] - INFO: load:1.38 valid_run:903.05 task_valid:874.15 collect_output:21.54
2023-01-11 23:53:28 - train.py[line:549] - INFO: 1400 / 4988
2023-01-11 23:53:28 - train.py[line:551] - INFO: load:1.41 valid_run:1055.27 task_valid:1020.46 collect_output:26.40
2023-01-11 23:55:58 - train.py[line:549] - INFO: 1600 / 4988
2023-01-11 23:55:58 - train.py[line:551] - INFO: load:1.43 valid_run:1205.27 task_valid:1161.70 collect_output:34.13
2023-01-11 23:58:27 - train.py[line:549] - INFO: 1800 / 4988
2023-01-11 23:58:27 - train.py[line:551] - INFO: load:1.46 valid_run:1354.05 task_valid:1306.57 collect_output:36.99
2023-01-12 00:00:55 - train.py[line:549] - INFO: 2000 / 4988
2023-01-12 00:00:55 - train.py[line:551] - INFO: load:1.49 valid_run:1502.09 task_valid:1450.07 collect_output:40.50
2023-01-12 00:03:24 - train.py[line:549] - INFO: 2200 / 4988
2023-01-12 00:03:24 - train.py[line:551] - INFO: load:1.51 valid_run:1651.25 task_valid:1595.25 collect_output:43.45
2023-01-12 00:05:54 - train.py[line:549] - INFO: 2400 / 4988
2023-01-12 00:05:54 - train.py[line:551] - INFO: load:1.54 valid_run:1800.32 task_valid:1740.21 collect_output:46.54
2023-01-12 00:08:22 - train.py[line:549] - INFO: 2600 / 4988
2023-01-12 00:08:22 - train.py[line:551] - INFO: load:1.56 valid_run:1948.96 task_valid:1882.26 collect_output:52.09
2023-01-12 00:10:53 - train.py[line:549] - INFO: 2800 / 4988
2023-01-12 00:10:53 - train.py[line:551] - INFO: load:1.59 valid_run:2099.16 task_valid:2028.04 collect_output:55.48
2023-01-12 00:13:22 - train.py[line:549] - INFO: 3000 / 4988
2023-01-12 00:13:22 - train.py[line:551] - INFO: load:1.61 valid_run:2248.95 task_valid:2174.77 collect_output:57.52
2023-01-12 00:15:52 - train.py[line:549] - INFO: 3200 / 4988
2023-01-12 00:15:52 - train.py[line:551] - INFO: load:1.64 valid_run:2398.57 task_valid:2319.48 collect_output:61.38
2023-01-12 00:18:23 - train.py[line:549] - INFO: 3400 / 4988
2023-01-12 00:18:23 - train.py[line:551] - INFO: load:1.66 valid_run:2549.12 task_valid:2465.23 collect_output:65.13
2023-01-12 00:20:53 - train.py[line:549] - INFO: 3600 / 4988
2023-01-12 00:20:53 - train.py[line:551] - INFO: load:1.69 valid_run:2699.61 task_valid:2612.57 collect_output:67.25
2023-01-12 00:23:21 - train.py[line:549] - INFO: 3800 / 4988
2023-01-12 00:23:21 - train.py[line:551] - INFO: load:1.71 valid_run:2847.05 task_valid:2754.59 collect_output:71.61
2023-01-12 00:25:51 - train.py[line:549] - INFO: 4000 / 4988
2023-01-12 00:25:51 - train.py[line:551] - INFO: load:1.74 valid_run:2996.72 task_valid:2900.24 collect_output:74.56
2023-01-12 00:28:21 - train.py[line:549] - INFO: 4200 / 4988
2023-01-12 00:28:21 - train.py[line:551] - INFO: load:1.76 valid_run:3147.23 task_valid:3045.11 collect_output:79.16
2023-01-12 00:30:51 - train.py[line:549] - INFO: 4400 / 4988
2023-01-12 00:30:51 - train.py[line:551] - INFO: load:1.79 valid_run:3296.44 task_valid:3190.26 collect_output:82.20
2023-01-12 00:33:21 - train.py[line:549] - INFO: 4600 / 4988
2023-01-12 00:33:21 - train.py[line:551] - INFO: load:1.82 valid_run:3446.78 task_valid:3336.79 collect_output:84.97
2023-01-12 00:35:52 - train.py[line:549] - INFO: 4800 / 4988
2023-01-12 00:35:52 - train.py[line:551] - INFO: load:1.84 valid_run:3597.58 task_valid:3483.71 collect_output:87.82

====================================================================================================
SGG eval:     R @ 50: 0.4074;     R @ 100: 0.4766;     R @ 500: 0.5058;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2526;    mR @ 100: 0.3054;    mR @ 500: 0.3395;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6537) (covered in:0.6458) (covering:0.3714) (eating:0.5294) (flying in:0.0000) (growing on:0.1250) (hanging from:0.2742) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.4788) (says:0.0000) (sitting on:0.7426) (standing on:0.1827) (using:0.6500) (walking in:0.0000) (walking on:0.1802) (watching:0.1806) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.4074;     R @ 100: 0.4766;     R @ 500: 0.5058;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2526;    mR @ 100: 0.3054;    mR @ 500: 0.3395;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6537) (covered in:0.6458) (covering:0.3714) (eating:0.5294) (flying in:0.0000) (growing on:0.1250) (hanging from:0.2742) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.8438) (playing:0.0000) (riding:0.4788) (says:0.0000) (sitting on:0.7426) (standing on:0.1827) (using:0.6500) (walking in:0.0000) (walking on:0.1802) (watching:0.1806) 
--------------------------------------------------------
====================================================================================================

2023-01-12 00:38:23 - train.py[line:487] - INFO: 0.47661484593837533
2023-01-12 00:38:23 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-12 00:38:23 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.374 | loss_v1 0 | loss_v2 0 | nll_loss 0.223 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.476615 | ppl 1.17 | vqa_score 0.3863 | wps 119.6 | wpb 89.9 | bsz 30 | num_updates 38000 | best_R@100 0.69005
2023-01-12 00:38:24 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 38000 updates
2023-01-12 00:38:24 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_38000.pt
2023-01-12 00:39:01 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_38000.pt
2023-01-12 00:40:24 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_38000.pt (epoch 1 @ 38000 updates, score 0.47661484593837533) (writing took 120.27756711654365 seconds)
2023-01-12 00:40:26 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-12 00:40:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:40:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:40:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:40:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:40:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:40:41 - progress_bar.py[line:274] - INFO: epoch 001:  38063 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4423, wps=0.4, ups=0, wpb=108.4, bsz=40, num_updates=38010, lr=3.22865e-05, gnorm=0.423, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=211628
2023-01-12 00:40:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:40:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:40:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:40:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:40:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:40:58 - progress_bar.py[line:274] - INFO: epoch 001:  38073 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4742, wps=100.6, ups=0.62, wpb=108.8, bsz=40, num_updates=38020, lr=3.22813e-05, gnorm=0.375, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=211644
2023-01-12 00:41:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:14 - progress_bar.py[line:274] - INFO: epoch 001:  38083 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4062, wps=102.6, ups=0.62, wpb=110.4, bsz=40, num_updates=38030, lr=3.2276e-05, gnorm=0.201, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=211660
2023-01-12 00:41:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:31 - progress_bar.py[line:274] - INFO: epoch 001:  38093 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3793, wps=104.7, ups=0.63, wpb=111.5, bsz=40, num_updates=38040, lr=3.22708e-05, gnorm=0.338, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=211677
2023-01-12 00:41:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:47 - progress_bar.py[line:274] - INFO: epoch 001:  38103 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3964, wps=101.7, ups=0.63, wpb=108.4, bsz=40, num_updates=38050, lr=3.22656e-05, gnorm=0.304, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=211693
2023-01-12 00:41:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:41:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:03 - progress_bar.py[line:274] - INFO: epoch 001:  38113 / 100000 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4468, wps=102.2, ups=0.61, wpb=111.3, bsz=40, num_updates=38060, lr=3.22604e-05, gnorm=0.243, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=211710
2023-01-12 00:42:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:20 - progress_bar.py[line:274] - INFO: epoch 001:  38123 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4896, wps=101.1, ups=0.61, wpb=110.6, bsz=40, num_updates=38070, lr=3.22552e-05, gnorm=0.255, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=211726
2023-01-12 00:42:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:36 - progress_bar.py[line:274] - INFO: epoch 001:  38133 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4579, wps=102.9, ups=0.63, wpb=109.5, bsz=40, num_updates=38080, lr=3.225e-05, gnorm=0.228, clip=0, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=211742
2023-01-12 00:42:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:53 - progress_bar.py[line:274] - INFO: epoch 001:  38143 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4409, wps=101.1, ups=0.61, wpb=110.8, bsz=40, num_updates=38090, lr=3.22448e-05, gnorm=0.286, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=211759
2023-01-12 00:42:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:42:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:10 - progress_bar.py[line:274] - INFO: epoch 001:  38153 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.47, wps=101.7, ups=0.62, wpb=109.9, bsz=40, num_updates=38100, lr=3.22396e-05, gnorm=0.258, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=211776
2023-01-12 00:43:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:26 - progress_bar.py[line:274] - INFO: epoch 001:  38163 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4421, wps=100.1, ups=0.61, wpb=110, bsz=40, num_updates=38110, lr=3.22344e-05, gnorm=0.305, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=211793
2023-01-12 00:43:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:43 - progress_bar.py[line:274] - INFO: epoch 001:  38173 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5648, wps=100.7, ups=0.62, wpb=108.6, bsz=40, num_updates=38120, lr=3.22292e-05, gnorm=0.293, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=211809
2023-01-12 00:43:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:43:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:00 - progress_bar.py[line:274] - INFO: epoch 001:  38183 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4694, wps=98.3, ups=0.6, wpb=109.5, bsz=40, num_updates=38130, lr=3.2224e-05, gnorm=0.858, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=211826
2023-01-12 00:44:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:16 - progress_bar.py[line:274] - INFO: epoch 001:  38193 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.533, nsentences=40, sample_size=107.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.475, wps=99.8, ups=0.62, wpb=107.5, bsz=40, num_updates=38140, lr=3.22188e-05, gnorm=0.341, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=211843
2023-01-12 00:44:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:33 - progress_bar.py[line:274] - INFO: epoch 001:  38203 / 100000 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=111.667, nsentences=40, sample_size=111.667, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5341, wps=99.5, ups=0.59, wpb=111.7, bsz=40, num_updates=38150, lr=3.22135e-05, gnorm=0.184, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=211860
2023-01-12 00:44:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:50 - progress_bar.py[line:274] - INFO: epoch 001:  38213 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4554, wps=98.8, ups=0.6, wpb=108.9, bsz=40, num_updates=38160, lr=3.22083e-05, gnorm=0.418, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=211876
2023-01-12 00:44:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:44:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:07 - progress_bar.py[line:274] - INFO: epoch 001:  38223 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4381, wps=101.3, ups=0.61, wpb=110.9, bsz=40, num_updates=38170, lr=3.22031e-05, gnorm=0.245, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=211893
2023-01-12 00:45:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:23 - progress_bar.py[line:274] - INFO: epoch 001:  38233 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.42, wps=101, ups=0.62, wpb=109, bsz=40, num_updates=38180, lr=3.21979e-05, gnorm=0.215, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=211910
2023-01-12 00:45:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:40 - progress_bar.py[line:274] - INFO: epoch 001:  38243 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4396, wps=101.4, ups=0.61, wpb=111.4, bsz=40, num_updates=38190, lr=3.21927e-05, gnorm=0.295, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=211926
2023-01-12 00:45:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:45:56 - progress_bar.py[line:274] - INFO: epoch 001:  38253 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4653, wps=103.1, ups=0.62, wpb=110.3, bsz=40, num_updates=38200, lr=3.21875e-05, gnorm=1.025, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=211943
2023-01-12 00:45:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:13 - progress_bar.py[line:274] - INFO: epoch 001:  38263 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4536, wps=103, ups=0.62, wpb=110.3, bsz=40, num_updates=38210, lr=3.21823e-05, gnorm=0.136, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=211959
2023-01-12 00:46:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:29 - progress_bar.py[line:274] - INFO: epoch 001:  38273 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4128, wps=103.5, ups=0.63, wpb=110.1, bsz=40, num_updates=38220, lr=3.21771e-05, gnorm=0.578, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=211975
2023-01-12 00:46:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:46 - progress_bar.py[line:274] - INFO: epoch 001:  38283 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4632, wps=99.2, ups=0.61, wpb=108.2, bsz=40, num_updates=38230, lr=3.21719e-05, gnorm=0.247, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=211992
2023-01-12 00:46:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:46:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:03 - progress_bar.py[line:274] - INFO: epoch 001:  38293 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5545, wps=99, ups=0.6, wpb=109.4, bsz=40, num_updates=38240, lr=3.21667e-05, gnorm=0.364, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=212009
2023-01-12 00:47:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:20 - progress_bar.py[line:274] - INFO: epoch 001:  38303 / 100000 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5111, wps=100.5, ups=0.61, wpb=110.5, bsz=40, num_updates=38250, lr=3.21615e-05, gnorm=0.123, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=212026
2023-01-12 00:47:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:36 - progress_bar.py[line:274] - INFO: epoch 001:  38313 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4854, wps=102.7, ups=0.62, wpb=109.9, bsz=40, num_updates=38260, lr=3.21562e-05, gnorm=0.161, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212042
2023-01-12 00:47:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:53 - progress_bar.py[line:274] - INFO: epoch 001:  38323 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3883, wps=101.4, ups=0.62, wpb=109.9, bsz=40, num_updates=38270, lr=3.2151e-05, gnorm=0.15, clip=0, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=212059
2023-01-12 00:47:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:47:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:09 - progress_bar.py[line:274] - INFO: epoch 001:  38333 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4848, wps=100, ups=0.61, wpb=109.9, bsz=40, num_updates=38280, lr=3.21458e-05, gnorm=0.158, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=212076
2023-01-12 00:48:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:26 - progress_bar.py[line:274] - INFO: epoch 001:  38343 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4623, wps=96.8, ups=0.59, wpb=109.3, bsz=40, num_updates=38290, lr=3.21406e-05, gnorm=0.246, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=212093
2023-01-12 00:48:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:43 - progress_bar.py[line:274] - INFO: epoch 001:  38353 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.505, wps=104.7, ups=0.64, wpb=109.9, bsz=40, num_updates=38300, lr=3.21354e-05, gnorm=0.238, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=212109
2023-01-12 00:48:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:48:59 - progress_bar.py[line:274] - INFO: epoch 001:  38363 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3434, wps=101.1, ups=0.62, wpb=109.3, bsz=40, num_updates=38310, lr=3.21302e-05, gnorm=0.192, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=212125
2023-01-12 00:49:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:15 - progress_bar.py[line:274] - INFO: epoch 001:  38373 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4111, wps=102.9, ups=0.62, wpb=110.2, bsz=40, num_updates=38320, lr=3.2125e-05, gnorm=0.537, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=212142
2023-01-12 00:49:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:32 - progress_bar.py[line:274] - INFO: epoch 001:  38383 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111, nsentences=40, sample_size=111, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4674, wps=101.5, ups=0.61, wpb=111, bsz=40, num_updates=38330, lr=3.21198e-05, gnorm=0.386, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=212158
2023-01-12 00:49:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:48 - progress_bar.py[line:274] - INFO: epoch 001:  38393 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=107.467, nsentences=40, sample_size=107.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5368, wps=101.1, ups=0.63, wpb=107.5, bsz=40, num_updates=38340, lr=3.21146e-05, gnorm=0.324, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212175
2023-01-12 00:49:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:49:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:05 - progress_bar.py[line:274] - INFO: epoch 001:  38403 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4894, wps=103.5, ups=0.62, wpb=110.8, bsz=40, num_updates=38350, lr=3.21094e-05, gnorm=0.743, clip=20, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=212191
2023-01-12 00:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:21 - progress_bar.py[line:274] - INFO: epoch 001:  38413 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3936, wps=103.1, ups=0.61, wpb=111.9, bsz=40, num_updates=38360, lr=3.21042e-05, gnorm=0.158, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212207
2023-01-12 00:50:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:37 - progress_bar.py[line:274] - INFO: epoch 001:  38423 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.402, wps=103.1, ups=0.63, wpb=109.7, bsz=40, num_updates=38370, lr=3.2099e-05, gnorm=0.773, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=212224
2023-01-12 00:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:54 - progress_bar.py[line:274] - INFO: epoch 001:  38433 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.47, wps=99.7, ups=0.61, wpb=109.3, bsz=40, num_updates=38380, lr=3.20937e-05, gnorm=0.272, clip=10, loss_scale=256, train_wall=16, gb_free=10.9, ema_decay=0.9999, wall=212240
2023-01-12 00:50:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:50:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:11 - progress_bar.py[line:274] - INFO: epoch 001:  38443 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=38390, lr=3.20885e-05, gnorm=0.195, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212257
2023-01-12 00:51:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:27 - progress_bar.py[line:274] - INFO: epoch 001:  38453 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4894, wps=105.9, ups=0.64, wpb=111.1, bsz=40, num_updates=38400, lr=3.20833e-05, gnorm=0.179, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=212273
2023-01-12 00:51:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:43 - progress_bar.py[line:274] - INFO: epoch 001:  38463 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4224, wps=99.8, ups=0.61, wpb=108.6, bsz=40, num_updates=38410, lr=3.20781e-05, gnorm=0.161, clip=0, loss_scale=256, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=212290
2023-01-12 00:51:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:51:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:00 - progress_bar.py[line:274] - INFO: epoch 001:  38473 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4815, wps=101.6, ups=0.61, wpb=110.3, bsz=40, num_updates=38420, lr=3.20729e-05, gnorm=0.135, clip=0, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=212306
2023-01-12 00:52:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:17 - progress_bar.py[line:274] - INFO: epoch 001:  38483 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4257, wps=101.1, ups=0.61, wpb=110.5, bsz=40, num_updates=38430, lr=3.20677e-05, gnorm=0.276, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=212323
2023-01-12 00:52:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:33 - progress_bar.py[line:274] - INFO: epoch 001:  38493 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4574, wps=102.6, ups=0.61, wpb=111.9, bsz=40, num_updates=38440, lr=3.20625e-05, gnorm=0.18, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=212339
2023-01-12 00:52:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:50 - progress_bar.py[line:274] - INFO: epoch 001:  38503 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3956, wps=101, ups=0.61, wpb=110.7, bsz=40, num_updates=38450, lr=3.20573e-05, gnorm=0.476, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=212356
2023-01-12 00:52:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:52:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:06 - progress_bar.py[line:274] - INFO: epoch 001:  38513 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4369, wps=100, ups=0.61, wpb=109.4, bsz=40, num_updates=38460, lr=3.20521e-05, gnorm=0.236, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=212373
2023-01-12 00:53:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:23 - progress_bar.py[line:274] - INFO: epoch 001:  38523 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.41, wps=100.4, ups=0.61, wpb=109.1, bsz=40, num_updates=38470, lr=3.20469e-05, gnorm=0.415, clip=0, loss_scale=256, train_wall=16, gb_free=9.6, ema_decay=0.9999, wall=212389
2023-01-12 00:53:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:40 - progress_bar.py[line:274] - INFO: epoch 001:  38533 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3608, wps=100.4, ups=0.61, wpb=110.3, bsz=40, num_updates=38480, lr=3.20417e-05, gnorm=0.222, clip=0, loss_scale=256, train_wall=16, gb_free=9.5, ema_decay=0.9999, wall=212406
2023-01-12 00:53:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:53:56 - progress_bar.py[line:274] - INFO: epoch 001:  38543 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.46, wps=99.6, ups=0.61, wpb=109.1, bsz=40, num_updates=38490, lr=3.20365e-05, gnorm=0.727, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212423
2023-01-12 00:53:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:13 - progress_bar.py[line:274] - INFO: epoch 001:  38553 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4804, wps=99.1, ups=0.6, wpb=109.9, bsz=40, num_updates=38500, lr=3.20313e-05, gnorm=0.097, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=212440
2023-01-12 00:54:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:30 - progress_bar.py[line:274] - INFO: epoch 001:  38563 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4752, wps=100, ups=0.61, wpb=109.4, bsz=40, num_updates=38510, lr=3.2026e-05, gnorm=0.458, clip=10, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=212456
2023-01-12 00:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:47 - progress_bar.py[line:274] - INFO: epoch 001:  38573 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.153, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3451, wps=98.6, ups=0.6, wpb=109, bsz=40, num_updates=38520, lr=3.20208e-05, gnorm=0.132, clip=0, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=212473
2023-01-12 00:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:54:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:04 - progress_bar.py[line:274] - INFO: epoch 001:  38583 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.45, wps=100.7, ups=0.61, wpb=110.9, bsz=40, num_updates=38530, lr=3.20156e-05, gnorm=0.205, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212490
2023-01-12 00:55:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:21 - progress_bar.py[line:274] - INFO: epoch 001:  38593 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4167, wps=99.9, ups=0.6, wpb=110.9, bsz=40, num_updates=38540, lr=3.20104e-05, gnorm=0.243, clip=0, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=212507
2023-01-12 00:55:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:37 - progress_bar.py[line:274] - INFO: epoch 001:  38603 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5088, wps=100.2, ups=0.62, wpb=108.5, bsz=40, num_updates=38550, lr=3.20052e-05, gnorm=0.158, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=212523
2023-01-12 00:55:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:54 - progress_bar.py[line:274] - INFO: epoch 001:  38613 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4762, wps=100.1, ups=0.6, wpb=111.3, bsz=40, num_updates=38560, lr=3.2e-05, gnorm=0.203, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=212540
2023-01-12 00:55:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:55:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:11 - progress_bar.py[line:274] - INFO: epoch 001:  38623 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5046, wps=101.5, ups=0.61, wpb=110.3, bsz=40, num_updates=38570, lr=3.19948e-05, gnorm=0.118, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212557
2023-01-12 00:56:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:27 - progress_bar.py[line:274] - INFO: epoch 001:  38633 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5701, wps=102.9, ups=0.62, wpb=110.2, bsz=40, num_updates=38580, lr=3.19896e-05, gnorm=0.397, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212573
2023-01-12 00:56:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:44 - progress_bar.py[line:274] - INFO: epoch 001:  38643 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.37, wps=99, ups=0.6, wpb=110.3, bsz=40, num_updates=38590, lr=3.19844e-05, gnorm=0.256, clip=0, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=212590
2023-01-12 00:56:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:56:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:00 - progress_bar.py[line:274] - INFO: epoch 001:  38653 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4958, wps=100.8, ups=0.62, wpb=108.6, bsz=40, num_updates=38600, lr=3.19792e-05, gnorm=0.335, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212607
2023-01-12 00:57:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:17 - progress_bar.py[line:274] - INFO: epoch 001:  38663 / 100000 loss=0.324, loss_v1=0, loss_v2=0, nll_loss=0.169, ntokens=106.533, nsentences=40, sample_size=106.533, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4711, wps=96.5, ups=0.6, wpb=106.5, bsz=40, num_updates=38610, lr=3.1974e-05, gnorm=0.365, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=212623
2023-01-12 00:57:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:34 - progress_bar.py[line:274] - INFO: epoch 001:  38673 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4947, wps=99.2, ups=0.6, wpb=109.7, bsz=40, num_updates=38620, lr=3.19688e-05, gnorm=0.392, clip=10, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=212640
2023-01-12 00:57:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:51 - progress_bar.py[line:274] - INFO: epoch 001:  38683 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4639, wps=102.2, ups=0.62, wpb=110.5, bsz=40, num_updates=38630, lr=3.19635e-05, gnorm=0.412, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=212657
2023-01-12 00:57:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:57:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:07 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-12 00:58:08 - progress_bar.py[line:274] - INFO: epoch 001:  38694 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4444, wps=94.4, ups=0.58, wpb=109.1, bsz=40, num_updates=38640, lr=3.19583e-05, gnorm=0.332, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=212674
2023-01-12 00:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:25 - progress_bar.py[line:274] - INFO: epoch 001:  38704 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4679, wps=100, ups=0.61, wpb=108.5, bsz=40, num_updates=38650, lr=3.19531e-05, gnorm=0.17, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=212691
2023-01-12 00:58:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:42 - progress_bar.py[line:274] - INFO: epoch 001:  38714 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.53, wps=100.3, ups=0.61, wpb=108.9, bsz=40, num_updates=38660, lr=3.19479e-05, gnorm=0.179, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=212707
2023-01-12 00:58:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:58:58 - progress_bar.py[line:274] - INFO: epoch 001:  38724 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4639, wps=103.5, ups=0.63, wpb=109.2, bsz=40, num_updates=38670, lr=3.19427e-05, gnorm=0.167, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=212724
2023-01-12 00:58:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:14 - progress_bar.py[line:274] - INFO: epoch 001:  38734 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4563, wps=101.7, ups=0.62, wpb=109.7, bsz=40, num_updates=38680, lr=3.19375e-05, gnorm=0.139, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=212740
2023-01-12 00:59:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:31 - progress_bar.py[line:274] - INFO: epoch 001:  38744 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4086, wps=102.2, ups=0.62, wpb=110.7, bsz=40, num_updates=38690, lr=3.19323e-05, gnorm=0.196, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=212757
2023-01-12 00:59:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:47 - progress_bar.py[line:274] - INFO: epoch 001:  38754 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4679, wps=103.3, ups=0.63, wpb=108.7, bsz=40, num_updates=38700, lr=3.19271e-05, gnorm=0.209, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212773
2023-01-12 00:59:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 00:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:04 - progress_bar.py[line:274] - INFO: epoch 001:  38764 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.413, wps=99.4, ups=0.6, wpb=110.5, bsz=40, num_updates=38710, lr=3.19219e-05, gnorm=0.104, clip=0, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=212790
2023-01-12 01:00:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:20 - progress_bar.py[line:274] - INFO: epoch 001:  38774 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4851, wps=101.1, ups=0.62, wpb=109.4, bsz=40, num_updates=38720, lr=3.19167e-05, gnorm=1.405, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212806
2023-01-12 01:00:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:37 - progress_bar.py[line:274] - INFO: epoch 001:  38784 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.2784, wps=100.5, ups=0.61, wpb=110, bsz=40, num_updates=38730, lr=3.19115e-05, gnorm=1.099, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=212823
2023-01-12 01:00:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:53 - progress_bar.py[line:274] - INFO: epoch 001:  38794 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4737, wps=102.4, ups=0.62, wpb=110.9, bsz=40, num_updates=38740, lr=3.19063e-05, gnorm=0.274, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=212839
2023-01-12 01:00:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:00:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:10 - progress_bar.py[line:274] - INFO: epoch 001:  38804 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.158, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3645, wps=102, ups=0.62, wpb=109.5, bsz=40, num_updates=38750, lr=3.1901e-05, gnorm=0.594, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212856
2023-01-12 01:01:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:26 - progress_bar.py[line:274] - INFO: epoch 001:  38814 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.466, wps=102.7, ups=0.62, wpb=109.9, bsz=40, num_updates=38760, lr=3.18958e-05, gnorm=0.239, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=212872
2023-01-12 01:01:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:42 - progress_bar.py[line:274] - INFO: epoch 001:  38824 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4632, wps=103.5, ups=0.62, wpb=110.6, bsz=40, num_updates=38770, lr=3.18906e-05, gnorm=2.157, clip=20, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=212888
2023-01-12 01:01:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:01:59 - progress_bar.py[line:274] - INFO: epoch 001:  38834 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4752, wps=100.1, ups=0.6, wpb=111.1, bsz=40, num_updates=38780, lr=3.18854e-05, gnorm=0.413, clip=10, loss_scale=256, train_wall=17, gb_free=10, ema_decay=0.9999, wall=212905
2023-01-12 01:02:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:15 - progress_bar.py[line:274] - INFO: epoch 001:  38844 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4316, wps=103, ups=0.62, wpb=109.9, bsz=40, num_updates=38790, lr=3.18802e-05, gnorm=0.198, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212921
2023-01-12 01:02:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:32 - progress_bar.py[line:274] - INFO: epoch 001:  38854 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4352, wps=102, ups=0.61, wpb=110.7, bsz=40, num_updates=38800, lr=3.1875e-05, gnorm=0.257, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212938
2023-01-12 01:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:48 - progress_bar.py[line:274] - INFO: epoch 001:  38864 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.413, wps=102.5, ups=0.62, wpb=110.5, bsz=40, num_updates=38810, lr=3.18698e-05, gnorm=0.221, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=212954
2023-01-12 01:02:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:02:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:04 - progress_bar.py[line:274] - INFO: epoch 001:  38874 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3684, wps=101.4, ups=0.62, wpb=108.5, bsz=40, num_updates=38820, lr=3.18646e-05, gnorm=0.101, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=212971
2023-01-12 01:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:20 - progress_bar.py[line:274] - INFO: epoch 001:  38884 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4574, wps=105.3, ups=0.64, wpb=110.2, bsz=40, num_updates=38830, lr=3.18594e-05, gnorm=0.273, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=212987
2023-01-12 01:03:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:37 - progress_bar.py[line:274] - INFO: epoch 001:  38894 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4158, wps=101, ups=0.62, wpb=108.3, bsz=40, num_updates=38840, lr=3.18542e-05, gnorm=0.156, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=213003
2023-01-12 01:03:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:53 - progress_bar.py[line:274] - INFO: epoch 001:  38904 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4095, wps=100.4, ups=0.62, wpb=108.5, bsz=40, num_updates=38850, lr=3.1849e-05, gnorm=0.193, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=213019
2023-01-12 01:03:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:03:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:10 - progress_bar.py[line:274] - INFO: epoch 001:  38914 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5333, wps=98.8, ups=0.6, wpb=109.5, bsz=40, num_updates=38860, lr=3.18438e-05, gnorm=0.207, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=213036
2023-01-12 01:04:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:27 - progress_bar.py[line:274] - INFO: epoch 001:  38924 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4609, wps=100, ups=0.61, wpb=108.7, bsz=40, num_updates=38870, lr=3.18385e-05, gnorm=0.285, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=213053
2023-01-12 01:04:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:43 - progress_bar.py[line:274] - INFO: epoch 001:  38934 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3644, wps=98.9, ups=0.61, wpb=108.5, bsz=40, num_updates=38880, lr=3.18333e-05, gnorm=0.33, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=213070
2023-01-12 01:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:04:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:00 - progress_bar.py[line:274] - INFO: epoch 001:  38944 / 100000 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=112.333, nsentences=40, sample_size=112.333, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4667, wps=101.9, ups=0.6, wpb=112.3, bsz=40, num_updates=38890, lr=3.18281e-05, gnorm=0.174, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=213086
2023-01-12 01:05:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:17 - progress_bar.py[line:274] - INFO: epoch 001:  38954 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3889, wps=101.2, ups=0.62, wpb=108.4, bsz=40, num_updates=38900, lr=3.18229e-05, gnorm=0.274, clip=0, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=213103
2023-01-12 01:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:33 - progress_bar.py[line:274] - INFO: epoch 001:  38964 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4149, wps=101.7, ups=0.62, wpb=109.9, bsz=40, num_updates=38910, lr=3.18177e-05, gnorm=0.607, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=213119
2023-01-12 01:05:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:50 - progress_bar.py[line:274] - INFO: epoch 001:  38974 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4, wps=101.1, ups=0.61, wpb=110.1, bsz=40, num_updates=38920, lr=3.18125e-05, gnorm=0.199, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=213136
2023-01-12 01:05:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:05:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:06 - progress_bar.py[line:274] - INFO: epoch 001:  38984 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4038, wps=100.8, ups=0.61, wpb=109.9, bsz=40, num_updates=38930, lr=3.18073e-05, gnorm=0.177, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=213152
2023-01-12 01:06:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:23 - progress_bar.py[line:274] - INFO: epoch 001:  38994 / 100000 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5269, wps=101.4, ups=0.61, wpb=110.6, bsz=40, num_updates=38940, lr=3.18021e-05, gnorm=0.194, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=213169
2023-01-12 01:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:39 - progress_bar.py[line:274] - INFO: epoch 001:  39004 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4851, wps=101.6, ups=0.61, wpb=110.2, bsz=40, num_updates=38950, lr=3.17969e-05, gnorm=0.658, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=213186
2023-01-12 01:06:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:56 - progress_bar.py[line:274] - INFO: epoch 001:  39014 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4773, wps=99.4, ups=0.6, wpb=110.1, bsz=40, num_updates=38960, lr=3.17917e-05, gnorm=0.276, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=213203
2023-01-12 01:06:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:06:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:13 - progress_bar.py[line:274] - INFO: epoch 001:  39024 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4909, wps=100.6, ups=0.62, wpb=108.6, bsz=40, num_updates=38970, lr=3.17865e-05, gnorm=1.308, clip=10, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=213219
2023-01-12 01:07:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:30 - progress_bar.py[line:274] - INFO: epoch 001:  39034 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3689, wps=98.9, ups=0.61, wpb=108.3, bsz=40, num_updates=38980, lr=3.17813e-05, gnorm=0.246, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=213236
2023-01-12 01:07:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:46 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 128.0
2023-01-12 01:07:48 - progress_bar.py[line:274] - INFO: epoch 001:  39045 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4545, wps=92.8, ups=0.57, wpb=109, bsz=40, num_updates=38990, lr=3.1776e-05, gnorm=0.327, clip=10, loss_scale=128, train_wall=18, gb_free=10.3, ema_decay=0.9999, wall=213254
2023-01-12 01:07:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:07:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 01:08:04 - progress_bar.py[line:274] - INFO: epoch 001:  39055 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4771, wps=102, ups=0.63, wpb=108.8, bsz=40, num_updates=39000, lr=3.17708e-05, gnorm=0.193, clip=0, loss_scale=128, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=213270
2023-01-12 01:08:04 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-12 01:08:06 - train.py[line:549] - INFO: 0 / 4988
2023-01-12 01:08:06 - train.py[line:551] - INFO: load:1.32 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-12 01:10:41 - train.py[line:549] - INFO: 200 / 4988
2023-01-12 01:10:41 - train.py[line:551] - INFO: load:1.35 valid_run:155.19 task_valid:152.37 collect_output:1.71
2023-01-12 01:13:09 - train.py[line:549] - INFO: 400 / 4988
2023-01-12 01:13:09 - train.py[line:551] - INFO: load:1.37 valid_run:303.11 task_valid:296.22 collect_output:4.70
2023-01-12 01:15:40 - train.py[line:549] - INFO: 600 / 4988
2023-01-12 01:15:40 - train.py[line:551] - INFO: load:1.40 valid_run:454.57 task_valid:439.94 collect_output:11.37
2023-01-12 01:18:09 - train.py[line:549] - INFO: 800 / 4988
2023-01-12 01:18:09 - train.py[line:551] - INFO: load:1.43 valid_run:603.14 task_valid:585.14 collect_output:13.69
2023-01-12 01:20:41 - train.py[line:549] - INFO: 1000 / 4988
2023-01-12 01:20:41 - train.py[line:551] - INFO: load:1.45 valid_run:754.99 task_valid:733.00 collect_output:16.64
2023-01-12 01:23:12 - train.py[line:549] - INFO: 1200 / 4988
2023-01-12 01:23:12 - train.py[line:551] - INFO: load:1.48 valid_run:906.17 task_valid:878.86 collect_output:20.92
2023-01-12 01:25:44 - train.py[line:549] - INFO: 1400 / 4988
2023-01-12 01:25:44 - train.py[line:551] - INFO: load:1.51 valid_run:1058.01 task_valid:1025.09 collect_output:25.48
2023-01-12 01:28:14 - train.py[line:549] - INFO: 1600 / 4988
2023-01-12 01:28:14 - train.py[line:551] - INFO: load:1.53 valid_run:1207.83 task_valid:1166.34 collect_output:33.02
2023-01-12 01:30:43 - train.py[line:549] - INFO: 1800 / 4988
2023-01-12 01:30:43 - train.py[line:551] - INFO: load:1.56 valid_run:1356.76 task_valid:1311.37 collect_output:35.85
2023-01-12 01:33:11 - train.py[line:549] - INFO: 2000 / 4988
2023-01-12 01:33:11 - train.py[line:551] - INFO: load:1.59 valid_run:1504.61 task_valid:1454.73 collect_output:39.29
2023-01-12 01:35:40 - train.py[line:549] - INFO: 2200 / 4988
2023-01-12 01:35:40 - train.py[line:551] - INFO: load:1.62 valid_run:1653.73 task_valid:1599.82 collect_output:42.28
2023-01-12 01:38:09 - train.py[line:549] - INFO: 2400 / 4988
2023-01-12 01:38:09 - train.py[line:551] - INFO: load:1.64 valid_run:1802.86 task_valid:1744.95 collect_output:45.22
2023-01-12 01:40:38 - train.py[line:549] - INFO: 2600 / 4988
2023-01-12 01:40:38 - train.py[line:551] - INFO: load:1.67 valid_run:1951.81 task_valid:1887.14 collect_output:50.95
2023-01-12 01:43:09 - train.py[line:549] - INFO: 2800 / 4988
2023-01-12 01:43:09 - train.py[line:551] - INFO: load:1.70 valid_run:2101.93 task_valid:2032.93 collect_output:54.22
2023-01-12 01:45:39 - train.py[line:549] - INFO: 3000 / 4988
2023-01-12 01:45:39 - train.py[line:551] - INFO: load:1.73 valid_run:2251.72 task_valid:2179.65 collect_output:56.26
2023-01-12 01:48:08 - train.py[line:549] - INFO: 3200 / 4988
2023-01-12 01:48:08 - train.py[line:551] - INFO: load:1.75 valid_run:2401.18 task_valid:2324.01 collect_output:60.31
2023-01-12 01:50:39 - train.py[line:549] - INFO: 3400 / 4988
2023-01-12 01:50:39 - train.py[line:551] - INFO: load:1.78 valid_run:2552.27 task_valid:2469.94 collect_output:64.42
2023-01-12 01:53:10 - train.py[line:549] - INFO: 3600 / 4988
2023-01-12 01:53:10 - train.py[line:551] - INFO: load:1.81 valid_run:2702.69 task_valid:2617.20 collect_output:66.54
2023-01-12 01:55:37 - train.py[line:549] - INFO: 3800 / 4988
2023-01-12 01:55:37 - train.py[line:551] - INFO: load:1.84 valid_run:2850.08 task_valid:2759.10 collect_output:70.98
2023-01-12 01:58:07 - train.py[line:549] - INFO: 4000 / 4988
2023-01-12 01:58:07 - train.py[line:551] - INFO: load:1.86 valid_run:2999.62 task_valid:2904.59 collect_output:73.97
2023-01-12 02:00:38 - train.py[line:549] - INFO: 4200 / 4988
2023-01-12 02:00:38 - train.py[line:551] - INFO: load:1.89 valid_run:3150.43 task_valid:3049.67 collect_output:78.64
2023-01-12 02:03:07 - train.py[line:549] - INFO: 4400 / 4988
2023-01-12 02:03:07 - train.py[line:551] - INFO: load:1.92 valid_run:3299.28 task_valid:3194.47 collect_output:81.64
2023-01-12 02:05:37 - train.py[line:549] - INFO: 4600 / 4988
2023-01-12 02:05:37 - train.py[line:551] - INFO: load:1.95 valid_run:3449.53 task_valid:3340.75 collect_output:84.57
2023-01-12 02:08:08 - train.py[line:549] - INFO: 4800 / 4988
2023-01-12 02:08:08 - train.py[line:551] - INFO: load:1.97 valid_run:3600.16 task_valid:3487.44 collect_output:87.48

====================================================================================================
SGG eval:     R @ 50: 0.3887;     R @ 100: 0.4555;     R @ 500: 0.4811;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2352;    mR @ 100: 0.2856;    mR @ 500: 0.3191;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6537) (covered in:0.5208) (covering:0.3714) (eating:0.5294) (flying in:0.0000) (growing on:0.1250) (hanging from:0.2742) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7604) (playing:0.0000) (riding:0.4493) (says:0.0000) (sitting on:0.7435) (standing on:0.1727) (using:0.6500) (walking in:0.0000) (walking on:0.0721) (watching:0.1389) 
--------------------------------------------------------
====================================================================================================

2023-01-12 02:10:38 - train.py[line:487] - INFO: 0.455481512605042

====================================================================================================
SGG eval:     R @ 50: 0.3887;     R @ 100: 0.4555;     R @ 500: 0.4811;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2352;    mR @ 100: 0.2856;    mR @ 500: 0.3191;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.6537) (covered in:0.5208) (covering:0.3714) (eating:0.5294) (flying in:0.0000) (growing on:0.1250) (hanging from:0.2742) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.7604) (playing:0.0000) (riding:0.4493) (says:0.0000) (sitting on:0.7435) (standing on:0.1727) (using:0.6500) (walking in:0.0000) (walking on:0.0721) (watching:0.1389) 
--------------------------------------------------------
====================================================================================================

2023-01-12 02:10:39 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-12 02:10:39 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.366 | loss_v1 0 | loss_v2 0 | nll_loss 0.216 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.455482 | ppl 1.16 | vqa_score 0.3795 | wps 119.5 | wpb 89.9 | bsz 30 | num_updates 39000 | best_R@100 0.69005
2023-01-12 02:10:39 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 39000 updates
2023-01-12 02:10:39 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_39000.pt
2023-01-12 02:11:15 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_39000.pt
2023-01-12 02:12:36 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_39000.pt (epoch 1 @ 39000 updates, score 0.455481512605042) (writing took 117.24709221348166 seconds)
2023-01-12 02:12:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:12:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:12:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:12:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:12:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:12:53 - progress_bar.py[line:274] - INFO: epoch 001:  39065 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4105, wps=0.4, ups=0, wpb=110.4, bsz=40, num_updates=39010, lr=3.17656e-05, gnorm=0.313, clip=0, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=217159
2023-01-12 02:12:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:12:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:12:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:10 - progress_bar.py[line:274] - INFO: epoch 001:  39075 / 100000 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5281, wps=101.5, ups=0.61, wpb=110.8, bsz=40, num_updates=39020, lr=3.17604e-05, gnorm=0.134, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217176
2023-01-12 02:13:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:26 - progress_bar.py[line:274] - INFO: epoch 001:  39085 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4423, wps=100.7, ups=0.61, wpb=109.3, bsz=40, num_updates=39030, lr=3.17552e-05, gnorm=0.313, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217193
2023-01-12 02:13:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:43 - progress_bar.py[line:274] - INFO: epoch 001:  39095 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4216, wps=102.2, ups=0.62, wpb=109.8, bsz=40, num_updates=39040, lr=3.175e-05, gnorm=1.126, clip=10, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=217209
2023-01-12 02:13:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:13:59 - progress_bar.py[line:274] - INFO: epoch 001:  39105 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.505, wps=104.4, ups=0.63, wpb=110, bsz=40, num_updates=39050, lr=3.17448e-05, gnorm=0.23, clip=10, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=217225
2023-01-12 02:13:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:15 - progress_bar.py[line:274] - INFO: epoch 001:  39115 / 100000 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5, wps=104.7, ups=0.63, wpb=110.1, bsz=40, num_updates=39060, lr=3.17396e-05, gnorm=0.225, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=217241
2023-01-12 02:14:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:32 - progress_bar.py[line:274] - INFO: epoch 001:  39125 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4894, wps=97.7, ups=0.6, wpb=109.3, bsz=40, num_updates=39070, lr=3.17344e-05, gnorm=0.282, clip=0, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=217258
2023-01-12 02:14:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:48 - progress_bar.py[line:274] - INFO: epoch 001:  39135 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4434, wps=103.5, ups=0.63, wpb=110.3, bsz=40, num_updates=39080, lr=3.17292e-05, gnorm=0.381, clip=10, loss_scale=128, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=217274
2023-01-12 02:14:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:14:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:05 - progress_bar.py[line:274] - INFO: epoch 001:  39145 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5258, wps=101.7, ups=0.61, wpb=110.3, bsz=40, num_updates=39090, lr=3.1724e-05, gnorm=0.295, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217291
2023-01-12 02:15:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:21 - progress_bar.py[line:274] - INFO: epoch 001:  39155 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3608, wps=101.8, ups=0.63, wpb=108.3, bsz=40, num_updates=39100, lr=3.17188e-05, gnorm=0.29, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217307
2023-01-12 02:15:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:37 - progress_bar.py[line:274] - INFO: epoch 001:  39165 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5408, wps=100.6, ups=0.61, wpb=109.1, bsz=40, num_updates=39110, lr=3.17135e-05, gnorm=0.328, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217324
2023-01-12 02:15:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:54 - progress_bar.py[line:274] - INFO: epoch 001:  39175 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4545, wps=103.3, ups=0.62, wpb=110.2, bsz=40, num_updates=39120, lr=3.17083e-05, gnorm=0.467, clip=20, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=217340
2023-01-12 02:15:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:15:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:10 - progress_bar.py[line:274] - INFO: epoch 001:  39185 / 100000 loss=0.272, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4938, wps=100.7, ups=0.6, wpb=111.5, bsz=40, num_updates=39130, lr=3.17031e-05, gnorm=0.298, clip=0, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=217357
2023-01-12 02:16:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:27 - progress_bar.py[line:274] - INFO: epoch 001:  39195 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4242, wps=98.8, ups=0.6, wpb=110.3, bsz=40, num_updates=39140, lr=3.16979e-05, gnorm=0.282, clip=10, loss_scale=128, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=217374
2023-01-12 02:16:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:44 - progress_bar.py[line:274] - INFO: epoch 001:  39205 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4954, wps=99.3, ups=0.61, wpb=108.7, bsz=40, num_updates=39150, lr=3.16927e-05, gnorm=0.326, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=217390
2023-01-12 02:16:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:16:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:01 - progress_bar.py[line:274] - INFO: epoch 001:  39215 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4906, wps=99.3, ups=0.6, wpb=109.6, bsz=40, num_updates=39160, lr=3.16875e-05, gnorm=0.165, clip=0, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=217407
2023-01-12 02:17:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:17 - progress_bar.py[line:274] - INFO: epoch 001:  39225 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.434, wps=101.5, ups=0.62, wpb=109.4, bsz=40, num_updates=39170, lr=3.16823e-05, gnorm=0.154, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217423
2023-01-12 02:17:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:34 - progress_bar.py[line:274] - INFO: epoch 001:  39235 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4149, wps=100.1, ups=0.6, wpb=110.6, bsz=40, num_updates=39180, lr=3.16771e-05, gnorm=0.465, clip=10, loss_scale=128, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=217440
2023-01-12 02:17:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:50 - progress_bar.py[line:274] - INFO: epoch 001:  39245 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4194, wps=101.8, ups=0.62, wpb=110.3, bsz=40, num_updates=39190, lr=3.16719e-05, gnorm=0.719, clip=10, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=217457
2023-01-12 02:17:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:17:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:07 - progress_bar.py[line:274] - INFO: epoch 001:  39255 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4059, wps=99.3, ups=0.6, wpb=109.6, bsz=40, num_updates=39200, lr=3.16667e-05, gnorm=0.192, clip=0, loss_scale=128, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=217473
2023-01-12 02:18:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:24 - progress_bar.py[line:274] - INFO: epoch 001:  39265 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.867, nsentences=40, sample_size=111.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5714, wps=102.3, ups=0.61, wpb=111.9, bsz=40, num_updates=39210, lr=3.16615e-05, gnorm=0.208, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217490
2023-01-12 02:18:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:40 - progress_bar.py[line:274] - INFO: epoch 001:  39275 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4074, wps=100.4, ups=0.62, wpb=108.6, bsz=40, num_updates=39220, lr=3.16563e-05, gnorm=0.326, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217506
2023-01-12 02:18:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:57 - progress_bar.py[line:274] - INFO: epoch 001:  39285 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3824, wps=101.2, ups=0.61, wpb=110.2, bsz=40, num_updates=39230, lr=3.1651e-05, gnorm=0.188, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217523
2023-01-12 02:18:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:18:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:13 - progress_bar.py[line:274] - INFO: epoch 001:  39295 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4412, wps=102.6, ups=0.62, wpb=110.8, bsz=40, num_updates=39240, lr=3.16458e-05, gnorm=0.757, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217539
2023-01-12 02:19:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:30 - progress_bar.py[line:274] - INFO: epoch 001:  39305 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4455, wps=98.3, ups=0.6, wpb=109.8, bsz=40, num_updates=39250, lr=3.16406e-05, gnorm=0.527, clip=20, loss_scale=128, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=217556
2023-01-12 02:19:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:46 - progress_bar.py[line:274] - INFO: epoch 001:  39315 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4731, wps=102.9, ups=0.62, wpb=110.5, bsz=40, num_updates=39260, lr=3.16354e-05, gnorm=0.187, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=217573
2023-01-12 02:19:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:03 - progress_bar.py[line:274] - INFO: epoch 001:  39325 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5044, wps=100.4, ups=0.61, wpb=109.5, bsz=40, num_updates=39270, lr=3.16302e-05, gnorm=0.281, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=217589
2023-01-12 02:20:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:20 - progress_bar.py[line:274] - INFO: epoch 001:  39335 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4842, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=39280, lr=3.1625e-05, gnorm=0.199, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=217606
2023-01-12 02:20:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:37 - progress_bar.py[line:274] - INFO: epoch 001:  39345 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=100.9, ups=0.62, wpb=108.7, bsz=40, num_updates=39290, lr=3.16198e-05, gnorm=0.31, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=217622
2023-01-12 02:20:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:53 - progress_bar.py[line:274] - INFO: epoch 001:  39355 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4412, wps=101.1, ups=0.61, wpb=110.6, bsz=40, num_updates=39300, lr=3.16146e-05, gnorm=0.331, clip=10, loss_scale=128, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=217640
2023-01-12 02:20:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:20:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:10 - progress_bar.py[line:274] - INFO: epoch 001:  39365 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=112, nsentences=40, sample_size=112, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3571, wps=102.5, ups=0.61, wpb=112, bsz=40, num_updates=39310, lr=3.16094e-05, gnorm=0.142, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217656
2023-01-12 02:21:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:27 - progress_bar.py[line:274] - INFO: epoch 001:  39375 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4545, wps=100.3, ups=0.6, wpb=110.7, bsz=40, num_updates=39320, lr=3.16042e-05, gnorm=0.93, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217673
2023-01-12 02:21:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:43 - progress_bar.py[line:274] - INFO: epoch 001:  39385 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4455, wps=101.1, ups=0.61, wpb=110.3, bsz=40, num_updates=39330, lr=3.1599e-05, gnorm=0.329, clip=0, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=217690
2023-01-12 02:21:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:00 - progress_bar.py[line:274] - INFO: epoch 001:  39395 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4712, wps=98.6, ups=0.6, wpb=108.7, bsz=40, num_updates=39340, lr=3.15937e-05, gnorm=0.166, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=217706
2023-01-12 02:22:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:17 - progress_bar.py[line:274] - INFO: epoch 001:  39405 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3846, wps=101, ups=0.61, wpb=109.5, bsz=40, num_updates=39350, lr=3.15885e-05, gnorm=0.293, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=217723
2023-01-12 02:22:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:33 - progress_bar.py[line:274] - INFO: epoch 001:  39415 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3511, wps=103, ups=0.62, wpb=111.1, bsz=40, num_updates=39360, lr=3.15833e-05, gnorm=0.156, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217739
2023-01-12 02:22:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:49 - progress_bar.py[line:274] - INFO: epoch 001:  39425 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=107.933, nsentences=40, sample_size=107.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.434, wps=100.8, ups=0.62, wpb=107.9, bsz=40, num_updates=39370, lr=3.15781e-05, gnorm=0.468, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=217756
2023-01-12 02:22:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:22:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:06 - progress_bar.py[line:274] - INFO: epoch 001:  39435 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4561, wps=99, ups=0.6, wpb=109.9, bsz=40, num_updates=39380, lr=3.15729e-05, gnorm=0.582, clip=20, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=217773
2023-01-12 02:23:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:23 - progress_bar.py[line:274] - INFO: epoch 001:  39445 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4409, wps=101.1, ups=0.61, wpb=110.5, bsz=40, num_updates=39390, lr=3.15677e-05, gnorm=0.235, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217789
2023-01-12 02:23:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:39 - progress_bar.py[line:274] - INFO: epoch 001:  39455 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4512, wps=102, ups=0.61, wpb=110.6, bsz=40, num_updates=39400, lr=3.15625e-05, gnorm=0.979, clip=20, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=217806
2023-01-12 02:23:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:56 - progress_bar.py[line:274] - INFO: epoch 001:  39465 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4118, wps=98.4, ups=0.6, wpb=108.7, bsz=40, num_updates=39410, lr=3.15573e-05, gnorm=0.283, clip=10, loss_scale=128, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=217822
2023-01-12 02:23:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:23:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:13 - progress_bar.py[line:274] - INFO: epoch 001:  39475 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4854, wps=99.8, ups=0.61, wpb=109.7, bsz=40, num_updates=39420, lr=3.15521e-05, gnorm=0.488, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217839
2023-01-12 02:24:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:29 - progress_bar.py[line:274] - INFO: epoch 001:  39485 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.3913, wps=101.9, ups=0.62, wpb=110.2, bsz=40, num_updates=39430, lr=3.15469e-05, gnorm=0.286, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=217856
2023-01-12 02:24:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:46 - progress_bar.py[line:274] - INFO: epoch 001:  39495 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4757, wps=99.9, ups=0.61, wpb=109.5, bsz=40, num_updates=39440, lr=3.15417e-05, gnorm=0.255, clip=0, loss_scale=128, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=217872
2023-01-12 02:24:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:24:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:03 - progress_bar.py[line:274] - INFO: epoch 001:  39505 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3654, wps=98.8, ups=0.6, wpb=109.9, bsz=40, num_updates=39450, lr=3.15365e-05, gnorm=0.146, clip=0, loss_scale=128, train_wall=17, gb_free=9.7, ema_decay=0.9999, wall=217889
2023-01-12 02:25:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:19 - progress_bar.py[line:274] - INFO: epoch 001:  39515 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3939, wps=104, ups=0.63, wpb=110.2, bsz=40, num_updates=39460, lr=3.15313e-05, gnorm=0.195, clip=0, loss_scale=128, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=217905
2023-01-12 02:25:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:35 - progress_bar.py[line:274] - INFO: epoch 001:  39525 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.333, nsentences=40, sample_size=111.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4941, wps=103.4, ups=0.62, wpb=111.3, bsz=40, num_updates=39470, lr=3.1526e-05, gnorm=0.359, clip=0, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217922
2023-01-12 02:25:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:52 - progress_bar.py[line:274] - INFO: epoch 001:  39535 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.44, wps=101, ups=0.62, wpb=109.2, bsz=40, num_updates=39480, lr=3.15208e-05, gnorm=0.117, clip=0, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=217938
2023-01-12 02:25:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:25:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:08 - progress_bar.py[line:274] - INFO: epoch 001:  39545 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3839, wps=99.8, ups=0.62, wpb=107.6, bsz=40, num_updates=39490, lr=3.15156e-05, gnorm=0.456, clip=10, loss_scale=128, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217954
2023-01-12 02:26:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:25 - progress_bar.py[line:274] - INFO: epoch 001:  39555 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4955, wps=101, ups=0.62, wpb=108.8, bsz=40, num_updates=39500, lr=3.15104e-05, gnorm=0.424, clip=10, loss_scale=128, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=217971
2023-01-12 02:26:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:41 - progress_bar.py[line:274] - INFO: epoch 001:  39565 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5437, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=39510, lr=3.15052e-05, gnorm=0.363, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=217987
2023-01-12 02:26:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:26:58 - progress_bar.py[line:274] - INFO: epoch 001:  39575 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4227, wps=102.3, ups=0.62, wpb=110.6, bsz=40, num_updates=39520, lr=3.15e-05, gnorm=0.602, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=218004
2023-01-12 02:26:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:14 - progress_bar.py[line:274] - INFO: epoch 001:  39585 / 100000 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.113, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4835, wps=103.5, ups=0.62, wpb=110.7, bsz=40, num_updates=39530, lr=3.14948e-05, gnorm=0.119, clip=0, loss_scale=256, train_wall=16, gb_free=10.7, ema_decay=0.9999, wall=218020
2023-01-12 02:27:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:31 - progress_bar.py[line:274] - INFO: epoch 001:  39595 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4952, wps=99.3, ups=0.61, wpb=108.8, bsz=40, num_updates=39540, lr=3.14896e-05, gnorm=0.393, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=218037
2023-01-12 02:27:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:47 - progress_bar.py[line:274] - INFO: epoch 001:  39605 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.4, nsentences=40, sample_size=107.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4216, wps=98.6, ups=0.61, wpb=107.4, bsz=40, num_updates=39550, lr=3.14844e-05, gnorm=0.209, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=218053
2023-01-12 02:27:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:27:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:03 - progress_bar.py[line:274] - INFO: epoch 001:  39615 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=106.8, nsentences=40, sample_size=106.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4087, wps=99.9, ups=0.62, wpb=106.8, bsz=40, num_updates=39560, lr=3.14792e-05, gnorm=0.462, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=218070
2023-01-12 02:28:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:20 - progress_bar.py[line:274] - INFO: epoch 001:  39625 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4717, wps=103.4, ups=0.62, wpb=111.1, bsz=40, num_updates=39570, lr=3.1474e-05, gnorm=0.317, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=218086
2023-01-12 02:28:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:37 - progress_bar.py[line:274] - INFO: epoch 001:  39635 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4158, wps=98.7, ups=0.6, wpb=109.1, bsz=40, num_updates=39580, lr=3.14688e-05, gnorm=0.473, clip=30, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=218103
2023-01-12 02:28:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:53 - progress_bar.py[line:274] - INFO: epoch 001:  39645 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4444, wps=100.8, ups=0.62, wpb=107.9, bsz=40, num_updates=39590, lr=3.14635e-05, gnorm=0.182, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=218119
2023-01-12 02:28:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:28:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:10 - progress_bar.py[line:274] - INFO: epoch 001:  39655 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4316, wps=99, ups=0.61, wpb=108.7, bsz=40, num_updates=39600, lr=3.14583e-05, gnorm=0.439, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=218136
2023-01-12 02:29:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:26 - progress_bar.py[line:274] - INFO: epoch 001:  39665 / 100000 loss=0.302, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4479, wps=102.4, ups=0.62, wpb=109.9, bsz=40, num_updates=39610, lr=3.14531e-05, gnorm=2.709, clip=40, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=218152
2023-01-12 02:29:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:42 - progress_bar.py[line:274] - INFO: epoch 001:  39675 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4078, wps=101.8, ups=0.62, wpb=109.8, bsz=40, num_updates=39620, lr=3.14479e-05, gnorm=0.196, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=218168
2023-01-12 02:29:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:29:58 - progress_bar.py[line:274] - INFO: epoch 001:  39685 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.425, wps=106.3, ups=0.64, wpb=111.5, bsz=40, num_updates=39630, lr=3.14427e-05, gnorm=0.626, clip=30, loss_scale=256, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=218184
2023-01-12 02:29:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:14 - progress_bar.py[line:274] - INFO: epoch 001:  39695 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4747, wps=104.2, ups=0.63, wpb=110.9, bsz=40, num_updates=39640, lr=3.14375e-05, gnorm=0.265, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=218201
2023-01-12 02:30:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:31 - progress_bar.py[line:274] - INFO: epoch 001:  39705 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5135, wps=101.4, ups=0.62, wpb=109.1, bsz=40, num_updates=39650, lr=3.14323e-05, gnorm=0.194, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=218217
2023-01-12 02:30:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:47 - progress_bar.py[line:274] - INFO: epoch 001:  39715 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4953, wps=100, ups=0.61, wpb=109.1, bsz=40, num_updates=39660, lr=3.14271e-05, gnorm=0.686, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=218234
2023-01-12 02:30:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:30:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:04 - progress_bar.py[line:274] - INFO: epoch 001:  39725 / 100000 loss=0.309, loss_v1=0, loss_v2=0, nll_loss=0.156, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4167, wps=98.8, ups=0.6, wpb=109.7, bsz=40, num_updates=39670, lr=3.14219e-05, gnorm=0.35, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=218250
2023-01-12 02:31:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:21 - progress_bar.py[line:274] - INFO: epoch 001:  39735 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4038, wps=96.9, ups=0.59, wpb=108.6, bsz=40, num_updates=39680, lr=3.14167e-05, gnorm=0.235, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=218268
2023-01-12 02:31:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:38 - progress_bar.py[line:274] - INFO: epoch 001:  39745 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4688, wps=101.5, ups=0.61, wpb=111.1, bsz=40, num_updates=39690, lr=3.14115e-05, gnorm=0.282, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=218284
2023-01-12 02:31:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:54 - progress_bar.py[line:274] - INFO: epoch 001:  39755 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5049, wps=100.2, ups=0.62, wpb=108.5, bsz=40, num_updates=39700, lr=3.14063e-05, gnorm=0.981, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=218301
2023-01-12 02:31:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:31:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:11 - progress_bar.py[line:274] - INFO: epoch 001:  39765 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5208, wps=99.5, ups=0.6, wpb=110, bsz=40, num_updates=39710, lr=3.1401e-05, gnorm=0.257, clip=0, loss_scale=256, train_wall=17, gb_free=9.5, ema_decay=0.9999, wall=218317
2023-01-12 02:32:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:27 - progress_bar.py[line:274] - INFO: epoch 001:  39775 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=103.5, ups=0.63, wpb=110.1, bsz=40, num_updates=39720, lr=3.13958e-05, gnorm=0.508, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=218334
2023-01-12 02:32:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:44 - progress_bar.py[line:274] - INFO: epoch 001:  39785 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.381, wps=100.4, ups=0.61, wpb=109, bsz=40, num_updates=39730, lr=3.13906e-05, gnorm=0.211, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=218350
2023-01-12 02:32:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:32:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:01 - progress_bar.py[line:274] - INFO: epoch 001:  39795 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4579, wps=101.1, ups=0.61, wpb=110.2, bsz=40, num_updates=39740, lr=3.13854e-05, gnorm=0.526, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=218367
2023-01-12 02:33:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:17 - progress_bar.py[line:274] - INFO: epoch 001:  39805 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=107.2, nsentences=40, sample_size=107.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4182, wps=98.3, ups=0.61, wpb=107.2, bsz=40, num_updates=39750, lr=3.13802e-05, gnorm=0.24, clip=0, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=218383
2023-01-12 02:33:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:34 - progress_bar.py[line:274] - INFO: epoch 001:  39815 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.383, wps=99.9, ups=0.6, wpb=110.5, bsz=40, num_updates=39760, lr=3.1375e-05, gnorm=0.162, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=218400
2023-01-12 02:33:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:51 - progress_bar.py[line:274] - INFO: epoch 001:  39825 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.45, wps=101.9, ups=0.62, wpb=109.9, bsz=40, num_updates=39770, lr=3.13698e-05, gnorm=0.247, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=218417
2023-01-12 02:33:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:33:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:07 - progress_bar.py[line:274] - INFO: epoch 001:  39835 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4369, wps=103.3, ups=0.63, wpb=109.1, bsz=40, num_updates=39780, lr=3.13646e-05, gnorm=0.257, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=218433
2023-01-12 02:34:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:23 - progress_bar.py[line:274] - INFO: epoch 001:  39845 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5258, wps=102.5, ups=0.62, wpb=111.1, bsz=40, num_updates=39790, lr=3.13594e-05, gnorm=0.147, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=218449
2023-01-12 02:34:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:40 - progress_bar.py[line:274] - INFO: epoch 001:  39855 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=108.133, nsentences=40, sample_size=108.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4474, wps=97.4, ups=0.6, wpb=108.1, bsz=40, num_updates=39800, lr=3.13542e-05, gnorm=0.235, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=218466
2023-01-12 02:34:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:57 - progress_bar.py[line:274] - INFO: epoch 001:  39865 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3587, wps=98.6, ups=0.6, wpb=109.8, bsz=40, num_updates=39810, lr=3.1349e-05, gnorm=0.282, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=218483
2023-01-12 02:34:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:34:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:14 - progress_bar.py[line:274] - INFO: epoch 001:  39875 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5455, wps=99.9, ups=0.6, wpb=110.5, bsz=40, num_updates=39820, lr=3.13438e-05, gnorm=0.433, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=218500
2023-01-12 02:35:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:31 - progress_bar.py[line:274] - INFO: epoch 001:  39885 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4848, wps=99.1, ups=0.6, wpb=110.5, bsz=40, num_updates=39830, lr=3.13385e-05, gnorm=0.29, clip=0, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=218517
2023-01-12 02:35:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:47 - progress_bar.py[line:274] - INFO: epoch 001:  39895 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.549, wps=100.4, ups=0.62, wpb=108.7, bsz=40, num_updates=39840, lr=3.13333e-05, gnorm=0.26, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=218534
2023-01-12 02:35:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:35:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:04 - progress_bar.py[line:274] - INFO: epoch 001:  39905 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3854, wps=100.9, ups=0.61, wpb=110.5, bsz=40, num_updates=39850, lr=3.13281e-05, gnorm=0.283, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=218550
2023-01-12 02:36:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:20 - progress_bar.py[line:274] - INFO: epoch 001:  39915 / 100000 loss=0.307, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4087, wps=101.4, ups=0.62, wpb=108.9, bsz=40, num_updates=39860, lr=3.13229e-05, gnorm=0.276, clip=10, loss_scale=256, train_wall=16, gb_free=11.1, ema_decay=0.9999, wall=218567
2023-01-12 02:36:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:37 - progress_bar.py[line:274] - INFO: epoch 001:  39925 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4949, wps=102.4, ups=0.61, wpb=111.1, bsz=40, num_updates=39870, lr=3.13177e-05, gnorm=0.326, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=218583
2023-01-12 02:36:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:54 - progress_bar.py[line:274] - INFO: epoch 001:  39935 / 100000 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.111, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5326, wps=101.2, ups=0.61, wpb=110.7, bsz=40, num_updates=39880, lr=3.13125e-05, gnorm=0.195, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=218600
2023-01-12 02:36:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:36:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:10 - progress_bar.py[line:274] - INFO: epoch 001:  39945 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4414, wps=100.2, ups=0.61, wpb=108.7, bsz=40, num_updates=39890, lr=3.13073e-05, gnorm=0.244, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=218616
2023-01-12 02:37:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:27 - progress_bar.py[line:274] - INFO: epoch 001:  39955 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3786, wps=99.5, ups=0.61, wpb=109.5, bsz=40, num_updates=39900, lr=3.13021e-05, gnorm=0.177, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=218633
2023-01-12 02:37:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:44 - progress_bar.py[line:274] - INFO: epoch 001:  39965 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=108.2, nsentences=40, sample_size=108.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4425, wps=100.3, ups=0.62, wpb=108.2, bsz=40, num_updates=39910, lr=3.12969e-05, gnorm=0.269, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=218650
2023-01-12 02:37:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:37:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:00 - progress_bar.py[line:274] - INFO: epoch 001:  39975 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4717, wps=102.1, ups=0.62, wpb=109.5, bsz=40, num_updates=39920, lr=3.12917e-05, gnorm=0.331, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=218666
2023-01-12 02:38:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:17 - progress_bar.py[line:274] - INFO: epoch 001:  39985 / 100000 loss=0.31, loss_v1=0, loss_v2=0, nll_loss=0.157, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4057, wps=102.2, ups=0.62, wpb=110.5, bsz=40, num_updates=39930, lr=3.12865e-05, gnorm=1.214, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=218683
2023-01-12 02:38:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:34 - progress_bar.py[line:274] - INFO: epoch 001:  39995 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4078, wps=98.4, ups=0.6, wpb=109.5, bsz=40, num_updates=39940, lr=3.12812e-05, gnorm=0.256, clip=10, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=218700
2023-01-12 02:38:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:50 - progress_bar.py[line:274] - INFO: epoch 001:  40005 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4057, wps=102, ups=0.63, wpb=108.5, bsz=40, num_updates=39950, lr=3.1276e-05, gnorm=0.996, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=218716
2023-01-12 02:38:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:38:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:07 - progress_bar.py[line:274] - INFO: epoch 001:  40015 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5392, wps=98.6, ups=0.6, wpb=109.3, bsz=40, num_updates=39960, lr=3.12708e-05, gnorm=0.382, clip=20, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=218733
2023-01-12 02:39:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:24 - progress_bar.py[line:274] - INFO: epoch 001:  40025 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4343, wps=99.3, ups=0.6, wpb=110.1, bsz=40, num_updates=39970, lr=3.12656e-05, gnorm=0.292, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=218750
2023-01-12 02:39:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:41 - progress_bar.py[line:274] - INFO: epoch 001:  40035 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4554, wps=98.5, ups=0.59, wpb=110.7, bsz=40, num_updates=39980, lr=3.12604e-05, gnorm=0.308, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=218767
2023-01-12 02:39:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:57 - progress_bar.py[line:274] - INFO: epoch 001:  40045 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=101.2, ups=0.62, wpb=109.3, bsz=40, num_updates=39990, lr=3.12552e-05, gnorm=0.102, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=218783
2023-01-12 02:39:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:39:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:40:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:40:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:40:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 02:40:14 - progress_bar.py[line:274] - INFO: epoch 001:  40055 / 100000 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=112.067, nsentences=40, sample_size=112.067, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4483, wps=101.2, ups=0.6, wpb=112.1, bsz=40, num_updates=40000, lr=3.125e-05, gnorm=0.396, clip=10, loss_scale=256, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=218800
2023-01-12 02:40:14 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-12 02:40:16 - train.py[line:549] - INFO: 0 / 4988
2023-01-12 02:40:16 - train.py[line:551] - INFO: load:1.47 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-12 02:42:47 - train.py[line:549] - INFO: 200 / 4988
2023-01-12 02:42:47 - train.py[line:551] - INFO: load:1.50 valid_run:151.29 task_valid:148.34 collect_output:1.87
2023-01-12 02:45:15 - train.py[line:549] - INFO: 400 / 4988
2023-01-12 02:45:15 - train.py[line:551] - INFO: load:1.52 valid_run:299.34 task_valid:291.94 collect_output:5.27
2023-01-12 02:47:47 - train.py[line:549] - INFO: 600 / 4988
2023-01-12 02:47:47 - train.py[line:551] - INFO: load:1.55 valid_run:450.80 task_valid:435.63 collect_output:11.98
2023-01-12 02:50:16 - train.py[line:549] - INFO: 800 / 4988
2023-01-12 02:50:16 - train.py[line:551] - INFO: load:1.57 valid_run:599.37 task_valid:580.71 collect_output:14.44
2023-01-12 02:52:47 - train.py[line:549] - INFO: 1000 / 4988
2023-01-12 02:52:47 - train.py[line:551] - INFO: load:1.60 valid_run:750.92 task_valid:728.34 collect_output:17.34
2023-01-12 02:55:18 - train.py[line:549] - INFO: 1200 / 4988
2023-01-12 02:55:18 - train.py[line:551] - INFO: load:1.62 valid_run:902.06 task_valid:874.18 collect_output:21.61
2023-01-12 02:57:51 - train.py[line:549] - INFO: 1400 / 4988
2023-01-12 02:57:51 - train.py[line:551] - INFO: load:1.65 valid_run:1054.47 task_valid:1020.26 collect_output:26.91
2023-01-12 03:00:21 - train.py[line:549] - INFO: 1600 / 4988
2023-01-12 03:00:21 - train.py[line:551] - INFO: load:1.67 valid_run:1204.53 task_valid:1161.36 collect_output:34.85
2023-01-12 03:02:50 - train.py[line:549] - INFO: 1800 / 4988
2023-01-12 03:02:50 - train.py[line:551] - INFO: load:1.70 valid_run:1353.43 task_valid:1306.34 collect_output:37.74
2023-01-12 03:05:18 - train.py[line:549] - INFO: 2000 / 4988
2023-01-12 03:05:18 - train.py[line:551] - INFO: load:1.73 valid_run:1501.38 task_valid:1449.94 collect_output:41.04
2023-01-12 03:07:47 - train.py[line:549] - INFO: 2200 / 4988
2023-01-12 03:07:47 - train.py[line:551] - INFO: load:1.75 valid_run:1650.63 task_valid:1595.00 collect_output:44.19
2023-01-12 03:10:17 - train.py[line:549] - INFO: 2400 / 4988
2023-01-12 03:10:17 - train.py[line:551] - INFO: load:1.78 valid_run:1799.82 task_valid:1740.09 collect_output:47.26
2023-01-12 03:12:46 - train.py[line:549] - INFO: 2600 / 4988
2023-01-12 03:12:46 - train.py[line:551] - INFO: load:1.80 valid_run:1948.70 task_valid:1882.09 collect_output:53.12
2023-01-12 03:15:16 - train.py[line:549] - INFO: 2800 / 4988
2023-01-12 03:15:16 - train.py[line:551] - INFO: load:1.83 valid_run:2098.81 task_valid:2027.89 collect_output:56.41
2023-01-12 03:17:46 - train.py[line:549] - INFO: 3000 / 4988
2023-01-12 03:17:46 - train.py[line:551] - INFO: load:1.86 valid_run:2248.61 task_valid:2174.53 collect_output:58.53
2023-01-12 03:20:15 - train.py[line:549] - INFO: 3200 / 4988
2023-01-12 03:20:15 - train.py[line:551] - INFO: load:1.88 valid_run:2398.17 task_valid:2319.10 collect_output:62.49
2023-01-12 03:22:46 - train.py[line:549] - INFO: 3400 / 4988
2023-01-12 03:22:46 - train.py[line:551] - INFO: load:1.91 valid_run:2548.87 task_valid:2465.01 collect_output:66.24
2023-01-12 03:25:17 - train.py[line:549] - INFO: 3600 / 4988
2023-01-12 03:25:17 - train.py[line:551] - INFO: load:1.93 valid_run:2699.26 task_valid:2612.56 collect_output:68.05
2023-01-12 03:27:44 - train.py[line:549] - INFO: 3800 / 4988
2023-01-12 03:27:44 - train.py[line:551] - INFO: load:1.96 valid_run:2846.86 task_valid:2754.50 collect_output:72.65
2023-01-12 03:30:14 - train.py[line:549] - INFO: 4000 / 4988
2023-01-12 03:30:14 - train.py[line:551] - INFO: load:1.98 valid_run:2996.45 task_valid:2900.02 collect_output:75.70
2023-01-12 03:32:45 - train.py[line:549] - INFO: 4200 / 4988
2023-01-12 03:32:45 - train.py[line:551] - INFO: load:2.01 valid_run:3147.29 task_valid:3045.22 collect_output:80.31
2023-01-12 03:35:14 - train.py[line:549] - INFO: 4400 / 4988
2023-01-12 03:35:14 - train.py[line:551] - INFO: load:2.04 valid_run:3295.99 task_valid:3190.10 collect_output:83.09
2023-01-12 03:37:44 - train.py[line:549] - INFO: 4600 / 4988
2023-01-12 03:37:45 - train.py[line:551] - INFO: load:2.06 valid_run:3446.75 task_valid:3337.02 collect_output:85.87
2023-01-12 03:40:15 - train.py[line:549] - INFO: 4800 / 4988
2023-01-12 03:40:15 - train.py[line:551] - INFO: load:2.09 valid_run:3597.54 task_valid:3484.00 collect_output:88.65

====================================================================================================
SGG eval:     R @ 50: 0.3794;     R @ 100: 0.4456;     R @ 500: 0.4719;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2306;    mR @ 100: 0.2767;    mR @ 500: 0.3094;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5927) (covered in:0.5208) (covering:0.3714) (eating:0.5294) (flying in:0.0000) (growing on:0.1250) (hanging from:0.2742) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.6979) (playing:0.0000) (riding:0.4379) (says:0.0000) (sitting on:0.7367) (standing on:0.1643) (using:0.6500) (walking in:0.0000) (walking on:0.0450) (watching:0.1389) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.3794;     R @ 100: 0.4456;     R @ 500: 0.4719;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2306;    mR @ 100: 0.2767;    mR @ 500: 0.3094;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5927) (covered in:0.5208) (covering:0.3714) (eating:0.5294) (flying in:0.0000) (growing on:0.1250) (hanging from:0.2742) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.6979) (playing:0.0000) (riding:0.4379) (says:0.0000) (sitting on:0.7367) (standing on:0.1643) (using:0.6500) (walking in:0.0000) (walking on:0.0450) (watching:0.1389) 
--------------------------------------------------------
====================================================================================================

2023-01-12 03:42:46 - train.py[line:487] - INFO: 0.44561484593837536
2023-01-12 03:42:46 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-12 03:42:47 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.374 | loss_v1 0 | loss_v2 0 | nll_loss 0.222 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.445615 | ppl 1.17 | vqa_score 0.3694 | wps 119.6 | wpb 89.9 | bsz 30 | num_updates 40000 | best_R@100 0.69005
2023-01-12 03:42:47 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 40000 updates
2023-01-12 03:42:47 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_40000.pt
2023-01-12 03:43:23 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_40000.pt
2023-01-12 03:44:44 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_40000.pt (epoch 1 @ 40000 updates, score 0.44561484593837536) (writing took 117.65302486717701 seconds)
2023-01-12 03:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:44:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:44:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:44:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:44:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:01 - progress_bar.py[line:274] - INFO: epoch 001:  40065 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.505, wps=0.4, ups=0, wpb=110.3, bsz=40, num_updates=40010, lr=3.12448e-05, gnorm=0.228, clip=0, loss_scale=256, train_wall=16, gb_free=10, ema_decay=0.9999, wall=222687
2023-01-12 03:45:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:17 - progress_bar.py[line:274] - INFO: epoch 001:  40075 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3978, wps=101.4, ups=0.62, wpb=109.9, bsz=40, num_updates=40020, lr=3.12396e-05, gnorm=0.361, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=222703
2023-01-12 03:45:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:34 - progress_bar.py[line:274] - INFO: epoch 001:  40085 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.5, wps=101.2, ups=0.62, wpb=108.1, bsz=40, num_updates=40030, lr=3.12344e-05, gnorm=0.505, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=222720
2023-01-12 03:45:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:50 - progress_bar.py[line:274] - INFO: epoch 001:  40095 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4722, wps=104.3, ups=0.63, wpb=109.9, bsz=40, num_updates=40040, lr=3.12292e-05, gnorm=0.169, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=222736
2023-01-12 03:45:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:45:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:06 - progress_bar.py[line:274] - INFO: epoch 001:  40105 / 100000 loss=0.3, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4444, wps=98, ups=0.6, wpb=108.1, bsz=40, num_updates=40050, lr=3.1224e-05, gnorm=0.169, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=222753
2023-01-12 03:46:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:23 - progress_bar.py[line:274] - INFO: epoch 001:  40115 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.404, wps=101.9, ups=0.62, wpb=110.3, bsz=40, num_updates=40060, lr=3.12188e-05, gnorm=0.311, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=222769
2023-01-12 03:46:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:39 - progress_bar.py[line:274] - INFO: epoch 001:  40125 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5728, wps=104.6, ups=0.64, wpb=109.3, bsz=40, num_updates=40070, lr=3.12135e-05, gnorm=0.235, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=222785
2023-01-12 03:46:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:55 - progress_bar.py[line:274] - INFO: epoch 001:  40135 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.44, wps=102.7, ups=0.62, wpb=110, bsz=40, num_updates=40080, lr=3.12083e-05, gnorm=0.196, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=222801
2023-01-12 03:46:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:46:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:12 - progress_bar.py[line:274] - INFO: epoch 001:  40145 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4149, wps=99.8, ups=0.6, wpb=110.4, bsz=40, num_updates=40090, lr=3.12031e-05, gnorm=0.225, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=222818
2023-01-12 03:47:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:29 - progress_bar.py[line:274] - INFO: epoch 001:  40155 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4343, wps=98.1, ups=0.6, wpb=109.3, bsz=40, num_updates=40100, lr=3.11979e-05, gnorm=0.837, clip=30, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=222835
2023-01-12 03:47:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:45 - progress_bar.py[line:274] - INFO: epoch 001:  40165 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5591, wps=101, ups=0.61, wpb=110.7, bsz=40, num_updates=40110, lr=3.11927e-05, gnorm=1.533, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=222852
2023-01-12 03:47:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:47:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:02 - progress_bar.py[line:274] - INFO: epoch 001:  40175 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4352, wps=101.2, ups=0.62, wpb=109.3, bsz=40, num_updates=40120, lr=3.11875e-05, gnorm=0.294, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=222868
2023-01-12 03:48:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:18 - progress_bar.py[line:274] - INFO: epoch 001:  40185 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4646, wps=101.8, ups=0.62, wpb=109.3, bsz=40, num_updates=40130, lr=3.11823e-05, gnorm=0.214, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=222884
2023-01-12 03:48:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:35 - progress_bar.py[line:274] - INFO: epoch 001:  40195 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4556, wps=100.1, ups=0.61, wpb=109.5, bsz=40, num_updates=40140, lr=3.11771e-05, gnorm=0.168, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=222901
2023-01-12 03:48:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:51 - progress_bar.py[line:274] - INFO: epoch 001:  40205 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4216, wps=101.7, ups=0.62, wpb=110, bsz=40, num_updates=40150, lr=3.11719e-05, gnorm=1.438, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=222918
2023-01-12 03:48:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:48:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:08 - progress_bar.py[line:274] - INFO: epoch 001:  40215 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4592, wps=102.2, ups=0.62, wpb=110.5, bsz=40, num_updates=40160, lr=3.11667e-05, gnorm=0.629, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=222934
2023-01-12 03:49:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:24 - progress_bar.py[line:274] - INFO: epoch 001:  40225 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4066, wps=99.3, ups=0.61, wpb=109.1, bsz=40, num_updates=40170, lr=3.11615e-05, gnorm=0.304, clip=10, loss_scale=512, train_wall=16, gb_free=9.9, ema_decay=0.9999, wall=222951
2023-01-12 03:49:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:41 - progress_bar.py[line:274] - INFO: epoch 001:  40235 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4894, wps=99.7, ups=0.6, wpb=110.9, bsz=40, num_updates=40180, lr=3.11562e-05, gnorm=0.206, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=222968
2023-01-12 03:49:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:49:58 - progress_bar.py[line:274] - INFO: epoch 001:  40245 / 100000 loss=0.274, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5119, wps=101, ups=0.6, wpb=111.4, bsz=40, num_updates=40190, lr=3.1151e-05, gnorm=0.44, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=222984
2023-01-12 03:49:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:15 - progress_bar.py[line:274] - INFO: epoch 001:  40255 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3804, wps=101.8, ups=0.61, wpb=110.7, bsz=40, num_updates=40200, lr=3.11458e-05, gnorm=0.284, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=223001
2023-01-12 03:50:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:31 - progress_bar.py[line:274] - INFO: epoch 001:  40265 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3738, wps=100.8, ups=0.61, wpb=110.5, bsz=40, num_updates=40210, lr=3.11406e-05, gnorm=1.104, clip=30, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=223018
2023-01-12 03:50:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:48 - progress_bar.py[line:274] - INFO: epoch 001:  40275 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4536, wps=100.7, ups=0.61, wpb=110.9, bsz=40, num_updates=40220, lr=3.11354e-05, gnorm=0.314, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=223034
2023-01-12 03:50:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:50:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:04 - progress_bar.py[line:274] - INFO: epoch 001:  40285 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.267, nsentences=40, sample_size=111.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4362, wps=104.3, ups=0.62, wpb=111.3, bsz=40, num_updates=40230, lr=3.11302e-05, gnorm=0.342, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223051
2023-01-12 03:51:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:21 - progress_bar.py[line:274] - INFO: epoch 001:  40295 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3846, wps=101.6, ups=0.61, wpb=111.1, bsz=40, num_updates=40240, lr=3.1125e-05, gnorm=0.16, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=223067
2023-01-12 03:51:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:38 - progress_bar.py[line:274] - INFO: epoch 001:  40305 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4524, wps=100.8, ups=0.61, wpb=110.7, bsz=40, num_updates=40250, lr=3.11198e-05, gnorm=0.341, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223084
2023-01-12 03:51:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:54 - progress_bar.py[line:274] - INFO: epoch 001:  40315 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4149, wps=101.5, ups=0.62, wpb=108.8, bsz=40, num_updates=40260, lr=3.11146e-05, gnorm=0.143, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223100
2023-01-12 03:51:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:51:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:11 - progress_bar.py[line:274] - INFO: epoch 001:  40325 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5556, wps=100.3, ups=0.61, wpb=109.8, bsz=40, num_updates=40270, lr=3.11094e-05, gnorm=0.328, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=223117
2023-01-12 03:52:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:28 - progress_bar.py[line:274] - INFO: epoch 001:  40335 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.149, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4074, wps=98.4, ups=0.6, wpb=109.9, bsz=40, num_updates=40280, lr=3.11042e-05, gnorm=0.432, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=223134
2023-01-12 03:52:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:44 - progress_bar.py[line:274] - INFO: epoch 001:  40345 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4796, wps=102.3, ups=0.62, wpb=110.5, bsz=40, num_updates=40290, lr=3.1099e-05, gnorm=0.201, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223150
2023-01-12 03:52:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:52:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:01 - progress_bar.py[line:274] - INFO: epoch 001:  40355 / 100000 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.11, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5114, wps=99.8, ups=0.6, wpb=110.7, bsz=40, num_updates=40300, lr=3.10938e-05, gnorm=0.342, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=223167
2023-01-12 03:53:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:17 - progress_bar.py[line:274] - INFO: epoch 001:  40365 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4667, wps=104.4, ups=0.63, wpb=110.4, bsz=40, num_updates=40310, lr=3.10885e-05, gnorm=0.611, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223183
2023-01-12 03:53:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:34 - progress_bar.py[line:274] - INFO: epoch 001:  40375 / 100000 loss=0.271, loss_v1=0, loss_v2=0, nll_loss=0.108, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4691, wps=101.7, ups=0.61, wpb=110.7, bsz=40, num_updates=40320, lr=3.10833e-05, gnorm=0.2, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=223200
2023-01-12 03:53:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:50 - progress_bar.py[line:274] - INFO: epoch 001:  40385 / 100000 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4516, wps=99.7, ups=0.61, wpb=109.5, bsz=40, num_updates=40330, lr=3.10781e-05, gnorm=0.187, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=223217
2023-01-12 03:53:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:53:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:07 - progress_bar.py[line:274] - INFO: epoch 001:  40395 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3861, wps=101, ups=0.61, wpb=109.5, bsz=40, num_updates=40340, lr=3.10729e-05, gnorm=0.231, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=223233
2023-01-12 03:54:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:24 - progress_bar.py[line:274] - INFO: epoch 001:  40405 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3789, wps=102.2, ups=0.61, wpb=111.5, bsz=40, num_updates=40350, lr=3.10677e-05, gnorm=0.467, clip=20, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=223250
2023-01-12 03:54:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:40 - progress_bar.py[line:274] - INFO: epoch 001:  40415 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4255, wps=99.6, ups=0.6, wpb=110, bsz=40, num_updates=40360, lr=3.10625e-05, gnorm=0.227, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=223267
2023-01-12 03:54:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:57 - progress_bar.py[line:274] - INFO: epoch 001:  40425 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.41, wps=102, ups=0.61, wpb=110.7, bsz=40, num_updates=40370, lr=3.10573e-05, gnorm=0.278, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=223283
2023-01-12 03:54:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:54:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:13 - progress_bar.py[line:274] - INFO: epoch 001:  40435 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3913, wps=101.6, ups=0.61, wpb=110.2, bsz=40, num_updates=40380, lr=3.10521e-05, gnorm=0.306, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223300
2023-01-12 03:55:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:30 - progress_bar.py[line:274] - INFO: epoch 001:  40445 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4808, wps=99.6, ups=0.61, wpb=109.2, bsz=40, num_updates=40390, lr=3.10469e-05, gnorm=0.229, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=223316
2023-01-12 03:55:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:46 - progress_bar.py[line:274] - INFO: epoch 001:  40455 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4757, wps=101.6, ups=0.62, wpb=108.8, bsz=40, num_updates=40400, lr=3.10417e-05, gnorm=0.139, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=223332
2023-01-12 03:55:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:55:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:03 - progress_bar.py[line:274] - INFO: epoch 001:  40465 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4835, wps=99.1, ups=0.6, wpb=109.5, bsz=40, num_updates=40410, lr=3.10365e-05, gnorm=0.226, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=223349
2023-01-12 03:56:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:20 - progress_bar.py[line:274] - INFO: epoch 001:  40475 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4421, wps=100.9, ups=0.61, wpb=110.9, bsz=40, num_updates=40420, lr=3.10312e-05, gnorm=0.36, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223366
2023-01-12 03:56:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:36 - progress_bar.py[line:274] - INFO: epoch 001:  40485 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.135, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4086, wps=101.2, ups=0.61, wpb=110.7, bsz=40, num_updates=40430, lr=3.1026e-05, gnorm=0.182, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=223383
2023-01-12 03:56:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:53 - progress_bar.py[line:274] - INFO: epoch 001:  40495 / 100000 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5106, wps=101.5, ups=0.61, wpb=110.5, bsz=40, num_updates=40440, lr=3.10208e-05, gnorm=0.163, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223399
2023-01-12 03:56:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:56:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:10 - progress_bar.py[line:274] - INFO: epoch 001:  40505 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.154, ntokens=107.8, nsentences=40, sample_size=107.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3894, wps=96.7, ups=0.6, wpb=107.8, bsz=40, num_updates=40450, lr=3.10156e-05, gnorm=0.586, clip=10, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=223416
2023-01-12 03:57:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:26 - progress_bar.py[line:274] - INFO: epoch 001:  40515 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.4, nsentences=40, sample_size=111.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5658, wps=103.2, ups=0.62, wpb=111.4, bsz=40, num_updates=40460, lr=3.10104e-05, gnorm=0.146, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=223433
2023-01-12 03:57:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:43 - progress_bar.py[line:274] - INFO: epoch 001:  40525 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4851, wps=99.3, ups=0.6, wpb=109.7, bsz=40, num_updates=40470, lr=3.10052e-05, gnorm=0.275, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=223450
2023-01-12 03:57:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:57:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:00 - progress_bar.py[line:274] - INFO: epoch 001:  40535 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4479, wps=101, ups=0.61, wpb=110.1, bsz=40, num_updates=40480, lr=3.1e-05, gnorm=0.292, clip=0, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=223466
2023-01-12 03:58:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:17 - progress_bar.py[line:274] - INFO: epoch 001:  40545 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5048, wps=100.9, ups=0.61, wpb=109.9, bsz=40, num_updates=40490, lr=3.09948e-05, gnorm=0.363, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=223483
2023-01-12 03:58:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:34 - progress_bar.py[line:274] - INFO: epoch 001:  40555 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4639, wps=99.9, ups=0.6, wpb=110.1, bsz=40, num_updates=40500, lr=3.09896e-05, gnorm=0.356, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=223500
2023-01-12 03:58:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:51 - progress_bar.py[line:274] - INFO: epoch 001:  40565 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4556, wps=100.4, ups=0.6, wpb=111.1, bsz=40, num_updates=40510, lr=3.09844e-05, gnorm=0.256, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=223517
2023-01-12 03:58:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:58:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:07 - progress_bar.py[line:274] - INFO: epoch 001:  40575 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4906, wps=103.2, ups=0.63, wpb=109.9, bsz=40, num_updates=40520, lr=3.09792e-05, gnorm=0.27, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223533
2023-01-12 03:59:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:24 - progress_bar.py[line:274] - INFO: epoch 001:  40585 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4857, wps=98.7, ups=0.61, wpb=108.5, bsz=40, num_updates=40530, lr=3.0974e-05, gnorm=0.231, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223550
2023-01-12 03:59:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:41 - progress_bar.py[line:274] - INFO: epoch 001:  40595 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4706, wps=98.2, ups=0.6, wpb=109.1, bsz=40, num_updates=40540, lr=3.09688e-05, gnorm=0.702, clip=20, loss_scale=1024, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=223567
2023-01-12 03:59:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:57 - progress_bar.py[line:274] - INFO: epoch 001:  40605 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4231, wps=101.2, ups=0.61, wpb=110.1, bsz=40, num_updates=40550, lr=3.09635e-05, gnorm=0.704, clip=10, loss_scale=1024, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=223583
2023-01-12 03:59:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 03:59:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:14 - progress_bar.py[line:274] - INFO: epoch 001:  40615 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4327, wps=99, ups=0.6, wpb=109.4, bsz=40, num_updates=40560, lr=3.09583e-05, gnorm=0.497, clip=10, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=223600
2023-01-12 04:00:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:31 - progress_bar.py[line:274] - INFO: epoch 001:  40625 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3814, wps=101.8, ups=0.62, wpb=109.9, bsz=40, num_updates=40570, lr=3.09531e-05, gnorm=0.447, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223617
2023-01-12 04:00:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:47 - progress_bar.py[line:274] - INFO: epoch 001:  40635 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=100.6, ups=0.61, wpb=110.5, bsz=40, num_updates=40580, lr=3.09479e-05, gnorm=0.611, clip=10, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=223633
2023-01-12 04:00:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:00:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:03 - progress_bar.py[line:274] - INFO: epoch 001:  40645 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4023, wps=103.7, ups=0.63, wpb=110.2, bsz=40, num_updates=40590, lr=3.09427e-05, gnorm=0.257, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223650
2023-01-12 04:01:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:20 - progress_bar.py[line:274] - INFO: epoch 001:  40655 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4955, wps=97.6, ups=0.6, wpb=108.1, bsz=40, num_updates=40600, lr=3.09375e-05, gnorm=0.495, clip=10, loss_scale=1024, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=223667
2023-01-12 04:01:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:37 - progress_bar.py[line:274] - INFO: epoch 001:  40665 / 100000 loss=0.304, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4375, wps=101.1, ups=0.61, wpb=110.7, bsz=40, num_updates=40610, lr=3.09323e-05, gnorm=0.573, clip=20, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=223683
2023-01-12 04:01:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:54 - progress_bar.py[line:274] - INFO: epoch 001:  40675 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3663, wps=99.3, ups=0.6, wpb=109.6, bsz=40, num_updates=40620, lr=3.09271e-05, gnorm=0.554, clip=20, loss_scale=1024, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=223700
2023-01-12 04:01:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:01:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:10 - progress_bar.py[line:274] - INFO: epoch 001:  40685 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=102.2, ups=0.62, wpb=109.6, bsz=40, num_updates=40630, lr=3.09219e-05, gnorm=0.399, clip=10, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=223716
2023-01-12 04:02:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:27 - progress_bar.py[line:274] - INFO: epoch 001:  40695 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4632, wps=102.2, ups=0.62, wpb=109.1, bsz=40, num_updates=40640, lr=3.09167e-05, gnorm=0.208, clip=0, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=223733
2023-01-12 04:02:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:43 - progress_bar.py[line:274] - INFO: epoch 001:  40705 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=108.933, nsentences=40, sample_size=108.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3861, wps=100.3, ups=0.61, wpb=108.9, bsz=40, num_updates=40650, lr=3.09115e-05, gnorm=0.207, clip=0, loss_scale=1024, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=223749
2023-01-12 04:02:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:02:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:00 - progress_bar.py[line:274] - INFO: epoch 001:  40715 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.46, wps=101.3, ups=0.61, wpb=109.8, bsz=40, num_updates=40660, lr=3.09063e-05, gnorm=0.349, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223766
2023-01-12 04:03:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:16 - progress_bar.py[line:274] - INFO: epoch 001:  40725 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4583, wps=101.1, ups=0.61, wpb=110.6, bsz=40, num_updates=40670, lr=3.0901e-05, gnorm=0.195, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223783
2023-01-12 04:03:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:33 - progress_bar.py[line:274] - INFO: epoch 001:  40735 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4667, wps=101.4, ups=0.61, wpb=110.5, bsz=40, num_updates=40680, lr=3.08958e-05, gnorm=0.194, clip=0, loss_scale=1024, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=223799
2023-01-12 04:03:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:49 - progress_bar.py[line:274] - INFO: epoch 001:  40745 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.486, wps=104.3, ups=0.63, wpb=110.7, bsz=40, num_updates=40690, lr=3.08906e-05, gnorm=0.132, clip=0, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=223815
2023-01-12 04:03:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:03:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:06 - progress_bar.py[line:274] - INFO: epoch 001:  40755 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4639, wps=100.6, ups=0.6, wpb=110.9, bsz=40, num_updates=40700, lr=3.08854e-05, gnorm=0.132, clip=0, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223832
2023-01-12 04:04:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:22 - progress_bar.py[line:274] - INFO: epoch 001:  40765 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5208, wps=103.7, ups=0.62, wpb=110.8, bsz=40, num_updates=40710, lr=3.08802e-05, gnorm=0.265, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223848
2023-01-12 04:04:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:39 - progress_bar.py[line:274] - INFO: epoch 001:  40775 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4615, wps=102.4, ups=0.62, wpb=110.3, bsz=40, num_updates=40720, lr=3.0875e-05, gnorm=0.157, clip=0, loss_scale=1024, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=223865
2023-01-12 04:04:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:55 - progress_bar.py[line:274] - INFO: epoch 001:  40785 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5283, wps=103.1, ups=0.63, wpb=108.8, bsz=40, num_updates=40730, lr=3.08698e-05, gnorm=0.115, clip=0, loss_scale=1024, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=223881
2023-01-12 04:04:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:04:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:11 - progress_bar.py[line:274] - INFO: epoch 001:  40795 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4423, wps=100.9, ups=0.62, wpb=109.1, bsz=40, num_updates=40740, lr=3.08646e-05, gnorm=0.693, clip=10, loss_scale=1024, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223897
2023-01-12 04:05:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:28 - progress_bar.py[line:274] - INFO: epoch 001:  40805 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4949, wps=101.8, ups=0.62, wpb=110.1, bsz=40, num_updates=40750, lr=3.08594e-05, gnorm=0.158, clip=0, loss_scale=1024, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=223914
2023-01-12 04:05:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:34 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-01-12 04:05:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:46 - progress_bar.py[line:274] - INFO: epoch 001:  40816 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.562, nsentences=40, sample_size=109.562, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4955, wps=98.1, ups=0.56, wpb=109.6, bsz=40, num_updates=40760, lr=3.08542e-05, gnorm=0.155, clip=0, loss_scale=512, train_wall=18, gb_free=10, ema_decay=0.9999, wall=223932
2023-01-12 04:05:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:05:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:02 - progress_bar.py[line:274] - INFO: epoch 001:  40826 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5052, wps=100.2, ups=0.61, wpb=109.5, bsz=40, num_updates=40770, lr=3.0849e-05, gnorm=0.604, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223949
2023-01-12 04:06:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:19 - progress_bar.py[line:274] - INFO: epoch 001:  40836 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4516, wps=99.7, ups=0.6, wpb=111.1, bsz=40, num_updates=40780, lr=3.08438e-05, gnorm=0.234, clip=0, loss_scale=512, train_wall=17, gb_free=10.6, ema_decay=0.9999, wall=223965
2023-01-12 04:06:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:36 - progress_bar.py[line:274] - INFO: epoch 001:  40846 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4412, wps=101.3, ups=0.61, wpb=110.4, bsz=40, num_updates=40790, lr=3.08385e-05, gnorm=1.099, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=223982
2023-01-12 04:06:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:53 - progress_bar.py[line:274] - INFO: epoch 001:  40856 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4537, wps=99.6, ups=0.61, wpb=109.3, bsz=40, num_updates=40800, lr=3.08333e-05, gnorm=0.336, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=223999
2023-01-12 04:06:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:06:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:09 - progress_bar.py[line:274] - INFO: epoch 001:  40866 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4646, wps=103, ups=0.62, wpb=110.4, bsz=40, num_updates=40810, lr=3.08281e-05, gnorm=0.173, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=224015
2023-01-12 04:07:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:25 - progress_bar.py[line:274] - INFO: epoch 001:  40876 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4023, wps=101.3, ups=0.61, wpb=110.1, bsz=40, num_updates=40820, lr=3.08229e-05, gnorm=0.732, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=224032
2023-01-12 04:07:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:42 - progress_bar.py[line:274] - INFO: epoch 001:  40886 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=108.8, nsentences=40, sample_size=108.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4021, wps=98.8, ups=0.61, wpb=108.8, bsz=40, num_updates=40830, lr=3.08177e-05, gnorm=0.17, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=224048
2023-01-12 04:07:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:07:59 - progress_bar.py[line:274] - INFO: epoch 001:  40896 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4476, wps=99.5, ups=0.6, wpb=110.3, bsz=40, num_updates=40840, lr=3.08125e-05, gnorm=0.233, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=224065
2023-01-12 04:07:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:16 - progress_bar.py[line:274] - INFO: epoch 001:  40906 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3889, wps=101.5, ups=0.61, wpb=110.7, bsz=40, num_updates=40850, lr=3.08073e-05, gnorm=0.64, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=224082
2023-01-12 04:08:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:32 - progress_bar.py[line:274] - INFO: epoch 001:  40916 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5385, wps=100, ups=0.61, wpb=109.4, bsz=40, num_updates=40860, lr=3.08021e-05, gnorm=0.399, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=224098
2023-01-12 04:08:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:49 - progress_bar.py[line:274] - INFO: epoch 001:  40926 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3846, wps=101, ups=0.61, wpb=109.9, bsz=40, num_updates=40870, lr=3.07969e-05, gnorm=0.327, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=224115
2023-01-12 04:08:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:08:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:05 - progress_bar.py[line:274] - INFO: epoch 001:  40936 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3774, wps=102.7, ups=0.62, wpb=109.5, bsz=40, num_updates=40880, lr=3.07917e-05, gnorm=0.252, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=224131
2023-01-12 04:09:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:21 - progress_bar.py[line:274] - INFO: epoch 001:  40946 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5098, wps=102.4, ups=0.62, wpb=109.4, bsz=40, num_updates=40890, lr=3.07865e-05, gnorm=0.185, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=224147
2023-01-12 04:09:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:38 - progress_bar.py[line:274] - INFO: epoch 001:  40956 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4681, wps=101.8, ups=0.61, wpb=111.5, bsz=40, num_updates=40900, lr=3.07812e-05, gnorm=0.233, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=224164
2023-01-12 04:09:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:55 - progress_bar.py[line:274] - INFO: epoch 001:  40966 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4369, wps=101.1, ups=0.61, wpb=110.6, bsz=40, num_updates=40910, lr=3.0776e-05, gnorm=0.132, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=224181
2023-01-12 04:09:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:09:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:11 - progress_bar.py[line:274] - INFO: epoch 001:  40976 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4167, wps=102.3, ups=0.62, wpb=110, bsz=40, num_updates=40920, lr=3.07708e-05, gnorm=0.103, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=224197
2023-01-12 04:10:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:28 - progress_bar.py[line:274] - INFO: epoch 001:  40986 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=112.133, nsentences=40, sample_size=112.133, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3778, wps=102.7, ups=0.61, wpb=112.1, bsz=40, num_updates=40930, lr=3.07656e-05, gnorm=0.223, clip=0, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=224214
2023-01-12 04:10:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:45 - progress_bar.py[line:274] - INFO: epoch 001:  40996 / 100000 loss=0.296, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.46, wps=99.8, ups=0.61, wpb=109.9, bsz=40, num_updates=40940, lr=3.07604e-05, gnorm=0.213, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=224231
2023-01-12 04:10:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:10:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:01 - progress_bar.py[line:274] - INFO: epoch 001:  41006 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4513, wps=102, ups=0.63, wpb=108.7, bsz=40, num_updates=40950, lr=3.07552e-05, gnorm=0.351, clip=10, loss_scale=512, train_wall=16, gb_free=9.7, ema_decay=0.9999, wall=224247
2023-01-12 04:11:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:17 - progress_bar.py[line:274] - INFO: epoch 001:  41016 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4571, wps=100.5, ups=0.61, wpb=109.5, bsz=40, num_updates=40960, lr=3.075e-05, gnorm=0.276, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=224264
2023-01-12 04:11:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:34 - progress_bar.py[line:274] - INFO: epoch 001:  41026 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5049, wps=100.1, ups=0.61, wpb=109.5, bsz=40, num_updates=40970, lr=3.07448e-05, gnorm=1.42, clip=30, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=224280
2023-01-12 04:11:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:51 - progress_bar.py[line:274] - INFO: epoch 001:  41036 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.067, nsentences=40, sample_size=110.067, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4615, wps=101.8, ups=0.62, wpb=110.1, bsz=40, num_updates=40980, lr=3.07396e-05, gnorm=0.253, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=224297
2023-01-12 04:11:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:11:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:12:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:12:07 - progress_bar.py[line:274] - INFO: epoch 001:  41046 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.127, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4583, wps=99.6, ups=0.6, wpb=109.7, bsz=40, num_updates=40990, lr=3.07344e-05, gnorm=0.232, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=224314
2023-01-12 04:12:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:12:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:12:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:12:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:12:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 04:12:24 - progress_bar.py[line:274] - INFO: epoch 001:  41056 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.933, nsentences=40, sample_size=109.933, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4095, wps=102.4, ups=0.62, wpb=109.9, bsz=40, num_updates=41000, lr=3.07292e-05, gnorm=0.306, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=224330
2023-01-12 04:12:24 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-12 04:12:26 - train.py[line:549] - INFO: 0 / 4988
2023-01-12 04:12:26 - train.py[line:551] - INFO: load:1.26 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-12 04:14:57 - train.py[line:549] - INFO: 200 / 4988
2023-01-12 04:14:57 - train.py[line:551] - INFO: load:1.28 valid_run:151.59 task_valid:148.66 collect_output:1.84
2023-01-12 04:17:25 - train.py[line:549] - INFO: 400 / 4988
2023-01-12 04:17:25 - train.py[line:551] - INFO: load:1.31 valid_run:299.52 task_valid:292.24 collect_output:5.15
2023-01-12 04:19:56 - train.py[line:549] - INFO: 600 / 4988
2023-01-12 04:19:56 - train.py[line:551] - INFO: load:1.34 valid_run:450.68 task_valid:436.08 collect_output:11.39
2023-01-12 04:22:25 - train.py[line:549] - INFO: 800 / 4988
2023-01-12 04:22:25 - train.py[line:551] - INFO: load:1.37 valid_run:599.56 task_valid:581.43 collect_output:13.85
2023-01-12 04:24:57 - train.py[line:549] - INFO: 1000 / 4988
2023-01-12 04:24:57 - train.py[line:551] - INFO: load:1.39 valid_run:751.48 task_valid:729.30 collect_output:16.81
2023-01-12 04:27:29 - train.py[line:549] - INFO: 1200 / 4988
2023-01-12 04:27:29 - train.py[line:551] - INFO: load:1.42 valid_run:902.79 task_valid:875.26 collect_output:21.11
2023-01-12 04:30:01 - train.py[line:549] - INFO: 1400 / 4988
2023-01-12 04:30:01 - train.py[line:551] - INFO: load:1.45 valid_run:1054.82 task_valid:1021.47 collect_output:25.86
2023-01-12 04:32:31 - train.py[line:549] - INFO: 1600 / 4988
2023-01-12 04:32:31 - train.py[line:551] - INFO: load:1.47 valid_run:1204.94 task_valid:1162.78 collect_output:33.65
2023-01-12 04:35:00 - train.py[line:549] - INFO: 1800 / 4988
2023-01-12 04:35:00 - train.py[line:551] - INFO: load:1.50 valid_run:1353.85 task_valid:1307.89 collect_output:36.42
2023-01-12 04:37:28 - train.py[line:549] - INFO: 2000 / 4988
2023-01-12 04:37:28 - train.py[line:551] - INFO: load:1.53 valid_run:1501.70 task_valid:1451.50 collect_output:39.62
2023-01-12 04:39:57 - train.py[line:549] - INFO: 2200 / 4988
2023-01-12 04:39:57 - train.py[line:551] - INFO: load:1.56 valid_run:1650.96 task_valid:1596.65 collect_output:42.69
2023-01-12 04:42:27 - train.py[line:549] - INFO: 2400 / 4988
2023-01-12 04:42:27 - train.py[line:551] - INFO: load:1.58 valid_run:1800.40 task_valid:1741.76 collect_output:45.97
2023-01-12 04:44:56 - train.py[line:549] - INFO: 2600 / 4988
2023-01-12 04:44:56 - train.py[line:551] - INFO: load:1.61 valid_run:1949.27 task_valid:1883.92 collect_output:51.63
2023-01-12 04:47:26 - train.py[line:549] - INFO: 2800 / 4988
2023-01-12 04:47:26 - train.py[line:551] - INFO: load:1.64 valid_run:2099.23 task_valid:2029.67 collect_output:54.80
2023-01-12 04:49:56 - train.py[line:549] - INFO: 3000 / 4988
2023-01-12 04:49:56 - train.py[line:551] - INFO: load:1.66 valid_run:2248.91 task_valid:2176.23 collect_output:56.85
2023-01-12 04:52:25 - train.py[line:549] - INFO: 3200 / 4988
2023-01-12 04:52:25 - train.py[line:551] - INFO: load:1.69 valid_run:2398.51 task_valid:2320.73 collect_output:60.90
2023-01-12 04:54:56 - train.py[line:549] - INFO: 3400 / 4988
2023-01-12 04:54:56 - train.py[line:551] - INFO: load:1.73 valid_run:2549.38 task_valid:2466.96 collect_output:64.48
2023-01-12 04:57:27 - train.py[line:549] - INFO: 3600 / 4988
2023-01-12 04:57:27 - train.py[line:551] - INFO: load:1.75 valid_run:2699.59 task_valid:2614.26 collect_output:66.35
2023-01-12 04:59:54 - train.py[line:549] - INFO: 3800 / 4988
2023-01-12 04:59:54 - train.py[line:551] - INFO: load:1.78 valid_run:2847.18 task_valid:2756.19 collect_output:70.95
2023-01-12 05:02:24 - train.py[line:549] - INFO: 4000 / 4988
2023-01-12 05:02:24 - train.py[line:551] - INFO: load:1.81 valid_run:2996.99 task_valid:2901.63 collect_output:74.24
2023-01-12 05:04:55 - train.py[line:549] - INFO: 4200 / 4988
2023-01-12 05:04:55 - train.py[line:551] - INFO: load:1.83 valid_run:3147.83 task_valid:3046.64 collect_output:79.04
2023-01-12 05:07:24 - train.py[line:549] - INFO: 4400 / 4988
2023-01-12 05:07:24 - train.py[line:551] - INFO: load:1.86 valid_run:3296.79 task_valid:3191.61 collect_output:81.98
2023-01-12 05:09:54 - train.py[line:549] - INFO: 4600 / 4988
2023-01-12 05:09:54 - train.py[line:551] - INFO: load:1.89 valid_run:3447.16 task_valid:3338.08 collect_output:84.80
2023-01-12 05:12:25 - train.py[line:549] - INFO: 4800 / 4988
2023-01-12 05:12:25 - train.py[line:551] - INFO: load:1.92 valid_run:3598.00 task_valid:3484.80 collect_output:87.87

====================================================================================================
SGG eval:     R @ 50: 0.3780;     R @ 100: 0.4448;     R @ 500: 0.4718;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2292;    mR @ 100: 0.2744;    mR @ 500: 0.3055;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5634) (covered in:0.5208) (covering:0.3714) (eating:0.5294) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3065) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.6562) (playing:0.0000) (riding:0.4395) (says:0.0000) (sitting on:0.7367) (standing on:0.1560) (using:0.6500) (walking in:0.0000) (walking on:0.0450) (watching:0.1389) 
--------------------------------------------------------
====================================================================================================

2023-01-12 05:14:57 - train.py[line:487] - INFO: 0.4448148459383753

====================================================================================================
SGG eval:     R @ 50: 0.3780;     R @ 100: 0.4448;     R @ 500: 0.4718;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2292;    mR @ 100: 0.2744;    mR @ 500: 0.3055;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(carrying:0.5634) (covered in:0.5208) (covering:0.3714) (eating:0.5294) (flying in:0.0000) (growing on:0.1250) (hanging from:0.3065) (lying on:0.0000) (mounted on:0.0000) (painted on:0.2500) (parked on:0.6562) (playing:0.0000) (riding:0.4395) (says:0.0000) (sitting on:0.7367) (standing on:0.1560) (using:0.6500) (walking in:0.0000) (walking on:0.0450) (watching:0.1389) 
--------------------------------------------------------
====================================================================================================

2023-01-12 05:14:57 - train.py[line:575] - INFO: logits:torch.Size([149614, 21]) sample_ids:torch.Size([149614])
2023-01-12 05:14:57 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.378 | loss_v1 0 | loss_v2 0 | nll_loss 0.228 | ntokens 89.926 | nsentences 29.995 | sample_size 89.926 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.444815 | ppl 1.17 | vqa_score 0.3592 | wps 119.6 | wpb 89.9 | bsz 30 | num_updates 41000 | best_R@100 0.69005
2023-01-12 05:14:57 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 41000 updates
2023-01-12 05:14:57 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_41000.pt
2023-01-12 05:15:41 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_41000.pt
2023-01-12 05:17:06 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/test_combine55_momentum0.995_alpha1.0/1_B20_A1_E1_0.04_5e-5_480/checkpoint_1_41000.pt (epoch 1 @ 41000 updates, score 0.4448148459383753) (writing took 128.80660392716527 seconds)
2023-01-12 05:17:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:23 - progress_bar.py[line:274] - INFO: epoch 001:  41066 / 100000 loss=0.315, loss_v1=0, loss_v2=0, nll_loss=0.165, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3559, wps=0.4, ups=0, wpb=108.3, bsz=40, num_updates=41010, lr=3.0724e-05, gnorm=0.354, clip=0, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=228229
2023-01-12 05:17:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:39 - progress_bar.py[line:274] - INFO: epoch 001:  41076 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.467, nsentences=40, sample_size=110.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4211, wps=102.6, ups=0.62, wpb=110.5, bsz=40, num_updates=41020, lr=3.07188e-05, gnorm=0.244, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228246
2023-01-12 05:17:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:53 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-01-12 05:17:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:17:58 - progress_bar.py[line:274] - INFO: epoch 001:  41087 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=108.625, nsentences=40, sample_size=108.625, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4219, wps=94.4, ups=0.54, wpb=108.6, bsz=40, num_updates=41030, lr=3.07135e-05, gnorm=0.405, clip=10, loss_scale=256, train_wall=18, gb_free=10.2, ema_decay=0.9999, wall=228264
2023-01-12 05:17:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:14 - progress_bar.py[line:274] - INFO: epoch 001:  41097 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.43, wps=102.3, ups=0.63, wpb=109.1, bsz=40, num_updates=41040, lr=3.07083e-05, gnorm=0.444, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=228280
2023-01-12 05:18:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:31 - progress_bar.py[line:274] - INFO: epoch 001:  41107 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.383, wps=103, ups=0.62, wpb=110.9, bsz=40, num_updates=41050, lr=3.07031e-05, gnorm=0.216, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228297
2023-01-12 05:18:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:47 - progress_bar.py[line:274] - INFO: epoch 001:  41117 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4124, wps=104.3, ups=0.63, wpb=110.6, bsz=40, num_updates=41060, lr=3.06979e-05, gnorm=0.324, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=228313
2023-01-12 05:18:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:18:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:03 - progress_bar.py[line:274] - INFO: epoch 001:  41127 / 100000 loss=0.267, loss_v1=0, loss_v2=0, nll_loss=0.105, ntokens=112.067, nsentences=40, sample_size=112.067, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5062, wps=104.9, ups=0.62, wpb=112.1, bsz=40, num_updates=41070, lr=3.06927e-05, gnorm=0.136, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=228329
2023-01-12 05:19:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:20 - progress_bar.py[line:274] - INFO: epoch 001:  41137 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4271, wps=101.2, ups=0.61, wpb=110.9, bsz=40, num_updates=41080, lr=3.06875e-05, gnorm=0.592, clip=10, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=228346
2023-01-12 05:19:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:36 - progress_bar.py[line:274] - INFO: epoch 001:  41147 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4624, wps=103.5, ups=0.62, wpb=110.5, bsz=40, num_updates=41090, lr=3.06823e-05, gnorm=0.226, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=228362
2023-01-12 05:19:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:53 - progress_bar.py[line:274] - INFO: epoch 001:  41157 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4364, wps=101.2, ups=0.62, wpb=108.9, bsz=40, num_updates=41100, lr=3.06771e-05, gnorm=0.38, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=228379
2023-01-12 05:19:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:19:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:10 - progress_bar.py[line:274] - INFO: epoch 001:  41167 / 100000 loss=0.311, loss_v1=0, loss_v2=0, nll_loss=0.163, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.4167, wps=101.7, ups=0.61, wpb=111.6, bsz=40, num_updates=41110, lr=3.06719e-05, gnorm=0.36, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228396
2023-01-12 05:20:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:26 - progress_bar.py[line:274] - INFO: epoch 001:  41177 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4857, wps=101.2, ups=0.62, wpb=108.4, bsz=40, num_updates=41120, lr=3.06667e-05, gnorm=0.384, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=228412
2023-01-12 05:20:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:43 - progress_bar.py[line:274] - INFO: epoch 001:  41187 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.6, nsentences=40, sample_size=111.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3646, wps=101.4, ups=0.61, wpb=111.6, bsz=40, num_updates=41130, lr=3.06615e-05, gnorm=0.212, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=228429
2023-01-12 05:20:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:20:59 - progress_bar.py[line:274] - INFO: epoch 001:  41197 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3491, wps=100.1, ups=0.61, wpb=109.5, bsz=40, num_updates=41140, lr=3.06562e-05, gnorm=1.068, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228446
2023-01-12 05:20:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:16 - progress_bar.py[line:274] - INFO: epoch 001:  41207 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.067, nsentences=40, sample_size=111.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.5049, wps=99.6, ups=0.6, wpb=111.1, bsz=40, num_updates=41150, lr=3.0651e-05, gnorm=1.905, clip=20, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=228463
2023-01-12 05:21:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:33 - progress_bar.py[line:274] - INFO: epoch 001:  41217 / 100000 loss=0.292, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4722, wps=101.6, ups=0.62, wpb=109.7, bsz=40, num_updates=41160, lr=3.06458e-05, gnorm=0.218, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=228479
2023-01-12 05:21:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:50 - progress_bar.py[line:274] - INFO: epoch 001:  41227 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=108.533, nsentences=40, sample_size=108.533, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4095, wps=99.3, ups=0.61, wpb=108.5, bsz=40, num_updates=41170, lr=3.06406e-05, gnorm=1.342, clip=30, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=228496
2023-01-12 05:21:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:21:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:06 - progress_bar.py[line:274] - INFO: epoch 001:  41237 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=108.267, nsentences=40, sample_size=108.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5758, wps=100.9, ups=0.62, wpb=108.3, bsz=40, num_updates=41180, lr=3.06354e-05, gnorm=0.734, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228512
2023-01-12 05:22:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:23 - progress_bar.py[line:274] - INFO: epoch 001:  41247 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.867, nsentences=40, sample_size=109.867, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3556, wps=100.1, ups=0.61, wpb=109.9, bsz=40, num_updates=41190, lr=3.06302e-05, gnorm=0.225, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228529
2023-01-12 05:22:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:40 - progress_bar.py[line:274] - INFO: epoch 001:  41257 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.2, nsentences=40, sample_size=109.2, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4851, wps=99, ups=0.6, wpb=109.2, bsz=40, num_updates=41200, lr=3.0625e-05, gnorm=1.558, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=228546
2023-01-12 05:22:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:56 - progress_bar.py[line:274] - INFO: epoch 001:  41267 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.467, nsentences=40, sample_size=108.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4272, wps=97.5, ups=0.6, wpb=108.5, bsz=40, num_updates=41210, lr=3.06198e-05, gnorm=0.192, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=228563
2023-01-12 05:22:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:22:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:13 - progress_bar.py[line:274] - INFO: epoch 001:  41277 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4135, wps=101.1, ups=0.6, wpb=111.5, bsz=40, num_updates=41220, lr=3.06146e-05, gnorm=0.374, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228579
2023-01-12 05:23:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:30 - progress_bar.py[line:274] - INFO: epoch 001:  41287 / 100000 loss=0.298, loss_v1=0, loss_v2=0, nll_loss=0.144, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3945, wps=103.6, ups=0.62, wpb=110.9, bsz=40, num_updates=41230, lr=3.06094e-05, gnorm=0.17, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228596
2023-01-12 05:23:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:46 - progress_bar.py[line:274] - INFO: epoch 001:  41297 / 100000 loss=0.276, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5056, wps=103.9, ups=0.63, wpb=110.6, bsz=40, num_updates=41240, lr=3.06042e-05, gnorm=0.184, clip=0, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=228612
2023-01-12 05:23:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:23:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:02 - progress_bar.py[line:274] - INFO: epoch 001:  41307 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.126, ntokens=111.8, nsentences=40, sample_size=111.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3918, wps=103.1, ups=0.61, wpb=111.8, bsz=40, num_updates=41250, lr=3.0599e-05, gnorm=0.497, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228628
2023-01-12 05:24:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:19 - progress_bar.py[line:274] - INFO: epoch 001:  41317 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.114, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.55, wps=100.1, ups=0.61, wpb=109, bsz=40, num_updates=41260, lr=3.05938e-05, gnorm=0.187, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228645
2023-01-12 05:24:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:35 - progress_bar.py[line:274] - INFO: epoch 001:  41327 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.533, nsentences=40, sample_size=111.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4592, wps=101.8, ups=0.61, wpb=111.5, bsz=40, num_updates=41270, lr=3.05885e-05, gnorm=0.614, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=228662
2023-01-12 05:24:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:52 - progress_bar.py[line:274] - INFO: epoch 001:  41337 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5094, wps=101.9, ups=0.62, wpb=109.6, bsz=40, num_updates=41280, lr=3.05833e-05, gnorm=0.368, clip=10, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=228678
2023-01-12 05:24:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:24:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:09 - progress_bar.py[line:274] - INFO: epoch 001:  41347 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.39, wps=100.1, ups=0.61, wpb=109.8, bsz=40, num_updates=41290, lr=3.05781e-05, gnorm=0.263, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=228695
2023-01-12 05:25:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:25 - progress_bar.py[line:274] - INFO: epoch 001:  41357 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.116, ntokens=111.733, nsentences=40, sample_size=111.733, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4731, wps=100.7, ups=0.6, wpb=111.7, bsz=40, num_updates=41300, lr=3.05729e-05, gnorm=0.635, clip=10, loss_scale=256, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=228712
2023-01-12 05:25:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:42 - progress_bar.py[line:274] - INFO: epoch 001:  41367 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.6, nsentences=40, sample_size=110.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4479, wps=100.7, ups=0.61, wpb=110.6, bsz=40, num_updates=41310, lr=3.05677e-05, gnorm=0.387, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=228728
2023-01-12 05:25:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:25:59 - progress_bar.py[line:274] - INFO: epoch 001:  41377 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=109.8, nsentences=40, sample_size=109.8, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.48, wps=101.2, ups=0.61, wpb=109.8, bsz=40, num_updates=41320, lr=3.05625e-05, gnorm=0.709, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=228745
2023-01-12 05:25:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:15 - progress_bar.py[line:274] - INFO: epoch 001:  41387 / 100000 loss=0.303, loss_v1=0, loss_v2=0, nll_loss=0.15, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3796, wps=102.5, ups=0.63, wpb=109.3, bsz=40, num_updates=41330, lr=3.05573e-05, gnorm=0.441, clip=20, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=228761
2023-01-12 05:26:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:32 - progress_bar.py[line:274] - INFO: epoch 001:  41397 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4078, wps=99, ups=0.6, wpb=109.1, bsz=40, num_updates=41340, lr=3.05521e-05, gnorm=0.19, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228778
2023-01-12 05:26:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:49 - progress_bar.py[line:274] - INFO: epoch 001:  41407 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4066, wps=100.1, ups=0.6, wpb=110.7, bsz=40, num_updates=41350, lr=3.05469e-05, gnorm=0.382, clip=10, loss_scale=256, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=228795
2023-01-12 05:26:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:26:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:05 - progress_bar.py[line:274] - INFO: epoch 001:  41417 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.404, wps=100.9, ups=0.61, wpb=110.3, bsz=40, num_updates=41360, lr=3.05417e-05, gnorm=0.716, clip=10, loss_scale=256, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=228811
2023-01-12 05:27:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:22 - progress_bar.py[line:274] - INFO: epoch 001:  41427 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3725, wps=101.8, ups=0.61, wpb=111.2, bsz=40, num_updates=41370, lr=3.05365e-05, gnorm=1.405, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=228828
2023-01-12 05:27:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:38 - progress_bar.py[line:274] - INFO: epoch 001:  41437 / 100000 loss=0.285, loss_v1=0, loss_v2=0, nll_loss=0.121, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5155, wps=100, ups=0.61, wpb=109.3, bsz=40, num_updates=41380, lr=3.05313e-05, gnorm=0.144, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=228845
2023-01-12 05:27:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:55 - progress_bar.py[line:274] - INFO: epoch 001:  41447 / 100000 loss=0.28, loss_v1=0, loss_v2=0, nll_loss=0.115, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5865, wps=98.5, ups=0.61, wpb=108.1, bsz=40, num_updates=41390, lr=3.0526e-05, gnorm=0.502, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228861
2023-01-12 05:27:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:27:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:12 - progress_bar.py[line:274] - INFO: epoch 001:  41457 / 100000 loss=0.27, loss_v1=0, loss_v2=0, nll_loss=0.102, ntokens=109.533, nsentences=40, sample_size=109.533, sample_size_v1=0, sample_size_v2=0, ppl=1.07, vqa_score=0.5699, wps=100.1, ups=0.61, wpb=109.5, bsz=40, num_updates=41400, lr=3.05208e-05, gnorm=0.092, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=228878
2023-01-12 05:28:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:29 - progress_bar.py[line:274] - INFO: epoch 001:  41467 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.267, nsentences=40, sample_size=110.267, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3936, wps=100.2, ups=0.61, wpb=110.3, bsz=40, num_updates=41410, lr=3.05156e-05, gnorm=0.152, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228895
2023-01-12 05:28:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:45 - progress_bar.py[line:274] - INFO: epoch 001:  41477 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5505, wps=103.6, ups=0.63, wpb=109.4, bsz=40, num_updates=41420, lr=3.05104e-05, gnorm=0.824, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=228911
2023-01-12 05:28:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:28:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:02 - progress_bar.py[line:274] - INFO: epoch 001:  41487 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3871, wps=99.4, ups=0.6, wpb=109.6, bsz=40, num_updates=41430, lr=3.05052e-05, gnorm=0.428, clip=10, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=228928
2023-01-12 05:29:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:18 - progress_bar.py[line:274] - INFO: epoch 001:  41497 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=108.867, nsentences=40, sample_size=108.867, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5055, wps=99.6, ups=0.61, wpb=108.9, bsz=40, num_updates=41440, lr=3.05e-05, gnorm=0.235, clip=0, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=228944
2023-01-12 05:29:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:35 - progress_bar.py[line:274] - INFO: epoch 001:  41507 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3438, wps=101.1, ups=0.61, wpb=110.3, bsz=40, num_updates=41450, lr=3.04948e-05, gnorm=0.749, clip=30, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228961
2023-01-12 05:29:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:52 - progress_bar.py[line:274] - INFO: epoch 001:  41517 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.142, ntokens=108.333, nsentences=40, sample_size=108.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5, wps=99.3, ups=0.61, wpb=108.3, bsz=40, num_updates=41460, lr=3.04896e-05, gnorm=0.542, clip=20, loss_scale=256, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=228978
2023-01-12 05:29:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:29:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:08 - progress_bar.py[line:274] - INFO: epoch 001:  41527 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4528, wps=101.1, ups=0.62, wpb=109.6, bsz=40, num_updates=41470, lr=3.04844e-05, gnorm=0.612, clip=20, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=228994
2023-01-12 05:30:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:25 - progress_bar.py[line:274] - INFO: epoch 001:  41537 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=109.4, nsentences=40, sample_size=109.4, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4216, wps=99, ups=0.6, wpb=109.4, bsz=40, num_updates=41480, lr=3.04792e-05, gnorm=0.314, clip=0, loss_scale=256, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=229011
2023-01-12 05:30:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:42 - progress_bar.py[line:274] - INFO: epoch 001:  41547 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3267, wps=100.4, ups=0.61, wpb=110.5, bsz=40, num_updates=41490, lr=3.0474e-05, gnorm=0.128, clip=0, loss_scale=256, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=229028
2023-01-12 05:30:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:30:58 - progress_bar.py[line:274] - INFO: epoch 001:  41557 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4082, wps=100.7, ups=0.61, wpb=110.1, bsz=40, num_updates=41500, lr=3.04688e-05, gnorm=0.32, clip=0, loss_scale=256, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=229044
2023-01-12 05:30:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:15 - progress_bar.py[line:274] - INFO: epoch 001:  41567 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.47, wps=101.5, ups=0.62, wpb=109.7, bsz=40, num_updates=41510, lr=3.04635e-05, gnorm=0.235, clip=0, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229061
2023-01-12 05:31:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:31 - progress_bar.py[line:274] - INFO: epoch 001:  41577 / 100000 loss=0.278, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4565, wps=100.3, ups=0.61, wpb=110.4, bsz=40, num_updates=41520, lr=3.04583e-05, gnorm=0.108, clip=0, loss_scale=256, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=229078
2023-01-12 05:31:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:48 - progress_bar.py[line:274] - INFO: epoch 001:  41587 / 100000 loss=0.297, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.44, wps=101.1, ups=0.62, wpb=109.3, bsz=40, num_updates=41530, lr=3.04531e-05, gnorm=0.37, clip=10, loss_scale=256, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229094
2023-01-12 05:31:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:31:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:05 - progress_bar.py[line:274] - INFO: epoch 001:  41597 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.867, nsentences=40, sample_size=110.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4078, wps=100.8, ups=0.61, wpb=110.9, bsz=40, num_updates=41540, lr=3.04479e-05, gnorm=0.195, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229111
2023-01-12 05:32:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:21 - progress_bar.py[line:274] - INFO: epoch 001:  41607 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.147, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3945, wps=101.6, ups=0.62, wpb=109.5, bsz=40, num_updates=41550, lr=3.04427e-05, gnorm=0.706, clip=20, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=229127
2023-01-12 05:32:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:37 - progress_bar.py[line:274] - INFO: epoch 001:  41617 / 100000 loss=0.314, loss_v1=0, loss_v2=0, nll_loss=0.16, ntokens=108.6, nsentences=40, sample_size=108.6, sample_size_v1=0, sample_size_v2=0, ppl=1.12, vqa_score=0.3962, wps=100.5, ups=0.62, wpb=108.6, bsz=40, num_updates=41560, lr=3.04375e-05, gnorm=0.363, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=229144
2023-01-12 05:32:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:54 - progress_bar.py[line:274] - INFO: epoch 001:  41627 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.132, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.5169, wps=101.5, ups=0.61, wpb=110.2, bsz=40, num_updates=41570, lr=3.04323e-05, gnorm=0.958, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=229160
2023-01-12 05:32:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:32:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:11 - progress_bar.py[line:274] - INFO: epoch 001:  41637 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.139, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4476, wps=99.3, ups=0.61, wpb=109, bsz=40, num_updates=41580, lr=3.04271e-05, gnorm=0.519, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229177
2023-01-12 05:33:11 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:28 - progress_bar.py[line:274] - INFO: epoch 001:  41647 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.137, ntokens=110.667, nsentences=40, sample_size=110.667, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4038, wps=99.9, ups=0.6, wpb=110.7, bsz=40, num_updates=41590, lr=3.04219e-05, gnorm=0.357, clip=10, loss_scale=512, train_wall=17, gb_free=9.9, ema_decay=0.9999, wall=229194
2023-01-12 05:33:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:45 - progress_bar.py[line:274] - INFO: epoch 001:  41657 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.12, ntokens=109, nsentences=40, sample_size=109, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5361, wps=97.4, ups=0.6, wpb=109, bsz=40, num_updates=41600, lr=3.04167e-05, gnorm=0.294, clip=0, loss_scale=512, train_wall=17, gb_free=10.1, ema_decay=0.9999, wall=229211
2023-01-12 05:33:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:33:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:01 - progress_bar.py[line:274] - INFO: epoch 001:  41667 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=109.267, nsentences=40, sample_size=109.267, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4519, wps=101.8, ups=0.62, wpb=109.3, bsz=40, num_updates=41610, lr=3.04115e-05, gnorm=0.304, clip=10, loss_scale=512, train_wall=16, gb_free=10.6, ema_decay=0.9999, wall=229227
2023-01-12 05:34:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:18 - progress_bar.py[line:274] - INFO: epoch 001:  41677 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.118, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5, wps=97.3, ups=0.59, wpb=109.6, bsz=40, num_updates=41620, lr=3.04063e-05, gnorm=0.588, clip=10, loss_scale=512, train_wall=17, gb_free=10.4, ema_decay=0.9999, wall=229244
2023-01-12 05:34:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:35 - progress_bar.py[line:274] - INFO: epoch 001:  41687 / 100000 loss=0.301, loss_v1=0, loss_v2=0, nll_loss=0.145, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.396, wps=100.5, ups=0.6, wpb=110.8, bsz=40, num_updates=41630, lr=3.0401e-05, gnorm=0.293, clip=0, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=229261
2023-01-12 05:34:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:51 - progress_bar.py[line:274] - INFO: epoch 001:  41697 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=108.733, nsentences=40, sample_size=108.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3426, wps=100.4, ups=0.62, wpb=108.7, bsz=40, num_updates=41640, lr=3.03958e-05, gnorm=0.347, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=229278
2023-01-12 05:34:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:34:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:08 - progress_bar.py[line:274] - INFO: epoch 001:  41707 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.129, ntokens=109.667, nsentences=40, sample_size=109.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4778, wps=101.5, ups=0.62, wpb=109.7, bsz=40, num_updates=41650, lr=3.03906e-05, gnorm=0.254, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=229294
2023-01-12 05:35:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:25 - progress_bar.py[line:274] - INFO: epoch 001:  41717 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4545, wps=99.3, ups=0.61, wpb=109.3, bsz=40, num_updates=41660, lr=3.03854e-05, gnorm=0.253, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229311
2023-01-12 05:35:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:41 - progress_bar.py[line:274] - INFO: epoch 001:  41727 / 100000 loss=0.273, loss_v1=0, loss_v2=0, nll_loss=0.109, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5417, wps=103, ups=0.62, wpb=110.8, bsz=40, num_updates=41670, lr=3.03802e-05, gnorm=0.122, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229327
2023-01-12 05:35:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:57 - progress_bar.py[line:274] - INFO: epoch 001:  41737 / 100000 loss=0.287, loss_v1=0, loss_v2=0, nll_loss=0.128, ntokens=108.4, nsentences=40, sample_size=108.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4775, wps=102.5, ups=0.63, wpb=108.4, bsz=40, num_updates=41680, lr=3.0375e-05, gnorm=0.26, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=229343
2023-01-12 05:35:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:35:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:14 - progress_bar.py[line:274] - INFO: epoch 001:  41747 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=110.533, nsentences=40, sample_size=110.533, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4842, wps=99.7, ups=0.6, wpb=110.5, bsz=40, num_updates=41690, lr=3.03698e-05, gnorm=0.257, clip=10, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=229360
2023-01-12 05:36:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:31 - progress_bar.py[line:274] - INFO: epoch 001:  41757 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.123, ntokens=110.933, nsentences=40, sample_size=110.933, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4375, wps=101.8, ups=0.61, wpb=110.9, bsz=40, num_updates=41700, lr=3.03646e-05, gnorm=0.819, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229377
2023-01-12 05:36:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:47 - progress_bar.py[line:274] - INFO: epoch 001:  41767 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.4, nsentences=40, sample_size=110.4, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.42, wps=101.1, ups=0.61, wpb=110.4, bsz=40, num_updates=41710, lr=3.03594e-05, gnorm=0.193, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229393
2023-01-12 05:36:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:36:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:04 - progress_bar.py[line:274] - INFO: epoch 001:  41777 / 100000 loss=0.275, loss_v1=0, loss_v2=0, nll_loss=0.112, ntokens=110.333, nsentences=40, sample_size=110.333, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.5263, wps=100.2, ups=0.61, wpb=110.3, bsz=40, num_updates=41720, lr=3.03542e-05, gnorm=0.083, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229410
2023-01-12 05:37:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:21 - progress_bar.py[line:274] - INFO: epoch 001:  41787 / 100000 loss=0.279, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.495, wps=102.2, ups=0.61, wpb=110.8, bsz=40, num_updates=41730, lr=3.0349e-05, gnorm=0.242, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=229427
2023-01-12 05:37:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:37 - progress_bar.py[line:274] - INFO: epoch 001:  41797 / 100000 loss=0.282, loss_v1=0, loss_v2=0, nll_loss=0.119, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4839, wps=102.3, ups=0.62, wpb=110.2, bsz=40, num_updates=41740, lr=3.03438e-05, gnorm=0.289, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=229443
2023-01-12 05:37:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:54 - progress_bar.py[line:274] - INFO: epoch 001:  41807 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.14, ntokens=110.2, nsentences=40, sample_size=110.2, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3232, wps=98.8, ups=0.6, wpb=110.2, bsz=40, num_updates=41750, lr=3.03385e-05, gnorm=0.35, clip=20, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=229460
2023-01-12 05:37:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:37:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:10 - progress_bar.py[line:274] - INFO: epoch 001:  41817 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=109.133, nsentences=40, sample_size=109.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4184, wps=101.3, ups=0.62, wpb=109.1, bsz=40, num_updates=41760, lr=3.03333e-05, gnorm=0.52, clip=10, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=229477
2023-01-12 05:38:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:27 - progress_bar.py[line:274] - INFO: epoch 001:  41827 / 100000 loss=0.293, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4095, wps=101.2, ups=0.61, wpb=110.1, bsz=40, num_updates=41770, lr=3.03281e-05, gnorm=0.985, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229493
2023-01-12 05:38:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:43 - progress_bar.py[line:274] - INFO: epoch 001:  41837 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4059, wps=102.8, ups=0.62, wpb=110.1, bsz=40, num_updates=41780, lr=3.03229e-05, gnorm=0.143, clip=0, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=229510
2023-01-12 05:38:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:38:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:00 - progress_bar.py[line:274] - INFO: epoch 001:  41847 / 100000 loss=0.288, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.6, nsentences=40, sample_size=109.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4216, wps=100.6, ups=0.61, wpb=109.6, bsz=40, num_updates=41790, lr=3.03177e-05, gnorm=0.157, clip=0, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=229526
2023-01-12 05:39:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:17 - progress_bar.py[line:274] - INFO: epoch 001:  41857 / 100000 loss=0.295, loss_v1=0, loss_v2=0, nll_loss=0.141, ntokens=110.133, nsentences=40, sample_size=110.133, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3939, wps=100.3, ups=0.61, wpb=110.1, bsz=40, num_updates=41800, lr=3.03125e-05, gnorm=0.25, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229543
2023-01-12 05:39:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:33 - progress_bar.py[line:274] - INFO: epoch 001:  41867 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.067, nsentences=40, sample_size=109.067, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.3878, wps=99.3, ups=0.61, wpb=109.1, bsz=40, num_updates=41810, lr=3.03073e-05, gnorm=0.109, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229559
2023-01-12 05:39:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:38 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:50 - progress_bar.py[line:274] - INFO: epoch 001:  41877 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4255, wps=100, ups=0.61, wpb=109.5, bsz=40, num_updates=41820, lr=3.03021e-05, gnorm=0.579, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229576
2023-01-12 05:39:50 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:39:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:02 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:07 - progress_bar.py[line:274] - INFO: epoch 001:  41887 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.13, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4124, wps=97.4, ups=0.59, wpb=109.5, bsz=40, num_updates=41830, lr=3.02969e-05, gnorm=0.201, clip=0, loss_scale=512, train_wall=17, gb_free=10, ema_decay=0.9999, wall=229593
2023-01-12 05:40:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:24 - progress_bar.py[line:274] - INFO: epoch 001:  41897 / 100000 loss=0.305, loss_v1=0, loss_v2=0, nll_loss=0.148, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.3608, wps=100, ups=0.62, wpb=107.9, bsz=40, num_updates=41840, lr=3.02917e-05, gnorm=1.069, clip=30, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229610
2023-01-12 05:40:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:40 - progress_bar.py[line:274] - INFO: epoch 001:  41907 / 100000 loss=0.308, loss_v1=0, loss_v2=0, nll_loss=0.151, ntokens=108.067, nsentences=40, sample_size=108.067, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.4196, wps=102.7, ups=0.63, wpb=108.1, bsz=40, num_updates=41850, lr=3.02865e-05, gnorm=0.528, clip=20, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229626
2023-01-12 05:40:40 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:56 - progress_bar.py[line:274] - INFO: epoch 001:  41917 / 100000 loss=0.29, loss_v1=0, loss_v2=0, nll_loss=0.133, ntokens=111.467, nsentences=40, sample_size=111.467, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4316, wps=103.6, ups=0.62, wpb=111.5, bsz=40, num_updates=41860, lr=3.02813e-05, gnorm=0.37, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=229642
2023-01-12 05:40:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:40:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:13 - progress_bar.py[line:274] - INFO: epoch 001:  41927 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=111.133, nsentences=40, sample_size=111.133, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4211, wps=103.5, ups=0.62, wpb=111.1, bsz=40, num_updates=41870, lr=3.0276e-05, gnorm=0.316, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229659
2023-01-12 05:41:13 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:15 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:17 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:24 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:29 - progress_bar.py[line:274] - INFO: epoch 001:  41937 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.138, ntokens=107.6, nsentences=40, sample_size=107.6, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4167, wps=100.5, ups=0.62, wpb=107.6, bsz=40, num_updates=41880, lr=3.02708e-05, gnorm=0.205, clip=10, loss_scale=512, train_wall=16, gb_free=10.4, ema_decay=0.9999, wall=229675
2023-01-12 05:41:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:46 - progress_bar.py[line:274] - INFO: epoch 001:  41947 / 100000 loss=0.284, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=108.667, nsentences=40, sample_size=108.667, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4862, wps=96.5, ups=0.59, wpb=108.7, bsz=40, num_updates=41890, lr=3.02656e-05, gnorm=0.295, clip=0, loss_scale=512, train_wall=17, gb_free=10.2, ema_decay=0.9999, wall=229692
2023-01-12 05:41:46 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:48 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:51 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:41:58 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:00 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:03 - progress_bar.py[line:274] - INFO: epoch 001:  41957 / 100000 loss=0.294, loss_v1=0, loss_v2=0, nll_loss=0.134, ntokens=107.867, nsentences=40, sample_size=107.867, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4505, wps=100.3, ups=0.62, wpb=107.9, bsz=40, num_updates=41900, lr=3.02604e-05, gnorm=0.34, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229709
2023-01-12 05:42:03 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:05 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:07 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:19 - progress_bar.py[line:274] - INFO: epoch 001:  41967 / 100000 loss=0.291, loss_v1=0, loss_v2=0, nll_loss=0.136, ntokens=110, nsentences=40, sample_size=110, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.3861, wps=104.5, ups=0.63, wpb=110, bsz=40, num_updates=41910, lr=3.02552e-05, gnorm=0.113, clip=0, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229725
2023-01-12 05:42:19 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:21 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:23 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:31 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:33 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:35 - progress_bar.py[line:274] - INFO: epoch 001:  41977 / 100000 loss=0.306, loss_v1=0, loss_v2=0, nll_loss=0.146, ntokens=106.867, nsentences=40, sample_size=106.867, sample_size_v1=0, sample_size_v2=0, ppl=1.11, vqa_score=0.487, wps=100, ups=0.62, wpb=106.9, bsz=40, num_updates=41920, lr=3.025e-05, gnorm=0.25, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=229741
2023-01-12 05:42:35 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:37 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:47 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:49 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:52 - progress_bar.py[line:274] - INFO: epoch 001:  41987 / 100000 loss=0.289, loss_v1=0, loss_v2=0, nll_loss=0.131, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4231, wps=98.8, ups=0.6, wpb=109.3, bsz=40, num_updates=41930, lr=3.02448e-05, gnorm=0.158, clip=0, loss_scale=512, train_wall=17, gb_free=10.3, ema_decay=0.9999, wall=229758
2023-01-12 05:42:52 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:54 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:42:56 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:04 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:06 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:08 - progress_bar.py[line:274] - INFO: epoch 001:  41997 / 100000 loss=0.281, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.3854, wps=103.4, ups=0.62, wpb=110.8, bsz=40, num_updates=41940, lr=3.02396e-05, gnorm=0.289, clip=10, loss_scale=512, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=229774
2023-01-12 05:43:08 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:10 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:20 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:22 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:25 - progress_bar.py[line:274] - INFO: epoch 001:  42007 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.125, ntokens=111.2, nsentences=40, sample_size=111.2, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.4149, wps=102.8, ups=0.62, wpb=111.2, bsz=40, num_updates=41950, lr=3.02344e-05, gnorm=0.299, clip=10, loss_scale=512, train_wall=16, gb_free=10.1, ema_decay=0.9999, wall=229791
2023-01-12 05:43:25 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:27 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:29 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:36 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:39 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:41 - progress_bar.py[line:274] - INFO: epoch 001:  42017 / 100000 loss=0.299, loss_v1=0, loss_v2=0, nll_loss=0.143, ntokens=109.333, nsentences=40, sample_size=109.333, sample_size_v1=0, sample_size_v2=0, ppl=1.1, vqa_score=0.4486, wps=101.3, ups=0.62, wpb=109.3, bsz=40, num_updates=41960, lr=3.02292e-05, gnorm=0.37, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229807
2023-01-12 05:43:41 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:43 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:45 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:53 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:55 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:57 - progress_bar.py[line:274] - INFO: epoch 001:  42027 / 100000 loss=inf, loss_v1=0, loss_v2=0, nll_loss=inf, ntokens=109.733, nsentences=40, sample_size=109.733, sample_size_v1=0, sample_size_v2=0, ppl=inf, vqa_score=0.4902, wps=103.3, ups=0.63, wpb=109.7, bsz=40, num_updates=41970, lr=3.0224e-05, gnorm=0.689, clip=10, loss_scale=512, train_wall=16, gb_free=10.2, ema_decay=0.9999, wall=229823
2023-01-12 05:43:57 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:43:59 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:01 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:09 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:12 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:14 - progress_bar.py[line:274] - INFO: epoch 001:  42037 / 100000 loss=0.283, loss_v1=0, loss_v2=0, nll_loss=0.122, ntokens=110.733, nsentences=40, sample_size=110.733, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.5319, wps=101.7, ups=0.61, wpb=110.7, bsz=40, num_updates=41980, lr=3.02188e-05, gnorm=0.15, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=229840
2023-01-12 05:44:14 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:16 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:18 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:26 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:28 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:30 - progress_bar.py[line:274] - INFO: epoch 001:  42047 / 100000 loss=0.277, loss_v1=0, loss_v2=0, nll_loss=0.117, ntokens=110.8, nsentences=40, sample_size=110.8, sample_size_v1=0, sample_size_v2=0, ppl=1.08, vqa_score=0.4792, wps=103.9, ups=0.62, wpb=110.8, bsz=40, num_updates=41990, lr=3.02135e-05, gnorm=0.187, clip=0, loss_scale=512, train_wall=16, gb_free=10.3, ema_decay=0.9999, wall=229856
2023-01-12 05:44:30 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:32 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:34 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:42 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:44 - trainer.py[line:1124] - INFO: Distill with momentum 0.995 alpha 1.0
2023-01-12 05:44:46 - progress_bar.py[line:274] - INFO: epoch 001:  42057 / 100000 loss=0.286, loss_v1=0, loss_v2=0, nll_loss=0.124, ntokens=109.467, nsentences=40, sample_size=109.467, sample_size_v1=0, sample_size_v2=0, ppl=1.09, vqa_score=0.433, wps=100.9, ups=0.61, wpb=109.5, bsz=40, num_updates=42000, lr=3.02083e-05, gnorm=1.09, clip=10, loss_scale=512, train_wall=16, gb_free=10, ema_decay=0.9999, wall=229873
2023-01-12 05:44:47 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-01-12 05:44:48 - train.py[line:549] - INFO: 0 / 4988
2023-01-12 05:44:48 - train.py[line:551] - INFO: load:1.46 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-01-12 05:47:20 - train.py[line:549] - INFO: 200 / 4988
2023-01-12 05:47:20 - train.py[line:551] - INFO: load:1.48 valid_run:151.72 task_valid:148.69 collect_output:1.87
2023-01-12 05:49:48 - train.py[line:549] - INFO: 400 / 4988
2023-01-12 05:49:48 - train.py[line:551] - INFO: load:1.51 valid_run:299.50 task_valid:292.14 collect_output:5.11
