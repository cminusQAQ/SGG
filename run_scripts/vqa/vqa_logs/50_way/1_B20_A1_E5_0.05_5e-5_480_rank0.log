Traceback (most recent call last):
Traceback (most recent call last):
  File "../../train.py", line 443, in <module>
  File "../../train.py", line 443, in <module>
    open(os.path.join(script_path, f'../datasets/OFA_data/sgg/{split}/{template_prefix}_{name}_sample_st_ed_{img_cnt}.json')))
FileNotFoundError: [Errno 2] No such file or directory: '../../../datasets/OFA_data/sgg/50_way/query_val_sample_st_ed_10.json'    
open(os.path.join(script_path, f'../datasets/OFA_data/sgg/{split}/{template_prefix}_{name}_sample_st_ed_{img_cnt}.json')))
FileNotFoundError: [Errno 2] No such file or directory: '../../../datasets/OFA_data/sgg/50_way/query_val_sample_st_ed_10.json'
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_val_10.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/50_way', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=12', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=5', '--warmup-ratio=0.05', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=200', '--validate-interval-updates=200', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 394553
Killing subprocess 394554
Traceback (most recent call last):
  File "../../train.py", line 443, in <module>
    open(os.path.join(script_path, f'../datasets/OFA_data/sgg/{split}/{template_prefix}_{name}_sample_st_ed_{img_cnt}.json')))
FileNotFoundError: [Errno 2] No such file or directory: '../../../datasets/OFA_data/sgg/50_way/query_val_sample_st_ed_10.json'
Traceback (most recent call last):
  File "../../train.py", line 443, in <module>
    open(os.path.join(script_path, f'../datasets/OFA_data/sgg/{split}/{template_prefix}_{name}_sample_st_ed_{img_cnt}.json')))
FileNotFoundError: [Errno 2] No such file or directory: '../../../datasets/OFA_data/sgg/50_way/query_val_sample_st_ed_10.json'
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/query_val_10.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/50_way', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=12', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=5', '--warmup-ratio=0.05', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=200', '--validate-interval-updates=200', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 394659
Killing subprocess 394660
Traceback (most recent call last):
  File "../../train.py", line 635, in <module>
Traceback (most recent call last):
  File "../../train.py", line 635, in <module>
    cli_main()
  File "../../train.py", line 614, in cli_main
    cli_main()
  File "../../train.py", line 614, in cli_main
    parser = options.get_training_parser()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/options.py", line 38, in get_training_parser
    parser = options.get_training_parser()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/options.py", line 38, in get_training_parser
    parser = get_parser("Trainer", default_task)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/options.py", line 227, in get_parser
    parser = get_parser("Trainer", default_task)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/options.py", line 227, in get_parser
    utils.import_user_module(usr_args)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/utils.py", line 489, in import_user_module
    utils.import_user_module(usr_args)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/utils.py", line 489, in import_user_module
    importlib.import_module(module_name)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/importlib/__init__.py", line 127, in import_module
    importlib.import_module(module_name)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1006, in _gcd_import
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 983, in _find_and_load
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 967, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap>", line 677, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap_external>", line 728, in exec_module
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "<frozen importlib._bootstrap>", line 219, in _call_with_frames_removed
  File "/data/private/yutianyu/OFA/ofa_module/__init__.py", line 3, in <module>
  File "/data/private/yutianyu/OFA/ofa_module/__init__.py", line 3, in <module>
    import tasks
  File "/data/private/yutianyu/OFA/tasks/__init__.py", line 2, in <module>
    import tasks
  File "/data/private/yutianyu/OFA/tasks/__init__.py", line 2, in <module>
    from .mm_tasks import *
  File "/data/private/yutianyu/OFA/tasks/mm_tasks/__init__.py", line 2, in <module>
    from .mm_tasks import *
  File "/data/private/yutianyu/OFA/tasks/mm_tasks/__init__.py", line 2, in <module>
    from .image_gen import ImageGenTask
  File "/data/private/yutianyu/OFA/tasks/mm_tasks/image_gen.py", line 26, in <module>
    from .image_gen import ImageGenTask
  File "/data/private/yutianyu/OFA/tasks/mm_tasks/image_gen.py", line 26, in <module>
    from models.taming.models.vqgan import GumbelVQ
  File "/data/private/yutianyu/OFA/models/taming/models/vqgan.py", line 3, in <module>
    from models.taming.models.vqgan import GumbelVQ
  File "/data/private/yutianyu/OFA/models/taming/models/vqgan.py", line 3, in <module>
    import pytorch_lightning as pl
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/pytorch_lightning/__init__.py", line 34, in <module>
    import pytorch_lightning as pl
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/pytorch_lightning/__init__.py", line 34, in <module>
    from pytorch_lightning.callbacks import Callback  # noqa: E402
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/pytorch_lightning/callbacks/__init__.py", line 14, in <module>
    from pytorch_lightning.callbacks import Callback  # noqa: E402
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/pytorch_lightning/callbacks/__init__.py", line 14, in <module>
    from pytorch_lightning.callbacks.callback import Callback
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/pytorch_lightning/callbacks/callback.py", line 25, in <module>
    from pytorch_lightning.callbacks.callback import Callback
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/pytorch_lightning/callbacks/callback.py", line 25, in <module>
        from pytorch_lightning.utilities.types import STEP_OUTPUTfrom pytorch_lightning.utilities.types import STEP_OUTPUT

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/pytorch_lightning/utilities/types.py", line 29, in <module>
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/pytorch_lightning/utilities/types.py", line 29, in <module>
    from torchmetrics import Metric
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchmetrics/__init__.py", line 14, in <module>
    from torchmetrics import Metric
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchmetrics/__init__.py", line 14, in <module>
    from torchmetrics import functional  # noqa: E402
      File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchmetrics/functional/__init__.py", line 76, in <module>
from torchmetrics import functional  # noqa: E402
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchmetrics/functional/__init__.py", line 76, in <module>
    from torchmetrics.functional.text.bleu import bleu_score
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchmetrics/functional/text/__init__.py", line 29, in <module>
    from torchmetrics.functional.text.bleu import bleu_score
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchmetrics/functional/text/__init__.py", line 29, in <module>
    from torchmetrics.functional.text.bert import bert_score  # noqa: F401
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchmetrics/functional/text/bert.py", line 29, in <module>
    from torchmetrics.functional.text.bert import bert_score  # noqa: F401
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchmetrics/functional/text/bert.py", line 29, in <module>
        from transformers.models.auto import AutoModel, AutoTokenizerfrom transformers.models.auto import AutoModel, AutoTokenizer

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/transformers/file_utils.py", line 1889, in __getattr__
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/transformers/file_utils.py", line 1889, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/transformers/models/auto/__init__.py", line 216, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
      File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py", line 223, in <module>
module = self._get_module(self._class_to_module[name])
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/transformers/models/auto/__init__.py", line 216, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py", line 223, in <module>
    from ..rag.modeling_rag import (  # noqa: F401 - need to import all RagModels to be in globals() function
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/transformers/models/rag/modeling_rag.py", line 30, in <module>
    from .retrieval_rag import RagRetriever
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/transformers/models/rag/retrieval_rag.py", line 33, in <module>
    from ..rag.modeling_rag import (  # noqa: F401 - need to import all RagModels to be in globals() function
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/transformers/models/rag/modeling_rag.py", line 30, in <module>
    from datasets import Dataset, load_dataset, load_from_disk
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/__init__.py", line 37, in <module>
    from .retrieval_rag import RagRetriever
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/transformers/models/rag/retrieval_rag.py", line 33, in <module>
    from datasets import Dataset, load_dataset, load_from_disk
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/__init__.py", line 37, in <module>
    from .arrow_dataset import Dataset
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/arrow_dataset.py", line 61, in <module>
    from .arrow_dataset import Dataset
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/arrow_dataset.py", line 61, in <module>
    from .arrow_reader import ArrowReader
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/arrow_reader.py", line 29, in <module>
    from .arrow_reader import ArrowReader
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/arrow_reader.py", line 29, in <module>
    from .download.download_config import DownloadConfig
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/download/__init__.py", line 10, in <module>
    from .download.download_config import DownloadConfig
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/download/__init__.py", line 10, in <module>
    from .streaming_download_manager import StreamingDownloadManager
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/download/streaming_download_manager.py", line 20, in <module>
    from .streaming_download_manager import StreamingDownloadManager
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/download/streaming_download_manager.py", line 20, in <module>
    from ..filesystems import COMPRESSION_FILESYSTEMS
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/filesystems/__init__.py", line 7, in <module>
    from ..filesystems import COMPRESSION_FILESYSTEMS
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/filesystems/__init__.py", line 7, in <module>
    from .hffilesystem import HfFileSystem
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/filesystems/hffilesystem.py", line 6, in <module>
    from .hffilesystem import HfFileSystem
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/datasets/filesystems/hffilesystem.py", line 6, in <module>
    from huggingface_hub.hf_api import DatasetInfo
    ImportErrorfrom huggingface_hub.hf_api import DatasetInfo: 
cannot import name 'DatasetInfo' from 'huggingface_hub.hf_api' (/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/huggingface_hub/hf_api.py)
ImportError: cannot import name 'DatasetInfo' from 'huggingface_hub.hf_api' (/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/huggingface_hub/hf_api.py)
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/50_way', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=12', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=5', '--warmup-ratio=0.05', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=200', '--validate-interval-updates=200', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 394768
Killing subprocess 394769
2023-03-13 21:42:26 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-03-13 21:42:26 - utils.py[line:261] - INFO: Start init
2023-03-13 21:42:26 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-03-13 21:42:26 - utils.py[line:261] - INFO: Start init
2023-03-13 21:42:27 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-13 21:42:27 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-13 21:42:27 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-03-13 21:42:27 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-03-13 21:42:31 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 200, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 200, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', save_interval=10, save_interval_updates=200, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=200, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-03-13 21:42:31 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-03-13 21:42:31 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-03-13 21:42:35 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-03-13 21:42:35 - train.py[line:118] - INFO: task: VqaGenTask
2023-03-13 21:42:35 - train.py[line:119] - INFO: model: OFAModel
2023-03-13 21:42:35 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-03-13 21:42:35 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-03-13 21:42:35 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv slice_id 0 row count 1267 total row count 2534
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv slice_id 1 row count 1267 total row count 2534
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 21:42:35 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-03-13 21:42:35 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-03-13 21:42:36 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-03-13 21:42:36 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 21:42:36 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 21:42:36 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-03-13 21:42:37 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-03-13 21:42:37 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-03-13 21:42:37 - trainer.py[line:500] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-03-13 21:42:46 - trainer.py[line:565] - INFO: Load Model_m together with Model 2
2023-03-13 21:42:46 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 21:42:46 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 21:42:46 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-03-13 21:42:46 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-03-13 21:42:47 - trainer.py[line:675] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-03-13 21:42:47 - trainer.py[line:695] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 315642 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 315642 total row count 631284
2023-03-13 21:42:48 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
@@@@ ERROR IN DATA @@@@ on
Traceback (most recent call last):
  File "../../train.py", line 635, in <module>
    cli_main()
  File "../../train.py", line 628, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 177, in main
    disable_iterator_cache=True,
  File "/data/private/yutianyu/OFA/utils/checkpoint_utils.py", line 288, in load_checkpoint
    epoch=1, load_dataset=True, **passthrough_args
  File "/data/private/yutianyu/OFA/trainer.py", line 722, in get_train_iterator
@@@@ ERROR IN DATA @@@@ on
Traceback (most recent call last):
  File "../../train.py", line 635, in <module>
    cli_main()
  File "../../train.py", line 628, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 177, in main
    disable_iterator_cache=True,
  File "/data/private/yutianyu/OFA/utils/checkpoint_utils.py", line 288, in load_checkpoint
    epoch=1, load_dataset=True, **passthrough_args
  File "/data/private/yutianyu/OFA/trainer.py", line 722, in get_train_iterator
    self.reset_dummy_batch(batch_iterator.first_batch)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in first_batch
    self.reset_dummy_batch(batch_iterator.first_batch)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in first_batch
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])    
return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in <listcomp>

  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in <listcomp>
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])    
return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 310, in __getitem__

  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 310, in __getitem__
    obj_pair_idx = [str(self.obj_name2idx[x]) for x in obj_pair_name]
  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 310, in <listcomp>
    obj_pair_idx = [str(self.obj_name2idx[x]) for x in obj_pair_name]
  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 310, in <listcomp>
    obj_pair_idx = [str(self.obj_name2idx[x]) for x in obj_pair_name]
KeyError: '"handle'
    obj_pair_idx = [str(self.obj_name2idx[x]) for x in obj_pair_name]
KeyError: '"sign'
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/50_way', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=12', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=5', '--warmup-ratio=0.05', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=200', '--validate-interval-updates=200', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 396177
Killing subprocess 396178
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 396637
Killing subprocess 396638
Main process received SIGINT, exiting
2023-03-13 21:47:53 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-03-13 21:47:53 - utils.py[line:261] - INFO: Start init
2023-03-13 21:47:53 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-03-13 21:47:53 - utils.py[line:261] - INFO: Start init
2023-03-13 21:47:53 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-13 21:47:53 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-13 21:47:53 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-03-13 21:47:53 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-03-13 21:47:57 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 200, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 200, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', save_interval=10, save_interval_updates=200, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=200, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-03-13 21:47:57 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-03-13 21:47:57 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv slice_id 1 row count 1267 total row count 2534
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 21:48:01 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-03-13 21:48:01 - train.py[line:118] - INFO: task: VqaGenTask
2023-03-13 21:48:01 - train.py[line:119] - INFO: model: OFAModel
2023-03-13 21:48:01 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-03-13 21:48:01 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-03-13 21:48:01 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv slice_id 0 row count 1267 total row count 2534
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 21:48:01 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-03-13 21:48:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-03-13 21:48:02 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-03-13 21:48:02 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 21:48:02 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 21:48:02 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-03-13 21:48:03 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-03-13 21:48:03 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-03-13 21:48:03 - trainer.py[line:500] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-03-13 21:48:05 - trainer.py[line:565] - INFO: Load Model_m together with Model 2
2023-03-13 21:48:06 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 21:48:06 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 21:48:06 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-03-13 21:48:06 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-03-13 21:48:06 - trainer.py[line:675] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-03-13 21:48:06 - trainer.py[line:695] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 315642 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 315642 total row count 631284
2023-03-13 21:48:07 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Traceback (most recent call last):
  File "../../train.py", line 635, in <module>
    cli_main()
  File "../../train.py", line 628, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 177, in main
    disable_iterator_cache=True,
  File "/data/private/yutianyu/OFA/utils/checkpoint_utils.py", line 288, in load_checkpoint
    epoch=1, load_dataset=True, **passthrough_args
  File "/data/private/yutianyu/OFA/trainer.py", line 722, in get_train_iterator
    self.reset_dummy_batch(batch_iterator.first_batch)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in first_batch
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in <listcomp>
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 314, in __getitem__
    obj_pair_idx = [str(self.obj_name2idx[x]) for x in obj_pair_name]
  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 314, in <listcomp>
    obj_pair_idx = [str(self.obj_name2idx[x]) for x in obj_pair_name]
TypeError: unhashable type: 'list'
Traceback (most recent call last):
  File "../../train.py", line 635, in <module>
    cli_main()
  File "../../train.py", line 628, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 177, in main
    disable_iterator_cache=True,
  File "/data/private/yutianyu/OFA/utils/checkpoint_utils.py", line 288, in load_checkpoint
    epoch=1, load_dataset=True, **passthrough_args
  File "/data/private/yutianyu/OFA/trainer.py", line 722, in get_train_iterator
    self.reset_dummy_batch(batch_iterator.first_batch)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in first_batch
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/data/iterators.py", line 322, in <listcomp>
    return self.collate_fn([self.dataset[i] for i in self.frozen_batches[0]])
  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 314, in __getitem__
    obj_pair_idx = [str(self.obj_name2idx[x]) for x in obj_pair_name]
  File "/data/private/yutianyu/OFA/data/mm_data/vqa_gen_dataset.py", line 314, in <listcomp>
    obj_pair_idx = [str(self.obj_name2idx[x]) for x in obj_pair_name]
TypeError: unhashable type: 'list'
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=1', '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/50_way', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=12', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=5', '--warmup-ratio=0.05', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=10', '--validate-interval=10', '--save-interval-updates=200', '--validate-interval-updates=200', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 396693
Killing subprocess 396694
2023-03-13 21:48:34 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-03-13 21:48:34 - utils.py[line:261] - INFO: Start init
2023-03-13 21:48:34 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-03-13 21:48:34 - utils.py[line:261] - INFO: Start init
2023-03-13 21:48:35 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-13 21:48:35 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-13 21:48:35 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-03-13 21:48:35 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-03-13 21:48:39 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 2, 'distributed_num_procs': 2, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 2, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 10, 'validate_interval_updates': 200, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 10, 'save_interval_updates': 200, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 2}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=2, distributed_port=-1, distributed_rank=0, distributed_world_size=2, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=2, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', save_interval=10, save_interval_updates=200, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=10, validate_interval_updates=200, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-03-13 21:48:39 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-03-13 21:48:39 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv slice_id 1 row count 1267 total row count 2534
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 21:48:43 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-03-13 21:48:43 - train.py[line:118] - INFO: task: VqaGenTask
2023-03-13 21:48:43 - train.py[line:119] - INFO: model: OFAModel
2023-03-13 21:48:43 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-03-13 21:48:43 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-03-13 21:48:43 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv slice_id 0 row count 1267 total row count 2534
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 21:48:44 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-03-13 21:48:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-03-13 21:48:44 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 2 workers***********************
2023-03-13 21:48:44 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 21:48:44 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 21:48:44 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 2 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-03-13 21:48:45 - train.py[line:161] - INFO: training on 2 devices (GPUs/TPUs)
2023-03-13 21:48:45 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-03-13 21:48:45 - trainer.py[line:500] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-03-13 21:48:47 - trainer.py[line:565] - INFO: Load Model_m together with Model 2
2023-03-13 21:48:48 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 21:48:48 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 21:48:48 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-03-13 21:48:48 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-03-13 21:48:49 - trainer.py[line:675] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-03-13 21:48:49 - trainer.py[line:695] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 315642 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 315642 total row count 631284
2023-03-13 21:48:49 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 78915, warmup steps 3945, warmup_factor 0.00025348542458808617
2023-03-13 21:48:50 - trainer.py[line:759] - INFO: begin training epoch 1
2023-03-13 21:48:50 - train.py[line:312] - INFO: Start iterating over samples
Total steps 78915, warmup steps 3945, warmup_factor 0.00025348542458808617
2023-03-13 21:49:11 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 15783 loss=1.323, loss_v1=0, loss_v2=0, nll_loss=1.099, ntokens=102.4, nsentences=40, sample_size=102.4, sample_size_v1=0, sample_size_v2=0, ppl=2.14, wps=71.4, ups=0.7, wpb=102.4, bsz=40, num_updates=10, lr=1.26743e-07, gnorm=14.88, clip=100, loss_scale=128, train_wall=15, gb_free=10.5, ema_decay=0.9999, wall=27
2023-03-13 21:49:24 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 15783 loss=1.339, loss_v1=0, loss_v2=0, nll_loss=1.118, ntokens=100.6, nsentences=40, sample_size=100.6, sample_size_v1=0, sample_size_v2=0, ppl=2.17, wps=81.1, ups=0.81, wpb=100.6, bsz=40, num_updates=20, lr=2.53485e-07, gnorm=13.492, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=40
2023-03-13 21:49:35 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 15783 loss=1.403, loss_v1=0, loss_v2=0, nll_loss=1.184, ntokens=100.6, nsentences=40, sample_size=100.6, sample_size_v1=0, sample_size_v2=0, ppl=2.27, wps=86.5, ups=0.86, wpb=100.6, bsz=40, num_updates=30, lr=3.80228e-07, gnorm=15.52, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=52
2023-03-13 21:49:47 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 15783 loss=1.282, loss_v1=0, loss_v2=0, nll_loss=1.077, ntokens=102.1, nsentences=40, sample_size=102.1, sample_size_v1=0, sample_size_v2=0, ppl=2.11, wps=86.2, ups=0.84, wpb=102.1, bsz=40, num_updates=40, lr=5.06971e-07, gnorm=11.348, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=63
2023-03-13 21:49:59 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 15783 loss=1.241, loss_v1=0, loss_v2=0, nll_loss=1.029, ntokens=102.4, nsentences=40, sample_size=102.4, sample_size_v1=0, sample_size_v2=0, ppl=2.04, wps=87.9, ups=0.86, wpb=102.4, bsz=40, num_updates=50, lr=6.33714e-07, gnorm=11.389, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=75
2023-03-13 21:50:11 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 15783 loss=1.238, loss_v1=0, loss_v2=0, nll_loss=1.036, ntokens=100.4, nsentences=40, sample_size=100.4, sample_size_v1=0, sample_size_v2=0, ppl=2.05, wps=86.9, ups=0.87, wpb=100.4, bsz=40, num_updates=60, lr=7.60456e-07, gnorm=10.003, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=87
2023-03-13 21:50:22 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 15783 loss=1.163, loss_v1=0, loss_v2=0, nll_loss=0.976, ntokens=102.7, nsentences=40, sample_size=102.7, sample_size_v1=0, sample_size_v2=0, ppl=1.97, wps=91.9, ups=0.9, wpb=102.7, bsz=40, num_updates=70, lr=8.87199e-07, gnorm=9.097, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=98
2023-03-13 21:50:33 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 15783 loss=1.125, loss_v1=0, loss_v2=0, nll_loss=0.939, ntokens=103.2, nsentences=40, sample_size=103.2, sample_size_v1=0, sample_size_v2=0, ppl=1.92, wps=89.4, ups=0.87, wpb=103.2, bsz=40, num_updates=80, lr=1.01394e-06, gnorm=8.372, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=109
Traceback (most recent call last):
  File "../../train.py", line 635, in <module>
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 396872
Killing subprocess 396873
Main process received SIGINT, exiting
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 183, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 109, in _get_module_details
    __import__(pkg_name)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/__init__.py", line 196, in <module>
    from torch._C import *
ImportError: numpy.core.multiarray failed to import
2023-03-13 21:51:49 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-03-13 21:51:49 - utils.py[line:261] - INFO: Start init
2023-03-13 21:51:49 - utils.py[line:258] - INFO: distributed init (rank 3): env://
2023-03-13 21:51:49 - utils.py[line:261] - INFO: Start init
2023-03-13 21:51:49 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2023-03-13 21:51:49 - utils.py[line:261] - INFO: Start init
2023-03-13 21:51:49 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-03-13 21:51:49 - utils.py[line:261] - INFO: Start init
2023-03-13 21:51:50 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-13 21:51:50 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2023-03-13 21:51:50 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-03-13 21:51:50 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-13 21:51:50 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-03-13 21:51:50 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-03-13 21:51:50 - utils.py[line:274] - INFO: initialized host node4 as rank 2
single-machine distributed training is initialized.
2023-03-13 21:51:50 - utils.py[line:274] - INFO: initialized host node4 as rank 3
single-machine distributed training is initialized.
2023-03-13 21:51:55 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 2000000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=4, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', save_interval=1, save_interval_updates=2000000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=2000000, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-03-13 21:51:55 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-03-13 21:51:55 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv slice_id 3 row count 633 total row count 2534
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv slice_id 2 row count 633 total row count 2534
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv slice_id 1 row count 634 total row count 2534
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 21:52:00 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-03-13 21:52:00 - train.py[line:118] - INFO: task: VqaGenTask
2023-03-13 21:52:00 - train.py[line:119] - INFO: model: OFAModel
2023-03-13 21:52:00 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-03-13 21:52:00 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-03-13 21:52:00 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_10.tsv slice_id 0 row count 634 total row count 2534
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 21:52:00 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-03-13 21:52:00 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-03-13 21:52:01 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 4 workers***********************
2023-03-13 21:52:01 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 21:52:01 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 21:52:01 - utils.py[line:765] - INFO: rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 21:52:01 - utils.py[line:765] - INFO: rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 21:52:01 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 4 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-03-13 21:52:03 - train.py[line:161] - INFO: training on 4 devices (GPUs/TPUs)
Done 0.95 cuda cpu, cpu
2023-03-13 21:52:03 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-03-13 21:52:03 - trainer.py[line:500] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
Done 0.95 cuda cpu, cpu
2023-03-13 21:52:06 - trainer.py[line:565] - INFO: Load Model_m together with Model 2
2023-03-13 21:52:06 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 21:52:06 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 21:52:06 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 21:52:06 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 21:52:06 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-03-13 21:52:06 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-03-13 21:52:07 - trainer.py[line:675] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-03-13 21:52:07 - trainer.py[line:695] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 157821 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 3 row count 157821 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 157821 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 2 row count 157821 total row count 631284
2023-03-13 21:52:07 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
2023-03-13 21:52:08 - trainer.py[line:759] - INFO: begin training epoch 1
2023-03-13 21:52:08 - train.py[line:312] - INFO: Start iterating over samples
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
2023-03-13 21:52:29 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 7892 loss=1.312, loss_v1=0, loss_v2=0, nll_loss=1.089, ntokens=205.4, nsentences=80, sample_size=205.4, sample_size_v1=0, sample_size_v2=0, ppl=2.13, wps=142.2, ups=0.69, wpb=205.4, bsz=80, num_updates=10, lr=2.53421e-07, gnorm=12.808, clip=100, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=27
2023-03-13 21:52:41 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 7892 loss=1.289, loss_v1=0, loss_v2=0, nll_loss=1.067, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=2.1, wps=160, ups=0.79, wpb=202.4, bsz=80, num_updates=20, lr=5.06842e-07, gnorm=10.961, clip=100, loss_scale=128, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=40
2023-03-13 21:52:53 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 7892 loss=1.299, loss_v1=0, loss_v2=0, nll_loss=1.089, ntokens=203.9, nsentences=80, sample_size=203.9, sample_size_v1=0, sample_size_v2=0, ppl=2.13, wps=175.9, ups=0.86, wpb=203.9, bsz=80, num_updates=30, lr=7.60264e-07, gnorm=11.751, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=52
2023-03-13 21:53:05 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 7892 loss=1.254, loss_v1=0, loss_v2=0, nll_loss=1.056, ntokens=202.5, nsentences=80, sample_size=202.5, sample_size_v1=0, sample_size_v2=0, ppl=2.08, wps=166.7, ups=0.82, wpb=202.5, bsz=80, num_updates=40, lr=1.01368e-06, gnorm=8.431, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=64
2023-03-13 21:53:17 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 7892 loss=1.154, loss_v1=0, loss_v2=0, nll_loss=0.968, ntokens=204.4, nsentences=80, sample_size=204.4, sample_size_v1=0, sample_size_v2=0, ppl=1.96, wps=177.2, ups=0.87, wpb=204.4, bsz=80, num_updates=50, lr=1.26711e-06, gnorm=6.161, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=75
2023-03-13 21:53:28 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 7892 loss=1.128, loss_v1=0, loss_v2=0, nll_loss=0.944, ntokens=201.1, nsentences=80, sample_size=201.1, sample_size_v1=0, sample_size_v2=0, ppl=1.92, wps=175.5, ups=0.87, wpb=201.1, bsz=80, num_updates=60, lr=1.52053e-06, gnorm=5.469, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=87
2023-03-13 21:53:39 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 7892 loss=1.031, loss_v1=0, loss_v2=0, nll_loss=0.854, ntokens=204.8, nsentences=80, sample_size=204.8, sample_size_v1=0, sample_size_v2=0, ppl=1.81, wps=181.7, ups=0.89, wpb=204.8, bsz=80, num_updates=70, lr=1.77395e-06, gnorm=4.084, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=98
2023-03-13 21:53:51 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 7892 loss=0.982, loss_v1=0, loss_v2=0, nll_loss=0.802, ntokens=206, nsentences=80, sample_size=206, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=176.4, ups=0.86, wpb=206, bsz=80, num_updates=80, lr=2.02737e-06, gnorm=3.837, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=110
2023-03-13 21:54:03 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 7892 loss=1.01, loss_v1=0, loss_v2=0, nll_loss=0.838, ntokens=203, nsentences=80, sample_size=203, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=170.7, ups=0.84, wpb=203, bsz=80, num_updates=90, lr=2.28079e-06, gnorm=3.442, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=122
2023-03-13 21:54:15 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 7892 loss=1.027, loss_v1=0, loss_v2=0, nll_loss=0.861, ntokens=200.3, nsentences=80, sample_size=200.3, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=174.6, ups=0.87, wpb=200.3, bsz=80, num_updates=100, lr=2.53421e-06, gnorm=2.969, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=133
2023-03-13 21:54:26 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 7892 loss=0.931, loss_v1=0, loss_v2=0, nll_loss=0.765, ntokens=205.6, nsentences=80, sample_size=205.6, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=175.9, ups=0.86, wpb=205.6, bsz=80, num_updates=110, lr=2.78763e-06, gnorm=3.081, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=145
2023-03-13 21:54:38 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 7892 loss=0.942, loss_v1=0, loss_v2=0, nll_loss=0.773, ntokens=203.4, nsentences=80, sample_size=203.4, sample_size_v1=0, sample_size_v2=0, ppl=1.71, wps=174.9, ups=0.86, wpb=203.4, bsz=80, num_updates=120, lr=3.04105e-06, gnorm=2.936, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=156
2023-03-13 21:54:49 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 7892 loss=0.938, loss_v1=0, loss_v2=0, nll_loss=0.773, ntokens=202, nsentences=80, sample_size=202, sample_size_v1=0, sample_size_v2=0, ppl=1.71, wps=174.1, ups=0.86, wpb=202, bsz=80, num_updates=130, lr=3.29448e-06, gnorm=2.5, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=168
2023-03-13 21:55:01 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 7892 loss=0.926, loss_v1=0, loss_v2=0, nll_loss=0.759, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=176.8, ups=0.87, wpb=202.9, bsz=80, num_updates=140, lr=3.5479e-06, gnorm=2.332, clip=100, loss_scale=128, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=179
2023-03-13 21:55:12 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 7892 loss=0.905, loss_v1=0, loss_v2=0, nll_loss=0.743, ntokens=205.9, nsentences=80, sample_size=205.9, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=180.1, ups=0.87, wpb=205.9, bsz=80, num_updates=150, lr=3.80132e-06, gnorm=2.251, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=191
2023-03-13 21:55:24 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 7892 loss=0.928, loss_v1=0, loss_v2=0, nll_loss=0.768, ntokens=202.2, nsentences=80, sample_size=202.2, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=176.9, ups=0.87, wpb=202.2, bsz=80, num_updates=160, lr=4.05474e-06, gnorm=2.361, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=202
2023-03-13 21:55:35 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 7892 loss=0.892, loss_v1=0, loss_v2=0, nll_loss=0.724, ntokens=201.7, nsentences=80, sample_size=201.7, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=174.2, ups=0.86, wpb=201.7, bsz=80, num_updates=170, lr=4.30816e-06, gnorm=2.112, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=214
2023-03-13 21:55:48 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 7892 loss=0.884, loss_v1=0, loss_v2=0, nll_loss=0.719, ntokens=204.4, nsentences=80, sample_size=204.4, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=168.2, ups=0.82, wpb=204.4, bsz=80, num_updates=180, lr=4.56158e-06, gnorm=1.994, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=226
2023-03-13 21:55:59 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 7892 loss=0.921, loss_v1=0, loss_v2=0, nll_loss=0.752, ntokens=199.7, nsentences=80, sample_size=199.7, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=173.2, ups=0.87, wpb=199.7, bsz=80, num_updates=190, lr=4.815e-06, gnorm=2.137, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=238
2023-03-13 21:56:11 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 7892 loss=0.895, loss_v1=0, loss_v2=0, nll_loss=0.737, ntokens=203.7, nsentences=80, sample_size=203.7, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=177.9, ups=0.87, wpb=203.7, bsz=80, num_updates=200, lr=5.06842e-06, gnorm=1.992, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=249
2023-03-13 21:56:22 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 7892 loss=0.915, loss_v1=0, loss_v2=0, nll_loss=0.756, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=1.69, wps=179, ups=0.88, wpb=202.4, bsz=80, num_updates=210, lr=5.32184e-06, gnorm=2.155, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=260
2023-03-13 21:56:34 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 7892 loss=0.875, loss_v1=0, loss_v2=0, nll_loss=0.712, ntokens=203.5, nsentences=80, sample_size=203.5, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=174.7, ups=0.86, wpb=203.5, bsz=80, num_updates=220, lr=5.57527e-06, gnorm=2.57, clip=100, loss_scale=128, train_wall=12, gb_free=9.6, ema_decay=0.9999, wall=272
2023-03-13 21:56:45 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 7892 loss=0.898, loss_v1=0, loss_v2=0, nll_loss=0.741, ntokens=204.4, nsentences=80, sample_size=204.4, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=176.5, ups=0.86, wpb=204.4, bsz=80, num_updates=230, lr=5.82869e-06, gnorm=1.981, clip=100, loss_scale=128, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=284
2023-03-13 21:56:57 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 7892 loss=0.889, loss_v1=0, loss_v2=0, nll_loss=0.729, ntokens=201.5, nsentences=80, sample_size=201.5, sample_size_v1=0, sample_size_v2=0, ppl=1.66, wps=172, ups=0.85, wpb=201.5, bsz=80, num_updates=240, lr=6.08211e-06, gnorm=2.28, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=295
2023-03-13 21:57:08 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 7892 loss=0.849, loss_v1=0, loss_v2=0, nll_loss=0.677, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=175.1, ups=0.86, wpb=202.4, bsz=80, num_updates=250, lr=6.33553e-06, gnorm=1.849, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=307
2023-03-13 21:57:20 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 7892 loss=0.858, loss_v1=0, loss_v2=0, nll_loss=0.685, ntokens=203.9, nsentences=80, sample_size=203.9, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=181.9, ups=0.89, wpb=203.9, bsz=80, num_updates=260, lr=6.58895e-06, gnorm=2.055, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=318
2023-03-13 21:57:31 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 7892 loss=0.868, loss_v1=0, loss_v2=0, nll_loss=0.702, ntokens=203.2, nsentences=80, sample_size=203.2, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=178, ups=0.88, wpb=203.2, bsz=80, num_updates=270, lr=6.84237e-06, gnorm=2.192, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=330
2023-03-13 21:57:43 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 7892 loss=0.888, loss_v1=0, loss_v2=0, nll_loss=0.72, ntokens=201.9, nsentences=80, sample_size=201.9, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=175.4, ups=0.87, wpb=201.9, bsz=80, num_updates=280, lr=7.09579e-06, gnorm=2.203, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=341
2023-03-13 21:57:54 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 7892 loss=0.865, loss_v1=0, loss_v2=0, nll_loss=0.696, ntokens=201.9, nsentences=80, sample_size=201.9, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=174.5, ups=0.86, wpb=201.9, bsz=80, num_updates=290, lr=7.34921e-06, gnorm=1.962, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=353
2023-03-13 21:58:05 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 7892 loss=0.88, loss_v1=0, loss_v2=0, nll_loss=0.71, ntokens=199.5, nsentences=80, sample_size=199.5, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=176.6, ups=0.89, wpb=199.5, bsz=80, num_updates=300, lr=7.60264e-06, gnorm=2.165, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=364
2023-03-13 21:58:17 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 7892 loss=0.84, loss_v1=0, loss_v2=0, nll_loss=0.668, ntokens=202.5, nsentences=80, sample_size=202.5, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=177.5, ups=0.88, wpb=202.5, bsz=80, num_updates=310, lr=7.85606e-06, gnorm=2.114, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=375
2023-03-13 21:58:28 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 7892 loss=0.853, loss_v1=0, loss_v2=0, nll_loss=0.679, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=178.1, ups=0.88, wpb=202.8, bsz=80, num_updates=320, lr=8.10948e-06, gnorm=2.162, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=387
2023-03-13 21:58:40 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 7892 loss=0.822, loss_v1=0, loss_v2=0, nll_loss=0.644, ntokens=204, nsentences=80, sample_size=204, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=180.6, ups=0.89, wpb=204, bsz=80, num_updates=330, lr=8.3629e-06, gnorm=2.197, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=398
2023-03-13 21:58:51 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 7892 loss=0.794, loss_v1=0, loss_v2=0, nll_loss=0.61, ntokens=201, nsentences=80, sample_size=201, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=178.7, ups=0.89, wpb=201, bsz=80, num_updates=340, lr=8.61632e-06, gnorm=2.225, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=409
2023-03-13 21:59:02 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 7892 loss=0.764, loss_v1=0, loss_v2=0, nll_loss=0.564, ntokens=203.8, nsentences=80, sample_size=203.8, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=178.3, ups=0.87, wpb=203.8, bsz=80, num_updates=350, lr=8.86974e-06, gnorm=2.051, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=421
2023-03-13 21:59:14 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 7892 loss=0.781, loss_v1=0, loss_v2=0, nll_loss=0.592, ntokens=204.5, nsentences=80, sample_size=204.5, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=177.2, ups=0.87, wpb=204.5, bsz=80, num_updates=360, lr=9.12316e-06, gnorm=1.898, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=432
2023-03-13 21:59:25 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 7892 loss=0.767, loss_v1=0, loss_v2=0, nll_loss=0.579, ntokens=203.3, nsentences=80, sample_size=203.3, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=181.4, ups=0.89, wpb=203.3, bsz=80, num_updates=370, lr=9.37658e-06, gnorm=2.1, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=443
2023-03-13 21:59:37 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 7892 loss=0.779, loss_v1=0, loss_v2=0, nll_loss=0.586, ntokens=200.8, nsentences=80, sample_size=200.8, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=170, ups=0.85, wpb=200.8, bsz=80, num_updates=380, lr=9.63001e-06, gnorm=2.151, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=455
2023-03-13 21:59:48 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 7892 loss=0.751, loss_v1=0, loss_v2=0, nll_loss=0.553, ntokens=201.8, nsentences=80, sample_size=201.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=175.1, ups=0.87, wpb=201.8, bsz=80, num_updates=390, lr=9.88343e-06, gnorm=1.936, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=467
2023-03-13 22:00:00 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 7892 loss=0.76, loss_v1=0, loss_v2=0, nll_loss=0.564, ntokens=203.7, nsentences=80, sample_size=203.7, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=178.7, ups=0.88, wpb=203.7, bsz=80, num_updates=400, lr=1.01368e-05, gnorm=2.087, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=478
2023-03-13 22:00:11 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 7892 loss=0.716, loss_v1=0, loss_v2=0, nll_loss=0.512, ntokens=203.9, nsentences=80, sample_size=203.9, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=176.9, ups=0.87, wpb=203.9, bsz=80, num_updates=410, lr=1.03903e-05, gnorm=2.045, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=490
2023-03-13 22:00:23 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 7892 loss=0.752, loss_v1=0, loss_v2=0, nll_loss=0.546, ntokens=198.8, nsentences=80, sample_size=198.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=172.1, ups=0.87, wpb=198.8, bsz=80, num_updates=420, lr=1.06437e-05, gnorm=2.129, clip=100, loss_scale=128, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=501
2023-03-13 22:00:34 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 7892 loss=0.766, loss_v1=0, loss_v2=0, nll_loss=0.568, ntokens=202.1, nsentences=80, sample_size=202.1, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=180.9, ups=0.89, wpb=202.1, bsz=80, num_updates=430, lr=1.08971e-05, gnorm=1.978, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=513
2023-03-13 22:00:46 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 7892 loss=0.738, loss_v1=0, loss_v2=0, nll_loss=0.544, ntokens=202.1, nsentences=80, sample_size=202.1, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=172, ups=0.85, wpb=202.1, bsz=80, num_updates=440, lr=1.11505e-05, gnorm=1.984, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=524
2023-03-13 22:00:57 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 7892 loss=0.728, loss_v1=0, loss_v2=0, nll_loss=0.528, ntokens=200.6, nsentences=80, sample_size=200.6, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=175.6, ups=0.88, wpb=200.6, bsz=80, num_updates=450, lr=1.1404e-05, gnorm=1.982, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=536
2023-03-13 22:01:09 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 7892 loss=0.74, loss_v1=0, loss_v2=0, nll_loss=0.536, ntokens=201.5, nsentences=80, sample_size=201.5, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=174.6, ups=0.87, wpb=201.5, bsz=80, num_updates=460, lr=1.16574e-05, gnorm=2.015, clip=100, loss_scale=128, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=547
2023-03-13 22:01:20 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 7892 loss=0.706, loss_v1=0, loss_v2=0, nll_loss=0.507, ntokens=203.1, nsentences=80, sample_size=203.1, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=175.8, ups=0.87, wpb=203.1, bsz=80, num_updates=470, lr=1.19108e-05, gnorm=2.101, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=559
2023-03-13 22:01:32 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 7892 loss=0.701, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=175.9, ups=0.87, wpb=202.9, bsz=80, num_updates=480, lr=1.21642e-05, gnorm=1.811, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=570
2023-03-13 22:01:43 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 7892 loss=0.688, loss_v1=0, loss_v2=0, nll_loss=0.481, ntokens=203.5, nsentences=80, sample_size=203.5, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=176.4, ups=0.87, wpb=203.5, bsz=80, num_updates=490, lr=1.24176e-05, gnorm=1.736, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=582
2023-03-13 22:01:55 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 7892 loss=0.692, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=202.2, nsentences=80, sample_size=202.2, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=179.8, ups=0.89, wpb=202.2, bsz=80, num_updates=500, lr=1.26711e-05, gnorm=1.642, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=593
2023-03-13 22:02:06 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 7892 loss=0.698, loss_v1=0, loss_v2=0, nll_loss=0.488, ntokens=202, nsentences=80, sample_size=202, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=174.4, ups=0.86, wpb=202, bsz=80, num_updates=510, lr=1.29245e-05, gnorm=1.728, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=605
2023-03-13 22:02:18 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 7892 loss=0.714, loss_v1=0, loss_v2=0, nll_loss=0.507, ntokens=200.4, nsentences=80, sample_size=200.4, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=175.9, ups=0.88, wpb=200.4, bsz=80, num_updates=520, lr=1.31779e-05, gnorm=1.785, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=616
2023-03-13 22:02:29 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 7892 loss=0.693, loss_v1=0, loss_v2=0, nll_loss=0.495, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=175.7, ups=0.87, wpb=202.9, bsz=80, num_updates=530, lr=1.34313e-05, gnorm=1.624, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=628
2023-03-13 22:02:41 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 7892 loss=0.711, loss_v1=0, loss_v2=0, nll_loss=0.506, ntokens=200.6, nsentences=80, sample_size=200.6, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=172.5, ups=0.86, wpb=200.6, bsz=80, num_updates=540, lr=1.36847e-05, gnorm=1.724, clip=100, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=639
2023-03-13 22:02:52 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 7892 loss=0.661, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=204.3, nsentences=80, sample_size=204.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=175.2, ups=0.86, wpb=204.3, bsz=80, num_updates=550, lr=1.39382e-05, gnorm=1.471, clip=100, loss_scale=256, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=651
2023-03-13 22:03:04 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 7892 loss=0.724, loss_v1=0, loss_v2=0, nll_loss=0.526, ntokens=200.5, nsentences=80, sample_size=200.5, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=176.5, ups=0.88, wpb=200.5, bsz=80, num_updates=560, lr=1.41916e-05, gnorm=1.838, clip=100, loss_scale=256, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=662
2023-03-13 22:03:15 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 7892 loss=0.676, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=204.4, nsentences=80, sample_size=204.4, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=177.4, ups=0.87, wpb=204.4, bsz=80, num_updates=570, lr=1.4445e-05, gnorm=1.6, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=674
2023-03-13 22:03:27 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 7892 loss=0.685, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=203.6, nsentences=80, sample_size=203.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=178.7, ups=0.88, wpb=203.6, bsz=80, num_updates=580, lr=1.46984e-05, gnorm=1.636, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=685
2023-03-13 22:03:38 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 7892 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=204.7, nsentences=80, sample_size=204.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=179.3, ups=0.88, wpb=204.7, bsz=80, num_updates=590, lr=1.49518e-05, gnorm=1.576, clip=100, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=697
2023-03-13 22:03:50 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 7892 loss=0.677, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=202.7, nsentences=80, sample_size=202.7, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=174.9, ups=0.86, wpb=202.7, bsz=80, num_updates=600, lr=1.52053e-05, gnorm=1.751, clip=100, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=708
2023-03-13 22:04:01 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 7892 loss=0.689, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=203.1, nsentences=80, sample_size=203.1, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=177.6, ups=0.87, wpb=203.1, bsz=80, num_updates=610, lr=1.54587e-05, gnorm=1.751, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=720
2023-03-13 22:04:13 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 7892 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=204.6, nsentences=80, sample_size=204.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=177.5, ups=0.87, wpb=204.6, bsz=80, num_updates=620, lr=1.57121e-05, gnorm=1.706, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=731
2023-03-13 22:04:24 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 7892 loss=0.679, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=205.7, nsentences=80, sample_size=205.7, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=180.4, ups=0.88, wpb=205.7, bsz=80, num_updates=630, lr=1.59655e-05, gnorm=1.598, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=743
2023-03-13 22:04:36 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 7892 loss=0.674, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=204.1, nsentences=80, sample_size=204.1, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=176.9, ups=0.87, wpb=204.1, bsz=80, num_updates=640, lr=1.6219e-05, gnorm=1.58, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=754
2023-03-13 22:04:47 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 7892 loss=0.702, loss_v1=0, loss_v2=0, nll_loss=0.501, ntokens=201.8, nsentences=80, sample_size=201.8, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=172.7, ups=0.86, wpb=201.8, bsz=80, num_updates=650, lr=1.64724e-05, gnorm=1.543, clip=100, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=766
2023-03-13 22:04:59 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 7892 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=203.3, nsentences=80, sample_size=203.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=181.1, ups=0.89, wpb=203.3, bsz=80, num_updates=660, lr=1.67258e-05, gnorm=1.538, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=777
2023-03-13 22:05:10 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 7892 loss=0.661, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=201.3, nsentences=80, sample_size=201.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=174.7, ups=0.87, wpb=201.3, bsz=80, num_updates=670, lr=1.69792e-05, gnorm=1.535, clip=100, loss_scale=256, train_wall=11, gb_free=11, ema_decay=0.9999, wall=789
2023-03-13 22:05:22 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 7892 loss=0.682, loss_v1=0, loss_v2=0, nll_loss=0.475, ntokens=200.9, nsentences=80, sample_size=200.9, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=175.8, ups=0.87, wpb=200.9, bsz=80, num_updates=680, lr=1.72326e-05, gnorm=1.595, clip=100, loss_scale=256, train_wall=11, gb_free=10, ema_decay=0.9999, wall=800
2023-03-13 22:05:33 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 7892 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=201.7, nsentences=80, sample_size=201.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=178.6, ups=0.89, wpb=201.7, bsz=80, num_updates=690, lr=1.74861e-05, gnorm=1.608, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=812
2023-03-13 22:05:45 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 7892 loss=0.683, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=201.8, nsentences=80, sample_size=201.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=175.1, ups=0.87, wpb=201.8, bsz=80, num_updates=700, lr=1.77395e-05, gnorm=1.614, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=823
2023-03-13 22:05:56 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 7892 loss=0.679, loss_v1=0, loss_v2=0, nll_loss=0.481, ntokens=203.4, nsentences=80, sample_size=203.4, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=178.3, ups=0.88, wpb=203.4, bsz=80, num_updates=710, lr=1.79929e-05, gnorm=1.582, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=835
2023-03-13 22:06:07 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 7892 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=206.3, nsentences=80, sample_size=206.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=183.2, ups=0.89, wpb=206.3, bsz=80, num_updates=720, lr=1.82463e-05, gnorm=1.38, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=846
2023-03-13 22:06:19 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 7892 loss=0.676, loss_v1=0, loss_v2=0, nll_loss=0.47, ntokens=202.2, nsentences=80, sample_size=202.2, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=176.9, ups=0.88, wpb=202.2, bsz=80, num_updates=730, lr=1.84997e-05, gnorm=1.656, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=857
2023-03-13 22:06:30 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 7892 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=203.9, nsentences=80, sample_size=203.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=179, ups=0.88, wpb=203.9, bsz=80, num_updates=740, lr=1.87532e-05, gnorm=1.513, clip=100, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=869
2023-03-13 22:06:42 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 7892 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.457, ntokens=203.1, nsentences=80, sample_size=203.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=176.2, ups=0.87, wpb=203.1, bsz=80, num_updates=750, lr=1.90066e-05, gnorm=1.434, clip=90, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=880
2023-03-13 22:06:53 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 7892 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=201.3, nsentences=80, sample_size=201.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=171.4, ups=0.85, wpb=201.3, bsz=80, num_updates=760, lr=1.926e-05, gnorm=1.42, clip=100, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=892
2023-03-13 22:07:05 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 7892 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=203.3, nsentences=80, sample_size=203.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=175.6, ups=0.86, wpb=203.3, bsz=80, num_updates=770, lr=1.95134e-05, gnorm=1.616, clip=100, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=904
2023-03-13 22:07:17 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 7892 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=204.2, nsentences=80, sample_size=204.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=176.8, ups=0.87, wpb=204.2, bsz=80, num_updates=780, lr=1.97669e-05, gnorm=1.516, clip=100, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=915
2023-03-13 22:07:28 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 7892 loss=0.702, loss_v1=0, loss_v2=0, nll_loss=0.507, ntokens=199.9, nsentences=80, sample_size=199.9, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=175.6, ups=0.88, wpb=199.9, bsz=80, num_updates=790, lr=2.00203e-05, gnorm=1.763, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=926
2023-03-13 22:07:39 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 7892 loss=0.666, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=200.8, nsentences=80, sample_size=200.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=175.7, ups=0.87, wpb=200.8, bsz=80, num_updates=800, lr=2.02737e-05, gnorm=1.537, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=938
2023-03-13 22:07:51 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 7892 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=204.2, nsentences=80, sample_size=204.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=176.6, ups=0.87, wpb=204.2, bsz=80, num_updates=810, lr=2.05271e-05, gnorm=1.385, clip=100, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=949
2023-03-13 22:08:03 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 7892 loss=0.668, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=203.9, nsentences=80, sample_size=203.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=176.3, ups=0.86, wpb=203.9, bsz=80, num_updates=820, lr=2.07805e-05, gnorm=1.523, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=961
2023-03-13 22:08:14 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 7892 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=204.3, nsentences=80, sample_size=204.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=179, ups=0.88, wpb=204.3, bsz=80, num_updates=830, lr=2.1034e-05, gnorm=1.341, clip=90, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=972
2023-03-13 22:08:25 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 7892 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=203.2, nsentences=80, sample_size=203.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=175.9, ups=0.87, wpb=203.2, bsz=80, num_updates=840, lr=2.12874e-05, gnorm=1.444, clip=100, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=984
2023-03-13 22:08:37 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 7892 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=203.6, nsentences=80, sample_size=203.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=178, ups=0.87, wpb=203.6, bsz=80, num_updates=850, lr=2.15408e-05, gnorm=1.382, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=995
2023-03-13 22:08:48 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 7892 loss=0.672, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=201.7, nsentences=80, sample_size=201.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=179.4, ups=0.89, wpb=201.7, bsz=80, num_updates=860, lr=2.17942e-05, gnorm=1.567, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1007
2023-03-13 22:09:00 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 7892 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.458, ntokens=201.3, nsentences=80, sample_size=201.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=172.1, ups=0.85, wpb=201.3, bsz=80, num_updates=870, lr=2.20476e-05, gnorm=1.314, clip=100, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1018
2023-03-13 22:09:11 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 7892 loss=0.667, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=202.6, nsentences=80, sample_size=202.6, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=177.8, ups=0.88, wpb=202.6, bsz=80, num_updates=880, lr=2.23011e-05, gnorm=1.604, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1030
2023-03-13 22:09:23 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 7892 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.458, ntokens=201.8, nsentences=80, sample_size=201.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=179.6, ups=0.89, wpb=201.8, bsz=80, num_updates=890, lr=2.25545e-05, gnorm=1.458, clip=90, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1041
2023-03-13 22:09:34 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 7892 loss=0.663, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=204, nsentences=80, sample_size=204, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=176.4, ups=0.86, wpb=204, bsz=80, num_updates=900, lr=2.28079e-05, gnorm=1.631, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1053
2023-03-13 22:09:46 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 7892 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=203.7, nsentences=80, sample_size=203.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=179, ups=0.88, wpb=203.7, bsz=80, num_updates=910, lr=2.30613e-05, gnorm=1.418, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1064
2023-03-13 22:09:57 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 7892 loss=0.659, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=201.2, nsentences=80, sample_size=201.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=174.5, ups=0.87, wpb=201.2, bsz=80, num_updates=920, lr=2.33147e-05, gnorm=1.433, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1076
2023-03-13 22:10:09 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 7892 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=202.1, nsentences=80, sample_size=202.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=175, ups=0.87, wpb=202.1, bsz=80, num_updates=930, lr=2.35682e-05, gnorm=1.393, clip=100, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1087
2023-03-13 22:10:20 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 7892 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=175.6, ups=0.87, wpb=202.4, bsz=80, num_updates=940, lr=2.38216e-05, gnorm=1.389, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1099
2023-03-13 22:10:32 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 7892 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=177.6, ups=0.88, wpb=202.4, bsz=80, num_updates=950, lr=2.4075e-05, gnorm=1.381, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1110
2023-03-13 22:10:43 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 7892 loss=0.663, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=202.6, nsentences=80, sample_size=202.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=174.9, ups=0.86, wpb=202.6, bsz=80, num_updates=960, lr=2.43284e-05, gnorm=1.509, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1122
2023-03-13 22:10:55 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 7892 loss=0.662, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=199.2, nsentences=80, sample_size=199.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=174.6, ups=0.88, wpb=199.2, bsz=80, num_updates=970, lr=2.45819e-05, gnorm=1.461, clip=100, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1133
2023-03-13 22:11:06 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 7892 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=206.9, nsentences=80, sample_size=206.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=176.4, ups=0.85, wpb=206.9, bsz=80, num_updates=980, lr=2.48353e-05, gnorm=1.388, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1145
2023-03-13 22:11:18 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 7892 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=199.6, nsentences=80, sample_size=199.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=175.1, ups=0.88, wpb=199.6, bsz=80, num_updates=990, lr=2.50887e-05, gnorm=1.459, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1156
2023-03-13 22:11:29 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 7892 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=205, nsentences=80, sample_size=205, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=180.1, ups=0.88, wpb=205, bsz=80, num_updates=1000, lr=2.53421e-05, gnorm=1.41, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1168
2023-03-13 22:11:41 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 7892 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=205.1, nsentences=80, sample_size=205.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=178, ups=0.87, wpb=205.1, bsz=80, num_updates=1010, lr=2.55955e-05, gnorm=1.364, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1179
2023-03-13 22:11:52 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 7892 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=204.9, nsentences=80, sample_size=204.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=177.1, ups=0.86, wpb=204.9, bsz=80, num_updates=1020, lr=2.5849e-05, gnorm=1.4, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1191
2023-03-13 22:12:04 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 7892 loss=0.655, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=201.6, nsentences=80, sample_size=201.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=174.8, ups=0.87, wpb=201.6, bsz=80, num_updates=1030, lr=2.61024e-05, gnorm=1.293, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1202
2023-03-13 22:12:15 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 7892 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=202.6, nsentences=80, sample_size=202.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=175.1, ups=0.86, wpb=202.6, bsz=80, num_updates=1040, lr=2.63558e-05, gnorm=1.392, clip=100, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1214
2023-03-13 22:12:27 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 7892 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=206.6, nsentences=80, sample_size=206.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=178.3, ups=0.86, wpb=206.6, bsz=80, num_updates=1050, lr=2.66092e-05, gnorm=1.299, clip=90, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1225
2023-03-13 22:12:38 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 7892 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=201.6, nsentences=80, sample_size=201.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=176.8, ups=0.88, wpb=201.6, bsz=80, num_updates=1060, lr=2.68626e-05, gnorm=1.339, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1237
2023-03-13 22:12:50 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 7892 loss=0.676, loss_v1=0, loss_v2=0, nll_loss=0.473, ntokens=200.6, nsentences=80, sample_size=200.6, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=176.2, ups=0.88, wpb=200.6, bsz=80, num_updates=1070, lr=2.71161e-05, gnorm=1.389, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1248
2023-03-13 22:13:01 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 7892 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=203, nsentences=80, sample_size=203, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=174.2, ups=0.86, wpb=203, bsz=80, num_updates=1080, lr=2.73695e-05, gnorm=1.643, clip=100, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1260
2023-03-13 22:13:13 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 7892 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=201.7, nsentences=80, sample_size=201.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=176.4, ups=0.87, wpb=201.7, bsz=80, num_updates=1090, lr=2.76229e-05, gnorm=1.321, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1271
2023-03-13 22:13:24 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 7892 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=205.3, nsentences=80, sample_size=205.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=177.7, ups=0.87, wpb=205.3, bsz=80, num_updates=1100, lr=2.78763e-05, gnorm=1.499, clip=100, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1283
2023-03-13 22:13:36 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 7892 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=202.1, nsentences=80, sample_size=202.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=176.4, ups=0.87, wpb=202.1, bsz=80, num_updates=1110, lr=2.81298e-05, gnorm=1.415, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1294
2023-03-13 22:13:47 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 7892 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=176.1, ups=0.87, wpb=202.8, bsz=80, num_updates=1120, lr=2.83832e-05, gnorm=1.162, clip=100, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1306
2023-03-13 22:13:59 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 7892 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=200.4, nsentences=80, sample_size=200.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=175.6, ups=0.88, wpb=200.4, bsz=80, num_updates=1130, lr=2.86366e-05, gnorm=1.41, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1317
2023-03-13 22:14:10 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 7892 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=182.1, ups=0.9, wpb=202.9, bsz=80, num_updates=1140, lr=2.889e-05, gnorm=1.378, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1328
2023-03-13 22:14:21 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 7892 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=204, nsentences=80, sample_size=204, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=179, ups=0.88, wpb=204, bsz=80, num_updates=1150, lr=2.91434e-05, gnorm=1.402, clip=100, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1340
2023-03-13 22:14:33 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 7892 loss=0.666, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=200.7, nsentences=80, sample_size=200.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=173.7, ups=0.87, wpb=200.7, bsz=80, num_updates=1160, lr=2.93969e-05, gnorm=1.508, clip=100, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1351
2023-03-13 22:14:44 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 7892 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=177.5, ups=0.88, wpb=202.8, bsz=80, num_updates=1170, lr=2.96503e-05, gnorm=1.57, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1363
2023-03-13 22:14:56 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 7892 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=203.6, nsentences=80, sample_size=203.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=178.3, ups=0.88, wpb=203.6, bsz=80, num_updates=1180, lr=2.99037e-05, gnorm=1.339, clip=100, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1374
2023-03-13 22:15:08 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 7892 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=202.3, nsentences=80, sample_size=202.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=172.3, ups=0.85, wpb=202.3, bsz=80, num_updates=1190, lr=3.01571e-05, gnorm=1.356, clip=100, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1386
2023-03-13 22:15:19 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 7892 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=202, nsentences=80, sample_size=202, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=174.1, ups=0.86, wpb=202, bsz=80, num_updates=1200, lr=3.04105e-05, gnorm=1.333, clip=90, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1398
2023-03-13 22:15:31 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 7892 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=199.4, nsentences=80, sample_size=199.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=172.9, ups=0.87, wpb=199.4, bsz=80, num_updates=1210, lr=3.0664e-05, gnorm=1.225, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1409
2023-03-13 22:15:42 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 7892 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=203.4, nsentences=80, sample_size=203.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=176.1, ups=0.87, wpb=203.4, bsz=80, num_updates=1220, lr=3.09174e-05, gnorm=1.216, clip=100, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1421
2023-03-13 22:15:54 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 7892 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=201.7, nsentences=80, sample_size=201.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=177.1, ups=0.88, wpb=201.7, bsz=80, num_updates=1230, lr=3.11708e-05, gnorm=1.315, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1432
2023-03-13 22:16:05 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 7892 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=202.1, nsentences=80, sample_size=202.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=174.4, ups=0.86, wpb=202.1, bsz=80, num_updates=1240, lr=3.14242e-05, gnorm=1.287, clip=100, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1444
2023-03-13 22:16:17 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 7892 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=202.5, nsentences=80, sample_size=202.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=175.2, ups=0.86, wpb=202.5, bsz=80, num_updates=1250, lr=3.16776e-05, gnorm=1.31, clip=100, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1455
2023-03-13 22:16:26 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-03-13 22:16:29 - progress_bar.py[line:274] - INFO: epoch 001:   1261 / 7892 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=204.2, nsentences=80, sample_size=204.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=161.9, ups=0.79, wpb=204.2, bsz=80, num_updates=1260, lr=3.19311e-05, gnorm=1.28, clip=100, loss_scale=256, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=1468
2023-03-13 22:16:41 - progress_bar.py[line:274] - INFO: epoch 001:   1271 / 7892 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=203.9, nsentences=80, sample_size=203.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=179.1, ups=0.88, wpb=203.9, bsz=80, num_updates=1270, lr=3.21845e-05, gnorm=1.18, clip=90, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1479
2023-03-13 22:16:52 - progress_bar.py[line:274] - INFO: epoch 001:   1281 / 7892 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=204.3, nsentences=80, sample_size=204.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=176.8, ups=0.87, wpb=204.3, bsz=80, num_updates=1280, lr=3.24379e-05, gnorm=1.228, clip=80, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1491
2023-03-13 22:17:04 - progress_bar.py[line:274] - INFO: epoch 001:   1291 / 7892 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=202.5, nsentences=80, sample_size=202.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=179.8, ups=0.89, wpb=202.5, bsz=80, num_updates=1290, lr=3.26913e-05, gnorm=1.355, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1502
2023-03-13 22:17:15 - progress_bar.py[line:274] - INFO: epoch 001:   1301 / 7892 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=205, nsentences=80, sample_size=205, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=175.3, ups=0.86, wpb=205, bsz=80, num_updates=1300, lr=3.29448e-05, gnorm=1.186, clip=80, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1514
2023-03-13 22:17:27 - progress_bar.py[line:274] - INFO: epoch 001:   1311 / 7892 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=203.7, nsentences=80, sample_size=203.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=175.6, ups=0.86, wpb=203.7, bsz=80, num_updates=1310, lr=3.31982e-05, gnorm=1.176, clip=70, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1525
2023-03-13 22:17:39 - progress_bar.py[line:274] - INFO: epoch 001:   1321 / 7892 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=175, ups=0.86, wpb=202.4, bsz=80, num_updates=1320, lr=3.34516e-05, gnorm=1.182, clip=70, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1537
2023-03-13 22:17:50 - progress_bar.py[line:274] - INFO: epoch 001:   1331 / 7892 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=202.7, nsentences=80, sample_size=202.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.4, ups=0.87, wpb=202.7, bsz=80, num_updates=1330, lr=3.3705e-05, gnorm=1.181, clip=80, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1549
2023-03-13 22:18:02 - progress_bar.py[line:274] - INFO: epoch 001:   1341 / 7892 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=201.9, nsentences=80, sample_size=201.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.3, ups=0.87, wpb=201.9, bsz=80, num_updates=1340, lr=3.39584e-05, gnorm=1.189, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1560
2023-03-13 22:18:13 - progress_bar.py[line:274] - INFO: epoch 001:   1351 / 7892 loss=0.672, loss_v1=0, loss_v2=0, nll_loss=0.467, ntokens=200, nsentences=80, sample_size=200, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=172.9, ups=0.86, wpb=200, bsz=80, num_updates=1350, lr=3.42119e-05, gnorm=1.248, clip=90, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1572
2023-03-13 22:18:25 - progress_bar.py[line:274] - INFO: epoch 001:   1361 / 7892 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=203.5, nsentences=80, sample_size=203.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=178.6, ups=0.88, wpb=203.5, bsz=80, num_updates=1360, lr=3.44653e-05, gnorm=1.272, clip=100, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1583
2023-03-13 22:18:36 - progress_bar.py[line:274] - INFO: epoch 001:   1371 / 7892 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=200.6, nsentences=80, sample_size=200.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=176.1, ups=0.88, wpb=200.6, bsz=80, num_updates=1370, lr=3.47187e-05, gnorm=1.196, clip=100, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=1595
2023-03-13 22:18:47 - progress_bar.py[line:274] - INFO: epoch 001:   1381 / 7892 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=203.7, nsentences=80, sample_size=203.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=178.8, ups=0.88, wpb=203.7, bsz=80, num_updates=1380, lr=3.49721e-05, gnorm=1.2, clip=80, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1606
2023-03-13 22:18:59 - progress_bar.py[line:274] - INFO: epoch 001:   1391 / 7892 loss=0.678, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=200, nsentences=80, sample_size=200, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=172.7, ups=0.86, wpb=200, bsz=80, num_updates=1390, lr=3.52255e-05, gnorm=1.141, clip=90, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1617
2023-03-13 22:19:10 - progress_bar.py[line:274] - INFO: epoch 001:   1401 / 7892 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=205, nsentences=80, sample_size=205, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=177.4, ups=0.87, wpb=205, bsz=80, num_updates=1400, lr=3.5479e-05, gnorm=1.17, clip=90, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1629
2023-03-13 22:19:22 - progress_bar.py[line:274] - INFO: epoch 001:   1411 / 7892 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=205.2, nsentences=80, sample_size=205.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=176.9, ups=0.86, wpb=205.2, bsz=80, num_updates=1410, lr=3.57324e-05, gnorm=1.078, clip=60, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1641
2023-03-13 22:19:34 - progress_bar.py[line:274] - INFO: epoch 001:   1421 / 7892 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=204, nsentences=80, sample_size=204, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=176.2, ups=0.86, wpb=204, bsz=80, num_updates=1420, lr=3.59858e-05, gnorm=1.173, clip=80, loss_scale=256, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=1652
2023-03-13 22:19:45 - progress_bar.py[line:274] - INFO: epoch 001:   1431 / 7892 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=206, nsentences=80, sample_size=206, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=177.5, ups=0.86, wpb=206, bsz=80, num_updates=1430, lr=3.62392e-05, gnorm=1.317, clip=100, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1664
2023-03-13 22:19:57 - progress_bar.py[line:274] - INFO: epoch 001:   1441 / 7892 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=203.1, nsentences=80, sample_size=203.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=178.1, ups=0.88, wpb=203.1, bsz=80, num_updates=1440, lr=3.64927e-05, gnorm=1.142, clip=70, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1675
2023-03-13 22:20:08 - progress_bar.py[line:274] - INFO: epoch 001:   1451 / 7892 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.6, ups=0.87, wpb=202.8, bsz=80, num_updates=1450, lr=3.67461e-05, gnorm=1.025, clip=60, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1687
2023-03-13 22:20:20 - progress_bar.py[line:274] - INFO: epoch 001:   1461 / 7892 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=201, nsentences=80, sample_size=201, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=174, ups=0.87, wpb=201, bsz=80, num_updates=1460, lr=3.69995e-05, gnorm=1.302, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1698
2023-03-13 22:20:31 - progress_bar.py[line:274] - INFO: epoch 001:   1471 / 7892 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=202.5, nsentences=80, sample_size=202.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=175.6, ups=0.87, wpb=202.5, bsz=80, num_updates=1470, lr=3.72529e-05, gnorm=1.23, clip=90, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1710
2023-03-13 22:20:43 - progress_bar.py[line:274] - INFO: epoch 001:   1481 / 7892 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=176, ups=0.87, wpb=202.8, bsz=80, num_updates=1480, lr=3.75063e-05, gnorm=1.141, clip=80, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1721
2023-03-13 22:20:54 - progress_bar.py[line:274] - INFO: epoch 001:   1491 / 7892 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=203.8, nsentences=80, sample_size=203.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=177.1, ups=0.87, wpb=203.8, bsz=80, num_updates=1490, lr=3.77598e-05, gnorm=1.233, clip=70, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1733
2023-03-13 22:21:06 - progress_bar.py[line:274] - INFO: epoch 001:   1501 / 7892 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=203.3, nsentences=80, sample_size=203.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=178.2, ups=0.88, wpb=203.3, bsz=80, num_updates=1500, lr=3.80132e-05, gnorm=1.244, clip=90, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1744
2023-03-13 22:21:17 - progress_bar.py[line:274] - INFO: epoch 001:   1511 / 7892 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=202.3, nsentences=80, sample_size=202.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=173.6, ups=0.86, wpb=202.3, bsz=80, num_updates=1510, lr=3.82666e-05, gnorm=1.098, clip=80, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=1756
2023-03-13 22:21:29 - progress_bar.py[line:274] - INFO: epoch 001:   1521 / 7892 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=201.4, nsentences=80, sample_size=201.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=174.3, ups=0.87, wpb=201.4, bsz=80, num_updates=1520, lr=3.852e-05, gnorm=1.093, clip=70, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=1768
2023-03-13 22:21:41 - progress_bar.py[line:274] - INFO: epoch 001:   1531 / 7892 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=202.5, nsentences=80, sample_size=202.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=175.3, ups=0.87, wpb=202.5, bsz=80, num_updates=1530, lr=3.87734e-05, gnorm=1.101, clip=50, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1779
2023-03-13 22:21:52 - progress_bar.py[line:274] - INFO: epoch 001:   1541 / 7892 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=203.6, nsentences=80, sample_size=203.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=176.6, ups=0.87, wpb=203.6, bsz=80, num_updates=1540, lr=3.90269e-05, gnorm=1.082, clip=50, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1791
2023-03-13 22:22:04 - progress_bar.py[line:274] - INFO: epoch 001:   1551 / 7892 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=199.8, nsentences=80, sample_size=199.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=174.5, ups=0.87, wpb=199.8, bsz=80, num_updates=1550, lr=3.92803e-05, gnorm=1.101, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1802
2023-03-13 22:22:15 - progress_bar.py[line:274] - INFO: epoch 001:   1561 / 7892 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=203.4, nsentences=80, sample_size=203.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=176.2, ups=0.87, wpb=203.4, bsz=80, num_updates=1560, lr=3.95337e-05, gnorm=1.119, clip=60, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1814
2023-03-13 22:22:27 - progress_bar.py[line:274] - INFO: epoch 001:   1571 / 7892 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=201.6, nsentences=80, sample_size=201.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=175.4, ups=0.87, wpb=201.6, bsz=80, num_updates=1570, lr=3.97871e-05, gnorm=1.077, clip=60, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1825
2023-03-13 22:22:38 - progress_bar.py[line:274] - INFO: epoch 001:   1581 / 7892 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=200.1, nsentences=80, sample_size=200.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=175.6, ups=0.88, wpb=200.1, bsz=80, num_updates=1580, lr=4.00405e-05, gnorm=1.045, clip=60, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1837
2023-03-13 22:22:49 - progress_bar.py[line:274] - INFO: epoch 001:   1591 / 7892 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=178.5, ups=0.88, wpb=202.9, bsz=80, num_updates=1590, lr=4.0294e-05, gnorm=1.232, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1848
2023-03-13 22:23:01 - progress_bar.py[line:274] - INFO: epoch 001:   1601 / 7892 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=178.1, ups=0.88, wpb=202.9, bsz=80, num_updates=1600, lr=4.05474e-05, gnorm=1.152, clip=80, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1859
2023-03-13 22:23:12 - progress_bar.py[line:274] - INFO: epoch 001:   1611 / 7892 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=203.4, nsentences=80, sample_size=203.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=178, ups=0.88, wpb=203.4, bsz=80, num_updates=1610, lr=4.08008e-05, gnorm=1.102, clip=60, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1871
2023-03-13 22:23:24 - progress_bar.py[line:274] - INFO: epoch 001:   1621 / 7892 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=202.2, nsentences=80, sample_size=202.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=174, ups=0.86, wpb=202.2, bsz=80, num_updates=1620, lr=4.10542e-05, gnorm=1.162, clip=90, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1882
2023-03-13 22:23:35 - progress_bar.py[line:274] - INFO: epoch 001:   1631 / 7892 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=205.3, nsentences=80, sample_size=205.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=178.1, ups=0.87, wpb=205.3, bsz=80, num_updates=1630, lr=4.13077e-05, gnorm=1.007, clip=60, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1894
2023-03-13 22:23:47 - progress_bar.py[line:274] - INFO: epoch 001:   1641 / 7892 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=206.7, nsentences=80, sample_size=206.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=181.2, ups=0.88, wpb=206.7, bsz=80, num_updates=1640, lr=4.15611e-05, gnorm=1.056, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1905
2023-03-13 22:23:58 - progress_bar.py[line:274] - INFO: epoch 001:   1651 / 7892 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=201.5, nsentences=80, sample_size=201.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=174.3, ups=0.87, wpb=201.5, bsz=80, num_updates=1650, lr=4.18145e-05, gnorm=1.157, clip=40, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1917
2023-03-13 22:24:10 - progress_bar.py[line:274] - INFO: epoch 001:   1661 / 7892 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=203.7, nsentences=80, sample_size=203.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=178.2, ups=0.87, wpb=203.7, bsz=80, num_updates=1660, lr=4.20679e-05, gnorm=1.087, clip=40, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1928
2023-03-13 22:24:21 - progress_bar.py[line:274] - INFO: epoch 001:   1671 / 7892 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=202.5, nsentences=80, sample_size=202.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=177.6, ups=0.88, wpb=202.5, bsz=80, num_updates=1670, lr=4.23213e-05, gnorm=0.97, clip=40, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1940
2023-03-13 22:24:33 - progress_bar.py[line:274] - INFO: epoch 001:   1681 / 7892 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=202.6, nsentences=80, sample_size=202.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=174.1, ups=0.86, wpb=202.6, bsz=80, num_updates=1680, lr=4.25748e-05, gnorm=1.098, clip=50, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1951
2023-03-13 22:24:44 - progress_bar.py[line:274] - INFO: epoch 001:   1691 / 7892 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=203.1, nsentences=80, sample_size=203.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=175.2, ups=0.86, wpb=203.1, bsz=80, num_updates=1690, lr=4.28282e-05, gnorm=1.193, clip=70, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1963
2023-03-13 22:24:56 - progress_bar.py[line:274] - INFO: epoch 001:   1701 / 7892 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=202, nsentences=80, sample_size=202, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=174.5, ups=0.86, wpb=202, bsz=80, num_updates=1700, lr=4.30816e-05, gnorm=1.041, clip=50, loss_scale=256, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=1975
2023-03-13 22:25:08 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 7892 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=202.5, nsentences=80, sample_size=202.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=175, ups=0.86, wpb=202.5, bsz=80, num_updates=1710, lr=4.3335e-05, gnorm=0.927, clip=20, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=1986
2023-03-13 22:25:19 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 7892 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=204.1, nsentences=80, sample_size=204.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=181.4, ups=0.89, wpb=204.1, bsz=80, num_updates=1720, lr=4.35884e-05, gnorm=1.039, clip=40, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1997
2023-03-13 22:25:31 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 7892 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=201.2, nsentences=80, sample_size=201.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=171.5, ups=0.85, wpb=201.2, bsz=80, num_updates=1730, lr=4.38419e-05, gnorm=1.129, clip=70, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2009
2023-03-13 22:25:42 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 7892 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=203.4, nsentences=80, sample_size=203.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=180.9, ups=0.89, wpb=203.4, bsz=80, num_updates=1740, lr=4.40953e-05, gnorm=1.019, clip=50, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2020
2023-03-13 22:25:53 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 7892 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=201.5, nsentences=80, sample_size=201.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=174.5, ups=0.87, wpb=201.5, bsz=80, num_updates=1750, lr=4.43487e-05, gnorm=1.021, clip=60, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2032
2023-03-13 22:26:05 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 7892 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=201.5, nsentences=80, sample_size=201.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=174.7, ups=0.87, wpb=201.5, bsz=80, num_updates=1760, lr=4.46021e-05, gnorm=1.021, clip=60, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2043
2023-03-13 22:26:16 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 7892 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=177.9, ups=0.88, wpb=202.4, bsz=80, num_updates=1770, lr=4.48555e-05, gnorm=1.147, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2055
2023-03-13 22:26:28 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 7892 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=201.5, nsentences=80, sample_size=201.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=174.9, ups=0.87, wpb=201.5, bsz=80, num_updates=1780, lr=4.5109e-05, gnorm=1.123, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2066
2023-03-13 22:26:39 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 7892 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=205.5, nsentences=80, sample_size=205.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=182.4, ups=0.89, wpb=205.5, bsz=80, num_updates=1790, lr=4.53624e-05, gnorm=0.917, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2078
2023-03-13 22:26:50 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 7892 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=204.4, nsentences=80, sample_size=204.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=179.2, ups=0.88, wpb=204.4, bsz=80, num_updates=1800, lr=4.56158e-05, gnorm=1.02, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2089
2023-03-13 22:27:02 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 7892 loss=0.654, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=177.6, ups=0.88, wpb=202.4, bsz=80, num_updates=1810, lr=4.58692e-05, gnorm=1.165, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2100
2023-03-13 22:27:13 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 7892 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=200.6, nsentences=80, sample_size=200.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=175.5, ups=0.87, wpb=200.6, bsz=80, num_updates=1820, lr=4.61227e-05, gnorm=1.082, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2112
2023-03-13 22:27:25 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 7892 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=174.6, ups=0.86, wpb=202.4, bsz=80, num_updates=1830, lr=4.63761e-05, gnorm=1.024, clip=50, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2123
2023-03-13 22:27:37 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 7892 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=202.1, nsentences=80, sample_size=202.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.7, ups=0.87, wpb=202.1, bsz=80, num_updates=1840, lr=4.66295e-05, gnorm=1.107, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2135
2023-03-13 22:27:48 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 7892 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=204.7, nsentences=80, sample_size=204.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=177.5, ups=0.87, wpb=204.7, bsz=80, num_updates=1850, lr=4.68829e-05, gnorm=0.991, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2147
2023-03-13 22:28:00 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 7892 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.5, ups=0.87, wpb=202.9, bsz=80, num_updates=1860, lr=4.71363e-05, gnorm=0.994, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2158
2023-03-13 22:28:11 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 7892 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=201, nsentences=80, sample_size=201, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=174.1, ups=0.87, wpb=201, bsz=80, num_updates=1870, lr=4.73898e-05, gnorm=0.933, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2170
2023-03-13 22:28:23 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 7892 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=175.3, ups=0.86, wpb=202.9, bsz=80, num_updates=1880, lr=4.76432e-05, gnorm=0.834, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2181
2023-03-13 22:28:34 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 7892 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=202, nsentences=80, sample_size=202, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=174.4, ups=0.86, wpb=202, bsz=80, num_updates=1890, lr=4.78966e-05, gnorm=1.031, clip=50, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2193
2023-03-13 22:28:46 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 7892 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=203.2, nsentences=80, sample_size=203.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=176, ups=0.87, wpb=203.2, bsz=80, num_updates=1900, lr=4.815e-05, gnorm=0.967, clip=40, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=2204
2023-03-13 22:28:57 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 7892 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=200.6, nsentences=80, sample_size=200.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=173.4, ups=0.86, wpb=200.6, bsz=80, num_updates=1910, lr=4.84034e-05, gnorm=1.002, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2216
2023-03-13 22:29:09 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 7892 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=201.3, nsentences=80, sample_size=201.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=176.2, ups=0.88, wpb=201.3, bsz=80, num_updates=1920, lr=4.86569e-05, gnorm=0.869, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=2227
2023-03-13 22:29:20 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 7892 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=204, nsentences=80, sample_size=204, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=176.8, ups=0.87, wpb=204, bsz=80, num_updates=1930, lr=4.89103e-05, gnorm=0.95, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2239
2023-03-13 22:29:32 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 7892 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=203.1, nsentences=80, sample_size=203.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=176.1, ups=0.87, wpb=203.1, bsz=80, num_updates=1940, lr=4.91637e-05, gnorm=1.04, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2251
2023-03-13 22:29:44 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 7892 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=174.9, ups=0.86, wpb=202.9, bsz=80, num_updates=1950, lr=4.94171e-05, gnorm=0.946, clip=20, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=2262
2023-03-13 22:29:55 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 7892 loss=0.656, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=200.9, nsentences=80, sample_size=200.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=174, ups=0.87, wpb=200.9, bsz=80, num_updates=1960, lr=4.96706e-05, gnorm=0.977, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2274
2023-03-13 22:30:06 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 7892 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=203.2, nsentences=80, sample_size=203.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=181, ups=0.89, wpb=203.2, bsz=80, num_updates=1970, lr=4.9924e-05, gnorm=0.925, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2285
2023-03-13 22:30:18 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 7892 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=204.3, nsentences=80, sample_size=204.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=181.1, ups=0.89, wpb=204.3, bsz=80, num_updates=1980, lr=4.99907e-05, gnorm=0.97, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2296
2023-03-13 22:30:29 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 7892 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=202.3, nsentences=80, sample_size=202.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=177.5, ups=0.88, wpb=202.3, bsz=80, num_updates=1990, lr=4.99773e-05, gnorm=0.877, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2308
2023-03-13 22:30:40 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 7892 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=201.3, nsentences=80, sample_size=201.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=179.5, ups=0.89, wpb=201.3, bsz=80, num_updates=2000, lr=4.9964e-05, gnorm=1.023, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2319
2023-03-13 22:30:52 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 7892 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=201.9, nsentences=80, sample_size=201.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=174.9, ups=0.87, wpb=201.9, bsz=80, num_updates=2010, lr=4.99506e-05, gnorm=1.049, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2330
2023-03-13 22:31:03 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 7892 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=203, nsentences=80, sample_size=203, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=180.1, ups=0.89, wpb=203, bsz=80, num_updates=2020, lr=4.99373e-05, gnorm=1.064, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=2342
2023-03-13 22:31:14 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 7892 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=205.5, nsentences=80, sample_size=205.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=181.7, ups=0.88, wpb=205.5, bsz=80, num_updates=2030, lr=4.9924e-05, gnorm=0.889, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2353
2023-03-13 22:31:26 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 7892 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=204.1, nsentences=80, sample_size=204.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=176.2, ups=0.86, wpb=204.1, bsz=80, num_updates=2040, lr=4.99106e-05, gnorm=0.932, clip=40, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=2365
2023-03-13 22:31:38 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 7892 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=203.5, nsentences=80, sample_size=203.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=176.9, ups=0.87, wpb=203.5, bsz=80, num_updates=2050, lr=4.98973e-05, gnorm=0.881, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2376
2023-03-13 22:31:49 - progress_bar.py[line:274] - INFO: epoch 001:   2061 / 7892 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=201.7, nsentences=80, sample_size=201.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=179.2, ups=0.89, wpb=201.7, bsz=80, num_updates=2060, lr=4.9884e-05, gnorm=1.002, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2387
2023-03-13 22:32:00 - progress_bar.py[line:274] - INFO: epoch 001:   2071 / 7892 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=203.1, nsentences=80, sample_size=203.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=180.6, ups=0.89, wpb=203.1, bsz=80, num_updates=2070, lr=4.98706e-05, gnorm=1.035, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2399
2023-03-13 22:32:11 - progress_bar.py[line:274] - INFO: epoch 001:   2081 / 7892 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=201.7, nsentences=80, sample_size=201.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=179.1, ups=0.89, wpb=201.7, bsz=80, num_updates=2080, lr=4.98573e-05, gnorm=0.981, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2410
2023-03-13 22:32:23 - progress_bar.py[line:274] - INFO: epoch 001:   2091 / 7892 loss=0.64, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=201.6, nsentences=80, sample_size=201.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=174.5, ups=0.87, wpb=201.6, bsz=80, num_updates=2090, lr=4.98439e-05, gnorm=0.843, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=2421
2023-03-13 22:32:34 - progress_bar.py[line:274] - INFO: epoch 001:   2101 / 7892 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=203.1, nsentences=80, sample_size=203.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=178.6, ups=0.88, wpb=203.1, bsz=80, num_updates=2100, lr=4.98306e-05, gnorm=0.87, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2433
2023-03-13 22:32:46 - progress_bar.py[line:274] - INFO: epoch 001:   2111 / 7892 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=177.3, ups=0.88, wpb=202.4, bsz=80, num_updates=2110, lr=4.98173e-05, gnorm=0.926, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2444
2023-03-13 22:32:57 - progress_bar.py[line:274] - INFO: epoch 001:   2121 / 7892 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=203.3, nsentences=80, sample_size=203.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=180.7, ups=0.89, wpb=203.3, bsz=80, num_updates=2120, lr=4.98039e-05, gnorm=1.012, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2455
2023-03-13 22:33:08 - progress_bar.py[line:274] - INFO: epoch 001:   2131 / 7892 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=204.4, nsentences=80, sample_size=204.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=179.5, ups=0.88, wpb=204.4, bsz=80, num_updates=2130, lr=4.97906e-05, gnorm=0.916, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2467
2023-03-13 22:33:20 - progress_bar.py[line:274] - INFO: epoch 001:   2141 / 7892 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=204.9, nsentences=80, sample_size=204.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=177.4, ups=0.87, wpb=204.9, bsz=80, num_updates=2140, lr=4.97773e-05, gnorm=0.895, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2478
2023-03-13 22:33:31 - progress_bar.py[line:274] - INFO: epoch 001:   2151 / 7892 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=202.6, nsentences=80, sample_size=202.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=175.3, ups=0.87, wpb=202.6, bsz=80, num_updates=2150, lr=4.97639e-05, gnorm=0.866, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2490
2023-03-13 22:33:32 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-03-13 22:33:44 - progress_bar.py[line:274] - INFO: epoch 001:   2162 / 7892 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=201.6, nsentences=80, sample_size=201.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=159.1, ups=0.79, wpb=201.6, bsz=80, num_updates=2160, lr=4.97506e-05, gnorm=1.006, clip=60, loss_scale=256, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=2503
2023-03-13 22:33:56 - progress_bar.py[line:274] - INFO: epoch 001:   2172 / 7892 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=206.2, nsentences=80, sample_size=206.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=178.5, ups=0.87, wpb=206.2, bsz=80, num_updates=2170, lr=4.97372e-05, gnorm=1.014, clip=60, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2514
2023-03-13 22:34:07 - progress_bar.py[line:274] - INFO: epoch 001:   2182 / 7892 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=203, nsentences=80, sample_size=203, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=178.4, ups=0.88, wpb=203, bsz=80, num_updates=2180, lr=4.97239e-05, gnorm=0.94, clip=30, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2526
2023-03-13 22:34:18 - progress_bar.py[line:274] - INFO: epoch 001:   2192 / 7892 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=202.7, nsentences=80, sample_size=202.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=177.9, ups=0.88, wpb=202.7, bsz=80, num_updates=2190, lr=4.97106e-05, gnorm=0.96, clip=30, loss_scale=256, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=2537
2023-03-13 22:34:30 - progress_bar.py[line:274] - INFO: epoch 001:   2202 / 7892 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=203.2, nsentences=80, sample_size=203.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=178.2, ups=0.88, wpb=203.2, bsz=80, num_updates=2200, lr=4.96972e-05, gnorm=0.957, clip=30, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2548
2023-03-13 22:34:41 - progress_bar.py[line:274] - INFO: epoch 001:   2212 / 7892 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.5, ups=0.87, wpb=202.4, bsz=80, num_updates=2210, lr=4.96839e-05, gnorm=0.968, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2560
2023-03-13 22:34:53 - progress_bar.py[line:274] - INFO: epoch 001:   2222 / 7892 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=201.1, nsentences=80, sample_size=201.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=177, ups=0.88, wpb=201.1, bsz=80, num_updates=2220, lr=4.96706e-05, gnorm=1.046, clip=50, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2571
2023-03-13 22:35:04 - progress_bar.py[line:274] - INFO: epoch 001:   2232 / 7892 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=203.4, nsentences=80, sample_size=203.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=178.2, ups=0.88, wpb=203.4, bsz=80, num_updates=2230, lr=4.96572e-05, gnorm=0.88, clip=20, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2583
2023-03-13 22:35:16 - progress_bar.py[line:274] - INFO: epoch 001:   2242 / 7892 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=203.2, nsentences=80, sample_size=203.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=178.4, ups=0.88, wpb=203.2, bsz=80, num_updates=2240, lr=4.96439e-05, gnorm=1.014, clip=60, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2594
2023-03-13 22:35:27 - progress_bar.py[line:274] - INFO: epoch 001:   2252 / 7892 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=203.5, nsentences=80, sample_size=203.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.6, ups=0.86, wpb=203.5, bsz=80, num_updates=2250, lr=4.96305e-05, gnorm=0.967, clip=40, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2606
2023-03-13 22:35:39 - progress_bar.py[line:274] - INFO: epoch 001:   2262 / 7892 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=203.2, nsentences=80, sample_size=203.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=178.3, ups=0.88, wpb=203.2, bsz=80, num_updates=2260, lr=4.96172e-05, gnorm=0.964, clip=40, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2617
2023-03-13 22:35:50 - progress_bar.py[line:274] - INFO: epoch 001:   2272 / 7892 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=202.1, nsentences=80, sample_size=202.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=176.1, ups=0.87, wpb=202.1, bsz=80, num_updates=2270, lr=4.96039e-05, gnorm=0.9, clip=20, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2629
2023-03-13 22:36:02 - progress_bar.py[line:274] - INFO: epoch 001:   2282 / 7892 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=202.2, nsentences=80, sample_size=202.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=177.2, ups=0.88, wpb=202.2, bsz=80, num_updates=2280, lr=4.95905e-05, gnorm=0.968, clip=50, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2640
2023-03-13 22:36:13 - progress_bar.py[line:274] - INFO: epoch 001:   2292 / 7892 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=200.1, nsentences=80, sample_size=200.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=173, ups=0.86, wpb=200.1, bsz=80, num_updates=2290, lr=4.95772e-05, gnorm=0.942, clip=40, loss_scale=256, train_wall=12, gb_free=11.2, ema_decay=0.9999, wall=2652
2023-03-13 22:36:25 - progress_bar.py[line:274] - INFO: epoch 001:   2302 / 7892 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=201.8, nsentences=80, sample_size=201.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=174.3, ups=0.86, wpb=201.8, bsz=80, num_updates=2300, lr=4.95638e-05, gnorm=1.037, clip=30, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=2663
2023-03-13 22:36:36 - progress_bar.py[line:274] - INFO: epoch 001:   2312 / 7892 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=203.4, nsentences=80, sample_size=203.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=180.7, ups=0.89, wpb=203.4, bsz=80, num_updates=2310, lr=4.95505e-05, gnorm=0.891, clip=30, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=2675
2023-03-13 22:36:48 - progress_bar.py[line:274] - INFO: epoch 001:   2322 / 7892 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=204.6, nsentences=80, sample_size=204.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=177.4, ups=0.87, wpb=204.6, bsz=80, num_updates=2320, lr=4.95372e-05, gnorm=0.851, clip=0, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=2686
2023-03-13 22:36:59 - progress_bar.py[line:274] - INFO: epoch 001:   2332 / 7892 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=202.5, nsentences=80, sample_size=202.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=177.4, ups=0.88, wpb=202.5, bsz=80, num_updates=2330, lr=4.95238e-05, gnorm=1.006, clip=30, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2698
2023-03-13 22:37:11 - progress_bar.py[line:274] - INFO: epoch 001:   2342 / 7892 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=201.7, nsentences=80, sample_size=201.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=174.8, ups=0.87, wpb=201.7, bsz=80, num_updates=2340, lr=4.95105e-05, gnorm=1.014, clip=40, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2709
2023-03-13 22:37:22 - progress_bar.py[line:274] - INFO: epoch 001:   2352 / 7892 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=201.8, nsentences=80, sample_size=201.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=174.8, ups=0.87, wpb=201.8, bsz=80, num_updates=2350, lr=4.94972e-05, gnorm=0.876, clip=10, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2721
2023-03-13 22:37:34 - progress_bar.py[line:274] - INFO: epoch 001:   2362 / 7892 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=203, nsentences=80, sample_size=203, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=176.7, ups=0.87, wpb=203, bsz=80, num_updates=2360, lr=4.94838e-05, gnorm=0.916, clip=20, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2732
2023-03-13 22:37:45 - progress_bar.py[line:274] - INFO: epoch 001:   2372 / 7892 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=200.5, nsentences=80, sample_size=200.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=170.9, ups=0.85, wpb=200.5, bsz=80, num_updates=2370, lr=4.94705e-05, gnorm=0.972, clip=20, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2744
2023-03-13 22:37:57 - progress_bar.py[line:274] - INFO: epoch 001:   2382 / 7892 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=205.2, nsentences=80, sample_size=205.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=176.3, ups=0.86, wpb=205.2, bsz=80, num_updates=2380, lr=4.94571e-05, gnorm=0.849, clip=20, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2756
2023-03-13 22:38:09 - progress_bar.py[line:274] - INFO: epoch 001:   2392 / 7892 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=200.8, nsentences=80, sample_size=200.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=174.5, ups=0.87, wpb=200.8, bsz=80, num_updates=2390, lr=4.94438e-05, gnorm=0.84, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2767
2023-03-13 22:38:20 - progress_bar.py[line:274] - INFO: epoch 001:   2402 / 7892 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=204.5, nsentences=80, sample_size=204.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=176.8, ups=0.86, wpb=204.5, bsz=80, num_updates=2400, lr=4.94305e-05, gnorm=0.791, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2779
2023-03-13 22:38:31 - progress_bar.py[line:274] - INFO: epoch 001:   2412 / 7892 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=201.5, nsentences=80, sample_size=201.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=179.6, ups=0.89, wpb=201.5, bsz=80, num_updates=2410, lr=4.94171e-05, gnorm=0.828, clip=10, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=2790
2023-03-13 22:38:43 - progress_bar.py[line:274] - INFO: epoch 001:   2422 / 7892 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=178, ups=0.88, wpb=202.9, bsz=80, num_updates=2420, lr=4.94038e-05, gnorm=0.955, clip=40, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=2801
2023-03-13 22:38:54 - progress_bar.py[line:274] - INFO: epoch 001:   2432 / 7892 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=202.7, nsentences=80, sample_size=202.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=182.7, ups=0.9, wpb=202.7, bsz=80, num_updates=2430, lr=4.93905e-05, gnorm=0.8, clip=10, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2812
2023-03-13 22:39:05 - progress_bar.py[line:274] - INFO: epoch 001:   2442 / 7892 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=202.3, nsentences=80, sample_size=202.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=175, ups=0.87, wpb=202.3, bsz=80, num_updates=2440, lr=4.93771e-05, gnorm=0.867, clip=10, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=2824
2023-03-13 22:39:17 - progress_bar.py[line:274] - INFO: epoch 001:   2452 / 7892 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=203.1, nsentences=80, sample_size=203.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=176, ups=0.87, wpb=203.1, bsz=80, num_updates=2450, lr=4.93638e-05, gnorm=0.97, clip=30, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2835
2023-03-13 22:39:28 - progress_bar.py[line:274] - INFO: epoch 001:   2462 / 7892 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=203.8, nsentences=80, sample_size=203.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=178.4, ups=0.88, wpb=203.8, bsz=80, num_updates=2460, lr=4.93504e-05, gnorm=0.91, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2847
2023-03-13 22:39:40 - progress_bar.py[line:274] - INFO: epoch 001:   2472 / 7892 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=204.2, nsentences=80, sample_size=204.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=176.4, ups=0.86, wpb=204.2, bsz=80, num_updates=2470, lr=4.93371e-05, gnorm=0.94, clip=40, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2858
2023-03-13 22:39:52 - progress_bar.py[line:274] - INFO: epoch 001:   2482 / 7892 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=201.5, nsentences=80, sample_size=201.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=170.6, ups=0.85, wpb=201.5, bsz=80, num_updates=2480, lr=4.93238e-05, gnorm=0.995, clip=50, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=2870
2023-03-13 22:40:03 - progress_bar.py[line:274] - INFO: epoch 001:   2492 / 7892 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=199.6, nsentences=80, sample_size=199.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=173.1, ups=0.87, wpb=199.6, bsz=80, num_updates=2490, lr=4.93104e-05, gnorm=0.843, clip=30, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=2882
2023-03-13 22:40:15 - progress_bar.py[line:274] - INFO: epoch 001:   2502 / 7892 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=203.7, nsentences=80, sample_size=203.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=176.4, ups=0.87, wpb=203.7, bsz=80, num_updates=2500, lr=4.92971e-05, gnorm=0.754, clip=0, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2893
2023-03-13 22:40:26 - progress_bar.py[line:274] - INFO: epoch 001:   2512 / 7892 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=204.4, nsentences=80, sample_size=204.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=179.6, ups=0.88, wpb=204.4, bsz=80, num_updates=2510, lr=4.92838e-05, gnorm=0.832, clip=0, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2905
2023-03-13 22:40:38 - progress_bar.py[line:274] - INFO: epoch 001:   2522 / 7892 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=201.2, nsentences=80, sample_size=201.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=176.9, ups=0.88, wpb=201.2, bsz=80, num_updates=2520, lr=4.92704e-05, gnorm=0.885, clip=20, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=2916
2023-03-13 22:40:49 - progress_bar.py[line:274] - INFO: epoch 001:   2532 / 7892 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=200.6, nsentences=80, sample_size=200.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=173.5, ups=0.87, wpb=200.6, bsz=80, num_updates=2530, lr=4.92571e-05, gnorm=0.892, clip=20, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2928
2023-03-13 22:41:01 - progress_bar.py[line:274] - INFO: epoch 001:   2542 / 7892 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=202.1, nsentences=80, sample_size=202.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=176.9, ups=0.88, wpb=202.1, bsz=80, num_updates=2540, lr=4.92437e-05, gnorm=0.808, clip=0, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2939
2023-03-13 22:41:12 - progress_bar.py[line:274] - INFO: epoch 001:   2552 / 7892 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=201.6, nsentences=80, sample_size=201.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=174.6, ups=0.87, wpb=201.6, bsz=80, num_updates=2550, lr=4.92304e-05, gnorm=0.801, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2951
2023-03-13 22:41:24 - progress_bar.py[line:274] - INFO: epoch 001:   2562 / 7892 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=202.2, nsentences=80, sample_size=202.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=175.3, ups=0.87, wpb=202.2, bsz=80, num_updates=2560, lr=4.92171e-05, gnorm=0.838, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2962
2023-03-13 22:41:35 - progress_bar.py[line:274] - INFO: epoch 001:   2572 / 7892 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=199.5, nsentences=80, sample_size=199.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=172.7, ups=0.87, wpb=199.5, bsz=80, num_updates=2570, lr=4.92037e-05, gnorm=0.953, clip=20, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2974
2023-03-13 22:41:47 - progress_bar.py[line:274] - INFO: epoch 001:   2582 / 7892 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=175.4, ups=0.86, wpb=202.8, bsz=80, num_updates=2580, lr=4.91904e-05, gnorm=0.829, clip=0, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2985
2023-03-13 22:41:59 - progress_bar.py[line:274] - INFO: epoch 001:   2592 / 7892 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=201, nsentences=80, sample_size=201, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=172.5, ups=0.86, wpb=201, bsz=80, num_updates=2590, lr=4.9177e-05, gnorm=0.829, clip=10, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2997
2023-03-13 22:42:10 - progress_bar.py[line:274] - INFO: epoch 001:   2602 / 7892 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=200.5, nsentences=80, sample_size=200.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=175.8, ups=0.88, wpb=200.5, bsz=80, num_updates=2600, lr=4.91637e-05, gnorm=0.915, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3008
2023-03-13 22:42:21 - progress_bar.py[line:274] - INFO: epoch 001:   2612 / 7892 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=200.2, nsentences=80, sample_size=200.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=173.5, ups=0.87, wpb=200.2, bsz=80, num_updates=2610, lr=4.91504e-05, gnorm=0.972, clip=40, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3020
2023-03-13 22:42:33 - progress_bar.py[line:274] - INFO: epoch 001:   2622 / 7892 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=203.3, nsentences=80, sample_size=203.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=176.2, ups=0.87, wpb=203.3, bsz=80, num_updates=2620, lr=4.9137e-05, gnorm=0.879, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=3032
2023-03-13 22:42:44 - progress_bar.py[line:274] - INFO: epoch 001:   2632 / 7892 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=204.6, nsentences=80, sample_size=204.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=181.7, ups=0.89, wpb=204.6, bsz=80, num_updates=2630, lr=4.91237e-05, gnorm=0.906, clip=30, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3043
2023-03-13 22:42:56 - progress_bar.py[line:274] - INFO: epoch 001:   2642 / 7892 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=203.6, nsentences=80, sample_size=203.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=178.2, ups=0.88, wpb=203.6, bsz=80, num_updates=2640, lr=4.91104e-05, gnorm=0.795, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3054
2023-03-13 22:43:07 - progress_bar.py[line:274] - INFO: epoch 001:   2652 / 7892 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=203.3, nsentences=80, sample_size=203.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=178.2, ups=0.88, wpb=203.3, bsz=80, num_updates=2650, lr=4.9097e-05, gnorm=0.799, clip=0, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3066
2023-03-13 22:43:19 - progress_bar.py[line:274] - INFO: epoch 001:   2662 / 7892 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=203.6, nsentences=80, sample_size=203.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=178.3, ups=0.88, wpb=203.6, bsz=80, num_updates=2660, lr=4.90837e-05, gnorm=0.741, clip=0, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3077
2023-03-13 22:43:30 - progress_bar.py[line:274] - INFO: epoch 001:   2672 / 7892 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=204.7, nsentences=80, sample_size=204.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=179.2, ups=0.88, wpb=204.7, bsz=80, num_updates=2670, lr=4.90703e-05, gnorm=0.849, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=3089
2023-03-13 22:43:41 - progress_bar.py[line:274] - INFO: epoch 001:   2682 / 7892 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=204.7, nsentences=80, sample_size=204.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=179.5, ups=0.88, wpb=204.7, bsz=80, num_updates=2680, lr=4.9057e-05, gnorm=0.88, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3100
2023-03-13 22:43:53 - progress_bar.py[line:274] - INFO: epoch 001:   2692 / 7892 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=201.4, nsentences=80, sample_size=201.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=175.4, ups=0.87, wpb=201.4, bsz=80, num_updates=2690, lr=4.90437e-05, gnorm=0.934, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3111
2023-03-13 22:44:05 - progress_bar.py[line:274] - INFO: epoch 001:   2702 / 7892 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=203, nsentences=80, sample_size=203, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=174.2, ups=0.86, wpb=203, bsz=80, num_updates=2700, lr=4.90303e-05, gnorm=0.835, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3123
2023-03-13 22:44:16 - progress_bar.py[line:274] - INFO: epoch 001:   2712 / 7892 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=200.8, nsentences=80, sample_size=200.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=174.1, ups=0.87, wpb=200.8, bsz=80, num_updates=2710, lr=4.9017e-05, gnorm=0.908, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3135
2023-03-13 22:44:27 - progress_bar.py[line:274] - INFO: epoch 001:   2722 / 7892 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=200.7, nsentences=80, sample_size=200.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.9, ups=0.88, wpb=200.7, bsz=80, num_updates=2720, lr=4.90037e-05, gnorm=0.874, clip=20, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=3146
2023-03-13 22:44:39 - progress_bar.py[line:274] - INFO: epoch 001:   2732 / 7892 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=203.4, nsentences=80, sample_size=203.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=176.1, ups=0.87, wpb=203.4, bsz=80, num_updates=2730, lr=4.89903e-05, gnorm=0.871, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3158
2023-03-13 22:44:50 - progress_bar.py[line:274] - INFO: epoch 001:   2742 / 7892 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=204.2, nsentences=80, sample_size=204.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=179, ups=0.88, wpb=204.2, bsz=80, num_updates=2740, lr=4.8977e-05, gnorm=0.765, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3169
2023-03-13 22:45:02 - progress_bar.py[line:274] - INFO: epoch 001:   2752 / 7892 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=201.2, nsentences=80, sample_size=201.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=179, ups=0.89, wpb=201.2, bsz=80, num_updates=2750, lr=4.89636e-05, gnorm=0.93, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3180
2023-03-13 22:45:13 - progress_bar.py[line:274] - INFO: epoch 001:   2762 / 7892 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=202.7, nsentences=80, sample_size=202.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=180.2, ups=0.89, wpb=202.7, bsz=80, num_updates=2760, lr=4.89503e-05, gnorm=0.828, clip=10, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=3191
2023-03-13 22:45:24 - progress_bar.py[line:274] - INFO: epoch 001:   2772 / 7892 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=203.4, nsentences=80, sample_size=203.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=178.6, ups=0.88, wpb=203.4, bsz=80, num_updates=2770, lr=4.8937e-05, gnorm=0.807, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=3203
2023-03-13 22:45:36 - progress_bar.py[line:274] - INFO: epoch 001:   2782 / 7892 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=202.1, nsentences=80, sample_size=202.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=175.5, ups=0.87, wpb=202.1, bsz=80, num_updates=2780, lr=4.89236e-05, gnorm=0.751, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3214
2023-03-13 22:45:47 - progress_bar.py[line:274] - INFO: epoch 001:   2792 / 7892 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=203.6, nsentences=80, sample_size=203.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=176.6, ups=0.87, wpb=203.6, bsz=80, num_updates=2790, lr=4.89103e-05, gnorm=0.929, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3226
2023-03-13 22:45:59 - progress_bar.py[line:274] - INFO: epoch 001:   2802 / 7892 loss=0.666, loss_v1=0, loss_v2=0, nll_loss=0.465, ntokens=202.7, nsentences=80, sample_size=202.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=174.6, ups=0.86, wpb=202.7, bsz=80, num_updates=2800, lr=4.8897e-05, gnorm=0.959, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3238
2023-03-13 22:46:11 - progress_bar.py[line:274] - INFO: epoch 001:   2812 / 7892 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=204.1, nsentences=80, sample_size=204.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=174.7, ups=0.86, wpb=204.1, bsz=80, num_updates=2810, lr=4.88836e-05, gnorm=0.875, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3249
2023-03-13 22:46:22 - progress_bar.py[line:274] - INFO: epoch 001:   2822 / 7892 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=204, nsentences=80, sample_size=204, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=176.8, ups=0.87, wpb=204, bsz=80, num_updates=2820, lr=4.88703e-05, gnorm=0.874, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3261
2023-03-13 22:46:34 - progress_bar.py[line:274] - INFO: epoch 001:   2832 / 7892 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=178.3, ups=0.88, wpb=202.9, bsz=80, num_updates=2830, lr=4.88569e-05, gnorm=0.848, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3272
2023-03-13 22:46:45 - progress_bar.py[line:274] - INFO: epoch 001:   2842 / 7892 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=204.1, nsentences=80, sample_size=204.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=178.8, ups=0.88, wpb=204.1, bsz=80, num_updates=2840, lr=4.88436e-05, gnorm=0.85, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3284
2023-03-13 22:46:56 - progress_bar.py[line:274] - INFO: epoch 001:   2852 / 7892 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=177.7, ups=0.88, wpb=202.8, bsz=80, num_updates=2850, lr=4.88303e-05, gnorm=0.853, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3295
2023-03-13 22:47:08 - progress_bar.py[line:274] - INFO: epoch 001:   2862 / 7892 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=201.8, nsentences=80, sample_size=201.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=176.9, ups=0.88, wpb=201.8, bsz=80, num_updates=2860, lr=4.88169e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3306
2023-03-13 22:47:19 - progress_bar.py[line:274] - INFO: epoch 001:   2872 / 7892 loss=0.666, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=200.9, nsentences=80, sample_size=200.9, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=174.3, ups=0.87, wpb=200.9, bsz=80, num_updates=2870, lr=4.88036e-05, gnorm=0.954, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=3318
2023-03-13 22:47:31 - progress_bar.py[line:274] - INFO: epoch 001:   2882 / 7892 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=178.1, ups=0.88, wpb=202.8, bsz=80, num_updates=2880, lr=4.87902e-05, gnorm=0.888, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3329
2023-03-13 22:47:42 - progress_bar.py[line:274] - INFO: epoch 001:   2892 / 7892 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=202.2, nsentences=80, sample_size=202.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=177.3, ups=0.88, wpb=202.2, bsz=80, num_updates=2890, lr=4.87769e-05, gnorm=0.793, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3341
2023-03-13 22:47:53 - progress_bar.py[line:274] - INFO: epoch 001:   2902 / 7892 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=203.3, nsentences=80, sample_size=203.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=181.2, ups=0.89, wpb=203.3, bsz=80, num_updates=2900, lr=4.87636e-05, gnorm=0.809, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3352
2023-03-13 22:48:05 - progress_bar.py[line:274] - INFO: epoch 001:   2912 / 7892 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=201, nsentences=80, sample_size=201, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=175.2, ups=0.87, wpb=201, bsz=80, num_updates=2910, lr=4.87502e-05, gnorm=0.84, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=3363
2023-03-13 22:48:16 - progress_bar.py[line:274] - INFO: epoch 001:   2922 / 7892 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=202.9, nsentences=80, sample_size=202.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=174.5, ups=0.86, wpb=202.9, bsz=80, num_updates=2920, lr=4.87369e-05, gnorm=0.904, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3375
2023-03-13 22:48:28 - progress_bar.py[line:274] - INFO: epoch 001:   2932 / 7892 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=201.5, nsentences=80, sample_size=201.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=176.8, ups=0.88, wpb=201.5, bsz=80, num_updates=2930, lr=4.87236e-05, gnorm=0.805, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=3386
2023-03-13 22:48:39 - progress_bar.py[line:274] - INFO: epoch 001:   2942 / 7892 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=201.9, nsentences=80, sample_size=201.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=179.4, ups=0.89, wpb=201.9, bsz=80, num_updates=2940, lr=4.87102e-05, gnorm=0.837, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3398
2023-03-13 22:48:51 - progress_bar.py[line:274] - INFO: epoch 001:   2952 / 7892 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=203.2, nsentences=80, sample_size=203.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=178, ups=0.88, wpb=203.2, bsz=80, num_updates=2950, lr=4.86969e-05, gnorm=0.843, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3409
2023-03-13 22:49:02 - progress_bar.py[line:274] - INFO: epoch 001:   2962 / 7892 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=201.5, nsentences=80, sample_size=201.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=177, ups=0.88, wpb=201.5, bsz=80, num_updates=2960, lr=4.86835e-05, gnorm=0.809, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3421
2023-03-13 22:49:13 - progress_bar.py[line:274] - INFO: epoch 001:   2972 / 7892 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=201.6, nsentences=80, sample_size=201.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=177, ups=0.88, wpb=201.6, bsz=80, num_updates=2970, lr=4.86702e-05, gnorm=0.939, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3432
2023-03-13 22:49:25 - progress_bar.py[line:274] - INFO: epoch 001:   2982 / 7892 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=202.6, nsentences=80, sample_size=202.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=177.8, ups=0.88, wpb=202.6, bsz=80, num_updates=2980, lr=4.86569e-05, gnorm=0.805, clip=10, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=3443
2023-03-13 22:49:36 - progress_bar.py[line:274] - INFO: epoch 001:   2992 / 7892 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=201.8, nsentences=80, sample_size=201.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=177.3, ups=0.88, wpb=201.8, bsz=80, num_updates=2990, lr=4.86435e-05, gnorm=0.911, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=3455
2023-03-13 22:49:48 - progress_bar.py[line:274] - INFO: epoch 001:   3002 / 7892 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=203.1, nsentences=80, sample_size=203.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=176.1, ups=0.87, wpb=203.1, bsz=80, num_updates=3000, lr=4.86302e-05, gnorm=0.824, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=3466
2023-03-13 22:49:59 - progress_bar.py[line:274] - INFO: epoch 001:   3012 / 7892 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=202, nsentences=80, sample_size=202, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.2, ups=0.87, wpb=202, bsz=80, num_updates=3010, lr=4.86169e-05, gnorm=0.815, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3478
2023-03-13 22:50:11 - progress_bar.py[line:274] - INFO: epoch 001:   3022 / 7892 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=174.5, ups=0.86, wpb=202.8, bsz=80, num_updates=3020, lr=4.86035e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=3489
2023-03-13 22:50:22 - progress_bar.py[line:274] - INFO: epoch 001:   3032 / 7892 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=204.3, nsentences=80, sample_size=204.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=177.1, ups=0.87, wpb=204.3, bsz=80, num_updates=3030, lr=4.85902e-05, gnorm=0.815, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=3501
2023-03-13 22:50:34 - progress_bar.py[line:274] - INFO: epoch 001:   3042 / 7892 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=202.6, nsentences=80, sample_size=202.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=175.2, ups=0.86, wpb=202.6, bsz=80, num_updates=3040, lr=4.85768e-05, gnorm=0.759, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3513
2023-03-13 22:50:46 - progress_bar.py[line:274] - INFO: epoch 001:   3052 / 7892 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=204.3, nsentences=80, sample_size=204.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=178.3, ups=0.87, wpb=204.3, bsz=80, num_updates=3050, lr=4.85635e-05, gnorm=0.698, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3524
2023-03-13 22:50:57 - progress_bar.py[line:274] - INFO: epoch 001:   3062 / 7892 loss=0.668, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=201.7, nsentences=80, sample_size=201.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=178.9, ups=0.89, wpb=201.7, bsz=80, num_updates=3060, lr=4.85502e-05, gnorm=0.852, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=3535
2023-03-13 22:51:08 - progress_bar.py[line:274] - INFO: epoch 001:   3072 / 7892 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=202.6, nsentences=80, sample_size=202.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=178.2, ups=0.88, wpb=202.6, bsz=80, num_updates=3070, lr=4.85368e-05, gnorm=0.805, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3547
2023-03-13 22:51:20 - progress_bar.py[line:274] - INFO: epoch 001:   3082 / 7892 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=203, nsentences=80, sample_size=203, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=176.3, ups=0.87, wpb=203, bsz=80, num_updates=3080, lr=4.85235e-05, gnorm=0.863, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3558
2023-03-13 22:51:31 - progress_bar.py[line:274] - INFO: epoch 001:   3092 / 7892 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=204, nsentences=80, sample_size=204, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=176.1, ups=0.86, wpb=204, bsz=80, num_updates=3090, lr=4.85102e-05, gnorm=0.88, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3570
2023-03-13 22:51:42 - progress_bar.py[line:274] - INFO: epoch 001:   3102 / 7892 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=203.5, nsentences=80, sample_size=203.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=185.2, ups=0.91, wpb=203.5, bsz=80, num_updates=3100, lr=4.84968e-05, gnorm=0.882, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3581
2023-03-13 22:51:54 - progress_bar.py[line:274] - INFO: epoch 001:   3112 / 7892 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=203, nsentences=80, sample_size=203, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=179.8, ups=0.89, wpb=203, bsz=80, num_updates=3110, lr=4.84835e-05, gnorm=0.917, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=3592
2023-03-13 22:52:05 - progress_bar.py[line:274] - INFO: epoch 001:   3122 / 7892 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=201.7, nsentences=80, sample_size=201.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=178.2, ups=0.88, wpb=201.7, bsz=80, num_updates=3120, lr=4.84701e-05, gnorm=0.82, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=3603
2023-03-13 22:52:17 - progress_bar.py[line:274] - INFO: epoch 001:   3132 / 7892 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=201.8, nsentences=80, sample_size=201.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=171.6, ups=0.85, wpb=201.8, bsz=80, num_updates=3130, lr=4.84568e-05, gnorm=0.768, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3615
2023-03-13 22:52:28 - progress_bar.py[line:274] - INFO: epoch 001:   3142 / 7892 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=204.8, nsentences=80, sample_size=204.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=179.6, ups=0.88, wpb=204.8, bsz=80, num_updates=3140, lr=4.84435e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3627
2023-03-13 22:52:40 - progress_bar.py[line:274] - INFO: epoch 001:   3152 / 7892 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=203.2, nsentences=80, sample_size=203.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.6, ups=0.86, wpb=203.2, bsz=80, num_updates=3150, lr=4.84301e-05, gnorm=0.867, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=3638
2023-03-13 22:52:51 - progress_bar.py[line:274] - INFO: epoch 001:   3162 / 7892 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=203.6, nsentences=80, sample_size=203.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=181.2, ups=0.89, wpb=203.6, bsz=80, num_updates=3160, lr=4.84168e-05, gnorm=0.751, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3649
2023-03-13 22:53:02 - progress_bar.py[line:274] - INFO: epoch 001:   3172 / 7892 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=178.2, ups=0.88, wpb=202.8, bsz=80, num_updates=3170, lr=4.84034e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=3661
2023-03-13 22:53:14 - progress_bar.py[line:274] - INFO: epoch 001:   3182 / 7892 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=205, nsentences=80, sample_size=205, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=179.8, ups=0.88, wpb=205, bsz=80, num_updates=3180, lr=4.83901e-05, gnorm=0.843, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3672
2023-03-13 22:53:24 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-13 22:53:26 - progress_bar.py[line:274] - INFO: epoch 001:   3193 / 7892 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=204, nsentences=80, sample_size=204, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=162.3, ups=0.8, wpb=204, bsz=80, num_updates=3190, lr=4.83768e-05, gnorm=0.828, clip=10, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=3685
2023-03-13 22:53:38 - progress_bar.py[line:274] - INFO: epoch 001:   3203 / 7892 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=203.3, nsentences=80, sample_size=203.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=175.9, ups=0.87, wpb=203.3, bsz=80, num_updates=3200, lr=4.83634e-05, gnorm=0.851, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3696
2023-03-13 22:53:49 - progress_bar.py[line:274] - INFO: epoch 001:   3213 / 7892 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=200.8, nsentences=80, sample_size=200.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.9, ups=0.88, wpb=200.8, bsz=80, num_updates=3210, lr=4.83501e-05, gnorm=0.929, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3708
2023-03-13 22:54:00 - progress_bar.py[line:274] - INFO: epoch 001:   3223 / 7892 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=203.5, nsentences=80, sample_size=203.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=180.9, ups=0.89, wpb=203.5, bsz=80, num_updates=3220, lr=4.83368e-05, gnorm=0.879, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3719
2023-03-13 22:54:12 - progress_bar.py[line:274] - INFO: epoch 001:   3233 / 7892 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=203.5, nsentences=80, sample_size=203.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=175.9, ups=0.86, wpb=203.5, bsz=80, num_updates=3230, lr=4.83234e-05, gnorm=0.802, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3731
2023-03-13 22:54:24 - progress_bar.py[line:274] - INFO: epoch 001:   3243 / 7892 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=175.2, ups=0.86, wpb=202.8, bsz=80, num_updates=3240, lr=4.83101e-05, gnorm=0.837, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3742
2023-03-13 22:54:35 - progress_bar.py[line:274] - INFO: epoch 001:   3253 / 7892 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=201, nsentences=80, sample_size=201, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=173.9, ups=0.86, wpb=201, bsz=80, num_updates=3250, lr=4.82967e-05, gnorm=0.824, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3754
2023-03-13 22:54:47 - progress_bar.py[line:274] - INFO: epoch 001:   3263 / 7892 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=201.4, nsentences=80, sample_size=201.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=177.1, ups=0.88, wpb=201.4, bsz=80, num_updates=3260, lr=4.82834e-05, gnorm=0.877, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3765
2023-03-13 22:54:58 - progress_bar.py[line:274] - INFO: epoch 001:   3273 / 7892 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=200.1, nsentences=80, sample_size=200.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=176, ups=0.88, wpb=200.1, bsz=80, num_updates=3270, lr=4.82701e-05, gnorm=0.879, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=3776
2023-03-13 22:55:09 - progress_bar.py[line:274] - INFO: epoch 001:   3283 / 7892 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=200.6, nsentences=80, sample_size=200.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=178.5, ups=0.89, wpb=200.6, bsz=80, num_updates=3280, lr=4.82567e-05, gnorm=0.876, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3788
2023-03-13 22:55:21 - progress_bar.py[line:274] - INFO: epoch 001:   3293 / 7892 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=202.8, nsentences=80, sample_size=202.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=175.9, ups=0.87, wpb=202.8, bsz=80, num_updates=3290, lr=4.82434e-05, gnorm=0.832, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3799
2023-03-13 22:55:32 - progress_bar.py[line:274] - INFO: epoch 001:   3303 / 7892 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=200.2, nsentences=80, sample_size=200.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=173.6, ups=0.87, wpb=200.2, bsz=80, num_updates=3300, lr=4.82301e-05, gnorm=0.777, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3811
2023-03-13 22:55:44 - progress_bar.py[line:274] - INFO: epoch 001:   3313 / 7892 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=202, nsentences=80, sample_size=202, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=177.2, ups=0.88, wpb=202, bsz=80, num_updates=3310, lr=4.82167e-05, gnorm=0.902, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3822
2023-03-13 22:55:55 - progress_bar.py[line:274] - INFO: epoch 001:   3323 / 7892 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=204.4, nsentences=80, sample_size=204.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=179.1, ups=0.88, wpb=204.4, bsz=80, num_updates=3320, lr=4.82034e-05, gnorm=0.821, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3834
2023-03-13 22:56:06 - progress_bar.py[line:274] - INFO: epoch 001:   3333 / 7892 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=201.8, nsentences=80, sample_size=201.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=179.3, ups=0.89, wpb=201.8, bsz=80, num_updates=3330, lr=4.819e-05, gnorm=0.855, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3845
2023-03-13 22:56:18 - progress_bar.py[line:274] - INFO: epoch 001:   3343 / 7892 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=204.2, nsentences=80, sample_size=204.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=173.8, ups=0.85, wpb=204.2, bsz=80, num_updates=3340, lr=4.81767e-05, gnorm=0.854, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3857
2023-03-13 22:56:30 - progress_bar.py[line:274] - INFO: epoch 001:   3353 / 7892 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=203.3, nsentences=80, sample_size=203.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=177.8, ups=0.87, wpb=203.3, bsz=80, num_updates=3350, lr=4.81634e-05, gnorm=0.875, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3868
2023-03-13 22:56:41 - progress_bar.py[line:274] - INFO: epoch 001:   3363 / 7892 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=204.3, nsentences=80, sample_size=204.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=177, ups=0.87, wpb=204.3, bsz=80, num_updates=3360, lr=4.815e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=3880
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 397934
Killing subprocess 397935
Killing subprocess 397936
Killing subprocess 397937
Main process received SIGINT, exiting
2023-03-13 22:59:32 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-03-13 22:59:32 - utils.py[line:261] - INFO: Start init
2023-03-13 22:59:32 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2023-03-13 22:59:32 - utils.py[line:261] - INFO: Start init
2023-03-13 22:59:32 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-03-13 22:59:32 - utils.py[line:261] - INFO: Start init
2023-03-13 22:59:32 - utils.py[line:258] - INFO: distributed init (rank 3): env://
2023-03-13 22:59:32 - utils.py[line:261] - INFO: Start init
2023-03-13 22:59:32 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2023-03-13 22:59:33 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-13 22:59:33 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-03-13 22:59:33 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-13 22:59:33 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-03-13 22:59:33 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-03-13 22:59:33 - utils.py[line:274] - INFO: initialized host node4 as rank 2
single-machine distributed training is initialized.
2023-03-13 22:59:33 - utils.py[line:274] - INFO: initialized host node4 as rank 3
single-machine distributed training is initialized.
2023-03-13 22:59:39 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 100, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 12, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 100, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='12', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=4, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', save_interval=1, save_interval_updates=100, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=100, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-03-13 22:59:39 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-03-13 22:59:39 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-03-13 22:59:44 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-03-13 22:59:44 - train.py[line:118] - INFO: task: VqaGenTask
2023-03-13 22:59:44 - train.py[line:119] - INFO: model: OFAModel
2023-03-13 22:59:44 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-03-13 22:59:44 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-03-13 22:59:44 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 2 row count 70515 total row count 282060file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 0 row count 70515 total row count 282060

file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 3 row count 70515 total row count 282060
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 1 row count 70515 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 22:59:44 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-03-13 22:59:44 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-03-13 22:59:46 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 4 workers***********************
2023-03-13 22:59:46 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 22:59:46 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 22:59:46 - utils.py[line:765] - INFO: rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 22:59:46 - utils.py[line:765] - INFO: rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 22:59:46 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 4 workers***********************
Done 0.95 cuda cpu, cpu
2023-03-13 22:59:47 - train.py[line:161] - INFO: training on 4 devices (GPUs/TPUs)
2023-03-13 22:59:47 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-03-13 22:59:47 - trainer.py[line:500] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-03-13 22:59:57 - trainer.py[line:565] - INFO: Load Model_m together with Model 2
2023-03-13 22:59:57 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 22:59:57 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 22:59:57 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 22:59:57 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 22:59:57 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-03-13 22:59:57 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-03-13 22:59:58 - trainer.py[line:675] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-03-13 22:59:58 - trainer.py[line:695] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 3 row count 157821 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 157821 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 157821 total row count 631284
2023-03-13 22:59:58 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 2 row count 157821 total row count 631284
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
2023-03-13 22:59:59 - trainer.py[line:759] - INFO: begin training epoch 1
2023-03-13 22:59:59 - train.py[line:312] - INFO: Start iterating over samples
2023-03-13 23:00:18 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 7892 loss=1.312, loss_v1=0, loss_v2=0, nll_loss=1.089, ntokens=205.4, nsentences=80, sample_size=205.4, sample_size_v1=0, sample_size_v2=0, ppl=2.13, wps=161.1, ups=0.78, wpb=205.4, bsz=80, num_updates=10, lr=2.53421e-07, gnorm=12.815, clip=100, loss_scale=128, train_wall=18, gb_free=10.5, ema_decay=0.9999, wall=32
2023-03-13 23:00:31 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 7892 loss=1.289, loss_v1=0, loss_v2=0, nll_loss=1.067, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=2.1, wps=161.6, ups=0.8, wpb=202.4, bsz=80, num_updates=20, lr=5.06842e-07, gnorm=10.98, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=45
2023-03-13 23:00:43 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 7892 loss=1.299, loss_v1=0, loss_v2=0, nll_loss=1.089, ntokens=203.9, nsentences=80, sample_size=203.9, sample_size_v1=0, sample_size_v2=0, ppl=2.13, wps=174.4, ups=0.86, wpb=203.9, bsz=80, num_updates=30, lr=7.60264e-07, gnorm=11.749, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=57
2023-03-13 23:00:55 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 7892 loss=1.254, loss_v1=0, loss_v2=0, nll_loss=1.056, ntokens=202.5, nsentences=80, sample_size=202.5, sample_size_v1=0, sample_size_v2=0, ppl=2.08, wps=163.2, ups=0.81, wpb=202.5, bsz=80, num_updates=40, lr=1.01368e-06, gnorm=8.433, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=69
2023-03-13 23:01:07 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 7892 loss=1.154, loss_v1=0, loss_v2=0, nll_loss=0.968, ntokens=204.4, nsentences=80, sample_size=204.4, sample_size_v1=0, sample_size_v2=0, ppl=1.96, wps=175.6, ups=0.86, wpb=204.4, bsz=80, num_updates=50, lr=1.26711e-06, gnorm=6.169, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=81
2023-03-13 23:01:18 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 7892 loss=1.128, loss_v1=0, loss_v2=0, nll_loss=0.944, ntokens=201.1, nsentences=80, sample_size=201.1, sample_size_v1=0, sample_size_v2=0, ppl=1.92, wps=174.7, ups=0.87, wpb=201.1, bsz=80, num_updates=60, lr=1.52053e-06, gnorm=5.469, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=92
2023-03-13 23:01:30 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 7892 loss=1.031, loss_v1=0, loss_v2=0, nll_loss=0.854, ntokens=204.8, nsentences=80, sample_size=204.8, sample_size_v1=0, sample_size_v2=0, ppl=1.81, wps=181.1, ups=0.88, wpb=204.8, bsz=80, num_updates=70, lr=1.77395e-06, gnorm=4.102, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=104
2023-03-13 23:01:42 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 7892 loss=0.982, loss_v1=0, loss_v2=0, nll_loss=0.802, ntokens=206, nsentences=80, sample_size=206, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=172.7, ups=0.84, wpb=206, bsz=80, num_updates=80, lr=2.02737e-06, gnorm=3.837, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=115
2023-03-13 23:01:53 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 7892 loss=1.01, loss_v1=0, loss_v2=0, nll_loss=0.838, ntokens=203, nsentences=80, sample_size=203, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=170.9, ups=0.84, wpb=203, bsz=80, num_updates=90, lr=2.28079e-06, gnorm=3.443, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=127
2023-03-13 23:02:05 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 7892 loss=1.027, loss_v1=0, loss_v2=0, nll_loss=0.861, ntokens=200.3, nsentences=80, sample_size=200.3, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=174.6, ups=0.87, wpb=200.3, bsz=80, num_updates=100, lr=2.53421e-06, gnorm=2.977, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=139
2023-03-13 23:02:05 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-03-13 23:02:05 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-03-13 23:02:07 - train.py[line:549] - INFO: 0 / 5877
2023-03-13 23:02:07 - train.py[line:551] - INFO: load:1.29 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-03-13 23:02:10 - trainer.py[line:1415] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 11.93 GiB (GPU 0; 39.59 GiB total capacity; 12.87 GiB already allocated; 9.30 GiB free; 27.48 GiB reserved in total by PyTorch)
2023-03-13 23:02:10 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   13181 MB |   15555 MB |   17673 GB |   17660 GB |
|       from large pool |   13036 MB |   15408 MB |   17661 GB |   17648 GB |
|       from small pool |     145 MB |     146 MB |      11 GB |      11 GB |
|---------------------------------------------------------------------------|
| Active memory         |   13181 MB |   15555 MB |   17673 GB |   17660 GB |
|       from large pool |   13036 MB |   15408 MB |   17661 GB |   17648 GB |
|       from small pool |     145 MB |     146 MB |      11 GB |      11 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   28140 MB |   36706 MB |  114228 MB |   86088 MB |
|       from large pool |   27994 MB |   36558 MB |  113924 MB |   85930 MB |
|       from small pool |     146 MB |     148 MB |     304 MB |     158 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   14958 MB |   16429 MB |   14466 GB |   14451 GB |
|       from large pool |   14957 MB |   16427 MB |   14453 GB |   14438 GB |
|       from small pool |       0 MB |       2 MB |      12 GB |      12 GB |
|---------------------------------------------------------------------------|
| Allocations           |    3641    |    3655    |  540335    |  536694    |
|       from large pool |     563    |     575    |  272237    |  271674    |
|       from small pool |    3078    |    3096    |  268098    |  265020    |
|---------------------------------------------------------------------------|
| Active allocs         |    3641    |    3655    |  540335    |  536694    |
|       from large pool |     563    |     575    |  272237    |  271674    |
|       from small pool |    3078    |    3096    |  268098    |  265020    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     192    |     233    |     474    |     282    |
|       from large pool |     119    |     159    |     322    |     203    |
|       from small pool |      73    |      74    |     152    |      79    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     146    |  279211    |  279071    |
|       from large pool |      99    |     103    |  145096    |  144997    |
|       from small pool |      41    |      48    |  134115    |  134074    |
|===========================================================================|

2023-03-13 23:02:10 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-13 23:02:10 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-13 23:02:10 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-13 23:02:10 - trainer.py[line:1164] - WARNING: ran out of memory in validation step, retrying batch
2023-03-13 23:02:13 - trainer.py[line:1415] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 9.90 GiB (GPU 2; 39.59 GiB total capacity; 11.71 GiB already allocated; 530.19 MiB free; 36.07 GiB reserved in total by PyTorch)
2023-03-13 23:02:13 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-13 23:02:13 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-13 23:02:13 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   11989 MB |   22122 MB |   17799 GB |   17787 GB |
|       from large pool |   11844 MB |   21977 MB |   17787 GB |   17775 GB |
|       from small pool |     145 MB |     146 MB |      11 GB |      11 GB |
|---------------------------------------------------------------------------|
| Active memory         |   11989 MB |   22122 MB |   17799 GB |   17787 GB |
|       from large pool |   11844 MB |   21977 MB |   17787 GB |   17775 GB |
|       from small pool |     145 MB |     146 MB |      11 GB |      11 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   36938 MB |   36940 MB |  120902 MB |   83964 MB |
|       from large pool |   36792 MB |   36792 MB |  120620 MB |   83828 MB |
|       from small pool |     146 MB |     148 MB |     282 MB |     136 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   24948 MB |   24948 MB |   14450 GB |   14426 GB |
|       from large pool |   24947 MB |   24947 MB |   14437 GB |   14413 GB |
|       from small pool |       0 MB |       2 MB |      12 GB |      12 GB |
|---------------------------------------------------------------------------|
| Allocations           |    3667    |    3681    |  549185    |  545518    |
|       from large pool |     563    |     575    |  273813    |  273250    |
|       from small pool |    3104    |    3112    |  275372    |  272268    |
|---------------------------------------------------------------------------|
| Active allocs         |    3667    |    3681    |  549185    |  545518    |
|       from large pool |     563    |     575    |  273813    |  273250    |
|       from small pool |    3104    |    3112    |  275372    |  272268    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     194    |     245    |     481    |     287    |
|       from large pool |     121    |     171    |     340    |     219    |
|       from small pool |      73    |      74    |     141    |      68    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     133    |     149    |  290593    |  290460    |
|       from large pool |      99    |     102    |  150609    |  150510    |
|       from small pool |      34    |      53    |  139984    |  139950    |
|===========================================================================|

2023-03-13 23:02:13 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-13 23:02:13 - trainer.py[line:1164] - WARNING: ran out of memory in validation step, retrying batch
2023-03-13 23:02:15 - trainer.py[line:1415] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 9.90 GiB (GPU 2; 39.59 GiB total capacity; 18.60 GiB already allocated; 8.78 GiB free; 27.81 GiB reserved in total by PyTorch)
2023-03-13 23:02:15 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-13 23:02:15 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-13 23:02:15 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   19046 MB |   22122 MB |   17923 GB |   17904 GB |
|       from large pool |   18928 MB |   21977 MB |   17911 GB |   17893 GB |
|       from small pool |     118 MB |     146 MB |      11 GB |      11 GB |
|---------------------------------------------------------------------------|
| Active memory         |   19046 MB |   22122 MB |   17923 GB |   17904 GB |
|       from large pool |   18928 MB |   21977 MB |   17911 GB |   17893 GB |
|       from small pool |     118 MB |     146 MB |      11 GB |      11 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   28476 MB |   36940 MB |  125970 MB |   97494 MB |
|       from large pool |   28354 MB |   36792 MB |  125688 MB |   97334 MB |
|       from small pool |     122 MB |     148 MB |     282 MB |     160 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    9429 MB |   24959 MB |   14567 GB |   14558 GB |
|       from large pool |    9425 MB |   24955 MB |   14554 GB |   14545 GB |
|       from small pool |       3 MB |       4 MB |      13 GB |      12 GB |
|---------------------------------------------------------------------------|
| Allocations           |    3023    |    3681    |  554261    |  551238    |
|       from large pool |     447    |     575    |  275385    |  274938    |
|       from small pool |    2576    |    3112    |  278876    |  276300    |
|---------------------------------------------------------------------------|
| Active allocs         |    3023    |    3681    |  554261    |  551238    |
|       from large pool |     447    |     575    |  275385    |  274938    |
|       from small pool |    2576    |    3112    |  278876    |  276300    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     110    |     245    |     482    |     372    |
|       from large pool |      49    |     171    |     341    |     292    |
|       from small pool |      61    |      74    |     141    |      80    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     117    |     186    |  294748    |  294631    |
|       from large pool |      51    |     102    |  151321    |  151270    |
|       from small pool |      66    |     110    |  143427    |  143361    |
|===========================================================================|

2023-03-13 23:02:15 - trainer.py[line:1418] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

Traceback (most recent call last):
  File "/data/private/yutianyu/OFA/trainer.py", line 1156, in valid_step
    sample, model, self.criterion, **extra_kwargs
  File "/data/private/yutianyu/OFA/tasks/mm_tasks/vqa_gen.py", line 294, in valid_step
    lprobs = eval_model.get_normalized_probs(decoder_out, log_probs=True)
  File "/data/private/yutianyu/OFA/models/ofa/unify_transformer.py", line 481, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_model.py", line 83, in get_normalized_probs_scriptable
    return self.decoder.get_normalized_probs(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_decoder.py", line 67, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_decoder.py", line 92, in get_normalized_probs_scriptable
    return utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/utils.py", line 521, in log_softmax
    return F.log_softmax(x, dim=dim, dtype=torch.float32)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/nn/functional.py", line 1674, in log_softmax
    ret = input.log_softmax(dim, dtype=dtype)
RuntimeError: CUDA out of memory. Tried to allocate 9.90 GiB (GPU 2; 39.59 GiB total capacity; 11.71 GiB already allocated; 530.19 MiB free; 36.07 GiB reserved in total by PyTorch)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "../../train.py", line 635, in <module>
    cli_main()
  File "../../train.py", line 628, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 206, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "../../train.py", line 332, in train
    cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch
  File "../../train.py", line 418, in validate_and_save
    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)
  File "../../train.py", line 556, in validate
    logging_output, logits, sample_ids, time_info = trainer.valid_step(sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/data/private/yutianyu/OFA/trainer.py", line 1171, in valid_step
    return self.valid_step(sample, raise_oom=True, do_distill=do_distill)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/data/private/yutianyu/OFA/trainer.py", line 1172, in valid_step
    raise e
  File "/data/private/yutianyu/OFA/trainer.py", line 1156, in valid_step
    sample, model, self.criterion, **extra_kwargs
  File "/data/private/yutianyu/OFA/tasks/mm_tasks/vqa_gen.py", line 294, in valid_step
    lprobs = eval_model.get_normalized_probs(decoder_out, log_probs=True)
  File "/data/private/yutianyu/OFA/models/ofa/unify_transformer.py", line 481, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_model.py", line 83, in get_normalized_probs_scriptable
    return self.decoder.get_normalized_probs(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_decoder.py", line 67, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_decoder.py", line 92, in get_normalized_probs_scriptable
    return utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/utils.py", line 521, in log_softmax
    return F.log_softmax(x, dim=dim, dtype=torch.float32)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/nn/functional.py", line 1674, in log_softmax
    ret = input.log_softmax(dim, dtype=dtype)
RuntimeError: CUDA out of memory. Tried to allocate 9.90 GiB (GPU 2; 39.59 GiB total capacity; 18.60 GiB already allocated; 8.78 GiB free; 27.81 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=3', '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/50_way', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=12', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=5', '--warmup-ratio=0.05', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=1', '--validate-interval=1', '--save-interval-updates=100', '--validate-interval-updates=100', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 407542
Killing subprocess 407543
Killing subprocess 407544
Killing subprocess 407545
2023-03-13 23:03:49 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-03-13 23:03:49 - utils.py[line:261] - INFO: Start init
2023-03-13 23:03:49 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-03-13 23:03:49 - utils.py[line:261] - INFO: Start init
2023-03-13 23:03:49 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2023-03-13 23:03:49 - utils.py[line:261] - INFO: Start init
2023-03-13 23:03:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-03-13 23:03:49 - utils.py[line:258] - INFO: distributed init (rank 3): env://
2023-03-13 23:03:49 - utils.py[line:261] - INFO: Start init
2023-03-13 23:03:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2023-03-13 23:03:50 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-13 23:03:50 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-13 23:03:50 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.2023-03-13 23:03:50 - utils.py[line:274] - INFO: initialized host node4 as rank 0

single-machine distributed training is initialized.
2023-03-13 23:03:50 - utils.py[line:274] - INFO: initialized host node4 as rank 2
single-machine distributed training is initialized.
2023-03-13 23:03:50 - utils.py[line:274] - INFO: initialized host node4 as rank 3
single-machine distributed training is initialized.
2023-03-13 23:03:56 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 4, 'distributed_num_procs': 4, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 100, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 100, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='8', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=4, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', save_interval=1, save_interval_updates=100, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=100, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-03-13 23:03:56 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-03-13 23:03:56 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-03-13 23:04:00 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-03-13 23:04:00 - train.py[line:118] - INFO: task: VqaGenTask
2023-03-13 23:04:00 - train.py[line:119] - INFO: model: OFAModel
2023-03-13 23:04:00 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-03-13 23:04:00 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-03-13 23:04:00 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 0 row count 70515 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 1 row count 70515 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 23:04:00 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 3 row count 70515 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 2 row count 70515 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-03-13 23:04:01 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-03-13 23:04:02 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 4 workers***********************
2023-03-13 23:04:02 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 23:04:02 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 23:04:02 - utils.py[line:765] - INFO: rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 23:04:02 - utils.py[line:765] - INFO: rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 23:04:02 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 4 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-03-13 23:04:03 - train.py[line:161] - INFO: training on 4 devices (GPUs/TPUs)
2023-03-13 23:04:03 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-03-13 23:04:03 - trainer.py[line:500] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-03-13 23:04:06 - trainer.py[line:565] - INFO: Load Model_m together with Model 2
2023-03-13 23:04:06 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 23:04:06 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 23:04:07 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 23:04:07 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 23:04:07 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-03-13 23:04:07 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-03-13 23:04:08 - trainer.py[line:675] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-03-13 23:04:08 - trainer.py[line:695] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 157821 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 2 row count 157821 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 157821 total row count 631284
2023-03-13 23:04:08 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 3 row count 157821 total row count 631284
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
2023-03-13 23:04:08 - trainer.py[line:759] - INFO: begin training epoch 1
2023-03-13 23:04:08 - train.py[line:312] - INFO: Start iterating over samples
Total steps 39460, warmup steps 1973, warmup_factor 0.0005068423720223011
2023-03-13 23:04:27 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 7892 loss=1.312, loss_v1=0, loss_v2=0, nll_loss=1.089, ntokens=205.4, nsentences=80, sample_size=205.4, sample_size_v1=0, sample_size_v2=0, ppl=2.13, wps=163.8, ups=0.8, wpb=205.4, bsz=80, num_updates=10, lr=2.53421e-07, gnorm=12.813, clip=100, loss_scale=128, train_wall=16, gb_free=10.5, ema_decay=0.9999, wall=25
2023-03-13 23:04:40 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 7892 loss=1.289, loss_v1=0, loss_v2=0, nll_loss=1.067, ntokens=202.4, nsentences=80, sample_size=202.4, sample_size_v1=0, sample_size_v2=0, ppl=2.1, wps=162.4, ups=0.8, wpb=202.4, bsz=80, num_updates=20, lr=5.06842e-07, gnorm=10.967, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=38
2023-03-13 23:04:51 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 7892 loss=1.299, loss_v1=0, loss_v2=0, nll_loss=1.089, ntokens=203.9, nsentences=80, sample_size=203.9, sample_size_v1=0, sample_size_v2=0, ppl=2.13, wps=175.2, ups=0.86, wpb=203.9, bsz=80, num_updates=30, lr=7.60264e-07, gnorm=11.733, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=49
2023-03-13 23:05:04 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 7892 loss=1.254, loss_v1=0, loss_v2=0, nll_loss=1.056, ntokens=202.5, nsentences=80, sample_size=202.5, sample_size_v1=0, sample_size_v2=0, ppl=2.08, wps=160.9, ups=0.79, wpb=202.5, bsz=80, num_updates=40, lr=1.01368e-06, gnorm=8.442, clip=100, loss_scale=128, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=62
2023-03-13 23:05:16 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 7892 loss=1.154, loss_v1=0, loss_v2=0, nll_loss=0.968, ntokens=204.4, nsentences=80, sample_size=204.4, sample_size_v1=0, sample_size_v2=0, ppl=1.96, wps=174.4, ups=0.85, wpb=204.4, bsz=80, num_updates=50, lr=1.26711e-06, gnorm=6.175, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=74
2023-03-13 23:05:27 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 7892 loss=1.128, loss_v1=0, loss_v2=0, nll_loss=0.944, ntokens=201.1, nsentences=80, sample_size=201.1, sample_size_v1=0, sample_size_v2=0, ppl=1.92, wps=174.3, ups=0.87, wpb=201.1, bsz=80, num_updates=60, lr=1.52053e-06, gnorm=5.477, clip=100, loss_scale=128, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=85
2023-03-13 23:05:38 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 7892 loss=1.031, loss_v1=0, loss_v2=0, nll_loss=0.854, ntokens=204.8, nsentences=80, sample_size=204.8, sample_size_v1=0, sample_size_v2=0, ppl=1.81, wps=181.2, ups=0.88, wpb=204.8, bsz=80, num_updates=70, lr=1.77395e-06, gnorm=4.111, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=96
2023-03-13 23:05:50 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 7892 loss=0.982, loss_v1=0, loss_v2=0, nll_loss=0.802, ntokens=206, nsentences=80, sample_size=206, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=177.1, ups=0.86, wpb=206, bsz=80, num_updates=80, lr=2.02737e-06, gnorm=3.866, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=108
2023-03-13 23:06:02 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 7892 loss=1.01, loss_v1=0, loss_v2=0, nll_loss=0.838, ntokens=203, nsentences=80, sample_size=203, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=173.1, ups=0.85, wpb=203, bsz=80, num_updates=90, lr=2.28079e-06, gnorm=3.449, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=120
2023-03-13 23:06:13 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 7892 loss=1.027, loss_v1=0, loss_v2=0, nll_loss=0.861, ntokens=200.3, nsentences=80, sample_size=200.3, sample_size_v1=0, sample_size_v2=0, ppl=1.82, wps=175.5, ups=0.88, wpb=200.3, bsz=80, num_updates=100, lr=2.53421e-06, gnorm=2.98, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=131
2023-03-13 23:06:13 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-03-13 23:06:13 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-03-13 23:06:15 - train.py[line:549] - INFO: 0 / 8815
2023-03-13 23:06:15 - train.py[line:551] - INFO: load:1.28 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-03-13 23:08:56 - train.py[line:549] - INFO: 200 / 8815
2023-03-13 23:08:56 - train.py[line:551] - INFO: load:1.30 valid_run:161.11 task_valid:150.74 collect_output:9.30
2023-03-13 23:11:32 - train.py[line:549] - INFO: 400 / 8815
2023-03-13 23:11:32 - train.py[line:551] - INFO: load:1.33 valid_run:317.56 task_valid:299.63 collect_output:15.80
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 409497
Killing subprocess 409498
Killing subprocess 409499
Killing subprocess 409500
Main process received SIGINT, exiting
2023-03-13 23:16:51 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-03-13 23:16:51 - utils.py[line:261] - INFO: Start init
2023-03-13 23:16:52 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2023-03-13 23:16:52 - utils.py[line:261] - INFO: Start init
2023-03-13 23:16:52 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-03-13 23:16:52 - utils.py[line:258] - INFO: distributed init (rank 4): env://
2023-03-13 23:16:52 - utils.py[line:261] - INFO: Start init
2023-03-13 23:16:52 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 4
2023-03-13 23:16:52 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-03-13 23:16:52 - utils.py[line:261] - INFO: Start init
2023-03-13 23:16:52 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-13 23:16:52 - utils.py[line:258] - INFO: distributed init (rank 3): env://
2023-03-13 23:16:52 - utils.py[line:261] - INFO: Start init
2023-03-13 23:16:52 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2023-03-13 23:16:52 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-13 23:16:52 - utils.py[line:274] - INFO: initialized host node4 as rank 3
single-machine distributed training is initialized.
2023-03-13 23:16:52 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-03-13 23:16:52 - utils.py[line:274] - INFO: initialized host node4 as rank 1
2023-03-13 23:16:52 - utils.py[line:274] - INFO: initialized host node4 as rank 2
2023-03-13 23:16:52 - utils.py[line:274] - INFO: initialized host node4 as rank 4
single-machine distributed training is initialized.
single-machine distributed training is initialized.
single-machine distributed training is initialized.
2023-03-13 23:16:58 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 5, 'distributed_num_procs': 5, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 5, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 100000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 100000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 5}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='8', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=5, distributed_port=-1, distributed_rank=0, distributed_world_size=5, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=5, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', save_interval=1, save_interval_updates=100000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=100000, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-03-13 23:16:59 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-03-13 23:16:59 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-03-13 23:17:03 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-03-13 23:17:03 - train.py[line:118] - INFO: task: VqaGenTask
2023-03-13 23:17:03 - train.py[line:119] - INFO: model: OFAModel
2023-03-13 23:17:03 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-03-13 23:17:03 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-03-13 23:17:03 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 4 row count 56412 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 1 row count 56412 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 3 row count 56412 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 0 row count 56412 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 23:17:03 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 2 row count 56412 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-03-13 23:17:03 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-03-13 23:17:05 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 5 workers***********************
2023-03-13 23:17:05 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 23:17:05 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 23:17:05 - utils.py[line:765] - INFO: rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 23:17:05 - utils.py[line:765] - INFO: rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 23:17:05 - utils.py[line:765] - INFO: rank   4: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-13 23:17:05 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 5 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-03-13 23:17:07 - train.py[line:161] - INFO: training on 5 devices (GPUs/TPUs)
2023-03-13 23:17:07 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-03-13 23:17:07 - trainer.py[line:500] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-03-13 23:17:10 - trainer.py[line:565] - INFO: Load Model_m together with Model 2
2023-03-13 23:17:10 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 23:17:10 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 23:17:10 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 23:17:10 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 23:17:10 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-13 23:17:10 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-03-13 23:17:11 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-03-13 23:17:11 - trainer.py[line:675] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-03-13 23:17:11 - trainer.py[line:695] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 4 row count 126256 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 3 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 2 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 126257 total row count 631284
2023-03-13 23:17:11 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 31565, warmup steps 1578, warmup_factor 0.0006337135614702154
Total steps 31565, warmup steps 1578, warmup_factor 0.0006337135614702154
Total steps 31565, warmup steps 1578, warmup_factor 0.0006337135614702154
Total steps 31565, warmup steps 1578, warmup_factor 0.0006337135614702154
Total steps 31565, warmup steps 1578, warmup_factor 0.0006337135614702154
2023-03-13 23:17:12 - trainer.py[line:759] - INFO: begin training epoch 1
2023-03-13 23:17:12 - train.py[line:312] - INFO: Start iterating over samples
2023-03-13 23:17:32 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 6313 loss=1.366, loss_v1=0, loss_v2=0, nll_loss=1.155, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=2.23, wps=195.5, ups=0.77, wpb=252.3, bsz=100, num_updates=10, lr=3.16857e-07, gnorm=12.561, clip=100, loss_scale=128, train_wall=17, gb_free=10.5, ema_decay=0.9999, wall=26
2023-03-13 23:17:44 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 6313 loss=1.282, loss_v1=0, loss_v2=0, nll_loss=1.062, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=2.09, wps=202, ups=0.8, wpb=253.3, bsz=100, num_updates=20, lr=6.33714e-07, gnorm=11.113, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39
2023-03-13 23:17:56 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 6313 loss=1.324, loss_v1=0, loss_v2=0, nll_loss=1.116, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=2.17, wps=214, ups=0.85, wpb=252.4, bsz=100, num_updates=30, lr=9.5057e-07, gnorm=10.111, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=50
2023-03-13 23:18:08 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 6313 loss=1.184, loss_v1=0, loss_v2=0, nll_loss=0.995, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.99, wps=210.9, ups=0.83, wpb=255.2, bsz=100, num_updates=40, lr=1.26743e-06, gnorm=6.794, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=63
2023-03-13 23:18:20 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 6313 loss=1.086, loss_v1=0, loss_v2=0, nll_loss=0.896, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=211.1, ups=0.83, wpb=254.8, bsz=100, num_updates=50, lr=1.58428e-06, gnorm=4.975, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=75
2023-03-13 23:18:32 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 6313 loss=1.108, loss_v1=0, loss_v2=0, nll_loss=0.923, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.9, wps=213.6, ups=0.85, wpb=251.2, bsz=100, num_updates=60, lr=1.90114e-06, gnorm=4.309, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=86
2023-03-13 23:18:44 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 6313 loss=1.005, loss_v1=0, loss_v2=0, nll_loss=0.825, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=218.5, ups=0.86, wpb=255.3, bsz=100, num_updates=70, lr=2.218e-06, gnorm=3.372, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=98
2023-03-13 23:18:55 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 6313 loss=0.966, loss_v1=0, loss_v2=0, nll_loss=0.79, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=221.1, ups=0.87, wpb=255.3, bsz=100, num_updates=80, lr=2.53485e-06, gnorm=2.892, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=110
2023-03-13 23:19:07 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 6313 loss=0.972, loss_v1=0, loss_v2=0, nll_loss=0.8, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=218, ups=0.86, wpb=252.8, bsz=100, num_updates=90, lr=2.85171e-06, gnorm=2.689, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=121
2023-03-13 23:19:18 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 6313 loss=0.966, loss_v1=0, loss_v2=0, nll_loss=0.796, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=216.8, ups=0.86, wpb=253.1, bsz=100, num_updates=100, lr=3.16857e-06, gnorm=2.526, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=133
2023-03-13 23:19:30 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 6313 loss=0.981, loss_v1=0, loss_v2=0, nll_loss=0.821, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=216.9, ups=0.86, wpb=251.7, bsz=100, num_updates=110, lr=3.48542e-06, gnorm=2.345, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=145
2023-03-13 23:19:42 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 6313 loss=0.935, loss_v1=0, loss_v2=0, nll_loss=0.77, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.71, wps=217.2, ups=0.86, wpb=252, bsz=100, num_updates=120, lr=3.80228e-06, gnorm=2.297, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=156
2023-03-13 23:19:53 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 6313 loss=0.95, loss_v1=0, loss_v2=0, nll_loss=0.789, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=213.1, ups=0.84, wpb=252.5, bsz=100, num_updates=130, lr=4.11914e-06, gnorm=2.077, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=168
2023-03-13 23:20:05 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 6313 loss=0.899, loss_v1=0, loss_v2=0, nll_loss=0.737, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=219.7, ups=0.86, wpb=254.8, bsz=100, num_updates=140, lr=4.43599e-06, gnorm=2.035, clip=100, loss_scale=128, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=180
2023-03-13 23:20:17 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 6313 loss=0.925, loss_v1=0, loss_v2=0, nll_loss=0.762, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=216.7, ups=0.86, wpb=251.4, bsz=100, num_updates=150, lr=4.75285e-06, gnorm=2.041, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=191
2023-03-13 23:20:28 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 6313 loss=0.939, loss_v1=0, loss_v2=0, nll_loss=0.783, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=213.9, ups=0.85, wpb=251.9, bsz=100, num_updates=160, lr=5.06971e-06, gnorm=2.1, clip=100, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=203
2023-03-13 23:20:40 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 6313 loss=0.886, loss_v1=0, loss_v2=0, nll_loss=0.717, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=216.4, ups=0.85, wpb=254, bsz=100, num_updates=170, lr=5.38657e-06, gnorm=2.061, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=215
2023-03-13 23:20:52 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 6313 loss=0.908, loss_v1=0, loss_v2=0, nll_loss=0.744, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=220.4, ups=0.87, wpb=251.9, bsz=100, num_updates=180, lr=5.70342e-06, gnorm=1.887, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=226
2023-03-13 23:21:04 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 6313 loss=0.907, loss_v1=0, loss_v2=0, nll_loss=0.743, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=212.8, ups=0.85, wpb=251.8, bsz=100, num_updates=190, lr=6.02028e-06, gnorm=1.845, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=238
2023-03-13 23:21:15 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 6313 loss=0.88, loss_v1=0, loss_v2=0, nll_loss=0.717, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=222.5, ups=0.88, wpb=254, bsz=100, num_updates=200, lr=6.33714e-06, gnorm=1.7, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=249
2023-03-13 23:21:26 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 6313 loss=0.863, loss_v1=0, loss_v2=0, nll_loss=0.698, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=223.1, ups=0.87, wpb=255.5, bsz=100, num_updates=210, lr=6.65399e-06, gnorm=1.666, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=261
2023-03-13 23:21:38 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 6313 loss=0.905, loss_v1=0, loss_v2=0, nll_loss=0.74, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=215.3, ups=0.86, wpb=250.9, bsz=100, num_updates=220, lr=6.97085e-06, gnorm=1.782, clip=100, loss_scale=128, train_wall=12, gb_free=9.6, ema_decay=0.9999, wall=273
2023-03-13 23:21:50 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 6313 loss=0.871, loss_v1=0, loss_v2=0, nll_loss=0.705, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=217.7, ups=0.86, wpb=253.2, bsz=100, num_updates=230, lr=7.28771e-06, gnorm=1.744, clip=100, loss_scale=128, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=284
2023-03-13 23:22:01 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 6313 loss=0.85, loss_v1=0, loss_v2=0, nll_loss=0.677, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=215.5, ups=0.86, wpb=251.7, bsz=100, num_updates=240, lr=7.60456e-06, gnorm=1.854, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=296
2023-03-13 23:22:13 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 6313 loss=0.835, loss_v1=0, loss_v2=0, nll_loss=0.654, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=218.1, ups=0.87, wpb=251.8, bsz=100, num_updates=250, lr=7.92142e-06, gnorm=1.8, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=307
2023-03-13 23:22:24 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 6313 loss=0.849, loss_v1=0, loss_v2=0, nll_loss=0.669, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=221.4, ups=0.87, wpb=253.6, bsz=100, num_updates=260, lr=8.23828e-06, gnorm=2.007, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=319
2023-03-13 23:22:36 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 6313 loss=0.831, loss_v1=0, loss_v2=0, nll_loss=0.656, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=217.4, ups=0.86, wpb=252.8, bsz=100, num_updates=270, lr=8.55513e-06, gnorm=1.965, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=331
2023-03-13 23:22:48 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 6313 loss=0.814, loss_v1=0, loss_v2=0, nll_loss=0.638, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=217.3, ups=0.86, wpb=252.3, bsz=100, num_updates=280, lr=8.87199e-06, gnorm=1.84, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=342
2023-03-13 23:22:59 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 6313 loss=0.8, loss_v1=0, loss_v2=0, nll_loss=0.617, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=216.9, ups=0.85, wpb=254.2, bsz=100, num_updates=290, lr=9.18885e-06, gnorm=1.881, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=354
2023-03-13 23:23:11 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 6313 loss=0.815, loss_v1=0, loss_v2=0, nll_loss=0.627, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=216.1, ups=0.86, wpb=250.3, bsz=100, num_updates=300, lr=9.5057e-06, gnorm=1.727, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=365
2023-03-13 23:23:23 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 6313 loss=0.75, loss_v1=0, loss_v2=0, nll_loss=0.554, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=219.9, ups=0.86, wpb=254.3, bsz=100, num_updates=310, lr=9.82256e-06, gnorm=2.13, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=377
2023-03-13 23:23:34 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 6313 loss=0.758, loss_v1=0, loss_v2=0, nll_loss=0.565, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=219.2, ups=0.86, wpb=254.8, bsz=100, num_updates=320, lr=1.01394e-05, gnorm=2.012, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=389
2023-03-13 23:23:46 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 6313 loss=0.744, loss_v1=0, loss_v2=0, nll_loss=0.551, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=221.2, ups=0.87, wpb=252.8, bsz=100, num_updates=330, lr=1.04563e-05, gnorm=1.815, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=400
2023-03-13 23:23:57 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 6313 loss=0.743, loss_v1=0, loss_v2=0, nll_loss=0.544, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=216.9, ups=0.86, wpb=251.1, bsz=100, num_updates=340, lr=1.07731e-05, gnorm=1.895, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=412
2023-03-13 23:24:09 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 6313 loss=0.733, loss_v1=0, loss_v2=0, nll_loss=0.529, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=222.3, ups=0.87, wpb=255.3, bsz=100, num_updates=350, lr=1.109e-05, gnorm=1.853, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=423
2023-03-13 23:24:20 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 6313 loss=0.732, loss_v1=0, loss_v2=0, nll_loss=0.536, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=216, ups=0.86, wpb=252.4, bsz=100, num_updates=360, lr=1.14068e-05, gnorm=1.81, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=435
2023-03-13 23:24:32 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 6313 loss=0.734, loss_v1=0, loss_v2=0, nll_loss=0.528, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=217.5, ups=0.86, wpb=252.9, bsz=100, num_updates=370, lr=1.17237e-05, gnorm=1.984, clip=100, loss_scale=128, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=447
2023-03-13 23:24:44 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 6313 loss=0.74, loss_v1=0, loss_v2=0, nll_loss=0.54, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=218, ups=0.86, wpb=253, bsz=100, num_updates=380, lr=1.20406e-05, gnorm=1.796, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=458
2023-03-13 23:24:55 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 6313 loss=0.722, loss_v1=0, loss_v2=0, nll_loss=0.52, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=217.7, ups=0.87, wpb=251.6, bsz=100, num_updates=390, lr=1.23574e-05, gnorm=1.757, clip=100, loss_scale=128, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=470
2023-03-13 23:25:07 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 6313 loss=0.735, loss_v1=0, loss_v2=0, nll_loss=0.541, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=219.8, ups=0.86, wpb=254.4, bsz=100, num_updates=400, lr=1.26743e-05, gnorm=1.634, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=481
2023-03-13 23:25:18 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 6313 loss=0.691, loss_v1=0, loss_v2=0, nll_loss=0.485, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=217.8, ups=0.85, wpb=255.6, bsz=100, num_updates=410, lr=1.29911e-05, gnorm=1.598, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=493
2023-03-13 23:25:30 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 6313 loss=0.701, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=217.7, ups=0.86, wpb=252.5, bsz=100, num_updates=420, lr=1.3308e-05, gnorm=1.604, clip=100, loss_scale=128, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=505
2023-03-13 23:25:42 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 6313 loss=0.729, loss_v1=0, loss_v2=0, nll_loss=0.53, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=218, ups=0.86, wpb=253.5, bsz=100, num_updates=430, lr=1.36248e-05, gnorm=1.677, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=516
2023-03-13 23:25:53 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 6313 loss=0.698, loss_v1=0, loss_v2=0, nll_loss=0.502, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=219.2, ups=0.86, wpb=253.9, bsz=100, num_updates=440, lr=1.39417e-05, gnorm=1.557, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=528
2023-03-13 23:26:05 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 6313 loss=0.695, loss_v1=0, loss_v2=0, nll_loss=0.488, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=219.7, ups=0.87, wpb=251.8, bsz=100, num_updates=450, lr=1.42586e-05, gnorm=1.558, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=539
2023-03-13 23:26:16 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 6313 loss=0.692, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=219.9, ups=0.86, wpb=254.5, bsz=100, num_updates=460, lr=1.45754e-05, gnorm=1.643, clip=100, loss_scale=128, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=551
2023-03-13 23:26:28 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 6313 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.454, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=223.2, ups=0.87, wpb=256, bsz=100, num_updates=470, lr=1.48923e-05, gnorm=1.455, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=562
2023-03-13 23:26:39 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 6313 loss=0.666, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=221, ups=0.87, wpb=255, bsz=100, num_updates=480, lr=1.52091e-05, gnorm=1.59, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=574
2023-03-13 23:26:51 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 6313 loss=0.671, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=218.2, ups=0.86, wpb=252.3, bsz=100, num_updates=490, lr=1.5526e-05, gnorm=1.67, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=585
2023-03-13 23:27:02 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 6313 loss=0.678, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=220.8, ups=0.88, wpb=252.2, bsz=100, num_updates=500, lr=1.58428e-05, gnorm=1.705, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=597
2023-03-13 23:27:14 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 6313 loss=0.685, loss_v1=0, loss_v2=0, nll_loss=0.479, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=218.7, ups=0.86, wpb=253.8, bsz=100, num_updates=510, lr=1.61597e-05, gnorm=1.526, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=608
2023-03-13 23:27:26 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 6313 loss=0.678, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=218.7, ups=0.86, wpb=253.2, bsz=100, num_updates=520, lr=1.64766e-05, gnorm=1.443, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=620
2023-03-13 23:27:37 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 6313 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=213.8, ups=0.84, wpb=254, bsz=100, num_updates=530, lr=1.67934e-05, gnorm=1.543, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=632
2023-03-13 23:27:49 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 6313 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=219.7, ups=0.86, wpb=254.2, bsz=100, num_updates=540, lr=1.71103e-05, gnorm=1.457, clip=100, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=644
2023-03-13 23:28:01 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 6313 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=220.3, ups=0.86, wpb=256.2, bsz=100, num_updates=550, lr=1.74271e-05, gnorm=1.223, clip=100, loss_scale=256, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=655
2023-03-13 23:28:12 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 6313 loss=0.673, loss_v1=0, loss_v2=0, nll_loss=0.471, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=220.4, ups=0.86, wpb=255.8, bsz=100, num_updates=560, lr=1.7744e-05, gnorm=1.43, clip=100, loss_scale=256, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=667
2023-03-13 23:28:24 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 6313 loss=0.667, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=221.6, ups=0.87, wpb=253.4, bsz=100, num_updates=570, lr=1.80608e-05, gnorm=1.513, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=678
2023-03-13 23:28:35 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 6313 loss=0.655, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=221.8, ups=0.88, wpb=253.3, bsz=100, num_updates=580, lr=1.83777e-05, gnorm=1.409, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=690
2023-03-13 23:28:47 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 6313 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=256.9, nsentences=100, sample_size=256.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=221.8, ups=0.86, wpb=256.9, bsz=100, num_updates=590, lr=1.86946e-05, gnorm=1.646, clip=100, loss_scale=256, train_wall=12, gb_free=11, ema_decay=0.9999, wall=701
2023-03-13 23:28:58 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 6313 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=222.1, ups=0.87, wpb=254, bsz=100, num_updates=600, lr=1.90114e-05, gnorm=1.69, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=713
2023-03-13 23:29:10 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 6313 loss=0.696, loss_v1=0, loss_v2=0, nll_loss=0.487, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=221, ups=0.87, wpb=252.6, bsz=100, num_updates=610, lr=1.93283e-05, gnorm=1.804, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=724
2023-03-13 23:29:21 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 6313 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=218.7, ups=0.86, wpb=254.7, bsz=100, num_updates=620, lr=1.96451e-05, gnorm=1.669, clip=100, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=736
2023-03-13 23:29:33 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 6313 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=217.6, ups=0.86, wpb=252.4, bsz=100, num_updates=630, lr=1.9962e-05, gnorm=1.42, clip=90, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=747
2023-03-13 23:29:44 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 6313 loss=0.678, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=220.5, ups=0.87, wpb=254.8, bsz=100, num_updates=640, lr=2.02788e-05, gnorm=1.543, clip=100, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=759
2023-03-13 23:29:56 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 6313 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=214.9, ups=0.85, wpb=253.7, bsz=100, num_updates=650, lr=2.05957e-05, gnorm=1.29, clip=90, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=771
2023-03-13 23:30:08 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 6313 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219.2, ups=0.86, wpb=253.6, bsz=100, num_updates=660, lr=2.09125e-05, gnorm=1.305, clip=100, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=782
2023-03-13 23:30:19 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 6313 loss=0.669, loss_v1=0, loss_v2=0, nll_loss=0.462, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=217.6, ups=0.86, wpb=252.1, bsz=100, num_updates=670, lr=2.12294e-05, gnorm=1.357, clip=100, loss_scale=256, train_wall=12, gb_free=11, ema_decay=0.9999, wall=794
2023-03-13 23:30:31 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 6313 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=213.7, ups=0.84, wpb=254.7, bsz=100, num_updates=680, lr=2.15463e-05, gnorm=1.252, clip=100, loss_scale=256, train_wall=12, gb_free=10, ema_decay=0.9999, wall=806
2023-03-13 23:30:43 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 6313 loss=0.667, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=218.4, ups=0.86, wpb=253, bsz=100, num_updates=690, lr=2.18631e-05, gnorm=1.261, clip=100, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=818
2023-03-13 23:30:55 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 6313 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219.6, ups=0.86, wpb=254.1, bsz=100, num_updates=700, lr=2.218e-05, gnorm=1.273, clip=100, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=829
2023-03-13 23:31:06 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.8, ups=0.86, wpb=253, bsz=100, num_updates=710, lr=2.24968e-05, gnorm=1.315, clip=100, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=841
2023-03-13 23:31:18 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 6313 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.2, ups=0.86, wpb=253.5, bsz=100, num_updates=720, lr=2.28137e-05, gnorm=1.36, clip=100, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=852
2023-03-13 23:31:29 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=224.7, ups=0.89, wpb=253.7, bsz=100, num_updates=730, lr=2.31305e-05, gnorm=1.33, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=864
2023-03-13 23:31:41 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 6313 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=256.6, nsentences=100, sample_size=256.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=221.1, ups=0.86, wpb=256.6, bsz=100, num_updates=740, lr=2.34474e-05, gnorm=1.325, clip=100, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=875
2023-03-13 23:31:52 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 6313 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=217.5, ups=0.86, wpb=252.8, bsz=100, num_updates=750, lr=2.37643e-05, gnorm=1.298, clip=90, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=887
2023-03-13 23:32:04 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.9, ups=0.86, wpb=254.9, bsz=100, num_updates=760, lr=2.40811e-05, gnorm=1.4, clip=100, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=898
2023-03-13 23:32:16 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=209.8, ups=0.83, wpb=252.9, bsz=100, num_updates=770, lr=2.4398e-05, gnorm=1.208, clip=100, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=910
2023-03-13 23:32:27 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 6313 loss=0.664, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=222.8, ups=0.88, wpb=253.7, bsz=100, num_updates=780, lr=2.47148e-05, gnorm=1.39, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=922
2023-03-13 23:32:39 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 6313 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=220, ups=0.87, wpb=254.3, bsz=100, num_updates=790, lr=2.50317e-05, gnorm=1.43, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=933
2023-03-13 23:32:51 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 6313 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=217.1, ups=0.86, wpb=251.9, bsz=100, num_updates=800, lr=2.53485e-05, gnorm=1.366, clip=90, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=945
2023-03-13 23:33:02 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 6313 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219, ups=0.87, wpb=250.3, bsz=100, num_updates=810, lr=2.56654e-05, gnorm=1.208, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=957
2023-03-13 23:33:14 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 6313 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=221.4, ups=0.87, wpb=253.6, bsz=100, num_updates=820, lr=2.59823e-05, gnorm=1.31, clip=90, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=968
2023-03-13 23:33:25 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 6313 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=216.4, ups=0.86, wpb=251.4, bsz=100, num_updates=830, lr=2.62991e-05, gnorm=1.159, clip=80, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=980
2023-03-13 23:33:37 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 6313 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=220.7, ups=0.87, wpb=254.1, bsz=100, num_updates=840, lr=2.6616e-05, gnorm=1.165, clip=70, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=992
2023-03-13 23:33:49 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 6313 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.3, ups=0.86, wpb=252.6, bsz=100, num_updates=850, lr=2.69328e-05, gnorm=1.199, clip=90, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1003
2023-03-13 23:34:00 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.4, ups=0.86, wpb=255.8, bsz=100, num_updates=860, lr=2.72497e-05, gnorm=1.205, clip=80, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1015
2023-03-13 23:34:12 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 6313 loss=0.677, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=249.4, nsentences=100, sample_size=249.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=215, ups=0.86, wpb=249.4, bsz=100, num_updates=870, lr=2.75665e-05, gnorm=1.282, clip=100, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1026
2023-03-13 23:34:23 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 6313 loss=0.655, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=216.6, ups=0.86, wpb=251.2, bsz=100, num_updates=880, lr=2.78834e-05, gnorm=1.236, clip=90, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1038
2023-03-13 23:34:35 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 6313 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=216.1, ups=0.85, wpb=255.5, bsz=100, num_updates=890, lr=2.82003e-05, gnorm=1.301, clip=90, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=1050
2023-03-13 23:34:47 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.7, ups=0.86, wpb=254.3, bsz=100, num_updates=900, lr=2.85171e-05, gnorm=1.127, clip=90, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1061
2023-03-13 23:34:59 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 6313 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=217.6, ups=0.86, wpb=252.3, bsz=100, num_updates=910, lr=2.8834e-05, gnorm=1.258, clip=90, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1073
2023-03-13 23:35:10 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 6313 loss=0.67, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=216.3, ups=0.86, wpb=251.2, bsz=100, num_updates=920, lr=2.91508e-05, gnorm=1.165, clip=60, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1085
2023-03-13 23:35:22 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.1, ups=0.86, wpb=253.9, bsz=100, num_updates=930, lr=2.94677e-05, gnorm=1.07, clip=60, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1096
2023-03-13 23:35:33 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 6313 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=249.5, nsentences=100, sample_size=249.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=215.3, ups=0.86, wpb=249.5, bsz=100, num_updates=940, lr=2.97845e-05, gnorm=1.189, clip=80, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1108
2023-03-13 23:35:45 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 6313 loss=0.667, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=216.9, ups=0.86, wpb=250.8, bsz=100, num_updates=950, lr=3.01014e-05, gnorm=1.164, clip=90, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1120
2023-03-13 23:35:57 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 6313 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.4, ups=0.87, wpb=253.6, bsz=100, num_updates=960, lr=3.04183e-05, gnorm=1.073, clip=70, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1131
2023-03-13 23:36:08 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 6313 loss=0.665, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=248.3, nsentences=100, sample_size=248.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=214, ups=0.86, wpb=248.3, bsz=100, num_updates=970, lr=3.07351e-05, gnorm=1.121, clip=70, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1143
2023-03-13 23:36:20 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 6313 loss=0.667, loss_v1=0, loss_v2=0, nll_loss=0.467, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=217.9, ups=0.86, wpb=252.3, bsz=100, num_updates=980, lr=3.1052e-05, gnorm=1.147, clip=80, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1154
2023-03-13 23:36:31 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 6313 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.1, ups=0.86, wpb=252.4, bsz=100, num_updates=990, lr=3.13688e-05, gnorm=0.998, clip=40, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1166
2023-03-13 23:36:43 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.1, ups=0.86, wpb=251.2, bsz=100, num_updates=1000, lr=3.16857e-05, gnorm=1.15, clip=90, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1177
2023-03-13 23:36:54 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 6313 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.9, ups=0.87, wpb=252.1, bsz=100, num_updates=1010, lr=3.20025e-05, gnorm=1.164, clip=70, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1189
2023-03-13 23:37:06 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 6313 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=218.6, ups=0.87, wpb=252.1, bsz=100, num_updates=1020, lr=3.23194e-05, gnorm=1.136, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1200
2023-03-13 23:37:18 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 6313 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=217.1, ups=0.86, wpb=251.4, bsz=100, num_updates=1030, lr=3.26362e-05, gnorm=1.182, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1212
2023-03-13 23:37:29 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 6313 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219.2, ups=0.86, wpb=253.9, bsz=100, num_updates=1040, lr=3.29531e-05, gnorm=1.006, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1224
2023-03-13 23:37:41 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 6313 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.9, ups=0.86, wpb=254.8, bsz=100, num_updates=1050, lr=3.327e-05, gnorm=0.972, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1235
2023-03-13 23:37:52 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 6313 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.1, ups=0.86, wpb=252.4, bsz=100, num_updates=1060, lr=3.35868e-05, gnorm=1.117, clip=70, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1247
2023-03-13 23:38:04 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 6313 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=249.3, nsentences=100, sample_size=249.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=216.1, ups=0.87, wpb=249.3, bsz=100, num_updates=1070, lr=3.39037e-05, gnorm=1.136, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1258
2023-03-13 23:38:15 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 6313 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=222.5, ups=0.88, wpb=254.2, bsz=100, num_updates=1080, lr=3.42205e-05, gnorm=1.185, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1270
2023-03-13 23:38:27 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=257.3, nsentences=100, sample_size=257.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=222, ups=0.86, wpb=257.3, bsz=100, num_updates=1090, lr=3.45374e-05, gnorm=1.266, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1281
2023-03-13 23:38:38 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 6313 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=220.4, ups=0.87, wpb=252.1, bsz=100, num_updates=1100, lr=3.48542e-05, gnorm=1.07, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1293
2023-03-13 23:38:50 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 6313 loss=0.644, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=221, ups=0.87, wpb=253.3, bsz=100, num_updates=1110, lr=3.51711e-05, gnorm=1.165, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1304
2023-03-13 23:39:01 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.4, ups=0.86, wpb=253.1, bsz=100, num_updates=1120, lr=3.5488e-05, gnorm=1.013, clip=30, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1316
2023-03-13 23:39:13 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.1, ups=0.86, wpb=254.7, bsz=100, num_updates=1130, lr=3.58048e-05, gnorm=1.111, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1328
2023-03-13 23:39:25 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 6313 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=217, ups=0.86, wpb=251.1, bsz=100, num_updates=1140, lr=3.61217e-05, gnorm=1.07, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1339
2023-03-13 23:39:36 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 6313 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=220.3, ups=0.87, wpb=252.7, bsz=100, num_updates=1150, lr=3.64385e-05, gnorm=1.047, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1351
2023-03-13 23:39:48 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 6313 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219.8, ups=0.86, wpb=254.6, bsz=100, num_updates=1160, lr=3.67554e-05, gnorm=1.137, clip=80, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1362
2023-03-13 23:39:59 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 6313 loss=0.662, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=219.7, ups=0.87, wpb=251.4, bsz=100, num_updates=1170, lr=3.70722e-05, gnorm=1.092, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1374
2023-03-13 23:40:11 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 6313 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.2, ups=0.86, wpb=255.1, bsz=100, num_updates=1180, lr=3.73891e-05, gnorm=1.206, clip=90, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1385
2023-03-13 23:40:22 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.1, ups=0.86, wpb=253.6, bsz=100, num_updates=1190, lr=3.7706e-05, gnorm=0.995, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1397
2023-03-13 23:40:34 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 6313 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219.3, ups=0.86, wpb=254, bsz=100, num_updates=1200, lr=3.80228e-05, gnorm=1.022, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1409
2023-03-13 23:40:46 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 6313 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=216.7, ups=0.86, wpb=250.8, bsz=100, num_updates=1210, lr=3.83397e-05, gnorm=1.026, clip=70, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1420
2023-03-13 23:40:57 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 6313 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.7, ups=0.86, wpb=253.7, bsz=100, num_updates=1220, lr=3.86565e-05, gnorm=0.997, clip=60, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1432
2023-03-13 23:41:09 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 6313 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.2, ups=0.86, wpb=253.4, bsz=100, num_updates=1230, lr=3.89734e-05, gnorm=0.963, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1443
2023-03-13 23:41:20 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=257.8, nsentences=100, sample_size=257.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=223.1, ups=0.87, wpb=257.8, bsz=100, num_updates=1240, lr=3.92902e-05, gnorm=0.99, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1455
2023-03-13 23:41:32 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 6313 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=216.7, ups=0.86, wpb=253, bsz=100, num_updates=1250, lr=3.96071e-05, gnorm=1.078, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1467
2023-03-13 23:41:44 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=213.4, ups=0.84, wpb=253.8, bsz=100, num_updates=1260, lr=3.9924e-05, gnorm=0.954, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1478
2023-03-13 23:41:56 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 6313 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.7, ups=0.87, wpb=253.9, bsz=100, num_updates=1270, lr=4.02408e-05, gnorm=1.078, clip=70, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1490
2023-03-13 23:42:07 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.7, ups=0.86, wpb=253.4, bsz=100, num_updates=1280, lr=4.05577e-05, gnorm=0.997, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1502
2023-03-13 23:42:19 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 6313 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=218.4, ups=0.86, wpb=252.8, bsz=100, num_updates=1290, lr=4.08745e-05, gnorm=1.09, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1513
2023-03-13 23:42:30 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 6313 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.6, ups=0.86, wpb=252.7, bsz=100, num_updates=1300, lr=4.11914e-05, gnorm=1.003, clip=50, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=1525
2023-03-13 23:42:42 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.7, ups=0.86, wpb=254.5, bsz=100, num_updates=1310, lr=4.15082e-05, gnorm=1.049, clip=50, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1536
2023-03-13 23:42:53 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 6313 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=218, ups=0.86, wpb=252.9, bsz=100, num_updates=1320, lr=4.18251e-05, gnorm=1.072, clip=80, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1548
2023-03-13 23:43:05 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 6313 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=221, ups=0.86, wpb=255.7, bsz=100, num_updates=1330, lr=4.2142e-05, gnorm=1.011, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1560
2023-03-13 23:43:16 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 6313 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=220.3, ups=0.88, wpb=251.7, bsz=100, num_updates=1340, lr=4.24588e-05, gnorm=0.966, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1571
2023-03-13 23:43:28 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 6313 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.2, ups=0.86, wpb=254.2, bsz=100, num_updates=1350, lr=4.27757e-05, gnorm=1.022, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1583
2023-03-13 23:43:40 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 6313 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=217.8, ups=0.86, wpb=252.4, bsz=100, num_updates=1360, lr=4.30925e-05, gnorm=1.045, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1594
2023-03-13 23:43:51 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 6313 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.4, ups=0.87, wpb=251.4, bsz=100, num_updates=1370, lr=4.34094e-05, gnorm=1.015, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1606
2023-03-13 23:44:03 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.6, ups=0.86, wpb=254.3, bsz=100, num_updates=1380, lr=4.37262e-05, gnorm=1.036, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1617
2023-03-13 23:44:14 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 6313 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=217.7, ups=0.86, wpb=251.9, bsz=100, num_updates=1390, lr=4.40431e-05, gnorm=0.912, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1629
2023-03-13 23:44:26 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 6313 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.8, ups=0.86, wpb=253.1, bsz=100, num_updates=1400, lr=4.43599e-05, gnorm=1.061, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1640
2023-03-13 23:44:37 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 6313 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=222.1, ups=0.87, wpb=254, bsz=100, num_updates=1410, lr=4.46768e-05, gnorm=0.84, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1652
2023-03-13 23:44:49 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=224.6, ups=0.88, wpb=255.9, bsz=100, num_updates=1420, lr=4.49937e-05, gnorm=0.871, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1663
2023-03-13 23:45:00 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 6313 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.5, ups=0.87, wpb=254.2, bsz=100, num_updates=1430, lr=4.53105e-05, gnorm=0.917, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=1675
2023-03-13 23:45:12 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 6313 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.6, ups=0.86, wpb=252.8, bsz=100, num_updates=1440, lr=4.56274e-05, gnorm=0.926, clip=30, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1686
2023-03-13 23:45:23 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 6313 loss=0.644, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=218.5, ups=0.86, wpb=253, bsz=100, num_updates=1450, lr=4.59442e-05, gnorm=0.981, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1698
2023-03-13 23:45:35 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.7, ups=0.86, wpb=255, bsz=100, num_updates=1460, lr=4.62611e-05, gnorm=0.971, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1710
2023-03-13 23:45:47 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 6313 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.4, ups=0.86, wpb=253.1, bsz=100, num_updates=1470, lr=4.65779e-05, gnorm=0.998, clip=30, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1721
2023-03-13 23:45:58 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 6313 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.1, ups=0.86, wpb=254.2, bsz=100, num_updates=1480, lr=4.68948e-05, gnorm=0.942, clip=30, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1733
2023-03-13 23:46:10 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 6313 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=222.6, ups=0.88, wpb=254.4, bsz=100, num_updates=1490, lr=4.72117e-05, gnorm=0.974, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1744
2023-03-13 23:46:22 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=214.7, ups=0.84, wpb=255.3, bsz=100, num_updates=1500, lr=4.75285e-05, gnorm=0.98, clip=30, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1756
2023-03-13 23:46:33 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=221.7, ups=0.87, wpb=253.5, bsz=100, num_updates=1510, lr=4.78454e-05, gnorm=1.054, clip=60, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1768
2023-03-13 23:46:45 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=221.7, ups=0.87, wpb=253.9, bsz=100, num_updates=1520, lr=4.81622e-05, gnorm=0.91, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1779
2023-03-13 23:46:56 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.9, ups=0.86, wpb=254, bsz=100, num_updates=1530, lr=4.84791e-05, gnorm=1.021, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1791
2023-03-13 23:47:07 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-13 23:47:09 - progress_bar.py[line:274] - INFO: epoch 001:   1541 / 6313 loss=0.665, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=201.2, ups=0.8, wpb=250.5, bsz=100, num_updates=1540, lr=4.87959e-05, gnorm=0.952, clip=30, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=1803
2023-03-13 23:47:20 - progress_bar.py[line:274] - INFO: epoch 001:   1551 / 6313 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=221.6, ups=0.88, wpb=252.5, bsz=100, num_updates=1550, lr=4.91128e-05, gnorm=0.88, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1815
2023-03-13 23:47:32 - progress_bar.py[line:274] - INFO: epoch 001:   1561 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.9, ups=0.86, wpb=252.8, bsz=100, num_updates=1560, lr=4.94297e-05, gnorm=0.799, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1826
2023-03-13 23:47:43 - progress_bar.py[line:274] - INFO: epoch 001:   1571 / 6313 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.9, ups=0.86, wpb=252.7, bsz=100, num_updates=1570, lr=4.97465e-05, gnorm=0.995, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1838
2023-03-13 23:47:55 - progress_bar.py[line:274] - INFO: epoch 001:   1581 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.7, ups=0.87, wpb=253.8, bsz=100, num_updates=1580, lr=4.99967e-05, gnorm=0.884, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1849
2023-03-13 23:48:06 - progress_bar.py[line:274] - INFO: epoch 001:   1591 / 6313 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.9, ups=0.86, wpb=252.5, bsz=100, num_updates=1590, lr=4.998e-05, gnorm=0.851, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1861
2023-03-13 23:48:18 - progress_bar.py[line:274] - INFO: epoch 001:   1601 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.8, ups=0.86, wpb=254.8, bsz=100, num_updates=1600, lr=4.99633e-05, gnorm=0.922, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1873
2023-03-13 23:48:30 - progress_bar.py[line:274] - INFO: epoch 001:   1611 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.9, ups=0.86, wpb=255.2, bsz=100, num_updates=1610, lr=4.99466e-05, gnorm=0.866, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1884
2023-03-13 23:48:41 - progress_bar.py[line:274] - INFO: epoch 001:   1621 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.4, ups=0.85, wpb=254.2, bsz=100, num_updates=1620, lr=4.993e-05, gnorm=0.903, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1896
2023-03-13 23:48:53 - progress_bar.py[line:274] - INFO: epoch 001:   1631 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.6, ups=0.86, wpb=251.8, bsz=100, num_updates=1630, lr=4.99133e-05, gnorm=1.014, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1908
2023-03-13 23:49:05 - progress_bar.py[line:274] - INFO: epoch 001:   1641 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.4, ups=0.86, wpb=253.9, bsz=100, num_updates=1640, lr=4.98966e-05, gnorm=0.932, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1919
2023-03-13 23:49:16 - progress_bar.py[line:274] - INFO: epoch 001:   1651 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.9, ups=0.86, wpb=255.2, bsz=100, num_updates=1650, lr=4.98799e-05, gnorm=0.841, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1931
2023-03-13 23:49:28 - progress_bar.py[line:274] - INFO: epoch 001:   1661 / 6313 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=222.4, ups=0.87, wpb=254.4, bsz=100, num_updates=1660, lr=4.98633e-05, gnorm=0.851, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1942
2023-03-13 23:49:39 - progress_bar.py[line:274] - INFO: epoch 001:   1671 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.5, ups=0.86, wpb=253.9, bsz=100, num_updates=1670, lr=4.98466e-05, gnorm=0.794, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1954
2023-03-13 23:49:50 - progress_bar.py[line:274] - INFO: epoch 001:   1681 / 6313 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=222, ups=0.89, wpb=250.1, bsz=100, num_updates=1680, lr=4.98299e-05, gnorm=1.039, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1965
2023-03-13 23:50:02 - progress_bar.py[line:274] - INFO: epoch 001:   1691 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=221.5, ups=0.87, wpb=253.5, bsz=100, num_updates=1690, lr=4.98133e-05, gnorm=0.914, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1976
2023-03-13 23:50:13 - progress_bar.py[line:274] - INFO: epoch 001:   1701 / 6313 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=216.9, ups=0.87, wpb=250.5, bsz=100, num_updates=1700, lr=4.97966e-05, gnorm=0.846, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=1988
2023-03-13 23:50:25 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 6313 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.1, ups=0.86, wpb=254.2, bsz=100, num_updates=1710, lr=4.97799e-05, gnorm=0.902, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2000
2023-03-13 23:50:37 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 6313 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=221, ups=0.87, wpb=253.4, bsz=100, num_updates=1720, lr=4.97632e-05, gnorm=0.883, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2011
2023-03-13 23:50:48 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 6313 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.7, ups=0.86, wpb=253.3, bsz=100, num_updates=1730, lr=4.97466e-05, gnorm=0.977, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2023
2023-03-13 23:51:00 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=215.3, ups=0.85, wpb=252.5, bsz=100, num_updates=1740, lr=4.97299e-05, gnorm=0.919, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2034
2023-03-13 23:51:11 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.7, ups=0.87, wpb=251.3, bsz=100, num_updates=1750, lr=4.97132e-05, gnorm=0.833, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2046
2023-03-13 23:51:23 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 6313 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.4, ups=0.86, wpb=253.6, bsz=100, num_updates=1760, lr=4.96965e-05, gnorm=0.87, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2057
2023-03-13 23:51:34 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.2, ups=0.87, wpb=252.7, bsz=100, num_updates=1770, lr=4.96799e-05, gnorm=0.916, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2069
2023-03-13 23:51:46 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.4, ups=0.86, wpb=254.7, bsz=100, num_updates=1780, lr=4.96632e-05, gnorm=0.98, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2081
2023-03-13 23:51:58 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=223.4, ups=0.87, wpb=255.5, bsz=100, num_updates=1790, lr=4.96465e-05, gnorm=0.873, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2092
2023-03-13 23:52:09 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=222.9, ups=0.87, wpb=255, bsz=100, num_updates=1800, lr=4.96298e-05, gnorm=0.799, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2104
2023-03-13 23:52:20 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=222.7, ups=0.88, wpb=254.3, bsz=100, num_updates=1810, lr=4.96132e-05, gnorm=0.887, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2115
2023-03-13 23:52:32 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 6313 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.7, ups=0.86, wpb=250.9, bsz=100, num_updates=1820, lr=4.95965e-05, gnorm=0.927, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2127
2023-03-13 23:52:44 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.5, ups=0.86, wpb=253.6, bsz=100, num_updates=1830, lr=4.95798e-05, gnorm=0.792, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2138
2023-03-13 23:52:55 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.9, ups=0.87, wpb=253.7, bsz=100, num_updates=1840, lr=4.95631e-05, gnorm=0.83, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2150
2023-03-13 23:53:07 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.7, ups=0.86, wpb=255.7, bsz=100, num_updates=1850, lr=4.95465e-05, gnorm=0.833, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=2161
2023-03-13 23:53:19 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=210, ups=0.83, wpb=253.8, bsz=100, num_updates=1860, lr=4.95298e-05, gnorm=0.873, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2173
2023-03-13 23:53:30 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 6313 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.5, ups=0.86, wpb=252.2, bsz=100, num_updates=1870, lr=4.95131e-05, gnorm=0.949, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2185
2023-03-13 23:53:42 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.9, ups=0.86, wpb=254.5, bsz=100, num_updates=1880, lr=4.94964e-05, gnorm=1.034, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2197
2023-03-13 23:53:53 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=223.6, ups=0.88, wpb=255.2, bsz=100, num_updates=1890, lr=4.94798e-05, gnorm=0.837, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2208
2023-03-13 23:54:05 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.1, ups=0.86, wpb=254.3, bsz=100, num_updates=1900, lr=4.94631e-05, gnorm=0.789, clip=0, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=2220
2023-03-13 23:54:17 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 6313 loss=0.654, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=210.9, ups=0.84, wpb=251.5, bsz=100, num_updates=1910, lr=4.94464e-05, gnorm=0.885, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2232
2023-03-13 23:54:29 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.6, ups=0.86, wpb=253.1, bsz=100, num_updates=1920, lr=4.94298e-05, gnorm=0.786, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2243
2023-03-13 23:54:40 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.6, ups=0.86, wpb=254.4, bsz=100, num_updates=1930, lr=4.94131e-05, gnorm=0.897, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2255
2023-03-13 23:54:52 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.9, ups=0.86, wpb=253.3, bsz=100, num_updates=1940, lr=4.93964e-05, gnorm=0.877, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2266
2023-03-13 23:55:03 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.2, ups=0.86, wpb=253, bsz=100, num_updates=1950, lr=4.93797e-05, gnorm=0.767, clip=10, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=2278
2023-03-13 23:55:15 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.6, ups=0.86, wpb=251.9, bsz=100, num_updates=1960, lr=4.93631e-05, gnorm=0.762, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2290
2023-03-13 23:55:27 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.5, ups=0.86, wpb=253.1, bsz=100, num_updates=1970, lr=4.93464e-05, gnorm=0.78, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2301
2023-03-13 23:55:38 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 6313 loss=0.668, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=214, ups=0.85, wpb=250.7, bsz=100, num_updates=1980, lr=4.93297e-05, gnorm=0.904, clip=30, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2313
2023-03-13 23:55:50 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.6, ups=0.86, wpb=253.8, bsz=100, num_updates=1990, lr=4.9313e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2324
2023-03-13 23:56:01 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 6313 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.1, ups=0.87, wpb=253, bsz=100, num_updates=2000, lr=4.92964e-05, gnorm=0.844, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2336
2023-03-13 23:56:13 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.6, ups=0.86, wpb=254.3, bsz=100, num_updates=2010, lr=4.92797e-05, gnorm=0.728, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2348
2023-03-13 23:56:25 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.8, ups=0.86, wpb=252.5, bsz=100, num_updates=2020, lr=4.9263e-05, gnorm=0.88, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2359
2023-03-13 23:56:36 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.9, ups=0.86, wpb=256.2, bsz=100, num_updates=2030, lr=4.92463e-05, gnorm=0.928, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2371
2023-03-13 23:56:48 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 6313 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=217.3, ups=0.86, wpb=252.5, bsz=100, num_updates=2040, lr=4.92297e-05, gnorm=0.844, clip=30, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=2382
2023-03-13 23:56:59 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.5, ups=0.86, wpb=253.8, bsz=100, num_updates=2050, lr=4.9213e-05, gnorm=0.832, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2394
2023-03-13 23:57:03 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-13 23:57:12 - progress_bar.py[line:274] - INFO: epoch 001:   2062 / 6313 loss=0.65, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=249.6, nsentences=100, sample_size=249.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=202.3, ups=0.81, wpb=249.6, bsz=100, num_updates=2060, lr=4.91963e-05, gnorm=0.974, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2406
2023-03-13 23:57:23 - progress_bar.py[line:274] - INFO: epoch 001:   2072 / 6313 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=222.4, ups=0.87, wpb=254.5, bsz=100, num_updates=2070, lr=4.91796e-05, gnorm=0.859, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2418
2023-03-13 23:57:35 - progress_bar.py[line:274] - INFO: epoch 001:   2082 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=222.8, ups=0.87, wpb=255.5, bsz=100, num_updates=2080, lr=4.9163e-05, gnorm=0.884, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2429
2023-03-13 23:57:46 - progress_bar.py[line:274] - INFO: epoch 001:   2092 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.3, ups=0.86, wpb=253.8, bsz=100, num_updates=2090, lr=4.91463e-05, gnorm=0.754, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2441
2023-03-13 23:57:58 - progress_bar.py[line:274] - INFO: epoch 001:   2102 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=214.8, ups=0.85, wpb=252.3, bsz=100, num_updates=2100, lr=4.91296e-05, gnorm=0.843, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=2453
2023-03-13 23:58:10 - progress_bar.py[line:274] - INFO: epoch 001:   2112 / 6313 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=221.3, ups=0.87, wpb=253.4, bsz=100, num_updates=2110, lr=4.91129e-05, gnorm=1.006, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2464
2023-03-13 23:58:21 - progress_bar.py[line:274] - INFO: epoch 001:   2122 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.6, ups=0.87, wpb=253.8, bsz=100, num_updates=2120, lr=4.90963e-05, gnorm=0.957, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2476
2023-03-13 23:58:33 - progress_bar.py[line:274] - INFO: epoch 001:   2132 / 6313 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.7, ups=0.87, wpb=253.8, bsz=100, num_updates=2130, lr=4.90796e-05, gnorm=0.802, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2487
2023-03-13 23:58:44 - progress_bar.py[line:274] - INFO: epoch 001:   2142 / 6313 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=217.3, ups=0.86, wpb=252.4, bsz=100, num_updates=2140, lr=4.90629e-05, gnorm=0.847, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2499
2023-03-13 23:58:56 - progress_bar.py[line:274] - INFO: epoch 001:   2152 / 6313 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=220.7, ups=0.88, wpb=252.1, bsz=100, num_updates=2150, lr=4.90463e-05, gnorm=0.776, clip=10, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=2510
2023-03-13 23:59:07 - progress_bar.py[line:274] - INFO: epoch 001:   2162 / 6313 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=220.9, ups=0.86, wpb=255.4, bsz=100, num_updates=2160, lr=4.90296e-05, gnorm=0.829, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2522
2023-03-13 23:59:19 - progress_bar.py[line:274] - INFO: epoch 001:   2172 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=221.6, ups=0.88, wpb=253.3, bsz=100, num_updates=2170, lr=4.90129e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2533
2023-03-13 23:59:30 - progress_bar.py[line:274] - INFO: epoch 001:   2182 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=223, ups=0.87, wpb=255.6, bsz=100, num_updates=2180, lr=4.89962e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2545
2023-03-13 23:59:42 - progress_bar.py[line:274] - INFO: epoch 001:   2192 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.1, ups=0.86, wpb=251.2, bsz=100, num_updates=2190, lr=4.89796e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=2556
2023-03-13 23:59:53 - progress_bar.py[line:274] - INFO: epoch 001:   2202 / 6313 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.9, ups=0.88, wpb=252.2, bsz=100, num_updates=2200, lr=4.89629e-05, gnorm=0.772, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2568
2023-03-14 00:00:05 - progress_bar.py[line:274] - INFO: epoch 001:   2212 / 6313 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=216.4, ups=0.87, wpb=250.2, bsz=100, num_updates=2210, lr=4.89462e-05, gnorm=0.876, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2579
2023-03-14 00:00:17 - progress_bar.py[line:274] - INFO: epoch 001:   2222 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=216.2, ups=0.85, wpb=254, bsz=100, num_updates=2220, lr=4.89295e-05, gnorm=0.79, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2591
2023-03-14 00:00:28 - progress_bar.py[line:274] - INFO: epoch 001:   2232 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.6, ups=0.85, wpb=252.5, bsz=100, num_updates=2230, lr=4.89129e-05, gnorm=0.775, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2603
2023-03-14 00:00:40 - progress_bar.py[line:274] - INFO: epoch 001:   2242 / 6313 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.3, ups=0.87, wpb=252.3, bsz=100, num_updates=2240, lr=4.88962e-05, gnorm=0.854, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2615
2023-03-14 00:00:51 - progress_bar.py[line:274] - INFO: epoch 001:   2252 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.4, ups=0.87, wpb=250.5, bsz=100, num_updates=2250, lr=4.88795e-05, gnorm=0.926, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2626
2023-03-14 00:01:03 - progress_bar.py[line:274] - INFO: epoch 001:   2262 / 6313 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=216.4, ups=0.86, wpb=251.4, bsz=100, num_updates=2260, lr=4.88628e-05, gnorm=0.79, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2638
2023-03-14 00:01:15 - progress_bar.py[line:274] - INFO: epoch 001:   2272 / 6313 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=215.8, ups=0.86, wpb=250.2, bsz=100, num_updates=2270, lr=4.88462e-05, gnorm=0.915, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2649
2023-03-14 00:01:26 - progress_bar.py[line:274] - INFO: epoch 001:   2282 / 6313 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.8, ups=0.86, wpb=252.6, bsz=100, num_updates=2280, lr=4.88295e-05, gnorm=0.864, clip=20, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=2661
2023-03-14 00:01:38 - progress_bar.py[line:274] - INFO: epoch 001:   2292 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.3, ups=0.86, wpb=255.4, bsz=100, num_updates=2290, lr=4.88128e-05, gnorm=0.763, clip=10, loss_scale=512, train_wall=12, gb_free=11.2, ema_decay=0.9999, wall=2672
2023-03-14 00:01:50 - progress_bar.py[line:274] - INFO: epoch 001:   2302 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=256.7, nsentences=100, sample_size=256.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.6, ups=0.86, wpb=256.7, bsz=100, num_updates=2300, lr=4.87961e-05, gnorm=0.795, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=2684
2023-03-14 00:02:01 - progress_bar.py[line:274] - INFO: epoch 001:   2312 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.9, ups=0.86, wpb=254.5, bsz=100, num_updates=2310, lr=4.87795e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=2696
2023-03-14 00:02:13 - progress_bar.py[line:274] - INFO: epoch 001:   2322 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.6, ups=0.86, wpb=253.9, bsz=100, num_updates=2320, lr=4.87628e-05, gnorm=0.747, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2707
2023-03-14 00:02:24 - progress_bar.py[line:274] - INFO: epoch 001:   2332 / 6313 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.7, ups=0.87, wpb=252.6, bsz=100, num_updates=2330, lr=4.87461e-05, gnorm=0.807, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2719
2023-03-14 00:02:36 - progress_bar.py[line:274] - INFO: epoch 001:   2342 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=214.2, ups=0.85, wpb=252.5, bsz=100, num_updates=2340, lr=4.87294e-05, gnorm=0.863, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2731
2023-03-14 00:02:48 - progress_bar.py[line:274] - INFO: epoch 001:   2352 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=214.4, ups=0.85, wpb=252.9, bsz=100, num_updates=2350, lr=4.87128e-05, gnorm=0.888, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2743
2023-03-14 00:03:00 - progress_bar.py[line:274] - INFO: epoch 001:   2362 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.9, ups=0.86, wpb=254.8, bsz=100, num_updates=2360, lr=4.86961e-05, gnorm=0.843, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2754
2023-03-14 00:03:11 - progress_bar.py[line:274] - INFO: epoch 001:   2372 / 6313 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.2, ups=0.86, wpb=251.6, bsz=100, num_updates=2370, lr=4.86794e-05, gnorm=0.87, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2766
2023-03-14 00:03:23 - progress_bar.py[line:274] - INFO: epoch 001:   2382 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.8, ups=0.86, wpb=255.6, bsz=100, num_updates=2380, lr=4.86628e-05, gnorm=0.876, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2777
2023-03-14 00:03:35 - progress_bar.py[line:274] - INFO: epoch 001:   2392 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=256.8, nsentences=100, sample_size=256.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.5, ups=0.86, wpb=256.8, bsz=100, num_updates=2390, lr=4.86461e-05, gnorm=0.715, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2789
2023-03-14 00:03:46 - progress_bar.py[line:274] - INFO: epoch 001:   2402 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.5, ups=0.86, wpb=256.4, bsz=100, num_updates=2400, lr=4.86294e-05, gnorm=0.726, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2801
2023-03-14 00:03:58 - progress_bar.py[line:274] - INFO: epoch 001:   2412 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.9, ups=0.86, wpb=253.4, bsz=100, num_updates=2410, lr=4.86127e-05, gnorm=0.764, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=2812
2023-03-14 00:04:09 - progress_bar.py[line:274] - INFO: epoch 001:   2422 / 6313 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.1, ups=0.86, wpb=251.2, bsz=100, num_updates=2420, lr=4.85961e-05, gnorm=0.781, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2824
2023-03-14 00:04:21 - progress_bar.py[line:274] - INFO: epoch 001:   2432 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.3, ups=0.86, wpb=250.8, bsz=100, num_updates=2430, lr=4.85794e-05, gnorm=0.786, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2835
2023-03-14 00:04:33 - progress_bar.py[line:274] - INFO: epoch 001:   2442 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.7, ups=0.86, wpb=251.3, bsz=100, num_updates=2440, lr=4.85627e-05, gnorm=0.804, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=2847
2023-03-14 00:04:44 - progress_bar.py[line:274] - INFO: epoch 001:   2452 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=223.6, ups=0.88, wpb=254.1, bsz=100, num_updates=2450, lr=4.8546e-05, gnorm=0.762, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2859
2023-03-14 00:04:56 - progress_bar.py[line:274] - INFO: epoch 001:   2462 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=214.4, ups=0.85, wpb=252.6, bsz=100, num_updates=2460, lr=4.85294e-05, gnorm=0.886, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2870
2023-03-14 00:05:07 - progress_bar.py[line:274] - INFO: epoch 001:   2472 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.5, ups=0.86, wpb=252.6, bsz=100, num_updates=2470, lr=4.85127e-05, gnorm=0.779, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2882
2023-03-14 00:05:19 - progress_bar.py[line:274] - INFO: epoch 001:   2482 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=221.2, ups=0.87, wpb=254, bsz=100, num_updates=2480, lr=4.8496e-05, gnorm=0.903, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2894
2023-03-14 00:05:31 - progress_bar.py[line:274] - INFO: epoch 001:   2492 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218, ups=0.86, wpb=253.3, bsz=100, num_updates=2490, lr=4.84793e-05, gnorm=0.862, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2905
2023-03-14 00:05:42 - progress_bar.py[line:274] - INFO: epoch 001:   2502 / 6313 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.9, ups=0.86, wpb=252.5, bsz=100, num_updates=2500, lr=4.84627e-05, gnorm=0.763, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2917
2023-03-14 00:05:54 - progress_bar.py[line:274] - INFO: epoch 001:   2512 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.4, ups=0.86, wpb=255.9, bsz=100, num_updates=2510, lr=4.8446e-05, gnorm=0.732, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2928
2023-03-14 00:06:05 - progress_bar.py[line:274] - INFO: epoch 001:   2522 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=224.3, ups=0.88, wpb=255.9, bsz=100, num_updates=2520, lr=4.84293e-05, gnorm=0.823, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=2940
2023-03-14 00:06:17 - progress_bar.py[line:274] - INFO: epoch 001:   2532 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221, ups=0.87, wpb=253.1, bsz=100, num_updates=2530, lr=4.84126e-05, gnorm=0.795, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2951
2023-03-14 00:06:28 - progress_bar.py[line:274] - INFO: epoch 001:   2542 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219, ups=0.86, wpb=254, bsz=100, num_updates=2540, lr=4.8396e-05, gnorm=0.852, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2963
2023-03-14 00:06:40 - progress_bar.py[line:274] - INFO: epoch 001:   2552 / 6313 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=216.9, ups=0.86, wpb=252.8, bsz=100, num_updates=2550, lr=4.83793e-05, gnorm=0.842, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2974
2023-03-14 00:06:52 - progress_bar.py[line:274] - INFO: epoch 001:   2562 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218, ups=0.86, wpb=252.4, bsz=100, num_updates=2560, lr=4.83626e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2986
2023-03-14 00:06:57 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 00:07:04 - progress_bar.py[line:274] - INFO: epoch 001:   2573 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=198.8, ups=0.79, wpb=251.5, bsz=100, num_updates=2570, lr=4.83459e-05, gnorm=0.84, clip=0, loss_scale=512, train_wall=13, gb_free=10.8, ema_decay=0.9999, wall=2999
2023-03-14 00:07:16 - progress_bar.py[line:274] - INFO: epoch 001:   2583 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.1, ups=0.85, wpb=253.6, bsz=100, num_updates=2580, lr=4.83293e-05, gnorm=0.781, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=3011
2023-03-14 00:07:23 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-03-14 00:07:29 - progress_bar.py[line:274] - INFO: epoch 001:   2594 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=198, ups=0.78, wpb=254.2, bsz=100, num_updates=2590, lr=4.83126e-05, gnorm=0.764, clip=10, loss_scale=256, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=3024
2023-03-14 00:07:41 - progress_bar.py[line:274] - INFO: epoch 001:   2604 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.3, ups=0.86, wpb=253, bsz=100, num_updates=2600, lr=4.82959e-05, gnorm=0.751, clip=0, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=3035
2023-03-14 00:07:52 - progress_bar.py[line:274] - INFO: epoch 001:   2614 / 6313 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=216.7, ups=0.86, wpb=251.8, bsz=100, num_updates=2610, lr=4.82793e-05, gnorm=0.845, clip=20, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3047
2023-03-14 00:08:04 - progress_bar.py[line:274] - INFO: epoch 001:   2624 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.9, ups=0.86, wpb=256.2, bsz=100, num_updates=2620, lr=4.82626e-05, gnorm=0.87, clip=20, loss_scale=256, train_wall=12, gb_free=11, ema_decay=0.9999, wall=3058
2023-03-14 00:08:16 - progress_bar.py[line:274] - INFO: epoch 001:   2634 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.8, ups=0.86, wpb=255.2, bsz=100, num_updates=2630, lr=4.82459e-05, gnorm=0.857, clip=10, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=3070
2023-03-14 00:08:27 - progress_bar.py[line:274] - INFO: epoch 001:   2644 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=224.3, ups=0.88, wpb=253.4, bsz=100, num_updates=2640, lr=4.82292e-05, gnorm=0.838, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3081
2023-03-14 00:08:38 - progress_bar.py[line:274] - INFO: epoch 001:   2654 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.2, ups=0.86, wpb=256.1, bsz=100, num_updates=2650, lr=4.82126e-05, gnorm=0.833, clip=10, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3093
2023-03-14 00:08:50 - progress_bar.py[line:274] - INFO: epoch 001:   2664 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.7, ups=0.86, wpb=252.2, bsz=100, num_updates=2660, lr=4.81959e-05, gnorm=0.846, clip=0, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=3105
2023-03-14 00:09:02 - progress_bar.py[line:274] - INFO: epoch 001:   2674 / 6313 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=216.2, ups=0.86, wpb=251.1, bsz=100, num_updates=2670, lr=4.81792e-05, gnorm=0.853, clip=20, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3116
2023-03-14 00:09:13 - progress_bar.py[line:274] - INFO: epoch 001:   2684 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=224.1, ups=0.88, wpb=254, bsz=100, num_updates=2680, lr=4.81625e-05, gnorm=0.788, clip=10, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=3128
2023-03-14 00:09:25 - progress_bar.py[line:274] - INFO: epoch 001:   2694 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.1, ups=0.86, wpb=252.6, bsz=100, num_updates=2690, lr=4.81459e-05, gnorm=0.811, clip=20, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3139
2023-03-14 00:09:36 - progress_bar.py[line:274] - INFO: epoch 001:   2704 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=257.9, nsentences=100, sample_size=257.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.3, ups=0.86, wpb=257.9, bsz=100, num_updates=2700, lr=4.81292e-05, gnorm=0.864, clip=20, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3151
2023-03-14 00:09:48 - progress_bar.py[line:274] - INFO: epoch 001:   2714 / 6313 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.9, ups=0.86, wpb=252.2, bsz=100, num_updates=2710, lr=4.81125e-05, gnorm=0.798, clip=10, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3162
2023-03-14 00:09:59 - progress_bar.py[line:274] - INFO: epoch 001:   2724 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=223.2, ups=0.88, wpb=252.4, bsz=100, num_updates=2720, lr=4.80958e-05, gnorm=0.822, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=3174
2023-03-14 00:10:11 - progress_bar.py[line:274] - INFO: epoch 001:   2734 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.5, ups=0.86, wpb=252.7, bsz=100, num_updates=2730, lr=4.80792e-05, gnorm=0.796, clip=0, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3185
2023-03-14 00:10:22 - progress_bar.py[line:274] - INFO: epoch 001:   2744 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220, ups=0.87, wpb=252, bsz=100, num_updates=2740, lr=4.80625e-05, gnorm=0.716, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3197
2023-03-14 00:10:34 - progress_bar.py[line:274] - INFO: epoch 001:   2754 / 6313 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.4, ups=0.87, wpb=252, bsz=100, num_updates=2750, lr=4.80458e-05, gnorm=0.79, clip=10, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3208
2023-03-14 00:10:45 - progress_bar.py[line:274] - INFO: epoch 001:   2764 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.9, ups=0.86, wpb=252.2, bsz=100, num_updates=2760, lr=4.80291e-05, gnorm=0.827, clip=0, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=3220
2023-03-14 00:10:57 - progress_bar.py[line:274] - INFO: epoch 001:   2774 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.5, ups=0.86, wpb=251.9, bsz=100, num_updates=2770, lr=4.80125e-05, gnorm=0.81, clip=20, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3231
2023-03-14 00:11:09 - progress_bar.py[line:274] - INFO: epoch 001:   2784 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.9, ups=0.86, wpb=253, bsz=100, num_updates=2780, lr=4.79958e-05, gnorm=0.745, clip=10, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3243
2023-03-14 00:11:20 - progress_bar.py[line:274] - INFO: epoch 001:   2794 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.4, ups=0.87, wpb=251.4, bsz=100, num_updates=2790, lr=4.79791e-05, gnorm=0.887, clip=10, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3255
2023-03-14 00:11:31 - progress_bar.py[line:274] - INFO: epoch 001:   2804 / 6313 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=223.8, ups=0.88, wpb=253, bsz=100, num_updates=2800, lr=4.79625e-05, gnorm=0.955, clip=40, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3266
2023-03-14 00:11:43 - progress_bar.py[line:274] - INFO: epoch 001:   2814 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.1, ups=0.86, wpb=254, bsz=100, num_updates=2810, lr=4.79458e-05, gnorm=0.818, clip=10, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=3278
2023-03-14 00:11:55 - progress_bar.py[line:274] - INFO: epoch 001:   2824 / 6313 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.2, ups=0.86, wpb=252.6, bsz=100, num_updates=2820, lr=4.79291e-05, gnorm=0.78, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3289
2023-03-14 00:12:06 - progress_bar.py[line:274] - INFO: epoch 001:   2834 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221.2, ups=0.87, wpb=254.2, bsz=100, num_updates=2830, lr=4.79124e-05, gnorm=0.899, clip=20, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=3301
2023-03-14 00:12:18 - progress_bar.py[line:274] - INFO: epoch 001:   2844 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.5, ups=0.87, wpb=253.2, bsz=100, num_updates=2840, lr=4.78958e-05, gnorm=0.766, clip=10, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3312
2023-03-14 00:12:29 - progress_bar.py[line:274] - INFO: epoch 001:   2854 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.6, ups=0.86, wpb=255.8, bsz=100, num_updates=2850, lr=4.78791e-05, gnorm=0.771, clip=20, loss_scale=256, train_wall=12, gb_free=10, ema_decay=0.9999, wall=3324
2023-03-14 00:12:41 - progress_bar.py[line:274] - INFO: epoch 001:   2864 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.1, ups=0.86, wpb=254.1, bsz=100, num_updates=2860, lr=4.78624e-05, gnorm=0.777, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3336
2023-03-14 00:12:53 - progress_bar.py[line:274] - INFO: epoch 001:   2874 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.7, ups=0.86, wpb=254.4, bsz=100, num_updates=2870, lr=4.78457e-05, gnorm=0.9, clip=30, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3347
2023-03-14 00:13:04 - progress_bar.py[line:274] - INFO: epoch 001:   2884 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.7, ups=0.86, wpb=253.4, bsz=100, num_updates=2880, lr=4.78291e-05, gnorm=0.898, clip=40, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=3359
2023-03-14 00:13:16 - progress_bar.py[line:274] - INFO: epoch 001:   2894 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.6, ups=0.86, wpb=253.8, bsz=100, num_updates=2890, lr=4.78124e-05, gnorm=0.775, clip=10, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=3370
2023-03-14 00:13:27 - progress_bar.py[line:274] - INFO: epoch 001:   2904 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=224.1, ups=0.88, wpb=255.8, bsz=100, num_updates=2900, lr=4.77957e-05, gnorm=0.819, clip=10, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3382
2023-03-14 00:13:39 - progress_bar.py[line:274] - INFO: epoch 001:   2914 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.1, ups=0.86, wpb=252.9, bsz=100, num_updates=2910, lr=4.7779e-05, gnorm=0.834, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3393
2023-03-14 00:13:50 - progress_bar.py[line:274] - INFO: epoch 001:   2924 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.8, ups=0.86, wpb=253.2, bsz=100, num_updates=2920, lr=4.77624e-05, gnorm=0.79, clip=0, loss_scale=256, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=3405
2023-03-14 00:14:02 - progress_bar.py[line:274] - INFO: epoch 001:   2934 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=221.3, ups=0.88, wpb=252.4, bsz=100, num_updates=2930, lr=4.77457e-05, gnorm=0.939, clip=30, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3416
2023-03-14 00:14:13 - progress_bar.py[line:274] - INFO: epoch 001:   2944 / 6313 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.1, ups=0.87, wpb=252.7, bsz=100, num_updates=2940, lr=4.7729e-05, gnorm=0.836, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3428
2023-03-14 00:14:25 - progress_bar.py[line:274] - INFO: epoch 001:   2954 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=213.8, ups=0.85, wpb=251.2, bsz=100, num_updates=2950, lr=4.77123e-05, gnorm=0.726, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3440
2023-03-14 00:14:37 - progress_bar.py[line:274] - INFO: epoch 001:   2964 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.2, ups=0.86, wpb=252.9, bsz=100, num_updates=2960, lr=4.76957e-05, gnorm=0.687, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3451
2023-03-14 00:14:48 - progress_bar.py[line:274] - INFO: epoch 001:   2974 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=249.4, nsentences=100, sample_size=249.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=215.5, ups=0.86, wpb=249.4, bsz=100, num_updates=2970, lr=4.7679e-05, gnorm=0.852, clip=0, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3463
2023-03-14 00:15:00 - progress_bar.py[line:274] - INFO: epoch 001:   2984 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.2, ups=0.86, wpb=254.2, bsz=100, num_updates=2980, lr=4.76623e-05, gnorm=0.763, clip=10, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3474
2023-03-14 00:15:11 - progress_bar.py[line:274] - INFO: epoch 001:   2994 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.6, ups=0.86, wpb=253.7, bsz=100, num_updates=2990, lr=4.76456e-05, gnorm=0.772, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3486
2023-03-14 00:15:23 - progress_bar.py[line:274] - INFO: epoch 001:   3004 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.7, ups=0.87, wpb=251.7, bsz=100, num_updates=3000, lr=4.7629e-05, gnorm=0.816, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3497
2023-03-14 00:15:34 - progress_bar.py[line:274] - INFO: epoch 001:   3014 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223.4, ups=0.87, wpb=256.1, bsz=100, num_updates=3010, lr=4.76123e-05, gnorm=0.817, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3509
2023-03-14 00:15:46 - progress_bar.py[line:274] - INFO: epoch 001:   3024 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.2, ups=0.87, wpb=253.1, bsz=100, num_updates=3020, lr=4.75956e-05, gnorm=0.799, clip=20, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3521
2023-03-14 00:15:58 - progress_bar.py[line:274] - INFO: epoch 001:   3034 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.7, ups=0.86, wpb=255, bsz=100, num_updates=3030, lr=4.7579e-05, gnorm=0.782, clip=10, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3532
2023-03-14 00:16:09 - progress_bar.py[line:274] - INFO: epoch 001:   3044 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.5, ups=0.86, wpb=252.9, bsz=100, num_updates=3040, lr=4.75623e-05, gnorm=0.874, clip=10, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3544
2023-03-14 00:16:21 - progress_bar.py[line:274] - INFO: epoch 001:   3054 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.2, ups=0.86, wpb=251.4, bsz=100, num_updates=3050, lr=4.75456e-05, gnorm=0.778, clip=10, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3555
2023-03-14 00:16:33 - progress_bar.py[line:274] - INFO: epoch 001:   3064 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.7, ups=0.86, wpb=253.3, bsz=100, num_updates=3060, lr=4.75289e-05, gnorm=0.868, clip=30, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3567
2023-03-14 00:16:44 - progress_bar.py[line:274] - INFO: epoch 001:   3074 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.3, ups=0.86, wpb=254.6, bsz=100, num_updates=3070, lr=4.75123e-05, gnorm=0.771, clip=10, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3579
2023-03-14 00:16:56 - progress_bar.py[line:274] - INFO: epoch 001:   3084 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.6, ups=0.87, wpb=251.4, bsz=100, num_updates=3080, lr=4.74956e-05, gnorm=0.797, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3590
2023-03-14 00:17:07 - progress_bar.py[line:274] - INFO: epoch 001:   3094 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.1, ups=0.86, wpb=254.6, bsz=100, num_updates=3090, lr=4.74789e-05, gnorm=0.757, clip=10, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3602
2023-03-14 00:17:19 - progress_bar.py[line:274] - INFO: epoch 001:   3104 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=213.6, ups=0.85, wpb=252.3, bsz=100, num_updates=3100, lr=4.74622e-05, gnorm=0.835, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3614
2023-03-14 00:17:31 - progress_bar.py[line:274] - INFO: epoch 001:   3114 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.2, ups=0.87, wpb=253.8, bsz=100, num_updates=3110, lr=4.74456e-05, gnorm=0.781, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=3625
2023-03-14 00:17:42 - progress_bar.py[line:274] - INFO: epoch 001:   3124 / 6313 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.5, ups=0.86, wpb=252.4, bsz=100, num_updates=3120, lr=4.74289e-05, gnorm=0.831, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3637
2023-03-14 00:17:54 - progress_bar.py[line:274] - INFO: epoch 001:   3134 / 6313 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.6, ups=0.87, wpb=250.4, bsz=100, num_updates=3130, lr=4.74122e-05, gnorm=0.813, clip=20, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=3648
2023-03-14 00:18:05 - progress_bar.py[line:274] - INFO: epoch 001:   3144 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=256.7, nsentences=100, sample_size=256.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.8, ups=0.86, wpb=256.7, bsz=100, num_updates=3140, lr=4.73955e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=3660
2023-03-14 00:18:17 - progress_bar.py[line:274] - INFO: epoch 001:   3154 / 6313 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.9, ups=0.87, wpb=252.7, bsz=100, num_updates=3150, lr=4.73789e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3671
2023-03-14 00:18:28 - progress_bar.py[line:274] - INFO: epoch 001:   3164 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=223.1, ups=0.87, wpb=255.1, bsz=100, num_updates=3160, lr=4.73622e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3683
2023-03-14 00:18:40 - progress_bar.py[line:274] - INFO: epoch 001:   3174 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=221.4, ups=0.87, wpb=253.4, bsz=100, num_updates=3170, lr=4.73455e-05, gnorm=0.732, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3694
2023-03-14 00:18:51 - progress_bar.py[line:274] - INFO: epoch 001:   3184 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.1, ups=0.86, wpb=253.5, bsz=100, num_updates=3180, lr=4.73288e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3706
2023-03-14 00:19:03 - progress_bar.py[line:274] - INFO: epoch 001:   3194 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.6, ups=0.85, wpb=255.1, bsz=100, num_updates=3190, lr=4.73122e-05, gnorm=0.787, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3718
2023-03-14 00:19:15 - progress_bar.py[line:274] - INFO: epoch 001:   3204 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.7, ups=0.86, wpb=252, bsz=100, num_updates=3200, lr=4.72955e-05, gnorm=0.744, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3729
2023-03-14 00:19:26 - progress_bar.py[line:274] - INFO: epoch 001:   3214 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.3, ups=0.86, wpb=253.1, bsz=100, num_updates=3210, lr=4.72788e-05, gnorm=0.771, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3741
2023-03-14 00:19:38 - progress_bar.py[line:274] - INFO: epoch 001:   3224 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.6, ups=0.86, wpb=253.3, bsz=100, num_updates=3220, lr=4.72621e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3753
2023-03-14 00:19:50 - progress_bar.py[line:274] - INFO: epoch 001:   3234 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222, ups=0.87, wpb=254.7, bsz=100, num_updates=3230, lr=4.72455e-05, gnorm=0.824, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3764
2023-03-14 00:20:01 - progress_bar.py[line:274] - INFO: epoch 001:   3244 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.6, ups=0.86, wpb=253.9, bsz=100, num_updates=3240, lr=4.72288e-05, gnorm=0.706, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3776
2023-03-14 00:20:13 - progress_bar.py[line:274] - INFO: epoch 001:   3254 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.8, ups=0.87, wpb=253.5, bsz=100, num_updates=3250, lr=4.72121e-05, gnorm=0.822, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3787
2023-03-14 00:20:24 - progress_bar.py[line:274] - INFO: epoch 001:   3264 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221, ups=0.87, wpb=254, bsz=100, num_updates=3260, lr=4.71955e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3799
2023-03-14 00:20:36 - progress_bar.py[line:274] - INFO: epoch 001:   3274 / 6313 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.1, ups=0.86, wpb=252.8, bsz=100, num_updates=3270, lr=4.71788e-05, gnorm=0.832, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3810
2023-03-14 00:20:47 - progress_bar.py[line:274] - INFO: epoch 001:   3284 / 6313 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=248.1, nsentences=100, sample_size=248.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.2, ups=0.88, wpb=248.1, bsz=100, num_updates=3280, lr=4.71621e-05, gnorm=0.766, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3822
2023-03-14 00:20:59 - progress_bar.py[line:274] - INFO: epoch 001:   3294 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.8, ups=0.86, wpb=255, bsz=100, num_updates=3290, lr=4.71454e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3833
2023-03-14 00:21:10 - progress_bar.py[line:274] - INFO: epoch 001:   3304 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.3, ups=0.86, wpb=252.9, bsz=100, num_updates=3300, lr=4.71288e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3845
2023-03-14 00:21:22 - progress_bar.py[line:274] - INFO: epoch 001:   3314 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.2, ups=0.86, wpb=253.8, bsz=100, num_updates=3310, lr=4.71121e-05, gnorm=0.749, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3857
2023-03-14 00:21:34 - progress_bar.py[line:274] - INFO: epoch 001:   3324 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.3, ups=0.86, wpb=253.2, bsz=100, num_updates=3320, lr=4.70954e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=3868
2023-03-14 00:21:45 - progress_bar.py[line:274] - INFO: epoch 001:   3334 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219, ups=0.87, wpb=251.2, bsz=100, num_updates=3330, lr=4.70787e-05, gnorm=0.745, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3880
2023-03-14 00:21:57 - progress_bar.py[line:274] - INFO: epoch 001:   3344 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.4, ups=0.86, wpb=253.4, bsz=100, num_updates=3340, lr=4.70621e-05, gnorm=0.804, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3891
2023-03-14 00:22:08 - progress_bar.py[line:274] - INFO: epoch 001:   3354 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.4, ups=0.86, wpb=253.6, bsz=100, num_updates=3350, lr=4.70454e-05, gnorm=0.789, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3903
2023-03-14 00:22:20 - progress_bar.py[line:274] - INFO: epoch 001:   3364 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=221, ups=0.87, wpb=253, bsz=100, num_updates=3360, lr=4.70287e-05, gnorm=0.859, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3914
2023-03-14 00:22:32 - progress_bar.py[line:274] - INFO: epoch 001:   3374 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.4, ups=0.86, wpb=255.5, bsz=100, num_updates=3370, lr=4.7012e-05, gnorm=0.833, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3926
2023-03-14 00:22:43 - progress_bar.py[line:274] - INFO: epoch 001:   3384 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.4, ups=0.86, wpb=255.9, bsz=100, num_updates=3380, lr=4.69954e-05, gnorm=0.75, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3938
2023-03-14 00:22:55 - progress_bar.py[line:274] - INFO: epoch 001:   3394 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218, ups=0.86, wpb=252.6, bsz=100, num_updates=3390, lr=4.69787e-05, gnorm=0.728, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3949
2023-03-14 00:23:06 - progress_bar.py[line:274] - INFO: epoch 001:   3404 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.6, ups=0.86, wpb=252.5, bsz=100, num_updates=3400, lr=4.6962e-05, gnorm=0.738, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=3961
2023-03-14 00:23:18 - progress_bar.py[line:274] - INFO: epoch 001:   3414 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.7, ups=0.86, wpb=254.2, bsz=100, num_updates=3410, lr=4.69453e-05, gnorm=0.788, clip=10, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=3972
2023-03-14 00:23:30 - progress_bar.py[line:274] - INFO: epoch 001:   3424 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=249.8, nsentences=100, sample_size=249.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=213.7, ups=0.86, wpb=249.8, bsz=100, num_updates=3420, lr=4.69287e-05, gnorm=0.759, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3984
2023-03-14 00:23:41 - progress_bar.py[line:274] - INFO: epoch 001:   3434 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.8, ups=0.86, wpb=255.6, bsz=100, num_updates=3430, lr=4.6912e-05, gnorm=0.763, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3996
2023-03-14 00:23:53 - progress_bar.py[line:274] - INFO: epoch 001:   3444 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=215, ups=0.86, wpb=251.3, bsz=100, num_updates=3440, lr=4.68953e-05, gnorm=0.716, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4008
2023-03-14 00:24:04 - progress_bar.py[line:274] - INFO: epoch 001:   3454 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.7, ups=0.87, wpb=250.3, bsz=100, num_updates=3450, lr=4.68786e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4019
2023-03-14 00:24:16 - progress_bar.py[line:274] - INFO: epoch 001:   3464 / 6313 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=248.3, nsentences=100, sample_size=248.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=213.9, ups=0.86, wpb=248.3, bsz=100, num_updates=3460, lr=4.6862e-05, gnorm=0.796, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4031
2023-03-14 00:24:28 - progress_bar.py[line:274] - INFO: epoch 001:   3474 / 6313 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.6, ups=0.87, wpb=252.6, bsz=100, num_updates=3470, lr=4.68453e-05, gnorm=0.804, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4042
2023-03-14 00:24:39 - progress_bar.py[line:274] - INFO: epoch 001:   3484 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.2, ups=0.86, wpb=253.2, bsz=100, num_updates=3480, lr=4.68286e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4054
2023-03-14 00:24:51 - progress_bar.py[line:274] - INFO: epoch 001:   3494 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=215.9, ups=0.86, wpb=251.2, bsz=100, num_updates=3490, lr=4.6812e-05, gnorm=0.763, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4065
2023-03-14 00:25:02 - progress_bar.py[line:274] - INFO: epoch 001:   3504 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.6, ups=0.86, wpb=255.2, bsz=100, num_updates=3500, lr=4.67953e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4077
2023-03-14 00:25:14 - progress_bar.py[line:274] - INFO: epoch 001:   3514 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.5, ups=0.86, wpb=252.8, bsz=100, num_updates=3510, lr=4.67786e-05, gnorm=0.712, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=4089
2023-03-14 00:25:26 - progress_bar.py[line:274] - INFO: epoch 001:   3524 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.4, ups=0.86, wpb=255, bsz=100, num_updates=3520, lr=4.67619e-05, gnorm=0.716, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=4100
2023-03-14 00:25:37 - progress_bar.py[line:274] - INFO: epoch 001:   3534 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.9, ups=0.86, wpb=252.8, bsz=100, num_updates=3530, lr=4.67453e-05, gnorm=0.766, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=4112
2023-03-14 00:25:49 - progress_bar.py[line:274] - INFO: epoch 001:   3544 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.9, ups=0.86, wpb=253, bsz=100, num_updates=3540, lr=4.67286e-05, gnorm=0.717, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4123
2023-03-14 00:26:00 - progress_bar.py[line:274] - INFO: epoch 001:   3554 / 6313 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.6, ups=0.86, wpb=254.8, bsz=100, num_updates=3550, lr=4.67119e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=4135
2023-03-14 00:26:12 - progress_bar.py[line:274] - INFO: epoch 001:   3564 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.3, ups=0.86, wpb=254.4, bsz=100, num_updates=3560, lr=4.66952e-05, gnorm=0.743, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4147
2023-03-14 00:26:24 - progress_bar.py[line:274] - INFO: epoch 001:   3574 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.5, ups=0.87, wpb=251.3, bsz=100, num_updates=3570, lr=4.66786e-05, gnorm=0.795, clip=20, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=4158
2023-03-14 00:26:35 - progress_bar.py[line:274] - INFO: epoch 001:   3584 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=216.8, ups=0.86, wpb=252.2, bsz=100, num_updates=3580, lr=4.66619e-05, gnorm=0.76, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4170
2023-03-14 00:26:47 - progress_bar.py[line:274] - INFO: epoch 001:   3594 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.3, ups=0.86, wpb=253.4, bsz=100, num_updates=3590, lr=4.66452e-05, gnorm=0.804, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=4181
2023-03-14 00:26:58 - progress_bar.py[line:274] - INFO: epoch 001:   3604 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.8, ups=0.87, wpb=252.9, bsz=100, num_updates=3600, lr=4.66285e-05, gnorm=0.787, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4193
2023-03-14 00:27:10 - progress_bar.py[line:274] - INFO: epoch 001:   3614 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=224.2, ups=0.88, wpb=253.9, bsz=100, num_updates=3610, lr=4.66119e-05, gnorm=0.718, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=4204
2023-03-14 00:27:12 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 00:27:22 - progress_bar.py[line:274] - INFO: epoch 001:   3625 / 6313 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=202.2, ups=0.8, wpb=253.2, bsz=100, num_updates=3620, lr=4.65952e-05, gnorm=0.78, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4217
2023-03-14 00:27:34 - progress_bar.py[line:274] - INFO: epoch 001:   3635 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.1, ups=0.86, wpb=253.8, bsz=100, num_updates=3630, lr=4.65785e-05, gnorm=0.724, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=4228
2023-03-14 00:27:46 - progress_bar.py[line:274] - INFO: epoch 001:   3645 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.9, ups=0.86, wpb=254.6, bsz=100, num_updates=3640, lr=4.65618e-05, gnorm=0.734, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4240
2023-03-14 00:27:57 - progress_bar.py[line:274] - INFO: epoch 001:   3655 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220, ups=0.87, wpb=253.8, bsz=100, num_updates=3650, lr=4.65452e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4252
2023-03-14 00:28:09 - progress_bar.py[line:274] - INFO: epoch 001:   3665 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.7, ups=0.86, wpb=252.5, bsz=100, num_updates=3660, lr=4.65285e-05, gnorm=0.721, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=4263
2023-03-14 00:28:21 - progress_bar.py[line:274] - INFO: epoch 001:   3675 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=214.4, ups=0.84, wpb=254.2, bsz=100, num_updates=3670, lr=4.65118e-05, gnorm=0.772, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=4275
2023-03-14 00:28:32 - progress_bar.py[line:274] - INFO: epoch 001:   3685 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.1, ups=0.86, wpb=253, bsz=100, num_updates=3680, lr=4.64951e-05, gnorm=0.676, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4287
2023-03-14 00:28:44 - progress_bar.py[line:274] - INFO: epoch 001:   3695 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.1, ups=0.87, wpb=252.3, bsz=100, num_updates=3690, lr=4.64785e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4298
2023-03-14 00:28:55 - progress_bar.py[line:274] - INFO: epoch 001:   3705 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.8, ups=0.86, wpb=253.5, bsz=100, num_updates=3700, lr=4.64618e-05, gnorm=0.793, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4310
2023-03-14 00:29:07 - progress_bar.py[line:274] - INFO: epoch 001:   3715 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.4, ups=0.86, wpb=255.9, bsz=100, num_updates=3710, lr=4.64451e-05, gnorm=0.727, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4321
2023-03-14 00:29:19 - progress_bar.py[line:274] - INFO: epoch 001:   3725 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.2, ups=0.86, wpb=254.2, bsz=100, num_updates=3720, lr=4.64285e-05, gnorm=0.736, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4333
2023-03-14 00:29:30 - progress_bar.py[line:274] - INFO: epoch 001:   3735 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.5, ups=0.86, wpb=252.6, bsz=100, num_updates=3730, lr=4.64118e-05, gnorm=0.765, clip=10, loss_scale=512, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=4345
2023-03-14 00:29:42 - progress_bar.py[line:274] - INFO: epoch 001:   3745 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.1, ups=0.86, wpb=254.5, bsz=100, num_updates=3740, lr=4.63951e-05, gnorm=0.744, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4356
2023-03-14 00:29:53 - progress_bar.py[line:274] - INFO: epoch 001:   3755 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.6, ups=0.86, wpb=254.3, bsz=100, num_updates=3750, lr=4.63784e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4368
2023-03-14 00:30:05 - progress_bar.py[line:274] - INFO: epoch 001:   3765 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.6, ups=0.87, wpb=254.6, bsz=100, num_updates=3760, lr=4.63618e-05, gnorm=0.783, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4379
2023-03-14 00:30:16 - progress_bar.py[line:274] - INFO: epoch 001:   3775 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.5, ups=0.87, wpb=252.5, bsz=100, num_updates=3770, lr=4.63451e-05, gnorm=0.712, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4391
2023-03-14 00:30:27 - progress_bar.py[line:274] - INFO: epoch 001:   3785 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=227.3, ups=0.9, wpb=253.8, bsz=100, num_updates=3780, lr=4.63284e-05, gnorm=0.654, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=4402
2023-03-14 00:30:39 - progress_bar.py[line:274] - INFO: epoch 001:   3795 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218, ups=0.85, wpb=255.5, bsz=100, num_updates=3790, lr=4.63117e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=4414
2023-03-14 00:30:51 - progress_bar.py[line:274] - INFO: epoch 001:   3805 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.9, ups=0.86, wpb=254.5, bsz=100, num_updates=3800, lr=4.62951e-05, gnorm=0.864, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4425
2023-03-14 00:31:02 - progress_bar.py[line:274] - INFO: epoch 001:   3815 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.2, ups=0.86, wpb=250.2, bsz=100, num_updates=3810, lr=4.62784e-05, gnorm=0.802, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4437
2023-03-14 00:31:14 - progress_bar.py[line:274] - INFO: epoch 001:   3825 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.4, ups=0.87, wpb=252.1, bsz=100, num_updates=3820, lr=4.62617e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4448
2023-03-14 00:31:26 - progress_bar.py[line:274] - INFO: epoch 001:   3835 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.9, ups=0.86, wpb=250.7, bsz=100, num_updates=3830, lr=4.6245e-05, gnorm=0.721, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4460
2023-03-14 00:31:37 - progress_bar.py[line:274] - INFO: epoch 001:   3845 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.7, ups=0.87, wpb=253.5, bsz=100, num_updates=3840, lr=4.62284e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4471
2023-03-14 00:31:49 - progress_bar.py[line:274] - INFO: epoch 001:   3855 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.5, ups=0.86, wpb=253.4, bsz=100, num_updates=3850, lr=4.62117e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4483
2023-03-14 00:32:00 - progress_bar.py[line:274] - INFO: epoch 001:   3865 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.7, ups=0.86, wpb=252.7, bsz=100, num_updates=3860, lr=4.6195e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4495
2023-03-14 00:32:12 - progress_bar.py[line:274] - INFO: epoch 001:   3875 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=222.1, ups=0.88, wpb=253.4, bsz=100, num_updates=3870, lr=4.61783e-05, gnorm=0.721, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4506
2023-03-14 00:32:23 - progress_bar.py[line:274] - INFO: epoch 001:   3885 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.8, ups=0.87, wpb=252.6, bsz=100, num_updates=3880, lr=4.61617e-05, gnorm=0.732, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=4518
2023-03-14 00:32:35 - progress_bar.py[line:274] - INFO: epoch 001:   3895 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219, ups=0.86, wpb=253.9, bsz=100, num_updates=3890, lr=4.6145e-05, gnorm=0.698, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=4529
2023-03-14 00:32:46 - progress_bar.py[line:274] - INFO: epoch 001:   3905 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.3, ups=0.87, wpb=254.6, bsz=100, num_updates=3900, lr=4.61283e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=4541
2023-03-14 00:32:58 - progress_bar.py[line:274] - INFO: epoch 001:   3915 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.9, ups=0.85, wpb=255.4, bsz=100, num_updates=3910, lr=4.61116e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4552
2023-03-14 00:33:09 - progress_bar.py[line:274] - INFO: epoch 001:   3925 / 6313 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=222.1, ups=0.87, wpb=255.2, bsz=100, num_updates=3920, lr=4.6095e-05, gnorm=0.771, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4564
2023-03-14 00:33:21 - progress_bar.py[line:274] - INFO: epoch 001:   3935 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.2, ups=0.87, wpb=252.7, bsz=100, num_updates=3930, lr=4.60783e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4575
2023-03-14 00:33:32 - progress_bar.py[line:274] - INFO: epoch 001:   3945 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=216.4, ups=0.87, wpb=250.1, bsz=100, num_updates=3940, lr=4.60616e-05, gnorm=0.744, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4587
2023-03-14 00:33:44 - progress_bar.py[line:274] - INFO: epoch 001:   3955 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.9, ups=0.87, wpb=252.9, bsz=100, num_updates=3950, lr=4.6045e-05, gnorm=0.776, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4598
2023-03-14 00:33:55 - progress_bar.py[line:274] - INFO: epoch 001:   3965 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=225.3, ups=0.88, wpb=254.7, bsz=100, num_updates=3960, lr=4.60283e-05, gnorm=0.727, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=4610
2023-03-14 00:34:07 - progress_bar.py[line:274] - INFO: epoch 001:   3975 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=248.3, nsentences=100, sample_size=248.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=214.1, ups=0.86, wpb=248.3, bsz=100, num_updates=3970, lr=4.60116e-05, gnorm=0.817, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4621
2023-03-14 00:34:18 - progress_bar.py[line:274] - INFO: epoch 001:   3985 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221.4, ups=0.86, wpb=256.3, bsz=100, num_updates=3980, lr=4.59949e-05, gnorm=0.651, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4633
2023-03-14 00:34:30 - progress_bar.py[line:274] - INFO: epoch 001:   3995 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.8, ups=0.88, wpb=253.4, bsz=100, num_updates=3990, lr=4.59783e-05, gnorm=0.698, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=4644
2023-03-14 00:34:41 - progress_bar.py[line:274] - INFO: epoch 001:   4005 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=222.4, ups=0.88, wpb=253.8, bsz=100, num_updates=4000, lr=4.59616e-05, gnorm=0.723, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=4656
2023-03-14 00:34:53 - progress_bar.py[line:274] - INFO: epoch 001:   4015 / 6313 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.8, ups=0.86, wpb=253.5, bsz=100, num_updates=4010, lr=4.59449e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=4667
2023-03-14 00:35:04 - progress_bar.py[line:274] - INFO: epoch 001:   4025 / 6313 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218, ups=0.86, wpb=253.4, bsz=100, num_updates=4020, lr=4.59282e-05, gnorm=0.762, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=4679
2023-03-14 00:35:16 - progress_bar.py[line:274] - INFO: epoch 001:   4035 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.1, ups=0.85, wpb=254.3, bsz=100, num_updates=4030, lr=4.59116e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4691
2023-03-14 00:35:28 - progress_bar.py[line:274] - INFO: epoch 001:   4045 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=216, ups=0.86, wpb=252.2, bsz=100, num_updates=4040, lr=4.58949e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4702
2023-03-14 00:35:39 - progress_bar.py[line:274] - INFO: epoch 001:   4055 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219, ups=0.86, wpb=253.7, bsz=100, num_updates=4050, lr=4.58782e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4714
2023-03-14 00:35:51 - progress_bar.py[line:274] - INFO: epoch 001:   4065 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.1, ups=0.86, wpb=253.7, bsz=100, num_updates=4060, lr=4.58615e-05, gnorm=0.787, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4726
2023-03-14 00:36:03 - progress_bar.py[line:274] - INFO: epoch 001:   4075 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.9, ups=0.87, wpb=251.5, bsz=100, num_updates=4070, lr=4.58449e-05, gnorm=0.728, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4737
2023-03-14 00:36:14 - progress_bar.py[line:274] - INFO: epoch 001:   4085 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.6, ups=0.87, wpb=253.9, bsz=100, num_updates=4080, lr=4.58282e-05, gnorm=0.739, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4749
2023-03-14 00:36:26 - progress_bar.py[line:274] - INFO: epoch 001:   4095 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.4, ups=0.86, wpb=252.8, bsz=100, num_updates=4090, lr=4.58115e-05, gnorm=0.799, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4760
2023-03-14 00:36:37 - progress_bar.py[line:274] - INFO: epoch 001:   4105 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.8, ups=0.88, wpb=251.1, bsz=100, num_updates=4100, lr=4.57948e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4772
2023-03-14 00:36:49 - progress_bar.py[line:274] - INFO: epoch 001:   4115 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.2, ups=0.86, wpb=253, bsz=100, num_updates=4110, lr=4.57782e-05, gnorm=0.717, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4783
2023-03-14 00:37:00 - progress_bar.py[line:274] - INFO: epoch 001:   4125 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.5, ups=0.86, wpb=255.1, bsz=100, num_updates=4120, lr=4.57615e-05, gnorm=0.745, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4795
2023-03-14 00:37:12 - progress_bar.py[line:274] - INFO: epoch 001:   4135 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.6, ups=0.86, wpb=254.8, bsz=100, num_updates=4130, lr=4.57448e-05, gnorm=0.698, clip=0, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=4807
2023-03-14 00:37:19 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 00:37:25 - progress_bar.py[line:274] - INFO: epoch 001:   4146 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=203, ups=0.79, wpb=255.9, bsz=100, num_updates=4140, lr=4.57281e-05, gnorm=0.767, clip=10, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=4819
2023-03-14 00:37:36 - progress_bar.py[line:274] - INFO: epoch 001:   4156 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=214.6, ups=0.85, wpb=252.6, bsz=100, num_updates=4150, lr=4.57115e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4831
2023-03-14 00:37:48 - progress_bar.py[line:274] - INFO: epoch 001:   4166 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.4, ups=0.85, wpb=254.7, bsz=100, num_updates=4160, lr=4.56948e-05, gnorm=0.793, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4843
2023-03-14 00:38:00 - progress_bar.py[line:274] - INFO: epoch 001:   4176 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.9, ups=0.86, wpb=253.5, bsz=100, num_updates=4170, lr=4.56781e-05, gnorm=0.692, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=4854
2023-03-14 00:38:11 - progress_bar.py[line:274] - INFO: epoch 001:   4186 / 6313 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221.3, ups=0.88, wpb=252.8, bsz=100, num_updates=4180, lr=4.56615e-05, gnorm=0.816, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4866
2023-03-14 00:38:23 - progress_bar.py[line:274] - INFO: epoch 001:   4196 / 6313 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.7, ups=0.86, wpb=251.5, bsz=100, num_updates=4190, lr=4.56448e-05, gnorm=0.734, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4877
2023-03-14 00:38:34 - progress_bar.py[line:274] - INFO: epoch 001:   4206 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.5, ups=0.86, wpb=253, bsz=100, num_updates=4200, lr=4.56281e-05, gnorm=0.757, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4889
2023-03-14 00:38:46 - progress_bar.py[line:274] - INFO: epoch 001:   4216 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.1, ups=0.86, wpb=253.3, bsz=100, num_updates=4210, lr=4.56114e-05, gnorm=0.831, clip=20, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=4901
2023-03-14 00:38:58 - progress_bar.py[line:274] - INFO: epoch 001:   4226 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.3, ups=0.86, wpb=253.9, bsz=100, num_updates=4220, lr=4.55948e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4912
2023-03-14 00:39:09 - progress_bar.py[line:274] - INFO: epoch 001:   4236 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222, ups=0.87, wpb=254.8, bsz=100, num_updates=4230, lr=4.55781e-05, gnorm=0.775, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4924
2023-03-14 00:39:21 - progress_bar.py[line:274] - INFO: epoch 001:   4246 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.8, ups=0.86, wpb=253.4, bsz=100, num_updates=4240, lr=4.55614e-05, gnorm=0.734, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=4935
2023-03-14 00:39:32 - progress_bar.py[line:274] - INFO: epoch 001:   4256 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.3, ups=0.86, wpb=254.1, bsz=100, num_updates=4250, lr=4.55447e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4947
2023-03-14 00:39:44 - progress_bar.py[line:274] - INFO: epoch 001:   4266 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.3, ups=0.86, wpb=255, bsz=100, num_updates=4260, lr=4.55281e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=4958
2023-03-14 00:39:56 - progress_bar.py[line:274] - INFO: epoch 001:   4276 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.7, ups=0.86, wpb=252.5, bsz=100, num_updates=4270, lr=4.55114e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4970
2023-03-14 00:40:07 - progress_bar.py[line:274] - INFO: epoch 001:   4286 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.3, ups=0.86, wpb=252.1, bsz=100, num_updates=4280, lr=4.54947e-05, gnorm=0.804, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4982
2023-03-14 00:40:19 - progress_bar.py[line:274] - INFO: epoch 001:   4296 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.4, ups=0.86, wpb=253.4, bsz=100, num_updates=4290, lr=4.5478e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4993
2023-03-14 00:40:30 - progress_bar.py[line:274] - INFO: epoch 001:   4306 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=221.8, ups=0.87, wpb=256.3, bsz=100, num_updates=4300, lr=4.54614e-05, gnorm=0.743, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5005
2023-03-14 00:40:42 - progress_bar.py[line:274] - INFO: epoch 001:   4316 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.2, ups=0.86, wpb=252.6, bsz=100, num_updates=4310, lr=4.54447e-05, gnorm=0.655, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5017
2023-03-14 00:40:54 - progress_bar.py[line:274] - INFO: epoch 001:   4326 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.9, ups=0.86, wpb=253.6, bsz=100, num_updates=4320, lr=4.5428e-05, gnorm=0.772, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5028
2023-03-14 00:41:05 - progress_bar.py[line:274] - INFO: epoch 001:   4336 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219, ups=0.86, wpb=253.5, bsz=100, num_updates=4330, lr=4.54113e-05, gnorm=0.749, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5040
2023-03-14 00:41:17 - progress_bar.py[line:274] - INFO: epoch 001:   4346 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.2, ups=0.86, wpb=254.4, bsz=100, num_updates=4340, lr=4.53947e-05, gnorm=0.733, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5051
2023-03-14 00:41:28 - progress_bar.py[line:274] - INFO: epoch 001:   4356 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.8, ups=0.86, wpb=251.5, bsz=100, num_updates=4350, lr=4.5378e-05, gnorm=0.728, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5063
2023-03-14 00:41:40 - progress_bar.py[line:274] - INFO: epoch 001:   4366 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.1, ups=0.87, wpb=253.3, bsz=100, num_updates=4360, lr=4.53613e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5075
2023-03-14 00:41:51 - progress_bar.py[line:274] - INFO: epoch 001:   4376 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.3, ups=0.88, wpb=251.7, bsz=100, num_updates=4370, lr=4.53446e-05, gnorm=0.695, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5086
2023-03-14 00:42:03 - progress_bar.py[line:274] - INFO: epoch 001:   4386 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.8, ups=0.86, wpb=253.3, bsz=100, num_updates=4380, lr=4.5328e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5098
2023-03-14 00:42:14 - progress_bar.py[line:274] - INFO: epoch 001:   4396 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.3, ups=0.87, wpb=252.2, bsz=100, num_updates=4390, lr=4.53113e-05, gnorm=0.771, clip=20, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5109
2023-03-14 00:42:26 - progress_bar.py[line:274] - INFO: epoch 001:   4406 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.7, ups=0.85, wpb=253.5, bsz=100, num_updates=4400, lr=4.52946e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=5121
2023-03-14 00:42:38 - progress_bar.py[line:274] - INFO: epoch 001:   4416 / 6313 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.3, ups=0.86, wpb=252.5, bsz=100, num_updates=4410, lr=4.5278e-05, gnorm=0.765, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5132
2023-03-14 00:42:49 - progress_bar.py[line:274] - INFO: epoch 001:   4426 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218, ups=0.86, wpb=253.1, bsz=100, num_updates=4420, lr=4.52613e-05, gnorm=0.784, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5144
2023-03-14 00:43:01 - progress_bar.py[line:274] - INFO: epoch 001:   4436 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.7, ups=0.86, wpb=252.8, bsz=100, num_updates=4430, lr=4.52446e-05, gnorm=0.739, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5156
2023-03-14 00:43:13 - progress_bar.py[line:274] - INFO: epoch 001:   4446 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221.1, ups=0.87, wpb=253.6, bsz=100, num_updates=4440, lr=4.52279e-05, gnorm=0.774, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5167
2023-03-14 00:43:24 - progress_bar.py[line:274] - INFO: epoch 001:   4456 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.4, ups=0.88, wpb=253, bsz=100, num_updates=4450, lr=4.52113e-05, gnorm=0.74, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=5179
2023-03-14 00:43:36 - progress_bar.py[line:274] - INFO: epoch 001:   4466 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.2, ups=0.86, wpb=250.6, bsz=100, num_updates=4460, lr=4.51946e-05, gnorm=0.732, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5190
2023-03-14 00:43:47 - progress_bar.py[line:274] - INFO: epoch 001:   4476 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.7, ups=0.86, wpb=253.1, bsz=100, num_updates=4470, lr=4.51779e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=5202
2023-03-14 00:43:59 - progress_bar.py[line:274] - INFO: epoch 001:   4486 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.6, ups=0.86, wpb=251.8, bsz=100, num_updates=4480, lr=4.51612e-05, gnorm=0.762, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5213
2023-03-14 00:44:10 - progress_bar.py[line:274] - INFO: epoch 001:   4496 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.9, ups=0.86, wpb=253.8, bsz=100, num_updates=4490, lr=4.51446e-05, gnorm=0.661, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5225
2023-03-14 00:44:22 - progress_bar.py[line:274] - INFO: epoch 001:   4506 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.6, ups=0.86, wpb=254.4, bsz=100, num_updates=4500, lr=4.51279e-05, gnorm=0.659, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5237
2023-03-14 00:44:34 - progress_bar.py[line:274] - INFO: epoch 001:   4516 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.9, ups=0.86, wpb=252.1, bsz=100, num_updates=4510, lr=4.51112e-05, gnorm=0.724, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5248
2023-03-14 00:44:45 - progress_bar.py[line:274] - INFO: epoch 001:   4526 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.8, ups=0.86, wpb=252.2, bsz=100, num_updates=4520, lr=4.50945e-05, gnorm=0.756, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5260
2023-03-14 00:44:57 - progress_bar.py[line:274] - INFO: epoch 001:   4536 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.9, ups=0.86, wpb=255.9, bsz=100, num_updates=4530, lr=4.50779e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5271
2023-03-14 00:45:08 - progress_bar.py[line:274] - INFO: epoch 001:   4546 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.8, ups=0.87, wpb=254.3, bsz=100, num_updates=4540, lr=4.50612e-05, gnorm=0.765, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5283
2023-03-14 00:45:20 - progress_bar.py[line:274] - INFO: epoch 001:   4556 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.8, ups=0.86, wpb=253.9, bsz=100, num_updates=4550, lr=4.50445e-05, gnorm=0.74, clip=10, loss_scale=512, train_wall=12, gb_free=9.6, ema_decay=0.9999, wall=5295
2023-03-14 00:45:32 - progress_bar.py[line:274] - INFO: epoch 001:   4566 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219, ups=0.86, wpb=253.6, bsz=100, num_updates=4560, lr=4.50278e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=5306
2023-03-14 00:45:43 - progress_bar.py[line:274] - INFO: epoch 001:   4576 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.5, ups=0.86, wpb=254.5, bsz=100, num_updates=4570, lr=4.50112e-05, gnorm=0.695, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5318
2023-03-14 00:45:55 - progress_bar.py[line:274] - INFO: epoch 001:   4586 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.6, ups=0.86, wpb=254.1, bsz=100, num_updates=4580, lr=4.49945e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5329
2023-03-14 00:46:06 - progress_bar.py[line:274] - INFO: epoch 001:   4596 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220, ups=0.87, wpb=251.6, bsz=100, num_updates=4590, lr=4.49778e-05, gnorm=0.835, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5341
2023-03-14 00:46:18 - progress_bar.py[line:274] - INFO: epoch 001:   4606 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.4, ups=0.87, wpb=252.7, bsz=100, num_updates=4600, lr=4.49611e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=5352
2023-03-14 00:46:30 - progress_bar.py[line:274] - INFO: epoch 001:   4616 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.8, ups=0.87, wpb=252.4, bsz=100, num_updates=4610, lr=4.49445e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5364
2023-03-14 00:46:41 - progress_bar.py[line:274] - INFO: epoch 001:   4626 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.5, ups=0.86, wpb=252.8, bsz=100, num_updates=4620, lr=4.49278e-05, gnorm=0.72, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=5376
2023-03-14 00:46:53 - progress_bar.py[line:274] - INFO: epoch 001:   4636 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.9, ups=0.87, wpb=254.1, bsz=100, num_updates=4630, lr=4.49111e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=5387
2023-03-14 00:47:05 - progress_bar.py[line:274] - INFO: epoch 001:   4646 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.1, ups=0.86, wpb=254, bsz=100, num_updates=4640, lr=4.48945e-05, gnorm=0.717, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=5399
2023-03-14 00:47:16 - progress_bar.py[line:274] - INFO: epoch 001:   4656 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221.9, ups=0.88, wpb=253.5, bsz=100, num_updates=4650, lr=4.48778e-05, gnorm=0.677, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5411
2023-03-14 00:47:22 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 00:47:29 - progress_bar.py[line:274] - INFO: epoch 001:   4667 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=200.8, ups=0.79, wpb=253.1, bsz=100, num_updates=4660, lr=4.48611e-05, gnorm=0.676, clip=0, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=5423
2023-03-14 00:47:40 - progress_bar.py[line:274] - INFO: epoch 001:   4677 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.6, ups=0.86, wpb=256.1, bsz=100, num_updates=4670, lr=4.48444e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=5435
2023-03-14 00:47:52 - progress_bar.py[line:274] - INFO: epoch 001:   4687 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.3, ups=0.86, wpb=253.9, bsz=100, num_updates=4680, lr=4.48278e-05, gnorm=0.752, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5446
2023-03-14 00:48:03 - progress_bar.py[line:274] - INFO: epoch 001:   4697 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=224.9, ups=0.89, wpb=253.5, bsz=100, num_updates=4690, lr=4.48111e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5458
2023-03-14 00:48:15 - progress_bar.py[line:274] - INFO: epoch 001:   4707 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.6, ups=0.87, wpb=251.5, bsz=100, num_updates=4700, lr=4.47944e-05, gnorm=0.72, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5469
2023-03-14 00:48:26 - progress_bar.py[line:274] - INFO: epoch 001:   4717 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.6, ups=0.86, wpb=254.6, bsz=100, num_updates=4710, lr=4.47777e-05, gnorm=0.732, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5481
2023-03-14 00:48:38 - progress_bar.py[line:274] - INFO: epoch 001:   4727 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.6, ups=0.86, wpb=254.2, bsz=100, num_updates=4720, lr=4.47611e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5492
2023-03-14 00:48:49 - progress_bar.py[line:274] - INFO: epoch 001:   4737 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.5, ups=0.86, wpb=250.8, bsz=100, num_updates=4730, lr=4.47444e-05, gnorm=0.758, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5504
2023-03-14 00:49:01 - progress_bar.py[line:274] - INFO: epoch 001:   4747 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=257.2, nsentences=100, sample_size=257.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=222.5, ups=0.87, wpb=257.2, bsz=100, num_updates=4740, lr=4.47277e-05, gnorm=0.814, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5516
2023-03-14 00:49:12 - progress_bar.py[line:274] - INFO: epoch 001:   4757 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.5, ups=0.87, wpb=252.7, bsz=100, num_updates=4750, lr=4.4711e-05, gnorm=0.734, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=5527
2023-03-14 00:49:24 - progress_bar.py[line:274] - INFO: epoch 001:   4767 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.1, ups=0.85, wpb=255.4, bsz=100, num_updates=4760, lr=4.46944e-05, gnorm=0.635, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=5539
2023-03-14 00:49:36 - progress_bar.py[line:274] - INFO: epoch 001:   4777 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.8, ups=0.86, wpb=253.9, bsz=100, num_updates=4770, lr=4.46777e-05, gnorm=0.672, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5550
2023-03-14 00:49:47 - progress_bar.py[line:274] - INFO: epoch 001:   4787 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.7, ups=0.87, wpb=254.7, bsz=100, num_updates=4780, lr=4.4661e-05, gnorm=0.761, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5562
2023-03-14 00:49:59 - progress_bar.py[line:274] - INFO: epoch 001:   4797 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.5, ups=0.86, wpb=255.9, bsz=100, num_updates=4790, lr=4.46443e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5573
2023-03-14 00:50:10 - progress_bar.py[line:274] - INFO: epoch 001:   4807 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223.6, ups=0.87, wpb=255.6, bsz=100, num_updates=4800, lr=4.46277e-05, gnorm=0.683, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=5585
2023-03-14 00:50:22 - progress_bar.py[line:274] - INFO: epoch 001:   4817 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223.7, ups=0.87, wpb=255.8, bsz=100, num_updates=4810, lr=4.4611e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5596
2023-03-14 00:50:33 - progress_bar.py[line:274] - INFO: epoch 001:   4827 / 6313 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.4, ups=0.86, wpb=252.7, bsz=100, num_updates=4820, lr=4.45943e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5608
2023-03-14 00:50:45 - progress_bar.py[line:274] - INFO: epoch 001:   4837 / 6313 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217, ups=0.86, wpb=251.1, bsz=100, num_updates=4830, lr=4.45777e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5619
2023-03-14 00:50:56 - progress_bar.py[line:274] - INFO: epoch 001:   4847 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.7, ups=0.88, wpb=254.4, bsz=100, num_updates=4840, lr=4.4561e-05, gnorm=0.73, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=5631
2023-03-14 00:51:08 - progress_bar.py[line:274] - INFO: epoch 001:   4857 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.2, ups=0.88, wpb=253.9, bsz=100, num_updates=4850, lr=4.45443e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5642
2023-03-14 00:51:19 - progress_bar.py[line:274] - INFO: epoch 001:   4867 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219, ups=0.86, wpb=253.9, bsz=100, num_updates=4860, lr=4.45276e-05, gnorm=0.785, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5654
2023-03-14 00:51:31 - progress_bar.py[line:274] - INFO: epoch 001:   4877 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=215.7, ups=0.86, wpb=252.1, bsz=100, num_updates=4870, lr=4.4511e-05, gnorm=0.771, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5666
2023-03-14 00:51:43 - progress_bar.py[line:274] - INFO: epoch 001:   4887 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.7, ups=0.86, wpb=251.8, bsz=100, num_updates=4880, lr=4.44943e-05, gnorm=0.733, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5677
2023-03-14 00:51:54 - progress_bar.py[line:274] - INFO: epoch 001:   4897 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219, ups=0.86, wpb=253.9, bsz=100, num_updates=4890, lr=4.44776e-05, gnorm=0.734, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5689
2023-03-14 00:52:06 - progress_bar.py[line:274] - INFO: epoch 001:   4907 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=222.7, ups=0.87, wpb=255.1, bsz=100, num_updates=4900, lr=4.44609e-05, gnorm=0.779, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5700
2023-03-14 00:52:17 - progress_bar.py[line:274] - INFO: epoch 001:   4917 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=223.4, ups=0.88, wpb=254.8, bsz=100, num_updates=4910, lr=4.44443e-05, gnorm=0.76, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5712
2023-03-14 00:52:29 - progress_bar.py[line:274] - INFO: epoch 001:   4927 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.6, ups=0.86, wpb=250.6, bsz=100, num_updates=4920, lr=4.44276e-05, gnorm=0.771, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5723
2023-03-14 00:52:40 - progress_bar.py[line:274] - INFO: epoch 001:   4937 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.7, ups=0.86, wpb=252.5, bsz=100, num_updates=4930, lr=4.44109e-05, gnorm=0.67, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5735
2023-03-14 00:52:52 - progress_bar.py[line:274] - INFO: epoch 001:   4947 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=225.2, ups=0.88, wpb=254.5, bsz=100, num_updates=4940, lr=4.43942e-05, gnorm=0.761, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5746
2023-03-14 00:53:03 - progress_bar.py[line:274] - INFO: epoch 001:   4957 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.4, ups=0.86, wpb=254.8, bsz=100, num_updates=4950, lr=4.43776e-05, gnorm=0.702, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5758
2023-03-14 00:53:15 - progress_bar.py[line:274] - INFO: epoch 001:   4967 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.4, ups=0.86, wpb=252.8, bsz=100, num_updates=4960, lr=4.43609e-05, gnorm=0.749, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5769
2023-03-14 00:53:26 - progress_bar.py[line:274] - INFO: epoch 001:   4977 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.8, ups=0.86, wpb=252.5, bsz=100, num_updates=4970, lr=4.43442e-05, gnorm=0.784, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5781
2023-03-14 00:53:38 - progress_bar.py[line:274] - INFO: epoch 001:   4987 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220, ups=0.86, wpb=254.6, bsz=100, num_updates=4980, lr=4.43275e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5792
2023-03-14 00:53:50 - progress_bar.py[line:274] - INFO: epoch 001:   4997 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=214.6, ups=0.85, wpb=252.9, bsz=100, num_updates=4990, lr=4.43109e-05, gnorm=0.629, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=5804
2023-03-14 00:54:01 - progress_bar.py[line:274] - INFO: epoch 001:   5007 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.1, ups=0.85, wpb=256.4, bsz=100, num_updates=5000, lr=4.42942e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5816
2023-03-14 00:54:13 - progress_bar.py[line:274] - INFO: epoch 001:   5017 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.7, ups=0.86, wpb=254.4, bsz=100, num_updates=5010, lr=4.42775e-05, gnorm=0.713, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5828
2023-03-14 00:54:25 - progress_bar.py[line:274] - INFO: epoch 001:   5027 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.6, ups=0.86, wpb=254.3, bsz=100, num_updates=5020, lr=4.42608e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5839
2023-03-14 00:54:36 - progress_bar.py[line:274] - INFO: epoch 001:   5037 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.3, ups=0.87, wpb=254.6, bsz=100, num_updates=5030, lr=4.42442e-05, gnorm=0.771, clip=10, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=5851
2023-03-14 00:54:48 - progress_bar.py[line:274] - INFO: epoch 001:   5047 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.2, ups=0.86, wpb=251.9, bsz=100, num_updates=5040, lr=4.42275e-05, gnorm=0.814, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5862
2023-03-14 00:54:59 - progress_bar.py[line:274] - INFO: epoch 001:   5057 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.5, ups=0.86, wpb=253, bsz=100, num_updates=5050, lr=4.42108e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5874
2023-03-14 00:55:11 - progress_bar.py[line:274] - INFO: epoch 001:   5067 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.2, ups=0.86, wpb=255.4, bsz=100, num_updates=5060, lr=4.41942e-05, gnorm=0.777, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5885
2023-03-14 00:55:22 - progress_bar.py[line:274] - INFO: epoch 001:   5077 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.1, ups=0.87, wpb=253.9, bsz=100, num_updates=5070, lr=4.41775e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5897
2023-03-14 00:55:34 - progress_bar.py[line:274] - INFO: epoch 001:   5087 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.9, ups=0.86, wpb=254.9, bsz=100, num_updates=5080, lr=4.41608e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5908
2023-03-14 00:55:45 - progress_bar.py[line:274] - INFO: epoch 001:   5097 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=224, ups=0.88, wpb=255.2, bsz=100, num_updates=5090, lr=4.41441e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5920
2023-03-14 00:55:57 - progress_bar.py[line:274] - INFO: epoch 001:   5107 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.7, ups=0.87, wpb=254.8, bsz=100, num_updates=5100, lr=4.41275e-05, gnorm=0.66, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5931
2023-03-14 00:56:09 - progress_bar.py[line:274] - INFO: epoch 001:   5117 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=213.8, ups=0.85, wpb=252.5, bsz=100, num_updates=5110, lr=4.41108e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5943
2023-03-14 00:56:20 - progress_bar.py[line:274] - INFO: epoch 001:   5127 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.1, ups=0.85, wpb=254.3, bsz=100, num_updates=5120, lr=4.40941e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=5955
2023-03-14 00:56:32 - progress_bar.py[line:274] - INFO: epoch 001:   5137 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=211.9, ups=0.84, wpb=251.4, bsz=100, num_updates=5130, lr=4.40774e-05, gnorm=0.743, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=5967
2023-03-14 00:56:44 - progress_bar.py[line:274] - INFO: epoch 001:   5147 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.8, ups=0.86, wpb=252.1, bsz=100, num_updates=5140, lr=4.40608e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=5978
2023-03-14 00:56:55 - progress_bar.py[line:274] - INFO: epoch 001:   5157 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.4, ups=0.88, wpb=250.6, bsz=100, num_updates=5150, lr=4.40441e-05, gnorm=0.69, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5990
2023-03-14 00:57:07 - progress_bar.py[line:274] - INFO: epoch 001:   5167 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=225.4, ups=0.89, wpb=254.5, bsz=100, num_updates=5160, lr=4.40274e-05, gnorm=0.68, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6001
2023-03-14 00:57:18 - progress_bar.py[line:274] - INFO: epoch 001:   5177 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=226.4, ups=0.89, wpb=255.3, bsz=100, num_updates=5170, lr=4.40107e-05, gnorm=0.692, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6012
2023-03-14 00:57:30 - progress_bar.py[line:274] - INFO: epoch 001:   5187 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.3, ups=0.86, wpb=251.6, bsz=100, num_updates=5180, lr=4.39941e-05, gnorm=0.682, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6024
2023-03-14 00:57:41 - progress_bar.py[line:274] - INFO: epoch 001:   5197 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221.9, ups=0.87, wpb=253.9, bsz=100, num_updates=5190, lr=4.39774e-05, gnorm=0.627, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6036
2023-03-14 00:57:53 - progress_bar.py[line:274] - INFO: epoch 001:   5207 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.2, ups=0.86, wpb=252, bsz=100, num_updates=5200, lr=4.39607e-05, gnorm=0.63, clip=0, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6047
2023-03-14 00:58:04 - progress_bar.py[line:274] - INFO: epoch 001:   5217 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.9, ups=0.86, wpb=254.2, bsz=100, num_updates=5210, lr=4.3944e-05, gnorm=0.714, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6059
2023-03-14 00:58:15 - progress_bar.py[line:274] - INFO: epoch 001:   5227 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=257.2, nsentences=100, sample_size=257.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=227.5, ups=0.88, wpb=257.2, bsz=100, num_updates=5220, lr=4.39274e-05, gnorm=0.656, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6070
2023-03-14 00:58:27 - progress_bar.py[line:274] - INFO: epoch 001:   5237 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.7, ups=0.85, wpb=252.5, bsz=100, num_updates=5230, lr=4.39107e-05, gnorm=0.671, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6082
2023-03-14 00:58:39 - progress_bar.py[line:274] - INFO: epoch 001:   5247 / 6313 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.5, ups=0.86, wpb=254.3, bsz=100, num_updates=5240, lr=4.3894e-05, gnorm=0.754, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6093
2023-03-14 00:58:50 - progress_bar.py[line:274] - INFO: epoch 001:   5257 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.5, ups=0.87, wpb=254.8, bsz=100, num_updates=5250, lr=4.38773e-05, gnorm=0.703, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6105
2023-03-14 00:59:02 - progress_bar.py[line:274] - INFO: epoch 001:   5267 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.9, ups=0.86, wpb=252.4, bsz=100, num_updates=5260, lr=4.38607e-05, gnorm=0.679, clip=0, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6117
2023-03-14 00:59:14 - progress_bar.py[line:274] - INFO: epoch 001:   5277 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.6, ups=0.86, wpb=253.2, bsz=100, num_updates=5270, lr=4.3844e-05, gnorm=0.723, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6128
2023-03-14 00:59:25 - progress_bar.py[line:274] - INFO: epoch 001:   5287 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.2, ups=0.86, wpb=254, bsz=100, num_updates=5280, lr=4.38273e-05, gnorm=0.771, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6140
2023-03-14 00:59:37 - progress_bar.py[line:274] - INFO: epoch 001:   5297 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=223.7, ups=0.87, wpb=255.7, bsz=100, num_updates=5290, lr=4.38107e-05, gnorm=0.707, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6151
2023-03-14 00:59:48 - progress_bar.py[line:274] - INFO: epoch 001:   5307 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=257.6, nsentences=100, sample_size=257.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=225.5, ups=0.88, wpb=257.6, bsz=100, num_updates=5300, lr=4.3794e-05, gnorm=0.623, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6163
2023-03-14 01:00:00 - progress_bar.py[line:274] - INFO: epoch 001:   5317 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.2, ups=0.86, wpb=253.9, bsz=100, num_updates=5310, lr=4.37773e-05, gnorm=0.694, clip=0, loss_scale=1024, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=6174
2023-03-14 01:00:11 - progress_bar.py[line:274] - INFO: epoch 001:   5327 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218, ups=0.86, wpb=252, bsz=100, num_updates=5320, lr=4.37606e-05, gnorm=0.672, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6186
2023-03-14 01:00:17 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 01:00:24 - progress_bar.py[line:274] - INFO: epoch 001:   5338 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=206.7, ups=0.81, wpb=254.4, bsz=100, num_updates=5330, lr=4.3744e-05, gnorm=0.655, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6198
2023-03-14 01:00:35 - progress_bar.py[line:274] - INFO: epoch 001:   5348 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.2, ups=0.86, wpb=251.7, bsz=100, num_updates=5340, lr=4.37273e-05, gnorm=0.646, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6210
2023-03-14 01:00:47 - progress_bar.py[line:274] - INFO: epoch 001:   5358 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.3, ups=0.85, wpb=254.5, bsz=100, num_updates=5350, lr=4.37106e-05, gnorm=0.687, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6222
2023-03-14 01:00:59 - progress_bar.py[line:274] - INFO: epoch 001:   5368 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.4, ups=0.86, wpb=253.9, bsz=100, num_updates=5360, lr=4.36939e-05, gnorm=0.792, clip=10, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=6233
2023-03-14 01:01:10 - progress_bar.py[line:274] - INFO: epoch 001:   5378 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=213.7, ups=0.85, wpb=250.7, bsz=100, num_updates=5370, lr=4.36773e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6245
2023-03-14 01:01:22 - progress_bar.py[line:274] - INFO: epoch 001:   5388 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.5, ups=0.86, wpb=252, bsz=100, num_updates=5380, lr=4.36606e-05, gnorm=0.765, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6257
2023-03-14 01:01:34 - progress_bar.py[line:274] - INFO: epoch 001:   5398 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.9, ups=0.87, wpb=252.8, bsz=100, num_updates=5390, lr=4.36439e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6268
2023-03-14 01:01:45 - progress_bar.py[line:274] - INFO: epoch 001:   5408 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219, ups=0.86, wpb=254.1, bsz=100, num_updates=5400, lr=4.36272e-05, gnorm=0.694, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6280
2023-03-14 01:01:57 - progress_bar.py[line:274] - INFO: epoch 001:   5418 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.4, ups=0.87, wpb=252.2, bsz=100, num_updates=5410, lr=4.36106e-05, gnorm=0.77, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6291
2023-03-14 01:02:08 - progress_bar.py[line:274] - INFO: epoch 001:   5428 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.4, ups=0.86, wpb=252.6, bsz=100, num_updates=5420, lr=4.35939e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6303
2023-03-14 01:02:20 - progress_bar.py[line:274] - INFO: epoch 001:   5438 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.4, ups=0.86, wpb=253.9, bsz=100, num_updates=5430, lr=4.35772e-05, gnorm=0.651, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6314
2023-03-14 01:02:31 - progress_bar.py[line:274] - INFO: epoch 001:   5448 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.2, ups=0.86, wpb=254.6, bsz=100, num_updates=5440, lr=4.35605e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6326
2023-03-14 01:02:43 - progress_bar.py[line:274] - INFO: epoch 001:   5458 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.6, ups=0.86, wpb=251.9, bsz=100, num_updates=5450, lr=4.35439e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6338
2023-03-14 01:02:55 - progress_bar.py[line:274] - INFO: epoch 001:   5468 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.1, ups=0.86, wpb=255.2, bsz=100, num_updates=5460, lr=4.35272e-05, gnorm=0.684, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6349
2023-03-14 01:03:06 - progress_bar.py[line:274] - INFO: epoch 001:   5478 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.3, ups=0.85, wpb=253.2, bsz=100, num_updates=5470, lr=4.35105e-05, gnorm=0.654, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6361
2023-03-14 01:03:18 - progress_bar.py[line:274] - INFO: epoch 001:   5488 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216, ups=0.86, wpb=250.4, bsz=100, num_updates=5480, lr=4.34938e-05, gnorm=0.698, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6372
2023-03-14 01:03:30 - progress_bar.py[line:274] - INFO: epoch 001:   5498 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.3, ups=0.85, wpb=253.8, bsz=100, num_updates=5490, lr=4.34772e-05, gnorm=0.768, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6384
2023-03-14 01:03:41 - progress_bar.py[line:274] - INFO: epoch 001:   5508 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.7, ups=0.86, wpb=256.3, bsz=100, num_updates=5500, lr=4.34605e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=6396
2023-03-14 01:03:53 - progress_bar.py[line:274] - INFO: epoch 001:   5518 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.1, ups=0.86, wpb=253.8, bsz=100, num_updates=5510, lr=4.34438e-05, gnorm=0.691, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6408
2023-03-14 01:04:05 - progress_bar.py[line:274] - INFO: epoch 001:   5528 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=250, nsentences=100, sample_size=250, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.2, ups=0.86, wpb=250, bsz=100, num_updates=5520, lr=4.34272e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6419
2023-03-14 01:04:16 - progress_bar.py[line:274] - INFO: epoch 001:   5538 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.9, ups=0.87, wpb=253.7, bsz=100, num_updates=5530, lr=4.34105e-05, gnorm=0.766, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6431
2023-03-14 01:04:28 - progress_bar.py[line:274] - INFO: epoch 001:   5548 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.2, ups=0.86, wpb=252.8, bsz=100, num_updates=5540, lr=4.33938e-05, gnorm=0.685, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6442
2023-03-14 01:04:39 - progress_bar.py[line:274] - INFO: epoch 001:   5558 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=223, ups=0.88, wpb=254.9, bsz=100, num_updates=5550, lr=4.33771e-05, gnorm=0.635, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6454
2023-03-14 01:04:51 - progress_bar.py[line:274] - INFO: epoch 001:   5568 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.4, ups=0.86, wpb=252.3, bsz=100, num_updates=5560, lr=4.33605e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6465
2023-03-14 01:05:02 - progress_bar.py[line:274] - INFO: epoch 001:   5578 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.5, ups=0.86, wpb=256.5, bsz=100, num_updates=5570, lr=4.33438e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6477
2023-03-14 01:05:14 - progress_bar.py[line:274] - INFO: epoch 001:   5588 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.2, ups=0.87, wpb=254.2, bsz=100, num_updates=5580, lr=4.33271e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6488
2023-03-14 01:05:26 - progress_bar.py[line:274] - INFO: epoch 001:   5598 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.7, ups=0.85, wpb=253.5, bsz=100, num_updates=5590, lr=4.33104e-05, gnorm=0.783, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6500
2023-03-14 01:05:37 - progress_bar.py[line:274] - INFO: epoch 001:   5608 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=214.6, ups=0.86, wpb=251, bsz=100, num_updates=5600, lr=4.32938e-05, gnorm=0.687, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6512
2023-03-14 01:05:49 - progress_bar.py[line:274] - INFO: epoch 001:   5618 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=214, ups=0.85, wpb=252.2, bsz=100, num_updates=5610, lr=4.32771e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6524
2023-03-14 01:06:00 - progress_bar.py[line:274] - INFO: epoch 001:   5628 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.9, ups=0.88, wpb=254.4, bsz=100, num_updates=5620, lr=4.32604e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6535
2023-03-14 01:06:12 - progress_bar.py[line:274] - INFO: epoch 001:   5638 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.3, ups=0.86, wpb=250.4, bsz=100, num_updates=5630, lr=4.32437e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=6547
2023-03-14 01:06:24 - progress_bar.py[line:274] - INFO: epoch 001:   5648 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.9, ups=0.86, wpb=255.9, bsz=100, num_updates=5640, lr=4.32271e-05, gnorm=0.617, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6558
2023-03-14 01:06:35 - progress_bar.py[line:274] - INFO: epoch 001:   5658 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.3, ups=0.86, wpb=255.8, bsz=100, num_updates=5650, lr=4.32104e-05, gnorm=0.665, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6570
2023-03-14 01:06:47 - progress_bar.py[line:274] - INFO: epoch 001:   5668 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=225.4, ups=0.88, wpb=255.1, bsz=100, num_updates=5660, lr=4.31937e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6581
2023-03-14 01:06:58 - progress_bar.py[line:274] - INFO: epoch 001:   5678 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217, ups=0.86, wpb=251.3, bsz=100, num_updates=5670, lr=4.3177e-05, gnorm=0.651, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6593
2023-03-14 01:07:10 - progress_bar.py[line:274] - INFO: epoch 001:   5688 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.1, ups=0.86, wpb=254.8, bsz=100, num_updates=5680, lr=4.31604e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6604
2023-03-14 01:07:21 - progress_bar.py[line:274] - INFO: epoch 001:   5698 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221.6, ups=0.88, wpb=253.2, bsz=100, num_updates=5690, lr=4.31437e-05, gnorm=0.665, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6616
2023-03-14 01:07:33 - progress_bar.py[line:274] - INFO: epoch 001:   5708 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.9, ups=0.86, wpb=252.8, bsz=100, num_updates=5700, lr=4.3127e-05, gnorm=0.639, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6627
2023-03-14 01:07:44 - progress_bar.py[line:274] - INFO: epoch 001:   5718 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.7, ups=0.86, wpb=253.8, bsz=100, num_updates=5710, lr=4.31103e-05, gnorm=0.733, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6639
2023-03-14 01:07:56 - progress_bar.py[line:274] - INFO: epoch 001:   5728 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.3, ups=0.86, wpb=253.9, bsz=100, num_updates=5720, lr=4.30937e-05, gnorm=0.687, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6651
2023-03-14 01:08:08 - progress_bar.py[line:274] - INFO: epoch 001:   5738 / 6313 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.6, ups=0.86, wpb=254, bsz=100, num_updates=5730, lr=4.3077e-05, gnorm=0.688, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6662
2023-03-14 01:08:19 - progress_bar.py[line:274] - INFO: epoch 001:   5748 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=222.2, ups=0.87, wpb=254.2, bsz=100, num_updates=5740, lr=4.30603e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6674
2023-03-14 01:08:31 - progress_bar.py[line:274] - INFO: epoch 001:   5758 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.1, ups=0.86, wpb=252.8, bsz=100, num_updates=5750, lr=4.30437e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=6685
2023-03-14 01:08:42 - progress_bar.py[line:274] - INFO: epoch 001:   5768 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.1, ups=0.86, wpb=254.3, bsz=100, num_updates=5760, lr=4.3027e-05, gnorm=0.761, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6697
2023-03-14 01:08:54 - progress_bar.py[line:274] - INFO: epoch 001:   5778 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.7, ups=0.87, wpb=253.8, bsz=100, num_updates=5770, lr=4.30103e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=6709
2023-03-14 01:09:06 - progress_bar.py[line:274] - INFO: epoch 001:   5788 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.2, ups=0.86, wpb=253.6, bsz=100, num_updates=5780, lr=4.29936e-05, gnorm=0.75, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6720
2023-03-14 01:09:17 - progress_bar.py[line:274] - INFO: epoch 001:   5798 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.9, ups=0.87, wpb=255.3, bsz=100, num_updates=5790, lr=4.2977e-05, gnorm=0.577, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6732
2023-03-14 01:09:29 - progress_bar.py[line:274] - INFO: epoch 001:   5808 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219, ups=0.87, wpb=253.1, bsz=100, num_updates=5800, lr=4.29603e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6743
2023-03-14 01:09:40 - progress_bar.py[line:274] - INFO: epoch 001:   5818 / 6313 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.2, ups=0.86, wpb=250.5, bsz=100, num_updates=5810, lr=4.29436e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6755
2023-03-14 01:09:52 - progress_bar.py[line:274] - INFO: epoch 001:   5828 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.4, ups=0.87, wpb=252.1, bsz=100, num_updates=5820, lr=4.29269e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6766
2023-03-14 01:10:03 - progress_bar.py[line:274] - INFO: epoch 001:   5838 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.1, ups=0.86, wpb=255.3, bsz=100, num_updates=5830, lr=4.29103e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6778
2023-03-14 01:10:15 - progress_bar.py[line:274] - INFO: epoch 001:   5848 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=215.7, ups=0.86, wpb=250.3, bsz=100, num_updates=5840, lr=4.28936e-05, gnorm=0.723, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6790
2023-03-14 01:10:27 - progress_bar.py[line:274] - INFO: epoch 001:   5858 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.9, ups=0.86, wpb=253.6, bsz=100, num_updates=5850, lr=4.28769e-05, gnorm=0.694, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6801
2023-03-14 01:10:34 - trainer.py[line:1008] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 01:10:39 - progress_bar.py[line:274] - INFO: epoch 001:   5869 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=201, ups=0.79, wpb=253.4, bsz=100, num_updates=5860, lr=4.28602e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=6814
2023-03-14 01:10:51 - progress_bar.py[line:274] - INFO: epoch 001:   5879 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.6, ups=0.86, wpb=253.2, bsz=100, num_updates=5870, lr=4.28436e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6825
2023-03-14 01:11:02 - progress_bar.py[line:274] - INFO: epoch 001:   5889 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.2, ups=0.87, wpb=253.3, bsz=100, num_updates=5880, lr=4.28269e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6837
2023-03-14 01:11:14 - progress_bar.py[line:274] - INFO: epoch 001:   5899 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.2, ups=0.86, wpb=254.1, bsz=100, num_updates=5890, lr=4.28102e-05, gnorm=0.694, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=6849
2023-03-14 01:11:26 - progress_bar.py[line:274] - INFO: epoch 001:   5909 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.3, ups=0.86, wpb=251.2, bsz=100, num_updates=5900, lr=4.27935e-05, gnorm=0.716, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6860
2023-03-14 01:11:37 - progress_bar.py[line:274] - INFO: epoch 001:   5919 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.9, ups=0.87, wpb=254.1, bsz=100, num_updates=5910, lr=4.27769e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6872
2023-03-14 01:11:49 - progress_bar.py[line:274] - INFO: epoch 001:   5929 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.8, ups=0.86, wpb=253.1, bsz=100, num_updates=5920, lr=4.27602e-05, gnorm=0.665, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6883
2023-03-14 01:12:00 - progress_bar.py[line:274] - INFO: epoch 001:   5939 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.9, ups=0.86, wpb=252.9, bsz=100, num_updates=5930, lr=4.27435e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6895
2023-03-14 01:12:12 - progress_bar.py[line:274] - INFO: epoch 001:   5949 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=222.3, ups=0.88, wpb=253.8, bsz=100, num_updates=5940, lr=4.27268e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6906
2023-03-14 01:12:23 - progress_bar.py[line:274] - INFO: epoch 001:   5959 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.4, ups=0.86, wpb=254.1, bsz=100, num_updates=5950, lr=4.27102e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6918
2023-03-14 01:12:35 - progress_bar.py[line:274] - INFO: epoch 001:   5969 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.9, ups=0.86, wpb=252, bsz=100, num_updates=5960, lr=4.26935e-05, gnorm=0.695, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6930
2023-03-14 01:12:47 - progress_bar.py[line:274] - INFO: epoch 001:   5979 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.7, ups=0.85, wpb=255.1, bsz=100, num_updates=5970, lr=4.26768e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6941
2023-03-14 01:12:58 - progress_bar.py[line:274] - INFO: epoch 001:   5989 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.9, ups=0.86, wpb=255.7, bsz=100, num_updates=5980, lr=4.26602e-05, gnorm=0.63, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6953
2023-03-14 01:13:10 - progress_bar.py[line:274] - INFO: epoch 001:   5999 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.5, ups=0.86, wpb=255.7, bsz=100, num_updates=5990, lr=4.26435e-05, gnorm=0.686, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6965
2023-03-14 01:13:22 - progress_bar.py[line:274] - INFO: epoch 001:   6009 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.1, ups=0.86, wpb=254.7, bsz=100, num_updates=6000, lr=4.26268e-05, gnorm=0.698, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6976
2023-03-14 01:13:33 - progress_bar.py[line:274] - INFO: epoch 001:   6019 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.8, ups=0.86, wpb=253.2, bsz=100, num_updates=6010, lr=4.26101e-05, gnorm=0.717, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6988
2023-03-14 01:13:45 - progress_bar.py[line:274] - INFO: epoch 001:   6029 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.7, ups=0.86, wpb=254.4, bsz=100, num_updates=6020, lr=4.25935e-05, gnorm=0.67, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6999
2023-03-14 01:13:56 - progress_bar.py[line:274] - INFO: epoch 001:   6039 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.6, ups=0.86, wpb=255.2, bsz=100, num_updates=6030, lr=4.25768e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=7011
2023-03-14 01:14:08 - progress_bar.py[line:274] - INFO: epoch 001:   6049 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.1, ups=0.86, wpb=254.3, bsz=100, num_updates=6040, lr=4.25601e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=7023
2023-03-14 01:14:20 - progress_bar.py[line:274] - INFO: epoch 001:   6059 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.7, ups=0.87, wpb=253.9, bsz=100, num_updates=6050, lr=4.25434e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=7034
2023-03-14 01:14:31 - progress_bar.py[line:274] - INFO: epoch 001:   6069 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.2, ups=0.86, wpb=252.5, bsz=100, num_updates=6060, lr=4.25268e-05, gnorm=0.625, clip=0, loss_scale=512, train_wall=12, gb_free=11.2, ema_decay=0.9999, wall=7046
2023-03-14 01:14:43 - progress_bar.py[line:274] - INFO: epoch 001:   6079 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.3, ups=0.86, wpb=251.7, bsz=100, num_updates=6070, lr=4.25101e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=7057
2023-03-14 01:14:54 - progress_bar.py[line:274] - INFO: epoch 001:   6089 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220, ups=0.87, wpb=253, bsz=100, num_updates=6080, lr=4.24934e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7069
2023-03-14 01:15:06 - progress_bar.py[line:274] - INFO: epoch 001:   6099 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.6, ups=0.85, wpb=255.7, bsz=100, num_updates=6090, lr=4.24767e-05, gnorm=0.698, clip=0, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=7081
2023-03-14 01:15:18 - progress_bar.py[line:274] - INFO: epoch 001:   6109 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=249.9, nsentences=100, sample_size=249.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216, ups=0.86, wpb=249.9, bsz=100, num_updates=6100, lr=4.24601e-05, gnorm=0.626, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=7092
2023-03-14 01:15:29 - progress_bar.py[line:274] - INFO: epoch 001:   6119 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218, ups=0.86, wpb=252.5, bsz=100, num_updates=6110, lr=4.24434e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=7104
2023-03-14 01:15:41 - progress_bar.py[line:274] - INFO: epoch 001:   6129 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.7, ups=0.86, wpb=251.7, bsz=100, num_updates=6120, lr=4.24267e-05, gnorm=0.655, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7115
2023-03-14 01:15:52 - progress_bar.py[line:274] - INFO: epoch 001:   6139 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=257.4, nsentences=100, sample_size=257.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.6, ups=0.86, wpb=257.4, bsz=100, num_updates=6130, lr=4.241e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7127
2023-03-14 01:16:04 - progress_bar.py[line:274] - INFO: epoch 001:   6149 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217, ups=0.86, wpb=251.4, bsz=100, num_updates=6140, lr=4.23934e-05, gnorm=0.708, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7139
2023-03-14 01:16:16 - progress_bar.py[line:274] - INFO: epoch 001:   6159 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219, ups=0.86, wpb=253.7, bsz=100, num_updates=6150, lr=4.23767e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=7150
2023-03-14 01:16:27 - progress_bar.py[line:274] - INFO: epoch 001:   6169 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.5, ups=0.86, wpb=251.8, bsz=100, num_updates=6160, lr=4.236e-05, gnorm=0.685, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=7162
2023-03-14 01:16:39 - progress_bar.py[line:274] - INFO: epoch 001:   6179 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.6, ups=0.87, wpb=252.4, bsz=100, num_updates=6170, lr=4.23433e-05, gnorm=0.721, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7173
2023-03-14 01:16:50 - progress_bar.py[line:274] - INFO: epoch 001:   6189 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.4, ups=0.86, wpb=253.7, bsz=100, num_updates=6180, lr=4.23267e-05, gnorm=0.714, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=7185
2023-03-14 01:17:02 - progress_bar.py[line:274] - INFO: epoch 001:   6199 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.2, ups=0.86, wpb=254.2, bsz=100, num_updates=6190, lr=4.231e-05, gnorm=0.698, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7196
2023-03-14 01:17:14 - progress_bar.py[line:274] - INFO: epoch 001:   6209 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.9, ups=0.85, wpb=255.2, bsz=100, num_updates=6200, lr=4.22933e-05, gnorm=0.748, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=7208
2023-03-14 01:17:25 - progress_bar.py[line:274] - INFO: epoch 001:   6219 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=215.6, ups=0.85, wpb=252.5, bsz=100, num_updates=6210, lr=4.22767e-05, gnorm=0.694, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7220
2023-03-14 01:17:37 - progress_bar.py[line:274] - INFO: epoch 001:   6229 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.5, ups=0.86, wpb=251.7, bsz=100, num_updates=6220, lr=4.226e-05, gnorm=0.752, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7231
2023-03-14 01:17:49 - progress_bar.py[line:274] - INFO: epoch 001:   6239 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.1, ups=0.86, wpb=253.7, bsz=100, num_updates=6230, lr=4.22433e-05, gnorm=0.733, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7243
2023-03-14 01:18:00 - progress_bar.py[line:274] - INFO: epoch 001:   6249 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.5, ups=0.86, wpb=253.2, bsz=100, num_updates=6240, lr=4.22266e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7255
2023-03-14 01:18:12 - progress_bar.py[line:274] - INFO: epoch 001:   6259 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.7, ups=0.86, wpb=253.1, bsz=100, num_updates=6250, lr=4.221e-05, gnorm=0.676, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=7266
2023-03-14 01:18:23 - progress_bar.py[line:274] - INFO: epoch 001:   6269 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.2, ups=0.86, wpb=254, bsz=100, num_updates=6260, lr=4.21933e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=7278
2023-03-14 01:18:35 - progress_bar.py[line:274] - INFO: epoch 001:   6279 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.6, ups=0.88, wpb=254.3, bsz=100, num_updates=6270, lr=4.21766e-05, gnorm=0.65, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=7289
2023-03-14 01:18:46 - progress_bar.py[line:274] - INFO: epoch 001:   6289 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.3, ups=0.86, wpb=250.4, bsz=100, num_updates=6280, lr=4.21599e-05, gnorm=0.746, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7301
2023-03-14 01:18:58 - progress_bar.py[line:274] - INFO: epoch 001:   6299 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223.2, ups=0.88, wpb=252.6, bsz=100, num_updates=6290, lr=4.21433e-05, gnorm=0.671, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7312
2023-03-14 01:19:09 - progress_bar.py[line:274] - INFO: epoch 001:   6309 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.8, ups=0.86, wpb=252.1, bsz=100, num_updates=6300, lr=4.21266e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7324
2023-03-14 01:19:14 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-03-14 01:19:14 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-03-14 01:19:15 - train.py[line:549] - INFO: 0 / 7052
2023-03-14 01:19:15 - train.py[line:551] - INFO: load:1.23 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-03-14 01:21:57 - train.py[line:549] - INFO: 200 / 7052
2023-03-14 01:21:57 - train.py[line:551] - INFO: load:1.25 valid_run:161.66 task_valid:149.51 collect_output:11.09
2023-03-14 01:24:31 - train.py[line:549] - INFO: 400 / 7052
2023-03-14 01:24:31 - train.py[line:551] - INFO: load:1.27 valid_run:315.75 task_valid:296.64 collect_output:17.00
2023-03-14 01:27:07 - train.py[line:549] - INFO: 600 / 7052
2023-03-14 01:27:07 - train.py[line:551] - INFO: load:1.30 valid_run:471.80 task_valid:441.75 collect_output:26.90
2023-03-14 01:29:46 - train.py[line:549] - INFO: 800 / 7052
2023-03-14 01:29:46 - train.py[line:551] - INFO: load:1.32 valid_run:630.69 task_valid:587.15 collect_output:39.36
2023-03-14 01:32:25 - train.py[line:549] - INFO: 1000 / 7052
2023-03-14 01:32:25 - train.py[line:551] - INFO: load:1.34 valid_run:789.05 task_valid:734.52 collect_output:49.31
2023-03-14 01:35:01 - train.py[line:549] - INFO: 1200 / 7052
2023-03-14 01:35:01 - train.py[line:551] - INFO: load:1.37 valid_run:945.49 task_valid:882.49 collect_output:56.76
2023-03-14 01:37:38 - train.py[line:549] - INFO: 1400 / 7052
2023-03-14 01:37:38 - train.py[line:551] - INFO: load:1.39 valid_run:1102.54 task_valid:1032.67 collect_output:62.59
2023-03-14 01:40:13 - train.py[line:549] - INFO: 1600 / 7052
2023-03-14 01:40:13 - train.py[line:551] - INFO: load:1.41 valid_run:1257.60 task_valid:1181.62 collect_output:67.66
2023-03-14 01:42:50 - train.py[line:549] - INFO: 1800 / 7052
2023-03-14 01:42:50 - train.py[line:551] - INFO: load:1.43 valid_run:1414.51 task_valid:1332.24 collect_output:72.91
2023-03-14 01:45:26 - train.py[line:549] - INFO: 2000 / 7052
2023-03-14 01:45:26 - train.py[line:551] - INFO: load:1.46 valid_run:1570.24 task_valid:1481.31 collect_output:78.53
2023-03-14 01:48:01 - train.py[line:549] - INFO: 2200 / 7052
2023-03-14 01:48:01 - train.py[line:551] - INFO: load:1.48 valid_run:1725.50 task_valid:1630.89 collect_output:83.16
2023-03-14 01:50:36 - train.py[line:549] - INFO: 2400 / 7052
2023-03-14 01:50:36 - train.py[line:551] - INFO: load:1.50 valid_run:1880.46 task_valid:1776.68 collect_output:91.29
2023-03-14 01:53:11 - train.py[line:549] - INFO: 2600 / 7052
2023-03-14 01:53:11 - train.py[line:551] - INFO: load:1.53 valid_run:2035.36 task_valid:1923.87 collect_output:97.96
2023-03-14 01:55:48 - train.py[line:549] - INFO: 2800 / 7052
2023-03-14 01:55:48 - train.py[line:551] - INFO: load:1.55 valid_run:2191.55 task_valid:2070.63 collect_output:106.34
2023-03-14 01:58:22 - train.py[line:549] - INFO: 3000 / 7052
2023-03-14 01:58:22 - train.py[line:551] - INFO: load:1.57 valid_run:2346.38 task_valid:2217.18 collect_output:113.57
2023-03-14 02:00:59 - train.py[line:549] - INFO: 3200 / 7052
2023-03-14 02:00:59 - train.py[line:551] - INFO: load:1.59 valid_run:2502.38 task_valid:2366.22 collect_output:119.50
2023-03-14 02:03:34 - train.py[line:549] - INFO: 3400 / 7052
2023-03-14 02:03:34 - train.py[line:551] - INFO: load:1.62 valid_run:2657.58 task_valid:2512.90 collect_output:127.00
2023-03-14 02:06:08 - train.py[line:549] - INFO: 3600 / 7052
2023-03-14 02:06:08 - train.py[line:551] - INFO: load:1.64 valid_run:2812.01 task_valid:2659.72 collect_output:133.59
2023-03-14 02:08:46 - train.py[line:549] - INFO: 3800 / 7052
2023-03-14 02:08:46 - train.py[line:551] - INFO: load:1.66 valid_run:2969.20 task_valid:2810.01 collect_output:139.47
2023-03-14 02:11:18 - train.py[line:549] - INFO: 4000 / 7052
2023-03-14 02:11:18 - train.py[line:551] - INFO: load:1.69 valid_run:3121.85 task_valid:2954.95 collect_output:146.14
2023-03-14 02:13:54 - train.py[line:549] - INFO: 4200 / 7052
2023-03-14 02:13:54 - train.py[line:551] - INFO: load:1.71 valid_run:3277.24 task_valid:3099.94 collect_output:155.49
2023-03-14 02:16:28 - train.py[line:549] - INFO: 4400 / 7052
2023-03-14 02:16:28 - train.py[line:551] - INFO: load:1.73 valid_run:3431.58 task_valid:3249.64 collect_output:159.09
2023-03-14 02:19:04 - train.py[line:549] - INFO: 4600 / 7052
2023-03-14 02:19:04 - train.py[line:551] - INFO: load:1.76 valid_run:3587.41 task_valid:3395.50 collect_output:168.03
2023-03-14 02:21:40 - train.py[line:549] - INFO: 4800 / 7052
2023-03-14 02:21:40 - train.py[line:551] - INFO: load:1.78 valid_run:3743.03 task_valid:3540.14 collect_output:177.99
2023-03-14 02:24:15 - train.py[line:549] - INFO: 5000 / 7052
2023-03-14 02:24:15 - train.py[line:551] - INFO: load:1.80 valid_run:3898.66 task_valid:3687.05 collect_output:185.69
2023-03-14 02:26:52 - train.py[line:549] - INFO: 5200 / 7052
2023-03-14 02:26:52 - train.py[line:551] - INFO: load:1.83 valid_run:4055.30 task_valid:3838.29 collect_output:190.06
2023-03-14 02:29:28 - train.py[line:549] - INFO: 5400 / 7052
2023-03-14 02:29:28 - train.py[line:551] - INFO: load:1.85 valid_run:4210.73 task_valid:3982.97 collect_output:199.78
2023-03-14 02:32:03 - train.py[line:549] - INFO: 5600 / 7052
2023-03-14 02:32:03 - train.py[line:551] - INFO: load:1.87 valid_run:4366.51 task_valid:4127.83 collect_output:209.65
2023-03-14 02:34:39 - train.py[line:549] - INFO: 5800 / 7052
2023-03-14 02:34:39 - train.py[line:551] - INFO: load:1.90 valid_run:4522.41 task_valid:4273.69 collect_output:218.65
2023-03-14 02:37:15 - train.py[line:549] - INFO: 6000 / 7052
2023-03-14 02:37:15 - train.py[line:551] - INFO: load:1.92 valid_run:4677.90 task_valid:4421.07 collect_output:225.71
2023-03-14 02:39:52 - train.py[line:549] - INFO: 6200 / 7052
2023-03-14 02:39:52 - train.py[line:551] - INFO: load:1.94 valid_run:4834.81 task_valid:4568.19 collect_output:234.47
2023-03-14 02:42:29 - train.py[line:549] - INFO: 6400 / 7052
2023-03-14 02:42:29 - train.py[line:551] - INFO: load:1.97 valid_run:4991.53 task_valid:4712.90 collect_output:245.43
2023-03-14 02:45:06 - train.py[line:549] - INFO: 6600 / 7052
2023-03-14 02:45:06 - train.py[line:551] - INFO: load:1.99 valid_run:5148.40 task_valid:4863.57 collect_output:250.58
2023-03-14 02:47:42 - train.py[line:549] - INFO: 6800 / 7052
2023-03-14 02:47:42 - train.py[line:551] - INFO: load:2.01 valid_run:5305.18 task_valid:5012.60 collect_output:257.29
2023-03-14 02:50:17 - train.py[line:549] - INFO: 7000 / 7052
2023-03-14 02:50:17 - train.py[line:551] - INFO: load:2.04 valid_run:5460.20 task_valid:5162.61 collect_output:261.25

====================================================================================================
SGG eval:     R @ 50: 0.6088;     R @ 100: 0.6364;     R @ 500: 0.6518;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1414;    mR @ 100: 0.1590;    mR @ 500: 0.1869;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0576) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.2842) (attached to:0.0000) (behind:0.2917) (belonging to:0.0000) (between:0.0000) (carrying:0.7202) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7409) (holding:0.2558) (in:0.3157) (in front of:0.1113) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4213) (of:0.4101) (on:0.9131) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.3333) (says:0.0000) (sitting on:0.2131) (standing on:0.0000) (to:0.0000) (under:0.1944) (using:1.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.3333) (wearing:0.9806) (wears:0.0000) (with:0.0845) 
--------------------------------------------------------
====================================================================================================

2023-03-14 02:51:19 - train.py[line:487] - INFO: 0.6363613849173533

====================================================================================================
SGG eval:     R @ 50: 0.6088;     R @ 100: 0.6364;     R @ 500: 0.6518;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1414;    mR @ 100: 0.1590;    mR @ 500: 0.1869;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0576) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.2842) (attached to:0.0000) (behind:0.2917) (belonging to:0.0000) (between:0.0000) (carrying:0.7202) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7409) (holding:0.2558) (in:0.3157) (in front of:0.1113) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4213) (of:0.4101) (on:0.9131) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.3333) (says:0.0000) (sitting on:0.2131) (standing on:0.0000) (to:0.0000) (under:0.1944) (using:1.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.3333) (wearing:0.9806) (wears:0.0000) (with:0.0845) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6088;     R @ 100: 0.6364;     R @ 500: 0.6518;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1414;    mR @ 100: 0.1590;    mR @ 500: 0.1869;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0576) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.2842) (attached to:0.0000) (behind:0.2917) (belonging to:0.0000) (between:0.0000) (carrying:0.7202) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7409) (holding:0.2558) (in:0.3157) (in front of:0.1113) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4213) (of:0.4101) (on:0.9131) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.3333) (says:0.0000) (sitting on:0.2131) (standing on:0.0000) (to:0.0000) (under:0.1944) (using:1.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.3333) (wearing:0.9806) (wears:0.0000) (with:0.0845) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6088;     R @ 100: 0.6364;     R @ 500: 0.6518;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1414;    mR @ 100: 0.1590;    mR @ 500: 0.1869;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0576) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.2842) (attached to:0.0000) (behind:0.2917) (belonging to:0.0000) (between:0.0000) (carrying:0.7202) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7409) (holding:0.2558) (in:0.3157) (in front of:0.1113) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4213) (of:0.4101) (on:0.9131) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.3333) (says:0.0000) (sitting on:0.2131) (standing on:0.0000) (to:0.0000) (under:0.1944) (using:1.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.3333) (wearing:0.9806) (wears:0.0000) (with:0.0845) 
--------------------------------------------------------
====================================================================================================

2023-03-14 02:51:19 - train.py[line:578] - INFO: logits:torch.Size([282060, 51]) sample_ids:torch.Size([282060])
2023-03-14 02:51:19 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.322 | loss_v1 0 | loss_v2 0 | nll_loss 0.135 | ntokens 119.044 | nsentences 39.997 | sample_size 119.044 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.636361 | ppl 1.1 | vqa_score 0.4935 | wps 152 | wpb 119 | bsz 40 | num_updates 6304

====================================================================================================
SGG eval:     R @ 50: 0.6088;     R @ 100: 0.6364;     R @ 500: 0.6518;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1414;    mR @ 100: 0.1590;    mR @ 500: 0.1869;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0576) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.2842) (attached to:0.0000) (behind:0.2917) (belonging to:0.0000) (between:0.0000) (carrying:0.7202) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7409) (holding:0.2558) (in:0.3157) (in front of:0.1113) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4213) (of:0.4101) (on:0.9131) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.3333) (says:0.0000) (sitting on:0.2131) (standing on:0.0000) (to:0.0000) (under:0.1944) (using:1.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.3333) (wearing:0.9806) (wears:0.0000) (with:0.0845) 
--------------------------------------------------------
====================================================================================================

2023-03-14 02:51:20 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6304 updates
2023-03-14 02:51:20 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint1.pt
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv slice_id 3 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv slice_id 4 row count 126256 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv slice_id 1 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv slice_id 2 row count 126257 total row count 631284
2023-03-14 02:51:26 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint1.pt
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 412928
Killing subprocess 412929
Killing subprocess 412930
Killing subprocess 412931
Killing subprocess 412932
Main process received SIGINT, exiting
2023-03-14 11:09:48 - utils.py[line:258] - INFO: distributed init (rank 3): env://
2023-03-14 11:09:48 - utils.py[line:258] - INFO: distributed init (rank 5): env://
2023-03-14 11:09:48 - utils.py[line:261] - INFO: Start init
2023-03-14 11:09:48 - utils.py[line:261] - INFO: Start init
2023-03-14 11:09:48 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-03-14 11:09:48 - utils.py[line:261] - INFO: Start init
2023-03-14 11:09:48 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2023-03-14 11:09:48 - utils.py[line:261] - INFO: Start init
2023-03-14 11:09:48 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-03-14 11:09:48 - utils.py[line:261] - INFO: Start init
2023-03-14 11:09:48 - utils.py[line:258] - INFO: distributed init (rank 4): env://
2023-03-14 11:09:48 - utils.py[line:261] - INFO: Start init
2023-03-14 11:09:48 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 4
2023-03-14 11:09:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 5
2023-03-14 11:09:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2023-03-14 11:09:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-14 11:09:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-03-14 11:09:49 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-14 11:09:49 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-03-14 11:09:49 - utils.py[line:274] - INFO: initialized host node4 as rank 5
single-machine distributed training is initialized.
2023-03-14 11:09:49 - utils.py[line:274] - INFO: initialized host node4 as rank 3
single-machine distributed training is initialized.
2023-03-14 11:09:49 - utils.py[line:274] - INFO: initialized host node4 as rank 2
single-machine distributed training is initialized.
2023-03-14 11:09:49 - utils.py[line:274] - INFO: initialized host node4 as rank 4
single-machine distributed training is initialized.
2023-03-14 11:09:49 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-03-14 11:09:59 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 6, 'distributed_num_procs': 6, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 6, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 500, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 500, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 6}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='8', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=6, distributed_port=-1, distributed_rank=0, distributed_world_size=6, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=6, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', save_interval=1, save_interval_updates=500, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=500, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-03-14 11:09:59 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-03-14 11:09:59 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-03-14 11:10:03 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-03-14 11:10:03 - train.py[line:118] - INFO: task: VqaGenTask
2023-03-14 11:10:03 - train.py[line:119] - INFO: model: OFAModel
2023-03-14 11:10:03 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-03-14 11:10:03 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-03-14 11:10:03 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv slice_id 0 row count 15233 total row count 91394
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv slice_id 1 row count 15233 total row count 91394
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv slice_id 3 row count 15232 total row count 91394file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv slice_id 5 row count 15232 total row count 91394

/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv slice_id 2 row count 15232 total row count 91394
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_500.tsv slice_id 4 row count 15232 total row count 91394
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-14 11:10:04 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-03-14 11:10:04 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-03-14 11:10:07 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 6 workers***********************
2023-03-14 11:10:07 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-14 11:10:07 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-14 11:10:07 - utils.py[line:765] - INFO: rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-14 11:10:07 - utils.py[line:765] - INFO: rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-14 11:10:07 - utils.py[line:765] - INFO: rank   4: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-14 11:10:07 - utils.py[line:765] - INFO: rank   5: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-14 11:10:07 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 6 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-03-14 11:10:08 - train.py[line:161] - INFO: training on 6 devices (GPUs/TPUs)
2023-03-14 11:10:08 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-03-14 11:10:08 - trainer.py[line:500] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-03-14 11:10:17 - trainer.py[line:565] - INFO: Load Model_m together with Model 2
2023-03-14 11:10:18 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-14 11:10:18 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-14 11:10:18 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-14 11:10:18 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-14 11:10:18 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-14 11:10:18 - trainer.py[line:646] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-14 11:10:18 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-03-14 11:10:18 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-03-14 11:10:19 - trainer.py[line:675] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-03-14 11:10:19 - trainer.py[line:695] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 105214 total row count 631284file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 2 row count 105214 total row count 631284

file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 105214 total row count 631284file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 5 row count 105214 total row count 631284

file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 4 row count 105214 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 3 row count 105214 total row count 631284
2023-03-14 11:10:20 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 26305, warmup steps 1315, warmup_factor 0.0007604562737642585
Total steps 26305, warmup steps 1315, warmup_factor 0.0007604562737642585
Total steps 26305, warmup steps 1315, warmup_factor 0.0007604562737642585
Total steps 26305, warmup steps 1315, warmup_factor 0.0007604562737642585
2023-03-14 11:10:21 - trainer.py[line:759] - INFO: begin training epoch 1
2023-03-14 11:10:21 - train.py[line:312] - INFO: Start iterating over samples
Total steps 26305, warmup steps 1315, warmup_factor 0.0007604562737642585
Total steps 26305, warmup steps 1315, warmup_factor 0.0007604562737642585
2023-03-14 11:10:43 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 5261 loss=1.335, loss_v1=0, loss_v2=0, nll_loss=1.111, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=2.16, wps=218.8, ups=0.72, wpb=303.9, bsz=120, num_updates=10, lr=3.80228e-07, gnorm=11.791, clip=100, loss_scale=128, train_wall=20, gb_free=10.5, ema_decay=0.9999, wall=36
2023-03-14 11:10:56 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 5261 loss=1.333, loss_v1=0, loss_v2=0, nll_loss=1.114, ntokens=301.6, nsentences=120, sample_size=301.6, sample_size_v1=0, sample_size_v2=0, ppl=2.16, wps=243.5, ups=0.81, wpb=301.6, bsz=120, num_updates=20, lr=7.60456e-07, gnorm=10.246, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=49
2023-03-14 11:11:08 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 5261 loss=1.265, loss_v1=0, loss_v2=0, nll_loss=1.059, ntokens=305.9, nsentences=120, sample_size=305.9, sample_size_v1=0, sample_size_v2=0, ppl=2.08, wps=246.8, ups=0.81, wpb=305.9, bsz=120, num_updates=30, lr=1.14068e-06, gnorm=8.813, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=61
2023-03-14 11:11:21 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 5261 loss=1.151, loss_v1=0, loss_v2=0, nll_loss=0.968, ntokens=308.2, nsentences=120, sample_size=308.2, sample_size_v1=0, sample_size_v2=0, ppl=1.96, wps=245.2, ups=0.8, wpb=308.2, bsz=120, num_updates=40, lr=1.52091e-06, gnorm=5.779, clip=100, loss_scale=128, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=74
2023-03-14 11:11:32 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 5261 loss=1.105, loss_v1=0, loss_v2=0, nll_loss=0.92, ntokens=304.6, nsentences=120, sample_size=304.6, sample_size_v1=0, sample_size_v2=0, ppl=1.89, wps=257.1, ups=0.84, wpb=304.6, bsz=120, num_updates=50, lr=1.90114e-06, gnorm=4.283, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=86
2023-03-14 11:11:44 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 5261 loss=1.052, loss_v1=0, loss_v2=0, nll_loss=0.871, ntokens=302.5, nsentences=120, sample_size=302.5, sample_size_v1=0, sample_size_v2=0, ppl=1.83, wps=255.5, ups=0.84, wpb=302.5, bsz=120, num_updates=60, lr=2.28137e-06, gnorm=3.599, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=97
2023-03-14 11:11:56 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 5261 loss=0.995, loss_v1=0, loss_v2=0, nll_loss=0.821, ntokens=305.7, nsentences=120, sample_size=305.7, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=261.9, ups=0.86, wpb=305.7, bsz=120, num_updates=70, lr=2.6616e-06, gnorm=2.721, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=109
2023-03-14 11:12:08 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 5261 loss=1.008, loss_v1=0, loss_v2=0, nll_loss=0.842, ntokens=302.7, nsentences=120, sample_size=302.7, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=260.6, ups=0.86, wpb=302.7, bsz=120, num_updates=80, lr=3.04183e-06, gnorm=2.613, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=121
2023-03-14 11:12:20 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 5261 loss=1.003, loss_v1=0, loss_v2=0, nll_loss=0.841, ntokens=301.3, nsentences=120, sample_size=301.3, sample_size_v1=0, sample_size_v2=0, ppl=1.79, wps=250.1, ups=0.83, wpb=301.3, bsz=120, num_updates=90, lr=3.42205e-06, gnorm=2.347, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=133
2023-03-14 11:12:31 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 5261 loss=0.944, loss_v1=0, loss_v2=0, nll_loss=0.779, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=258.8, ups=0.85, wpb=303.6, bsz=120, num_updates=100, lr=3.80228e-06, gnorm=1.98, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=145
2023-03-14 11:12:44 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 5261 loss=0.944, loss_v1=0, loss_v2=0, nll_loss=0.784, ntokens=303, nsentences=120, sample_size=303, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=248.7, ups=0.82, wpb=303, bsz=120, num_updates=110, lr=4.18251e-06, gnorm=1.887, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=157
2023-03-14 11:12:55 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 5261 loss=0.942, loss_v1=0, loss_v2=0, nll_loss=0.782, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=257.7, ups=0.85, wpb=302.9, bsz=120, num_updates=120, lr=4.56274e-06, gnorm=1.884, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=169
2023-03-14 11:13:07 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 5261 loss=0.951, loss_v1=0, loss_v2=0, nll_loss=0.795, ntokens=301.7, nsentences=120, sample_size=301.7, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=259, ups=0.86, wpb=301.7, bsz=120, num_updates=130, lr=4.94297e-06, gnorm=1.857, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=180
2023-03-14 11:13:19 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 5261 loss=0.883, loss_v1=0, loss_v2=0, nll_loss=0.718, ntokens=304.9, nsentences=120, sample_size=304.9, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=261.4, ups=0.86, wpb=304.9, bsz=120, num_updates=140, lr=5.32319e-06, gnorm=1.724, clip=100, loss_scale=128, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=192
2023-03-14 11:13:31 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 5261 loss=0.912, loss_v1=0, loss_v2=0, nll_loss=0.751, ntokens=303.5, nsentences=120, sample_size=303.5, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=255.5, ups=0.84, wpb=303.5, bsz=120, num_updates=150, lr=5.70342e-06, gnorm=1.641, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=204
2023-03-14 11:13:42 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 5261 loss=0.902, loss_v1=0, loss_v2=0, nll_loss=0.741, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=263.8, ups=0.87, wpb=303.8, bsz=120, num_updates=160, lr=6.08365e-06, gnorm=1.747, clip=100, loss_scale=128, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=215
2023-03-14 11:13:54 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 5261 loss=0.876, loss_v1=0, loss_v2=0, nll_loss=0.709, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=261.1, ups=0.86, wpb=304.1, bsz=120, num_updates=170, lr=6.46388e-06, gnorm=1.645, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=227
2023-03-14 11:14:06 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 5261 loss=0.885, loss_v1=0, loss_v2=0, nll_loss=0.718, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.65, wps=253.1, ups=0.83, wpb=303.7, bsz=120, num_updates=180, lr=6.84411e-06, gnorm=1.682, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=239
2023-03-14 11:14:17 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 5261 loss=0.865, loss_v1=0, loss_v2=0, nll_loss=0.691, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=261.3, ups=0.86, wpb=304.1, bsz=120, num_updates=190, lr=7.22433e-06, gnorm=1.901, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=251
2023-03-14 11:14:29 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 5261 loss=0.849, loss_v1=0, loss_v2=0, nll_loss=0.684, ntokens=306.6, nsentences=120, sample_size=306.6, sample_size_v1=0, sample_size_v2=0, ppl=1.61, wps=262.2, ups=0.86, wpb=306.6, bsz=120, num_updates=200, lr=7.60456e-06, gnorm=1.645, clip=100, loss_scale=128, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=262
2023-03-14 11:14:41 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 5261 loss=0.847, loss_v1=0, loss_v2=0, nll_loss=0.675, ntokens=303.7, nsentences=120, sample_size=303.7, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=260, ups=0.86, wpb=303.7, bsz=120, num_updates=210, lr=7.98479e-06, gnorm=1.705, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=274
2023-03-14 11:14:53 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 5261 loss=0.826, loss_v1=0, loss_v2=0, nll_loss=0.647, ntokens=303.9, nsentences=120, sample_size=303.9, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=261.1, ups=0.86, wpb=303.9, bsz=120, num_updates=220, lr=8.36502e-06, gnorm=1.679, clip=100, loss_scale=128, train_wall=12, gb_free=9.6, ema_decay=0.9999, wall=286
2023-03-14 11:15:04 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 5261 loss=0.836, loss_v1=0, loss_v2=0, nll_loss=0.662, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.58, wps=261.2, ups=0.86, wpb=303.8, bsz=120, num_updates=230, lr=8.74525e-06, gnorm=1.652, clip=100, loss_scale=128, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=297
2023-03-14 11:15:16 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 5261 loss=0.83, loss_v1=0, loss_v2=0, nll_loss=0.654, ntokens=302.9, nsentences=120, sample_size=302.9, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=253.2, ups=0.84, wpb=302.9, bsz=120, num_updates=240, lr=9.12548e-06, gnorm=1.628, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=309
2023-03-14 11:15:28 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 5261 loss=0.801, loss_v1=0, loss_v2=0, nll_loss=0.616, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=265.1, ups=0.87, wpb=304.3, bsz=120, num_updates=250, lr=9.5057e-06, gnorm=1.642, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=321
2023-03-14 11:15:39 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 5261 loss=0.792, loss_v1=0, loss_v2=0, nll_loss=0.605, ntokens=305.6, nsentences=120, sample_size=305.6, sample_size_v1=0, sample_size_v2=0, ppl=1.52, wps=261.2, ups=0.85, wpb=305.6, bsz=120, num_updates=260, lr=9.88593e-06, gnorm=1.789, clip=100, loss_scale=128, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=332
2023-03-14 11:15:51 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 5261 loss=0.789, loss_v1=0, loss_v2=0, nll_loss=0.597, ntokens=301.2, nsentences=120, sample_size=301.2, sample_size_v1=0, sample_size_v2=0, ppl=1.51, wps=260.2, ups=0.86, wpb=301.2, bsz=120, num_updates=270, lr=1.02662e-05, gnorm=1.71, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=344
2023-03-14 11:16:03 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 5261 loss=0.776, loss_v1=0, loss_v2=0, nll_loss=0.586, ntokens=305.4, nsentences=120, sample_size=305.4, sample_size_v1=0, sample_size_v2=0, ppl=1.5, wps=262.6, ups=0.86, wpb=305.4, bsz=120, num_updates=280, lr=1.06464e-05, gnorm=1.991, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=356
2023-03-14 11:16:14 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 5261 loss=0.756, loss_v1=0, loss_v2=0, nll_loss=0.564, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=262.3, ups=0.86, wpb=304.1, bsz=120, num_updates=290, lr=1.10266e-05, gnorm=1.558, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=367
2023-03-14 11:16:26 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 5261 loss=0.771, loss_v1=0, loss_v2=0, nll_loss=0.573, ntokens=299.2, nsentences=120, sample_size=299.2, sample_size_v1=0, sample_size_v2=0, ppl=1.49, wps=257.5, ups=0.86, wpb=299.2, bsz=120, num_updates=300, lr=1.14068e-05, gnorm=1.629, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=379
2023-03-14 11:16:37 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 5261 loss=0.735, loss_v1=0, loss_v2=0, nll_loss=0.534, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=265.6, ups=0.87, wpb=304.4, bsz=120, num_updates=310, lr=1.17871e-05, gnorm=1.781, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=390
2023-03-14 11:16:49 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 5261 loss=0.728, loss_v1=0, loss_v2=0, nll_loss=0.527, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=261.3, ups=0.86, wpb=303.4, bsz=120, num_updates=320, lr=1.21673e-05, gnorm=1.516, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=402
2023-03-14 11:17:00 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 5261 loss=0.729, loss_v1=0, loss_v2=0, nll_loss=0.529, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=265, ups=0.87, wpb=303.6, bsz=120, num_updates=330, lr=1.25475e-05, gnorm=1.551, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=413
2023-03-14 11:17:12 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 5261 loss=0.709, loss_v1=0, loss_v2=0, nll_loss=0.509, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=261.3, ups=0.86, wpb=303.3, bsz=120, num_updates=340, lr=1.29278e-05, gnorm=1.598, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=425
2023-03-14 11:17:24 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 5261 loss=0.709, loss_v1=0, loss_v2=0, nll_loss=0.502, ntokens=303.8, nsentences=120, sample_size=303.8, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=262.2, ups=0.86, wpb=303.8, bsz=120, num_updates=350, lr=1.3308e-05, gnorm=1.478, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=437
2023-03-14 11:17:35 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 5261 loss=0.703, loss_v1=0, loss_v2=0, nll_loss=0.503, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=261.1, ups=0.86, wpb=304.4, bsz=120, num_updates=360, lr=1.36882e-05, gnorm=1.492, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=448
2023-03-14 11:17:47 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 5261 loss=0.687, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=305.3, nsentences=120, sample_size=305.3, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=262.8, ups=0.86, wpb=305.3, bsz=120, num_updates=370, lr=1.40684e-05, gnorm=1.361, clip=100, loss_scale=128, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=460
2023-03-14 11:17:58 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 5261 loss=0.721, loss_v1=0, loss_v2=0, nll_loss=0.519, ntokens=302, nsentences=120, sample_size=302, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=260, ups=0.86, wpb=302, bsz=120, num_updates=380, lr=1.44487e-05, gnorm=1.395, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=471
2023-03-14 11:18:10 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 5261 loss=0.695, loss_v1=0, loss_v2=0, nll_loss=0.494, ntokens=305.2, nsentences=120, sample_size=305.2, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=266, ups=0.87, wpb=305.2, bsz=120, num_updates=390, lr=1.48289e-05, gnorm=1.569, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=483
2023-03-14 11:18:21 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 5261 loss=0.691, loss_v1=0, loss_v2=0, nll_loss=0.488, ntokens=303.6, nsentences=120, sample_size=303.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=262.2, ups=0.86, wpb=303.6, bsz=120, num_updates=400, lr=1.52091e-05, gnorm=1.527, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=495
2023-03-14 11:18:33 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 5261 loss=0.68, loss_v1=0, loss_v2=0, nll_loss=0.469, ntokens=304.3, nsentences=120, sample_size=304.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=265.3, ups=0.87, wpb=304.3, bsz=120, num_updates=410, lr=1.55894e-05, gnorm=1.484, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=506
2023-03-14 11:18:45 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 5261 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=303.4, nsentences=120, sample_size=303.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=261.5, ups=0.86, wpb=303.4, bsz=120, num_updates=420, lr=1.59696e-05, gnorm=1.271, clip=100, loss_scale=128, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=518
2023-03-14 11:18:56 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 5261 loss=0.709, loss_v1=0, loss_v2=0, nll_loss=0.508, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.42, wps=262.6, ups=0.86, wpb=304, bsz=120, num_updates=430, lr=1.63498e-05, gnorm=1.475, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=529
2023-03-14 11:19:08 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 5261 loss=0.681, loss_v1=0, loss_v2=0, nll_loss=0.481, ntokens=304.4, nsentences=120, sample_size=304.4, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=262, ups=0.86, wpb=304.4, bsz=120, num_updates=440, lr=1.673e-05, gnorm=1.483, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=541
2023-03-14 11:19:19 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 5261 loss=0.68, loss_v1=0, loss_v2=0, nll_loss=0.476, ntokens=303, nsentences=120, sample_size=303, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=264.9, ups=0.87, wpb=303, bsz=120, num_updates=450, lr=1.71103e-05, gnorm=1.488, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=552
2023-03-14 11:19:31 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 5261 loss=0.695, loss_v1=0, loss_v2=0, nll_loss=0.489, ntokens=304, nsentences=120, sample_size=304, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=261.3, ups=0.86, wpb=304, bsz=120, num_updates=460, lr=1.74905e-05, gnorm=1.432, clip=100, loss_scale=128, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=564
2023-03-14 11:19:43 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 5261 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=306.2, nsentences=120, sample_size=306.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=262.2, ups=0.86, wpb=306.2, bsz=120, num_updates=470, lr=1.78707e-05, gnorm=1.388, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=576
2023-03-14 11:19:54 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 5261 loss=0.663, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=307.8, nsentences=120, sample_size=307.8, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=266.4, ups=0.87, wpb=307.8, bsz=120, num_updates=480, lr=1.8251e-05, gnorm=1.459, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=587
2023-03-14 11:20:06 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 5261 loss=0.669, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=303.3, nsentences=120, sample_size=303.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=259.5, ups=0.86, wpb=303.3, bsz=120, num_updates=490, lr=1.86312e-05, gnorm=1.334, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=599
2023-03-14 11:20:17 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 5261 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=304.1, nsentences=120, sample_size=304.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=262.4, ups=0.86, wpb=304.1, bsz=120, num_updates=500, lr=1.90114e-05, gnorm=1.205, clip=90, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=610
2023-03-14 11:20:17 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-03-14 11:20:17 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-03-14 11:20:19 - train.py[line:549] - INFO: 0 / 1905
2023-03-14 11:20:19 - train.py[line:551] - INFO: load:1.15 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-03-14 11:23:08 - train.py[line:549] - INFO: 200 / 1905
2023-03-14 11:23:08 - train.py[line:551] - INFO: load:1.17 valid_run:168.78 task_valid:150.06 collect_output:17.68
2023-03-14 11:25:44 - train.py[line:549] - INFO: 400 / 1905
2023-03-14 11:25:44 - train.py[line:551] - INFO: load:1.19 valid_run:324.70 task_valid:297.57 collect_output:25.07
2023-03-14 11:28:19 - train.py[line:549] - INFO: 600 / 1905
2023-03-14 11:28:19 - train.py[line:551] - INFO: load:1.21 valid_run:480.44 task_valid:442.92 collect_output:34.44
2023-03-14 11:30:56 - train.py[line:549] - INFO: 800 / 1905
2023-03-14 11:30:56 - train.py[line:551] - INFO: load:1.23 valid_run:637.25 task_valid:588.77 collect_output:44.36
2023-03-14 11:33:33 - train.py[line:549] - INFO: 1000 / 1905
2023-03-14 11:33:33 - train.py[line:551] - INFO: load:1.26 valid_run:793.91 task_valid:736.12 collect_output:52.67
2023-03-14 11:36:11 - train.py[line:549] - INFO: 1200 / 1905
2023-03-14 11:36:11 - train.py[line:551] - INFO: load:1.28 valid_run:951.58 task_valid:884.28 collect_output:61.18
2023-03-14 11:38:50 - train.py[line:549] - INFO: 1400 / 1905
2023-03-14 11:38:50 - train.py[line:551] - INFO: load:1.30 valid_run:1111.06 task_valid:1034.79 collect_output:69.15
2023-03-14 11:41:25 - train.py[line:549] - INFO: 1600 / 1905
2023-03-14 11:41:25 - train.py[line:551] - INFO: load:1.32 valid_run:1265.65 task_valid:1183.76 collect_output:73.76
2023-03-14 11:44:01 - train.py[line:549] - INFO: 1800 / 1905
2023-03-14 11:44:01 - train.py[line:551] - INFO: load:1.35 valid_run:1421.47 task_valid:1334.87 collect_output:77.45

====================================================================================================
SGG eval:     R @ 50: 0.0372;     R @ 100: 0.0590;     R @ 500: 0.0863;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0072;    mR @ 100: 0.0135;    mR @ 500: 0.0217;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0000) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.0000) (behind:0.0000) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.0076) (holding:0.0769) (in:0.0000) (in front of:0.0510) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.0000) (of:0.1436) (on:0.0985) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.0000) (standing on:0.0000) (to:0.0000) (under:0.0000) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.0000) (wears:0.0000) (with:0.2955) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.0372;     R @ 100: 0.0590;     R @ 500: 0.0863;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0072;    mR @ 100: 0.0135;    mR @ 500: 0.0217;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0000) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.0000) (behind:0.0000) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.0076) (holding:0.0769) (in:0.0000) (in front of:0.0510) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.0000) (of:0.1436) (on:0.0985) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.0000) (standing on:0.0000) (to:0.0000) (under:0.0000) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.0000) (wears:0.0000) (with:0.2955) 
--------------------------------------------------------
====================================================================================================

2023-03-14 11:45:28 - train.py[line:487] - INFO: 0.05900453739071387

====================================================================================================
SGG eval:     R @ 50: 0.0372;     R @ 100: 0.0590;     R @ 500: 0.0863;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0072;    mR @ 100: 0.0135;    mR @ 500: 0.0217;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0000) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.0000) (behind:0.0000) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.0076) (holding:0.0769) (in:0.0000) (in front of:0.0510) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.0000) (of:0.1436) (on:0.0985) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.0000) (standing on:0.0000) (to:0.0000) (under:0.0000) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.0000) (wears:0.0000) (with:0.2955) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.0372;     R @ 100: 0.0590;     R @ 500: 0.0863;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0072;    mR @ 100: 0.0135;    mR @ 500: 0.0217;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0000) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.0000) (behind:0.0000) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.0076) (holding:0.0769) (in:0.0000) (in front of:0.0510) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.0000) (of:0.1436) (on:0.0985) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.0000) (standing on:0.0000) (to:0.0000) (under:0.0000) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.0000) (wears:0.0000) (with:0.2955) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.0372;     R @ 100: 0.0590;     R @ 500: 0.0863;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0072;    mR @ 100: 0.0135;    mR @ 500: 0.0217;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0000) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.0000) (behind:0.0000) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.0076) (holding:0.0769) (in:0.0000) (in front of:0.0510) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.0000) (of:0.1436) (on:0.0985) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.0000) (standing on:0.0000) (to:0.0000) (under:0.0000) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.0000) (wears:0.0000) (with:0.2955) 
--------------------------------------------------------
====================================================================================================

2023-03-14 11:45:28 - train.py[line:578] - INFO: logits:torch.Size([91426, 51]) sample_ids:torch.Size([91426])
2023-03-14 11:45:28 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.336 | loss_v1 0 | loss_v2 0 | nll_loss 0.132 | ntokens 142.72 | nsentences 47.976 | sample_size 142.72 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.0590045 | ppl 1.1 | vqa_score 0.028 | wps 180.5 | wpb 142.7 | bsz 48 | num_updates 500

====================================================================================================
SGG eval:     R @ 50: 0.0372;     R @ 100: 0.0590;     R @ 500: 0.0863;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.0072;    mR @ 100: 0.0135;    mR @ 500: 0.0217;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0000) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.0000) (attached to:0.0000) (behind:0.0000) (belonging to:0.0000) (between:0.0000) (carrying:0.0000) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0000) (has:0.0076) (holding:0.0769) (in:0.0000) (in front of:0.0510) (laying on:0.0000) (looking at:0.0000) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.0000) (of:0.1436) (on:0.0985) (on back of:0.0000) (over:0.0000) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.0000) (says:0.0000) (sitting on:0.0000) (standing on:0.0000) (to:0.0000) (under:0.0000) (using:0.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.0000) (wearing:0.0000) (wears:0.0000) (with:0.2955) 
--------------------------------------------------------
====================================================================================================

2023-03-14 11:45:28 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 500 updates
2023-03-14 11:45:28 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint_1_500.pt
2023-03-14 11:45:34 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint_1_500.pt
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 493295
Killing subprocess 493296
Killing subprocess 493297
Killing subprocess 493298
Killing subprocess 493299
Killing subprocess 493300
Main process received SIGINT, exiting
2023-03-14 19:31:54 - utils.py[line:258] - INFO: distributed init (rank 2): env://
2023-03-14 19:31:54 - utils.py[line:261] - INFO: Start init
2023-03-14 19:31:54 - utils.py[line:258] - INFO: distributed init (rank 1): env://
2023-03-14 19:31:54 - utils.py[line:261] - INFO: Start init
2023-03-14 19:31:54 - utils.py[line:258] - INFO: distributed init (rank 0): env://
2023-03-14 19:31:54 - utils.py[line:261] - INFO: Start init
2023-03-14 19:31:54 - utils.py[line:258] - INFO: distributed init (rank 3): env://
2023-03-14 19:31:54 - utils.py[line:261] - INFO: Start init
2023-03-14 19:31:54 - utils.py[line:258] - INFO: distributed init (rank 4): env://
2023-03-14 19:31:54 - utils.py[line:261] - INFO: Start init
2023-03-14 19:31:54 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 3
2023-03-14 19:31:54 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 4
2023-03-14 19:31:55 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 2
2023-03-14 19:31:55 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 0
2023-03-14 19:31:55 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:1 to store for rank: 1
2023-03-14 19:31:55 - utils.py[line:274] - INFO: initialized host node4 as rank 0
single-machine distributed training is initialized.
2023-03-14 19:31:55 - utils.py[line:274] - INFO: initialized host node4 as rank 1
single-machine distributed training is initialized.
2023-03-14 19:31:55 - utils.py[line:274] - INFO: initialized host node4 as rank 2
single-machine distributed training is initialized.
2023-03-14 19:31:55 - utils.py[line:274] - INFO: initialized host node4 as rank 3
single-machine distributed training is initialized.
2023-03-14 19:31:55 - utils.py[line:274] - INFO: initialized host node4 as rank 4
single-machine distributed training is initialized.
2023-03-14 19:32:04 - train.py[line:84] - INFO: {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': './vqa_tensorboard/50_way', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': 512, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': '../../ofa_module', 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 5, 'distributed_num_procs': 5, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 5, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 8, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 20, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 50000, 'validate_after_updates': 0, 'fixed_validation_seed': 7, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 5, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 1.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': './vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', 'restore_file': '/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 50000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'R@100', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1, 'use_ema_weights_to_init_param': False, 'use_latest_weights_to_init_ema': False}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 5}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='ofa_base', activation_fn='gelu', adam_betas='(0.9,0.999)', adam_eps=1e-08, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_object=True, add_type_embedding=True, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, ans2label_dict='{"no": 0, "yes":1}', ans2label_file='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', arch='ofa_base', attention_dropout=0.0, attn_scale_factor=2, azureml_logging=False, batch_size=20, batch_size_valid='8', best_checkpoint_metric='R@100', bf16=False, bitfit=False, bpe=None, bpe_dir='../../utils/BPE', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=1.0, code_dict_size=8192, code_image_size=128, code_layernorm_embedding=True, combine_valid_subsets=None, constraint_range=None, cpu=False, cpu_offload=False, criterion='adjust_label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=12, decoder_drop_path_rate=0.1, decoder_embed_dim=768, decoder_embed_path=None, decoder_ffn_embed_dim=3072, decoder_input_dim=768, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=True, decoder_output_dim=768, device_id=0, disable_entangle=True, disable_validation=False, distill='default', distill_alpha=1.0, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=5, distributed_port=-1, distributed_rank=0, distributed_world_size=5, drop_worst_after=0, drop_worst_ratio=0.0, dropout=0.1, ema_decay=0.9999, ema_fp32=True, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_drop_path_rate=0.1, encoder_embed_dim=768, encoder_embed_path=None, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=True, end_learning_rate=0.0, entangle_position_embedding=False, eos=2, eval_args='{"beam":5,"unnormalized":true,"temperature":1.0}', fast_stat_sync=False, find_unused_parameters=True, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=512, fp32_reduce_scatter=False, freeze_decoder_embedding=True, freeze_encoder_embedding=True, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_eos=False, ignore_prefix_size=0, ignore_unused_valid_subsets=False, image_bucket_size=42, imagenet_default_mean_and_std=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, label_proxy='answer', label_smoothing=0.1, layernorm_embedding=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='simple', log_interval=10, lr=[5e-05], lr_scheduler='polynomial_decay', max_epoch=5, max_object_length=30, max_source_positions=1024, max_src_length=128, max_target_positions=1024, max_tgt_length=30, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=5, num_bins=1000, num_shards=1, num_workers=8, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', orig_patch_image_size=256, pad=1, patch_image_size=480, patch_layernorm_embedding=True, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_classifier='mlp', pooler_dropout=0.0, power=1.0, profile=False, prompt_type='prev_output', quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, reg_alpha=1.0, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, resnet_drop_path_rate=0.0, resnet_type='resnet101', restore_file='/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', sample_patch_num=196, save_dir='./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', save_interval=1, save_interval_updates=50000, scale_attn=True, scale_fc=True, scale_heads=True, scale_resids=False, scoring='bleu', seed=1, selected_cols='0,5,2,3,4', sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=True, suppress_crashes=False, sync_bn=False, task='vqa_gen', tensorboard_logdir='./vqa_tensorboard/50_way', threshold_loss_scale=None, token_bucket_size=256, tokenizer=None, total_num_update=1000000, tpu=False, train_subset='train', unk=3, update_freq=[1], use_bmuf=False, use_ema_weights_to_init_param=False, use_latest_weights_to_init_ema=False, use_old_adam=False, use_plasma_view=False, use_rdrop=False, use_sharded_state=False, user_dir='../../ofa_module', uses_ema=True, val_inference_type='allcand', valid_batch_size=51, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=50000, wandb_project=None, warmup_ratio=0.05, warmup_updates=0, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'vqa_gen', 'data': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', 'selected_cols': '0,5,2,3,4', 'bpe': None, 'bpe_dir': '../../utils/BPE', 'max_source_positions': 1024, 'max_target_positions': 1024, 'max_src_length': 128, 'max_tgt_length': 30, 'code_dict_size': 8192, 'patch_image_size': 480, 'orig_patch_image_size': 256, 'num_bins': 1000, 'imagenet_default_mean_and_std': False, 'constraint_range': None, 'max_object_length': 30, 'ans2label_dict': '{"no": 0, "yes":1}', 'ans2label_file': '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', 'add_object': True, 'valid_batch_size': 51, 'prompt_type': 'prev_output', 'uses_ema': True, 'val_inference_type': 'allcand', 'eval_args': '{"beam":5,"unnormalized":true,"temperature":1.0}', 'label_proxy': 'answer', 'distill': 'default', 'distill_alpha': 1.0}, 'criterion': {'_name': 'adjust_label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'ignore_eos': False, 'sentence_avg': False, 'drop_worst_ratio': 0.0, 'drop_worst_after': 0, 'use_rdrop': False, 'reg_alpha': 1.0, 'sample_patch_num': 196, 'constraint_range': None}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.999)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 0, 'warmup_ratio': 0.05, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 1000000.0, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': True, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': True}}
2023-03-14 19:32:04 - ofa_task.py[line:111] - INFO: source dictionary: 59457 types
2023-03-14 19:32:04 - ofa_task.py[line:112] - INFO: target dictionary: 59457 types
2023-03-14 19:32:09 - train.py[line:117] - INFO: OFAModel(
  (encoder): TransformerEncoder(
    (encoder_dropout): Dropout(p=0.2, inplace=False)
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (type_embedding): Embedding(2, 768)
    (embed_images): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (drop_path): Identity()
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (6): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (7): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (8): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (9): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (10): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (11): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (12): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (13): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (14): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (15): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (16): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (17): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (18): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (19): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (20): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (21): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
        (22): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (drop_path): Identity()
        )
      )
    )
    (image_proj): Linear(in_features=1024, out_features=768, bias=True)
    (patch_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(59457, 768, padding_idx=1)
    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (embed_positions): Embedding(1026, 768)
    (embed_image_positions): Embedding(1765, 768)
    (pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (image_pos_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (self_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (self_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_q_linear): Linear(in_features=768, out_features=768, bias=True)
    (cross_pos_k_linear): Linear(in_features=768, out_features=768, bias=True)
    (code_layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): Identity()
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.019999999552965164)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.03999999910593033)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.06000000238418579)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.07999999821186066)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (self_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (cross_attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=768, out_features=768, bias=True)
          (v_proj): Linear(in_features=768, out_features=768, bias=True)
          (q_proj): Linear(in_features=768, out_features=768, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (drop_path): DropPath(p=0.10000000149011612)
      )
    )
    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=768, out_features=59457, bias=False)
    (token_rel_pos_table_list): ModuleList(
      (0): Embedding(511, 12)
      (1): Embedding(511, 12)
      (2): Embedding(511, 12)
      (3): Embedding(511, 12)
      (4): Embedding(511, 12)
      (5): Embedding(511, 12)
    )
    (image_rel_pos_table_list): ModuleList(
      (0): Embedding(6892, 12)
      (1): Embedding(6892, 12)
      (2): Embedding(6892, 12)
      (3): Embedding(6892, 12)
      (4): Embedding(6892, 12)
      (5): Embedding(6892, 12)
    )
  )
  (classification_heads): ModuleDict()
)
2023-03-14 19:32:09 - train.py[line:118] - INFO: task: VqaGenTask
2023-03-14 19:32:09 - train.py[line:119] - INFO: model: OFAModel
2023-03-14 19:32:09 - train.py[line:120] - INFO: criterion: AdjustLabelSmoothedCrossEntropyCriterion
2023-03-14 19:32:09 - train.py[line:124] - INFO: num. shared model params: 182,238,536 (num. trained: 136,575,560)
2023-03-14 19:32:09 - train.py[line:131] - INFO: num. expert model params: 0 (num. trained: 0)
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 0 row count 56412 total row count 282060
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 1 row count 56412 total row count 282060file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 4 row count 56412 total row count 282060

file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 2 row count 56412 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-14 19:32:09 - distributed_c10d.py[line:187] - INFO: Added key: store_based_barrier_key:2 to store for rank: 0
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv slice_id 3 row count 56412 total row count 282060
/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.0.downsample.0.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.1.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer1.2.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.0.downsample.0.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.1.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.2.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer2.3.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.0.downsample.0.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.1.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.2.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.3.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.4.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.5.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.6.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.7.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.8.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.9.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.10.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.11.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.12.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.13.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.14.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.15.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.16.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.17.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.18.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.19.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.20.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.21.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv1.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv2.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- encoder.embed_images.layer3.22.conv3.bias
2023-03-14 19:32:10 - trainer.py[line:126] - INFO: detected shared parameter: encoder.embed_images.conv1.bias <- decoder.output_projection.bias
2023-03-14 19:32:12 - utils.py[line:759] - INFO: ***********************CUDA enviroments for all 5 workers***********************
2023-03-14 19:32:12 - utils.py[line:765] - INFO: rank   0: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-14 19:32:12 - utils.py[line:765] - INFO: rank   1: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-14 19:32:12 - utils.py[line:765] - INFO: rank   2: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-14 19:32:12 - utils.py[line:765] - INFO: rank   3: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-14 19:32:12 - utils.py[line:765] - INFO: rank   4: capabilities =  8.0  ; total memory = 39.586 GB ; name = A100-SXM4-40GB                          
2023-03-14 19:32:12 - utils.py[line:767] - INFO: ***********************CUDA enviroments for all 5 workers***********************
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
Done 0.95 cuda cpu, cpu
2023-03-14 19:32:14 - train.py[line:161] - INFO: training on 5 devices (GPUs/TPUs)
2023-03-14 19:32:14 - train.py[line:167] - INFO: max tokens per device = None and max sentences per device = 20
2023-03-14 19:32:14 - trainer.py[line:499] - INFO: Preparing to load checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt
2023-03-14 19:32:23 - trainer.py[line:564] - INFO: Load Model_m together with Model 2
2023-03-14 19:32:23 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-14 19:32:23 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-14 19:32:23 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-14 19:32:23 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-14 19:32:23 - trainer.py[line:645] - WARNING: EMA not found in checkpoint. But store_ema is True. EMA is re-initialized from checkpoint.
2023-03-14 19:32:24 - ema.py[line:85] - INFO: Copying EMA model to device cuda
2023-03-14 19:32:24 - trainer.py[line:314] - INFO: Exponential Moving Average Shadow Model is initialized.
2023-03-14 19:32:24 - trainer.py[line:674] - INFO: Loaded checkpoint /data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt (epoch 48 @ 0 updates)
2023-03-14 19:32:24 - trainer.py[line:694] - INFO: loading train data for epoch 1
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 1 row count 126257 total row count 631284file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 4 row count 126256 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 3 row count 126257 total row count 631284

file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 0 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv slice_id 2 row count 126257 total row count 631284
2023-03-14 19:32:25 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
Total steps 31565, warmup steps 1578, warmup_factor 0.0006337135614702154
Total steps 31565, warmup steps 1578, warmup_factor 0.0006337135614702154
Total steps 31565, warmup steps 1578, warmup_factor 0.0006337135614702154
Total steps 31565, warmup steps 1578, warmup_factor 0.0006337135614702154
Total steps 31565, warmup steps 1578, warmup_factor 0.0006337135614702154
2023-03-14 19:32:26 - trainer.py[line:758] - INFO: begin training epoch 1
2023-03-14 19:32:26 - train.py[line:312] - INFO: Start iterating over samples
2023-03-14 19:32:46 - progress_bar.py[line:274] - INFO: epoch 001:     10 / 6313 loss=1.366, loss_v1=0, loss_v2=0, nll_loss=1.155, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=2.23, wps=196.4, ups=0.78, wpb=252.3, bsz=100, num_updates=10, lr=3.16857e-07, gnorm=12.575, clip=100, loss_scale=128, train_wall=18, gb_free=10.5, ema_decay=0.9999, wall=34
2023-03-14 19:32:59 - progress_bar.py[line:274] - INFO: epoch 001:     20 / 6313 loss=1.282, loss_v1=0, loss_v2=0, nll_loss=1.062, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=2.09, wps=201.1, ups=0.79, wpb=253.3, bsz=100, num_updates=20, lr=6.33714e-07, gnorm=11.118, clip=100, loss_scale=128, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=47
2023-03-14 19:33:11 - progress_bar.py[line:274] - INFO: epoch 001:     30 / 6313 loss=1.324, loss_v1=0, loss_v2=0, nll_loss=1.116, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=2.17, wps=213.9, ups=0.85, wpb=252.4, bsz=100, num_updates=30, lr=9.5057e-07, gnorm=10.118, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=58
2023-03-14 19:33:23 - progress_bar.py[line:274] - INFO: epoch 001:     40 / 6313 loss=1.184, loss_v1=0, loss_v2=0, nll_loss=0.995, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.99, wps=209.1, ups=0.82, wpb=255.2, bsz=100, num_updates=40, lr=1.26743e-06, gnorm=6.79, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=71
2023-03-14 19:33:35 - progress_bar.py[line:274] - INFO: epoch 001:     50 / 6313 loss=1.086, loss_v1=0, loss_v2=0, nll_loss=0.896, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.86, wps=209.3, ups=0.82, wpb=254.8, bsz=100, num_updates=50, lr=1.58428e-06, gnorm=4.979, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=83
2023-03-14 19:33:47 - progress_bar.py[line:274] - INFO: epoch 001:     60 / 6313 loss=1.108, loss_v1=0, loss_v2=0, nll_loss=0.923, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.9, wps=214, ups=0.85, wpb=251.2, bsz=100, num_updates=60, lr=1.90114e-06, gnorm=4.312, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=95
2023-03-14 19:33:58 - progress_bar.py[line:274] - INFO: epoch 001:     70 / 6313 loss=1.005, loss_v1=0, loss_v2=0, nll_loss=0.825, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=218.9, ups=0.86, wpb=255.3, bsz=100, num_updates=70, lr=2.218e-06, gnorm=3.374, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=106
2023-03-14 19:34:10 - progress_bar.py[line:274] - INFO: epoch 001:     80 / 6313 loss=0.966, loss_v1=0, loss_v2=0, nll_loss=0.79, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=222.1, ups=0.87, wpb=255.3, bsz=100, num_updates=80, lr=2.53485e-06, gnorm=2.899, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=118
2023-03-14 19:34:22 - progress_bar.py[line:274] - INFO: epoch 001:     90 / 6313 loss=0.972, loss_v1=0, loss_v2=0, nll_loss=0.8, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=218.9, ups=0.87, wpb=252.8, bsz=100, num_updates=90, lr=2.85171e-06, gnorm=2.697, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=129
2023-03-14 19:34:33 - progress_bar.py[line:274] - INFO: epoch 001:    100 / 6313 loss=0.966, loss_v1=0, loss_v2=0, nll_loss=0.796, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.74, wps=214.4, ups=0.85, wpb=253.1, bsz=100, num_updates=100, lr=3.16857e-06, gnorm=2.531, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=141
2023-03-14 19:34:45 - progress_bar.py[line:274] - INFO: epoch 001:    110 / 6313 loss=0.981, loss_v1=0, loss_v2=0, nll_loss=0.821, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.77, wps=217.3, ups=0.86, wpb=251.7, bsz=100, num_updates=110, lr=3.48542e-06, gnorm=2.36, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=153
2023-03-14 19:34:57 - progress_bar.py[line:274] - INFO: epoch 001:    120 / 6313 loss=0.935, loss_v1=0, loss_v2=0, nll_loss=0.77, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.71, wps=217.5, ups=0.86, wpb=252, bsz=100, num_updates=120, lr=3.80228e-06, gnorm=2.296, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=164
2023-03-14 19:35:08 - progress_bar.py[line:274] - INFO: epoch 001:    130 / 6313 loss=0.95, loss_v1=0, loss_v2=0, nll_loss=0.789, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.73, wps=213.7, ups=0.85, wpb=252.5, bsz=100, num_updates=130, lr=4.11914e-06, gnorm=2.084, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=176
2023-03-14 19:35:20 - progress_bar.py[line:274] - INFO: epoch 001:    140 / 6313 loss=0.899, loss_v1=0, loss_v2=0, nll_loss=0.737, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=219.4, ups=0.86, wpb=254.8, bsz=100, num_updates=140, lr=4.43599e-06, gnorm=2.047, clip=100, loss_scale=128, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=188
2023-03-14 19:35:32 - progress_bar.py[line:274] - INFO: epoch 001:    150 / 6313 loss=0.925, loss_v1=0, loss_v2=0, nll_loss=0.762, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.7, wps=217, ups=0.86, wpb=251.4, bsz=100, num_updates=150, lr=4.75285e-06, gnorm=2.044, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=199
2023-03-14 19:35:43 - progress_bar.py[line:274] - INFO: epoch 001:    160 / 6313 loss=0.94, loss_v1=0, loss_v2=0, nll_loss=0.783, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.72, wps=215, ups=0.85, wpb=251.9, bsz=100, num_updates=160, lr=5.06971e-06, gnorm=2.092, clip=100, loss_scale=128, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=211
2023-03-14 19:35:55 - progress_bar.py[line:274] - INFO: epoch 001:    170 / 6313 loss=0.886, loss_v1=0, loss_v2=0, nll_loss=0.717, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=216.8, ups=0.85, wpb=254, bsz=100, num_updates=170, lr=5.38657e-06, gnorm=2.067, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=223
2023-03-14 19:36:07 - progress_bar.py[line:274] - INFO: epoch 001:    180 / 6313 loss=0.908, loss_v1=0, loss_v2=0, nll_loss=0.744, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.68, wps=219.7, ups=0.87, wpb=251.9, bsz=100, num_updates=180, lr=5.70342e-06, gnorm=1.894, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=234
2023-03-14 19:36:18 - progress_bar.py[line:274] - INFO: epoch 001:    190 / 6313 loss=0.907, loss_v1=0, loss_v2=0, nll_loss=0.743, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=212.8, ups=0.85, wpb=251.8, bsz=100, num_updates=190, lr=6.02028e-06, gnorm=1.831, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=246
2023-03-14 19:36:30 - progress_bar.py[line:274] - INFO: epoch 001:    200 / 6313 loss=0.88, loss_v1=0, loss_v2=0, nll_loss=0.717, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.64, wps=222.5, ups=0.88, wpb=254, bsz=100, num_updates=200, lr=6.33714e-06, gnorm=1.703, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=258
2023-03-14 19:36:41 - progress_bar.py[line:274] - INFO: epoch 001:    210 / 6313 loss=0.863, loss_v1=0, loss_v2=0, nll_loss=0.698, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.62, wps=223.5, ups=0.87, wpb=255.5, bsz=100, num_updates=210, lr=6.65399e-06, gnorm=1.684, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=269
2023-03-14 19:36:53 - progress_bar.py[line:274] - INFO: epoch 001:    220 / 6313 loss=0.905, loss_v1=0, loss_v2=0, nll_loss=0.74, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.67, wps=216.1, ups=0.86, wpb=250.9, bsz=100, num_updates=220, lr=6.97085e-06, gnorm=1.775, clip=100, loss_scale=128, train_wall=12, gb_free=9.6, ema_decay=0.9999, wall=281
2023-03-14 19:37:04 - progress_bar.py[line:274] - INFO: epoch 001:    230 / 6313 loss=0.871, loss_v1=0, loss_v2=0, nll_loss=0.705, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.63, wps=218.3, ups=0.86, wpb=253.2, bsz=100, num_updates=230, lr=7.28771e-06, gnorm=1.749, clip=100, loss_scale=128, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=292
2023-03-14 19:37:16 - progress_bar.py[line:274] - INFO: epoch 001:    240 / 6313 loss=0.85, loss_v1=0, loss_v2=0, nll_loss=0.677, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.6, wps=216, ups=0.86, wpb=251.7, bsz=100, num_updates=240, lr=7.60456e-06, gnorm=1.829, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=304
2023-03-14 19:37:28 - progress_bar.py[line:274] - INFO: epoch 001:    250 / 6313 loss=0.834, loss_v1=0, loss_v2=0, nll_loss=0.654, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=218, ups=0.87, wpb=251.8, bsz=100, num_updates=250, lr=7.92142e-06, gnorm=1.796, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=316
2023-03-14 19:37:39 - progress_bar.py[line:274] - INFO: epoch 001:    260 / 6313 loss=0.849, loss_v1=0, loss_v2=0, nll_loss=0.669, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.59, wps=222.4, ups=0.88, wpb=253.6, bsz=100, num_updates=260, lr=8.23828e-06, gnorm=1.978, clip=100, loss_scale=128, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=327
2023-03-14 19:37:51 - progress_bar.py[line:274] - INFO: epoch 001:    270 / 6313 loss=0.831, loss_v1=0, loss_v2=0, nll_loss=0.655, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.57, wps=217.6, ups=0.86, wpb=252.8, bsz=100, num_updates=270, lr=8.55513e-06, gnorm=1.956, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=339
2023-03-14 19:38:02 - progress_bar.py[line:274] - INFO: epoch 001:    280 / 6313 loss=0.813, loss_v1=0, loss_v2=0, nll_loss=0.637, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.56, wps=218.3, ups=0.87, wpb=252.3, bsz=100, num_updates=280, lr=8.87199e-06, gnorm=1.858, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=350
2023-03-14 19:38:14 - progress_bar.py[line:274] - INFO: epoch 001:    290 / 6313 loss=0.8, loss_v1=0, loss_v2=0, nll_loss=0.617, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.53, wps=217.7, ups=0.86, wpb=254.2, bsz=100, num_updates=290, lr=9.18885e-06, gnorm=1.903, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=362
2023-03-14 19:38:26 - progress_bar.py[line:274] - INFO: epoch 001:    300 / 6313 loss=0.815, loss_v1=0, loss_v2=0, nll_loss=0.627, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.54, wps=216.6, ups=0.87, wpb=250.3, bsz=100, num_updates=300, lr=9.5057e-06, gnorm=1.724, clip=100, loss_scale=128, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=373
2023-03-14 19:38:37 - progress_bar.py[line:274] - INFO: epoch 001:    310 / 6313 loss=0.75, loss_v1=0, loss_v2=0, nll_loss=0.554, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=220.1, ups=0.87, wpb=254.3, bsz=100, num_updates=310, lr=9.82256e-06, gnorm=2.12, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=385
2023-03-14 19:38:49 - progress_bar.py[line:274] - INFO: epoch 001:    320 / 6313 loss=0.757, loss_v1=0, loss_v2=0, nll_loss=0.565, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.48, wps=220.2, ups=0.86, wpb=254.8, bsz=100, num_updates=320, lr=1.01394e-05, gnorm=1.96, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=397
2023-03-14 19:39:00 - progress_bar.py[line:274] - INFO: epoch 001:    330 / 6313 loss=0.744, loss_v1=0, loss_v2=0, nll_loss=0.551, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.47, wps=221.7, ups=0.88, wpb=252.8, bsz=100, num_updates=330, lr=1.04563e-05, gnorm=1.854, clip=100, loss_scale=128, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=408
2023-03-14 19:39:12 - progress_bar.py[line:274] - INFO: epoch 001:    340 / 6313 loss=0.743, loss_v1=0, loss_v2=0, nll_loss=0.543, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=217.3, ups=0.87, wpb=251.1, bsz=100, num_updates=340, lr=1.07731e-05, gnorm=1.874, clip=100, loss_scale=128, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=420
2023-03-14 19:39:23 - progress_bar.py[line:274] - INFO: epoch 001:    350 / 6313 loss=0.733, loss_v1=0, loss_v2=0, nll_loss=0.529, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=223.6, ups=0.88, wpb=255.3, bsz=100, num_updates=350, lr=1.109e-05, gnorm=1.883, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=431
2023-03-14 19:39:35 - progress_bar.py[line:274] - INFO: epoch 001:    360 / 6313 loss=0.732, loss_v1=0, loss_v2=0, nll_loss=0.536, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=216.2, ups=0.86, wpb=252.4, bsz=100, num_updates=360, lr=1.14068e-05, gnorm=1.819, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=443
2023-03-14 19:39:46 - progress_bar.py[line:274] - INFO: epoch 001:    370 / 6313 loss=0.734, loss_v1=0, loss_v2=0, nll_loss=0.528, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=217.9, ups=0.86, wpb=252.9, bsz=100, num_updates=370, lr=1.17237e-05, gnorm=1.953, clip=100, loss_scale=128, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=454
2023-03-14 19:39:58 - progress_bar.py[line:274] - INFO: epoch 001:    380 / 6313 loss=0.739, loss_v1=0, loss_v2=0, nll_loss=0.54, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.45, wps=218.5, ups=0.86, wpb=253, bsz=100, num_updates=380, lr=1.20406e-05, gnorm=1.799, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=466
2023-03-14 19:40:10 - progress_bar.py[line:274] - INFO: epoch 001:    390 / 6313 loss=0.722, loss_v1=0, loss_v2=0, nll_loss=0.521, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.43, wps=217.6, ups=0.86, wpb=251.6, bsz=100, num_updates=390, lr=1.23574e-05, gnorm=1.719, clip=100, loss_scale=128, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=477
2023-03-14 19:40:21 - progress_bar.py[line:274] - INFO: epoch 001:    400 / 6313 loss=0.735, loss_v1=0, loss_v2=0, nll_loss=0.541, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.46, wps=221.5, ups=0.87, wpb=254.4, bsz=100, num_updates=400, lr=1.26743e-05, gnorm=1.626, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=489
2023-03-14 19:40:33 - progress_bar.py[line:274] - INFO: epoch 001:    410 / 6313 loss=0.69, loss_v1=0, loss_v2=0, nll_loss=0.484, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=217.8, ups=0.85, wpb=255.6, bsz=100, num_updates=410, lr=1.29911e-05, gnorm=1.615, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=501
2023-03-14 19:40:45 - progress_bar.py[line:274] - INFO: epoch 001:    420 / 6313 loss=0.702, loss_v1=0, loss_v2=0, nll_loss=0.497, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=217.8, ups=0.86, wpb=252.5, bsz=100, num_updates=420, lr=1.3308e-05, gnorm=1.616, clip=100, loss_scale=128, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=512
2023-03-14 19:40:56 - progress_bar.py[line:274] - INFO: epoch 001:    430 / 6313 loss=0.729, loss_v1=0, loss_v2=0, nll_loss=0.53, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.44, wps=218.9, ups=0.86, wpb=253.5, bsz=100, num_updates=430, lr=1.36248e-05, gnorm=1.698, clip=100, loss_scale=128, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=524
2023-03-14 19:41:08 - progress_bar.py[line:274] - INFO: epoch 001:    440 / 6313 loss=0.698, loss_v1=0, loss_v2=0, nll_loss=0.501, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.41, wps=221.8, ups=0.87, wpb=253.9, bsz=100, num_updates=440, lr=1.39417e-05, gnorm=1.537, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=536
2023-03-14 19:41:19 - progress_bar.py[line:274] - INFO: epoch 001:    450 / 6313 loss=0.696, loss_v1=0, loss_v2=0, nll_loss=0.488, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=220.1, ups=0.87, wpb=251.8, bsz=100, num_updates=450, lr=1.42586e-05, gnorm=1.567, clip=100, loss_scale=128, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=547
2023-03-14 19:41:31 - progress_bar.py[line:274] - INFO: epoch 001:    460 / 6313 loss=0.692, loss_v1=0, loss_v2=0, nll_loss=0.485, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=219.9, ups=0.86, wpb=254.5, bsz=100, num_updates=460, lr=1.45754e-05, gnorm=1.669, clip=100, loss_scale=128, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=559
2023-03-14 19:41:42 - progress_bar.py[line:274] - INFO: epoch 001:    470 / 6313 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=224.1, ups=0.88, wpb=256, bsz=100, num_updates=470, lr=1.48923e-05, gnorm=1.524, clip=100, loss_scale=128, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=570
2023-03-14 19:41:54 - progress_bar.py[line:274] - INFO: epoch 001:    480 / 6313 loss=0.667, loss_v1=0, loss_v2=0, nll_loss=0.459, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=220.6, ups=0.87, wpb=255, bsz=100, num_updates=480, lr=1.52091e-05, gnorm=1.578, clip=100, loss_scale=128, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=582
2023-03-14 19:42:05 - progress_bar.py[line:274] - INFO: epoch 001:    490 / 6313 loss=0.671, loss_v1=0, loss_v2=0, nll_loss=0.467, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=218.4, ups=0.87, wpb=252.3, bsz=100, num_updates=490, lr=1.5526e-05, gnorm=1.675, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=593
2023-03-14 19:42:17 - progress_bar.py[line:274] - INFO: epoch 001:    500 / 6313 loss=0.677, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=221.4, ups=0.88, wpb=252.2, bsz=100, num_updates=500, lr=1.58428e-05, gnorm=1.697, clip=100, loss_scale=128, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=605
2023-03-14 19:42:28 - progress_bar.py[line:274] - INFO: epoch 001:    510 / 6313 loss=0.685, loss_v1=0, loss_v2=0, nll_loss=0.48, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=219.3, ups=0.86, wpb=253.8, bsz=100, num_updates=510, lr=1.61597e-05, gnorm=1.535, clip=100, loss_scale=128, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=616
2023-03-14 19:42:40 - progress_bar.py[line:274] - INFO: epoch 001:    520 / 6313 loss=0.679, loss_v1=0, loss_v2=0, nll_loss=0.477, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=219, ups=0.86, wpb=253.2, bsz=100, num_updates=520, lr=1.64766e-05, gnorm=1.454, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=628
2023-03-14 19:42:52 - progress_bar.py[line:274] - INFO: epoch 001:    530 / 6313 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.45, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=213.8, ups=0.84, wpb=254, bsz=100, num_updates=530, lr=1.67934e-05, gnorm=1.527, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=640
2023-03-14 19:43:03 - progress_bar.py[line:274] - INFO: epoch 001:    540 / 6313 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=219.8, ups=0.86, wpb=254.2, bsz=100, num_updates=540, lr=1.71103e-05, gnorm=1.491, clip=100, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=651
2023-03-14 19:43:15 - progress_bar.py[line:274] - INFO: epoch 001:    550 / 6313 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=220.9, ups=0.86, wpb=256.2, bsz=100, num_updates=550, lr=1.74271e-05, gnorm=1.247, clip=100, loss_scale=256, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=663
2023-03-14 19:43:27 - progress_bar.py[line:274] - INFO: epoch 001:    560 / 6313 loss=0.674, loss_v1=0, loss_v2=0, nll_loss=0.472, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=220.9, ups=0.86, wpb=255.8, bsz=100, num_updates=560, lr=1.7744e-05, gnorm=1.47, clip=100, loss_scale=256, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=674
2023-03-14 19:43:38 - progress_bar.py[line:274] - INFO: epoch 001:    570 / 6313 loss=0.667, loss_v1=0, loss_v2=0, nll_loss=0.464, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=221.5, ups=0.87, wpb=253.4, bsz=100, num_updates=570, lr=1.80608e-05, gnorm=1.529, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=686
2023-03-14 19:43:50 - progress_bar.py[line:274] - INFO: epoch 001:    580 / 6313 loss=0.655, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=221.7, ups=0.88, wpb=253.3, bsz=100, num_updates=580, lr=1.83777e-05, gnorm=1.451, clip=100, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=697
2023-03-14 19:44:01 - progress_bar.py[line:274] - INFO: epoch 001:    590 / 6313 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=256.9, nsentences=100, sample_size=256.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=221.9, ups=0.86, wpb=256.9, bsz=100, num_updates=590, lr=1.86946e-05, gnorm=1.679, clip=100, loss_scale=256, train_wall=12, gb_free=11, ema_decay=0.9999, wall=709
2023-03-14 19:44:13 - progress_bar.py[line:274] - INFO: epoch 001:    600 / 6313 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=222.5, ups=0.88, wpb=254, bsz=100, num_updates=600, lr=1.90114e-05, gnorm=1.704, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=720
2023-03-14 19:44:24 - progress_bar.py[line:274] - INFO: epoch 001:    610 / 6313 loss=0.698, loss_v1=0, loss_v2=0, nll_loss=0.49, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.4, wps=220.7, ups=0.87, wpb=252.6, bsz=100, num_updates=610, lr=1.93283e-05, gnorm=1.827, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=732
2023-03-14 19:44:36 - progress_bar.py[line:274] - INFO: epoch 001:    620 / 6313 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=220.9, ups=0.87, wpb=254.7, bsz=100, num_updates=620, lr=1.96451e-05, gnorm=1.648, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=743
2023-03-14 19:44:47 - progress_bar.py[line:274] - INFO: epoch 001:    630 / 6313 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.446, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=218, ups=0.86, wpb=252.4, bsz=100, num_updates=630, lr=1.9962e-05, gnorm=1.528, clip=100, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=755
2023-03-14 19:44:59 - progress_bar.py[line:274] - INFO: epoch 001:    640 / 6313 loss=0.678, loss_v1=0, loss_v2=0, nll_loss=0.478, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.39, wps=220.9, ups=0.87, wpb=254.8, bsz=100, num_updates=640, lr=2.02788e-05, gnorm=1.478, clip=100, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=767
2023-03-14 19:45:11 - progress_bar.py[line:274] - INFO: epoch 001:    650 / 6313 loss=0.653, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=214.9, ups=0.85, wpb=253.7, bsz=100, num_updates=650, lr=2.05957e-05, gnorm=1.293, clip=100, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=778
2023-03-14 19:45:22 - progress_bar.py[line:274] - INFO: epoch 001:    660 / 6313 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219.4, ups=0.87, wpb=253.6, bsz=100, num_updates=660, lr=2.09125e-05, gnorm=1.278, clip=90, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=790
2023-03-14 19:45:34 - progress_bar.py[line:274] - INFO: epoch 001:    670 / 6313 loss=0.669, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=217.6, ups=0.86, wpb=252.1, bsz=100, num_updates=670, lr=2.12294e-05, gnorm=1.36, clip=100, loss_scale=256, train_wall=12, gb_free=11, ema_decay=0.9999, wall=802
2023-03-14 19:45:46 - progress_bar.py[line:274] - INFO: epoch 001:    680 / 6313 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.447, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=213.9, ups=0.84, wpb=254.7, bsz=100, num_updates=680, lr=2.15463e-05, gnorm=1.289, clip=100, loss_scale=256, train_wall=12, gb_free=10, ema_decay=0.9999, wall=813
2023-03-14 19:45:57 - progress_bar.py[line:274] - INFO: epoch 001:    690 / 6313 loss=0.666, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=218.5, ups=0.86, wpb=253, bsz=100, num_updates=690, lr=2.18631e-05, gnorm=1.283, clip=100, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=825
2023-03-14 19:46:09 - progress_bar.py[line:274] - INFO: epoch 001:    700 / 6313 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219.8, ups=0.87, wpb=254.1, bsz=100, num_updates=700, lr=2.218e-05, gnorm=1.365, clip=100, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=837
2023-03-14 19:46:20 - progress_bar.py[line:274] - INFO: epoch 001:    710 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.9, ups=0.87, wpb=253, bsz=100, num_updates=710, lr=2.24968e-05, gnorm=1.317, clip=100, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=848
2023-03-14 19:46:32 - progress_bar.py[line:274] - INFO: epoch 001:    720 / 6313 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.1, ups=0.86, wpb=253.5, bsz=100, num_updates=720, lr=2.28137e-05, gnorm=1.364, clip=100, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=860
2023-03-14 19:46:44 - progress_bar.py[line:274] - INFO: epoch 001:    730 / 6313 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=225.1, ups=0.89, wpb=253.7, bsz=100, num_updates=730, lr=2.31305e-05, gnorm=1.322, clip=90, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=871
2023-03-14 19:46:55 - progress_bar.py[line:274] - INFO: epoch 001:    740 / 6313 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=256.6, nsentences=100, sample_size=256.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=221.3, ups=0.86, wpb=256.6, bsz=100, num_updates=740, lr=2.34474e-05, gnorm=1.386, clip=100, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=883
2023-03-14 19:47:07 - progress_bar.py[line:274] - INFO: epoch 001:    750 / 6313 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.3, ups=0.86, wpb=252.8, bsz=100, num_updates=750, lr=2.37643e-05, gnorm=1.323, clip=100, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=895
2023-03-14 19:47:18 - progress_bar.py[line:274] - INFO: epoch 001:    760 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220, ups=0.86, wpb=254.9, bsz=100, num_updates=760, lr=2.40811e-05, gnorm=1.361, clip=90, loss_scale=256, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=906
2023-03-14 19:47:30 - progress_bar.py[line:274] - INFO: epoch 001:    770 / 6313 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=208.8, ups=0.83, wpb=252.9, bsz=100, num_updates=770, lr=2.4398e-05, gnorm=1.214, clip=100, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=918
2023-03-14 19:47:42 - progress_bar.py[line:274] - INFO: epoch 001:    780 / 6313 loss=0.664, loss_v1=0, loss_v2=0, nll_loss=0.461, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=222.4, ups=0.88, wpb=253.7, bsz=100, num_updates=780, lr=2.47148e-05, gnorm=1.422, clip=100, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=930
2023-03-14 19:47:53 - progress_bar.py[line:274] - INFO: epoch 001:    790 / 6313 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=220, ups=0.87, wpb=254.3, bsz=100, num_updates=790, lr=2.50317e-05, gnorm=1.413, clip=90, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=941
2023-03-14 19:48:05 - progress_bar.py[line:274] - INFO: epoch 001:    800 / 6313 loss=0.657, loss_v1=0, loss_v2=0, nll_loss=0.452, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=217.5, ups=0.86, wpb=251.9, bsz=100, num_updates=800, lr=2.53485e-05, gnorm=1.373, clip=100, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=953
2023-03-14 19:48:17 - progress_bar.py[line:274] - INFO: epoch 001:    810 / 6313 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=216.5, ups=0.87, wpb=250.3, bsz=100, num_updates=810, lr=2.56654e-05, gnorm=1.197, clip=90, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=964
2023-03-14 19:48:28 - progress_bar.py[line:274] - INFO: epoch 001:    820 / 6313 loss=0.658, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=221.4, ups=0.87, wpb=253.6, bsz=100, num_updates=820, lr=2.59823e-05, gnorm=1.302, clip=90, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=976
2023-03-14 19:48:40 - progress_bar.py[line:274] - INFO: epoch 001:    830 / 6313 loss=0.648, loss_v1=0, loss_v2=0, nll_loss=0.442, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=217, ups=0.86, wpb=251.4, bsz=100, num_updates=830, lr=2.62991e-05, gnorm=1.182, clip=80, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=987
2023-03-14 19:48:51 - progress_bar.py[line:274] - INFO: epoch 001:    840 / 6313 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.443, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219.3, ups=0.86, wpb=254.1, bsz=100, num_updates=840, lr=2.6616e-05, gnorm=1.136, clip=80, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=999
2023-03-14 19:49:03 - progress_bar.py[line:274] - INFO: epoch 001:    850 / 6313 loss=0.644, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.1, ups=0.86, wpb=252.6, bsz=100, num_updates=850, lr=2.69328e-05, gnorm=1.19, clip=100, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1011
2023-03-14 19:49:14 - progress_bar.py[line:274] - INFO: epoch 001:    860 / 6313 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.7, ups=0.86, wpb=255.8, bsz=100, num_updates=860, lr=2.72497e-05, gnorm=1.242, clip=90, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1022
2023-03-14 19:49:26 - progress_bar.py[line:274] - INFO: epoch 001:    870 / 6313 loss=0.675, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=249.4, nsentences=100, sample_size=249.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=215, ups=0.86, wpb=249.4, bsz=100, num_updates=870, lr=2.75665e-05, gnorm=1.333, clip=100, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1034
2023-03-14 19:49:38 - progress_bar.py[line:274] - INFO: epoch 001:    880 / 6313 loss=0.654, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=217.4, ups=0.87, wpb=251.2, bsz=100, num_updates=880, lr=2.78834e-05, gnorm=1.26, clip=90, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1045
2023-03-14 19:49:49 - progress_bar.py[line:274] - INFO: epoch 001:    890 / 6313 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.43, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=214.8, ups=0.84, wpb=255.5, bsz=100, num_updates=890, lr=2.82003e-05, gnorm=1.292, clip=100, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=1057
2023-03-14 19:50:01 - progress_bar.py[line:274] - INFO: epoch 001:    900 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.5, ups=0.86, wpb=254.3, bsz=100, num_updates=900, lr=2.85171e-05, gnorm=1.173, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1069
2023-03-14 19:50:13 - progress_bar.py[line:274] - INFO: epoch 001:    910 / 6313 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=217.8, ups=0.86, wpb=252.3, bsz=100, num_updates=910, lr=2.8834e-05, gnorm=1.283, clip=100, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1081
2023-03-14 19:50:24 - progress_bar.py[line:274] - INFO: epoch 001:    920 / 6313 loss=0.671, loss_v1=0, loss_v2=0, nll_loss=0.468, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=216.8, ups=0.86, wpb=251.2, bsz=100, num_updates=920, lr=2.91508e-05, gnorm=1.146, clip=70, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1092
2023-03-14 19:50:36 - progress_bar.py[line:274] - INFO: epoch 001:    930 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.8, ups=0.86, wpb=253.9, bsz=100, num_updates=930, lr=2.94677e-05, gnorm=1.083, clip=80, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1104
2023-03-14 19:50:48 - progress_bar.py[line:274] - INFO: epoch 001:    940 / 6313 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=249.5, nsentences=100, sample_size=249.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=215.6, ups=0.86, wpb=249.5, bsz=100, num_updates=940, lr=2.97845e-05, gnorm=1.215, clip=70, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1115
2023-03-14 19:50:59 - progress_bar.py[line:274] - INFO: epoch 001:    950 / 6313 loss=0.665, loss_v1=0, loss_v2=0, nll_loss=0.456, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=217.4, ups=0.87, wpb=250.8, bsz=100, num_updates=950, lr=3.01014e-05, gnorm=1.264, clip=100, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1127
2023-03-14 19:51:11 - progress_bar.py[line:274] - INFO: epoch 001:    960 / 6313 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.3, ups=0.86, wpb=253.6, bsz=100, num_updates=960, lr=3.04183e-05, gnorm=1.137, clip=80, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1139
2023-03-14 19:51:22 - progress_bar.py[line:274] - INFO: epoch 001:    970 / 6313 loss=0.664, loss_v1=0, loss_v2=0, nll_loss=0.455, ntokens=248.3, nsentences=100, sample_size=248.3, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=214.2, ups=0.86, wpb=248.3, bsz=100, num_updates=970, lr=3.07351e-05, gnorm=1.142, clip=80, loss_scale=256, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1150
2023-03-14 19:51:34 - progress_bar.py[line:274] - INFO: epoch 001:    980 / 6313 loss=0.666, loss_v1=0, loss_v2=0, nll_loss=0.466, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=218.1, ups=0.86, wpb=252.3, bsz=100, num_updates=980, lr=3.1052e-05, gnorm=1.2, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1162
2023-03-14 19:51:45 - progress_bar.py[line:274] - INFO: epoch 001:    990 / 6313 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.3, ups=0.86, wpb=252.4, bsz=100, num_updates=990, lr=3.13688e-05, gnorm=1.081, clip=80, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1173
2023-03-14 19:51:57 - progress_bar.py[line:274] - INFO: epoch 001:   1000 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.5, ups=0.87, wpb=251.2, bsz=100, num_updates=1000, lr=3.16857e-05, gnorm=1.242, clip=100, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1185
2023-03-14 19:52:09 - progress_bar.py[line:274] - INFO: epoch 001:   1010 / 6313 loss=0.644, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.6, ups=0.87, wpb=252.1, bsz=100, num_updates=1010, lr=3.20025e-05, gnorm=1.221, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1197
2023-03-14 19:52:20 - progress_bar.py[line:274] - INFO: epoch 001:   1020 / 6313 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.453, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=218.5, ups=0.87, wpb=252.1, bsz=100, num_updates=1020, lr=3.23194e-05, gnorm=1.252, clip=80, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1208
2023-03-14 19:52:32 - progress_bar.py[line:274] - INFO: epoch 001:   1030 / 6313 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=217, ups=0.86, wpb=251.4, bsz=100, num_updates=1030, lr=3.26362e-05, gnorm=1.282, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1220
2023-03-14 19:52:43 - progress_bar.py[line:274] - INFO: epoch 001:   1040 / 6313 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.438, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219.4, ups=0.86, wpb=253.9, bsz=100, num_updates=1040, lr=3.29531e-05, gnorm=1.048, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1231
2023-03-14 19:52:55 - progress_bar.py[line:274] - INFO: epoch 001:   1050 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.2, ups=0.86, wpb=254.8, bsz=100, num_updates=1050, lr=3.327e-05, gnorm=1.024, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1243
2023-03-14 19:53:07 - progress_bar.py[line:274] - INFO: epoch 001:   1060 / 6313 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218, ups=0.86, wpb=252.4, bsz=100, num_updates=1060, lr=3.35868e-05, gnorm=1.177, clip=70, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1254
2023-03-14 19:53:18 - progress_bar.py[line:274] - INFO: epoch 001:   1070 / 6313 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=249.3, nsentences=100, sample_size=249.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=215.2, ups=0.86, wpb=249.3, bsz=100, num_updates=1070, lr=3.39037e-05, gnorm=1.159, clip=80, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1266
2023-03-14 19:53:30 - progress_bar.py[line:274] - INFO: epoch 001:   1080 / 6313 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=222.6, ups=0.88, wpb=254.2, bsz=100, num_updates=1080, lr=3.42205e-05, gnorm=1.216, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1277
2023-03-14 19:53:41 - progress_bar.py[line:274] - INFO: epoch 001:   1090 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=257.3, nsentences=100, sample_size=257.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221.9, ups=0.86, wpb=257.3, bsz=100, num_updates=1090, lr=3.45374e-05, gnorm=1.26, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1289
2023-03-14 19:53:53 - progress_bar.py[line:274] - INFO: epoch 001:   1100 / 6313 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=220.6, ups=0.88, wpb=252.1, bsz=100, num_updates=1100, lr=3.48542e-05, gnorm=1.09, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1300
2023-03-14 19:54:04 - progress_bar.py[line:274] - INFO: epoch 001:   1110 / 6313 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=221.7, ups=0.88, wpb=253.3, bsz=100, num_updates=1110, lr=3.51711e-05, gnorm=1.258, clip=100, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1312
2023-03-14 19:54:16 - progress_bar.py[line:274] - INFO: epoch 001:   1120 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.9, ups=0.86, wpb=253.1, bsz=100, num_updates=1120, lr=3.5488e-05, gnorm=1.048, clip=60, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1324
2023-03-14 19:54:27 - progress_bar.py[line:274] - INFO: epoch 001:   1130 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.4, ups=0.85, wpb=254.7, bsz=100, num_updates=1130, lr=3.58048e-05, gnorm=1.128, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1335
2023-03-14 19:54:39 - progress_bar.py[line:274] - INFO: epoch 001:   1140 / 6313 loss=0.651, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=217.8, ups=0.87, wpb=251.1, bsz=100, num_updates=1140, lr=3.61217e-05, gnorm=1.078, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1347
2023-03-14 19:54:50 - progress_bar.py[line:274] - INFO: epoch 001:   1150 / 6313 loss=0.655, loss_v1=0, loss_v2=0, nll_loss=0.449, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=219.8, ups=0.87, wpb=252.7, bsz=100, num_updates=1150, lr=3.64385e-05, gnorm=1.066, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1358
2023-03-14 19:55:02 - progress_bar.py[line:274] - INFO: epoch 001:   1160 / 6313 loss=0.643, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219.8, ups=0.86, wpb=254.6, bsz=100, num_updates=1160, lr=3.67554e-05, gnorm=1.153, clip=80, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1370
2023-03-14 19:55:14 - progress_bar.py[line:274] - INFO: epoch 001:   1170 / 6313 loss=0.662, loss_v1=0, loss_v2=0, nll_loss=0.46, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=219.6, ups=0.87, wpb=251.4, bsz=100, num_updates=1170, lr=3.70722e-05, gnorm=1.086, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1381
2023-03-14 19:55:25 - progress_bar.py[line:274] - INFO: epoch 001:   1180 / 6313 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.3, ups=0.86, wpb=255.1, bsz=100, num_updates=1180, lr=3.73891e-05, gnorm=1.266, clip=90, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1393
2023-03-14 19:55:37 - progress_bar.py[line:274] - INFO: epoch 001:   1190 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.5, ups=0.87, wpb=253.6, bsz=100, num_updates=1190, lr=3.7706e-05, gnorm=0.987, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1405
2023-03-14 19:55:48 - progress_bar.py[line:274] - INFO: epoch 001:   1200 / 6313 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=219.6, ups=0.86, wpb=254, bsz=100, num_updates=1200, lr=3.80228e-05, gnorm=1.02, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1416
2023-03-14 19:56:00 - progress_bar.py[line:274] - INFO: epoch 001:   1210 / 6313 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=217.4, ups=0.87, wpb=250.8, bsz=100, num_updates=1210, lr=3.83397e-05, gnorm=1.073, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1428
2023-03-14 19:56:11 - progress_bar.py[line:274] - INFO: epoch 001:   1220 / 6313 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.3, ups=0.86, wpb=253.7, bsz=100, num_updates=1220, lr=3.86565e-05, gnorm=1, clip=60, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1439
2023-03-14 19:56:23 - progress_bar.py[line:274] - INFO: epoch 001:   1230 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.8, ups=0.86, wpb=253.4, bsz=100, num_updates=1230, lr=3.89734e-05, gnorm=0.936, clip=30, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1451
2023-03-14 19:56:35 - progress_bar.py[line:274] - INFO: epoch 001:   1240 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=257.8, nsentences=100, sample_size=257.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=223.4, ups=0.87, wpb=257.8, bsz=100, num_updates=1240, lr=3.92902e-05, gnorm=1.018, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1462
2023-03-14 19:56:46 - progress_bar.py[line:274] - INFO: epoch 001:   1250 / 6313 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=215.5, ups=0.85, wpb=253, bsz=100, num_updates=1250, lr=3.96071e-05, gnorm=1.02, clip=70, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1474
2023-03-14 19:56:58 - progress_bar.py[line:274] - INFO: epoch 001:   1260 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=214.3, ups=0.84, wpb=253.8, bsz=100, num_updates=1260, lr=3.9924e-05, gnorm=0.974, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1486
2023-03-14 19:57:10 - progress_bar.py[line:274] - INFO: epoch 001:   1270 / 6313 loss=0.638, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.5, ups=0.86, wpb=253.9, bsz=100, num_updates=1270, lr=4.02408e-05, gnorm=1.155, clip=80, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1498
2023-03-14 19:57:21 - progress_bar.py[line:274] - INFO: epoch 001:   1280 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.3, ups=0.87, wpb=253.4, bsz=100, num_updates=1280, lr=4.05577e-05, gnorm=0.964, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1509
2023-03-14 19:57:33 - progress_bar.py[line:274] - INFO: epoch 001:   1290 / 6313 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.441, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=218.9, ups=0.87, wpb=252.8, bsz=100, num_updates=1290, lr=4.08745e-05, gnorm=1.066, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1521
2023-03-14 19:57:44 - progress_bar.py[line:274] - INFO: epoch 001:   1300 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.8, ups=0.87, wpb=252.7, bsz=100, num_updates=1300, lr=4.11914e-05, gnorm=0.996, clip=40, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=1532
2023-03-14 19:57:56 - progress_bar.py[line:274] - INFO: epoch 001:   1310 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.8, ups=0.86, wpb=254.5, bsz=100, num_updates=1310, lr=4.15082e-05, gnorm=1.025, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1544
2023-03-14 19:58:08 - progress_bar.py[line:274] - INFO: epoch 001:   1320 / 6313 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=218.7, ups=0.86, wpb=252.9, bsz=100, num_updates=1320, lr=4.18251e-05, gnorm=1.086, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1555
2023-03-14 19:58:19 - progress_bar.py[line:274] - INFO: epoch 001:   1330 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=222, ups=0.87, wpb=255.7, bsz=100, num_updates=1330, lr=4.2142e-05, gnorm=1.003, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1567
2023-03-14 19:58:31 - progress_bar.py[line:274] - INFO: epoch 001:   1340 / 6313 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=220.7, ups=0.88, wpb=251.7, bsz=100, num_updates=1340, lr=4.24588e-05, gnorm=0.946, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1578
2023-03-14 19:58:42 - progress_bar.py[line:274] - INFO: epoch 001:   1350 / 6313 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.1, ups=0.87, wpb=254.2, bsz=100, num_updates=1350, lr=4.27757e-05, gnorm=1.012, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1590
2023-03-14 19:58:54 - progress_bar.py[line:274] - INFO: epoch 001:   1360 / 6313 loss=0.645, loss_v1=0, loss_v2=0, nll_loss=0.445, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=218.2, ups=0.86, wpb=252.4, bsz=100, num_updates=1360, lr=4.30925e-05, gnorm=1.078, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1601
2023-03-14 19:59:05 - progress_bar.py[line:274] - INFO: epoch 001:   1370 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218, ups=0.87, wpb=251.4, bsz=100, num_updates=1370, lr=4.34094e-05, gnorm=1.069, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1613
2023-03-14 19:59:17 - progress_bar.py[line:274] - INFO: epoch 001:   1380 / 6313 loss=0.639, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=220.2, ups=0.87, wpb=254.3, bsz=100, num_updates=1380, lr=4.37262e-05, gnorm=1.056, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=1625
2023-03-14 19:59:28 - progress_bar.py[line:274] - INFO: epoch 001:   1390 / 6313 loss=0.649, loss_v1=0, loss_v2=0, nll_loss=0.451, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=217.8, ups=0.86, wpb=251.9, bsz=100, num_updates=1390, lr=4.40431e-05, gnorm=0.926, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1636
2023-03-14 19:59:40 - progress_bar.py[line:274] - INFO: epoch 001:   1400 / 6313 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.6, ups=0.86, wpb=253.1, bsz=100, num_updates=1400, lr=4.43599e-05, gnorm=1.043, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1648
2023-03-14 19:59:51 - progress_bar.py[line:274] - INFO: epoch 001:   1410 / 6313 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=222.5, ups=0.88, wpb=254, bsz=100, num_updates=1410, lr=4.46768e-05, gnorm=0.876, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1659
2023-03-14 20:00:03 - progress_bar.py[line:274] - INFO: epoch 001:   1420 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=224.4, ups=0.88, wpb=255.9, bsz=100, num_updates=1420, lr=4.49937e-05, gnorm=0.869, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1671
2023-03-14 20:00:14 - progress_bar.py[line:274] - INFO: epoch 001:   1430 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.5, ups=0.86, wpb=254.2, bsz=100, num_updates=1430, lr=4.53105e-05, gnorm=0.938, clip=40, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1682
2023-03-14 20:00:26 - progress_bar.py[line:274] - INFO: epoch 001:   1440 / 6313 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.6, ups=0.87, wpb=252.8, bsz=100, num_updates=1440, lr=4.56274e-05, gnorm=0.955, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=1694
2023-03-14 20:00:37 - progress_bar.py[line:274] - INFO: epoch 001:   1450 / 6313 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=218.9, ups=0.87, wpb=253, bsz=100, num_updates=1450, lr=4.59442e-05, gnorm=1.035, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1705
2023-03-14 20:00:49 - progress_bar.py[line:274] - INFO: epoch 001:   1460 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.9, ups=0.86, wpb=255, bsz=100, num_updates=1460, lr=4.62611e-05, gnorm=0.983, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1717
2023-03-14 20:01:01 - progress_bar.py[line:274] - INFO: epoch 001:   1470 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.5, ups=0.86, wpb=253.1, bsz=100, num_updates=1470, lr=4.65779e-05, gnorm=1.082, clip=60, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1728
2023-03-14 20:01:12 - progress_bar.py[line:274] - INFO: epoch 001:   1480 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.6, ups=0.86, wpb=254.2, bsz=100, num_updates=1480, lr=4.68948e-05, gnorm=0.904, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1740
2023-03-14 20:01:24 - progress_bar.py[line:274] - INFO: epoch 001:   1490 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.9, ups=0.87, wpb=254.4, bsz=100, num_updates=1490, lr=4.72117e-05, gnorm=0.966, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1752
2023-03-14 20:01:36 - progress_bar.py[line:274] - INFO: epoch 001:   1500 / 6313 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=216.5, ups=0.85, wpb=255.3, bsz=100, num_updates=1500, lr=4.75285e-05, gnorm=1.03, clip=50, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1763
2023-03-14 20:01:47 - progress_bar.py[line:274] - INFO: epoch 001:   1510 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=222.1, ups=0.88, wpb=253.5, bsz=100, num_updates=1510, lr=4.78454e-05, gnorm=1.039, clip=60, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=1775
2023-03-14 20:01:58 - progress_bar.py[line:274] - INFO: epoch 001:   1520 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=221.5, ups=0.87, wpb=253.9, bsz=100, num_updates=1520, lr=4.81622e-05, gnorm=0.944, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1786
2023-03-14 20:02:10 - progress_bar.py[line:274] - INFO: epoch 001:   1530 / 6313 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.1, ups=0.86, wpb=254, bsz=100, num_updates=1530, lr=4.84791e-05, gnorm=1.041, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1798
2023-03-14 20:02:21 - progress_bar.py[line:274] - INFO: epoch 001:   1540 / 6313 loss=0.66, loss_v1=0, loss_v2=0, nll_loss=0.458, ntokens=249.8, nsentences=100, sample_size=249.8, sample_size_v1=0, sample_size_v2=0, ppl=1.37, wps=219.1, ups=0.88, wpb=249.8, bsz=100, num_updates=1540, lr=4.87959e-05, gnorm=1.023, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1809
2023-03-14 20:02:25 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 20:02:34 - progress_bar.py[line:274] - INFO: epoch 001:   1551 / 6313 loss=0.63, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=203, ups=0.8, wpb=252.6, bsz=100, num_updates=1550, lr=4.91128e-05, gnorm=0.932, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1822
2023-03-14 20:02:46 - progress_bar.py[line:274] - INFO: epoch 001:   1561 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.3, ups=0.86, wpb=252.8, bsz=100, num_updates=1560, lr=4.94297e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1833
2023-03-14 20:02:57 - progress_bar.py[line:274] - INFO: epoch 001:   1571 / 6313 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=221.3, ups=0.88, wpb=252.7, bsz=100, num_updates=1570, lr=4.97465e-05, gnorm=0.979, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=1845
2023-03-14 20:03:09 - progress_bar.py[line:274] - INFO: epoch 001:   1581 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.5, ups=0.86, wpb=253.8, bsz=100, num_updates=1580, lr=4.99967e-05, gnorm=0.896, clip=30, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1856
2023-03-14 20:03:20 - progress_bar.py[line:274] - INFO: epoch 001:   1591 / 6313 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.9, ups=0.86, wpb=252.5, bsz=100, num_updates=1590, lr=4.998e-05, gnorm=0.891, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1868
2023-03-14 20:03:32 - progress_bar.py[line:274] - INFO: epoch 001:   1601 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=221.8, ups=0.87, wpb=254.8, bsz=100, num_updates=1600, lr=4.99633e-05, gnorm=0.963, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=1880
2023-03-14 20:03:43 - progress_bar.py[line:274] - INFO: epoch 001:   1611 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.1, ups=0.86, wpb=255.2, bsz=100, num_updates=1610, lr=4.99466e-05, gnorm=0.938, clip=40, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=1891
2023-03-14 20:03:55 - progress_bar.py[line:274] - INFO: epoch 001:   1621 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=216.8, ups=0.85, wpb=254.2, bsz=100, num_updates=1620, lr=4.993e-05, gnorm=0.9, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=1903
2023-03-14 20:04:07 - progress_bar.py[line:274] - INFO: epoch 001:   1631 / 6313 loss=0.628, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.3, ups=0.87, wpb=251.8, bsz=100, num_updates=1630, lr=4.99133e-05, gnorm=1.07, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=1915
2023-03-14 20:04:18 - progress_bar.py[line:274] - INFO: epoch 001:   1641 / 6313 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.3, ups=0.86, wpb=253.9, bsz=100, num_updates=1640, lr=4.98966e-05, gnorm=0.969, clip=30, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=1926
2023-03-14 20:04:30 - progress_bar.py[line:274] - INFO: epoch 001:   1651 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.2, ups=0.86, wpb=255.2, bsz=100, num_updates=1650, lr=4.98799e-05, gnorm=0.831, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=1938
2023-03-14 20:04:41 - progress_bar.py[line:274] - INFO: epoch 001:   1661 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=222.7, ups=0.88, wpb=254.4, bsz=100, num_updates=1660, lr=4.98633e-05, gnorm=0.856, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=1949
2023-03-14 20:04:53 - progress_bar.py[line:274] - INFO: epoch 001:   1671 / 6313 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.9, ups=0.87, wpb=253.9, bsz=100, num_updates=1670, lr=4.98466e-05, gnorm=0.846, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=1961
2023-03-14 20:05:04 - progress_bar.py[line:274] - INFO: epoch 001:   1681 / 6313 loss=0.646, loss_v1=0, loss_v2=0, nll_loss=0.439, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=222.2, ups=0.89, wpb=250.1, bsz=100, num_updates=1680, lr=4.98299e-05, gnorm=0.99, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1972
2023-03-14 20:05:16 - progress_bar.py[line:274] - INFO: epoch 001:   1691 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=222.4, ups=0.88, wpb=253.5, bsz=100, num_updates=1690, lr=4.98133e-05, gnorm=0.927, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=1983
2023-03-14 20:05:27 - progress_bar.py[line:274] - INFO: epoch 001:   1701 / 6313 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=216.3, ups=0.86, wpb=250.5, bsz=100, num_updates=1700, lr=4.97966e-05, gnorm=0.899, clip=20, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=1995
2023-03-14 20:05:39 - progress_bar.py[line:274] - INFO: epoch 001:   1711 / 6313 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=220.2, ups=0.87, wpb=254.2, bsz=100, num_updates=1710, lr=4.97799e-05, gnorm=0.911, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=2006
2023-03-14 20:05:50 - progress_bar.py[line:274] - INFO: epoch 001:   1721 / 6313 loss=0.641, loss_v1=0, loss_v2=0, nll_loss=0.444, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=222.8, ups=0.88, wpb=253.4, bsz=100, num_updates=1720, lr=4.97632e-05, gnorm=0.88, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2018
2023-03-14 20:06:02 - progress_bar.py[line:274] - INFO: epoch 001:   1731 / 6313 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.1, ups=0.86, wpb=253.3, bsz=100, num_updates=1730, lr=4.97466e-05, gnorm=1.018, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2030
2023-03-14 20:06:13 - progress_bar.py[line:274] - INFO: epoch 001:   1741 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=215.1, ups=0.85, wpb=252.5, bsz=100, num_updates=1740, lr=4.97299e-05, gnorm=0.876, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2041
2023-03-14 20:06:25 - progress_bar.py[line:274] - INFO: epoch 001:   1751 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.9, ups=0.88, wpb=251.3, bsz=100, num_updates=1750, lr=4.97132e-05, gnorm=0.847, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2053
2023-03-14 20:06:36 - progress_bar.py[line:274] - INFO: epoch 001:   1761 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.409, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.9, ups=0.86, wpb=253.6, bsz=100, num_updates=1760, lr=4.96965e-05, gnorm=0.855, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2064
2023-03-14 20:06:48 - progress_bar.py[line:274] - INFO: epoch 001:   1771 / 6313 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.3, ups=0.86, wpb=252.7, bsz=100, num_updates=1770, lr=4.96799e-05, gnorm=0.926, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2076
2023-03-14 20:07:00 - progress_bar.py[line:274] - INFO: epoch 001:   1781 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.432, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=220.2, ups=0.86, wpb=254.7, bsz=100, num_updates=1780, lr=4.96632e-05, gnorm=0.888, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2087
2023-03-14 20:07:11 - progress_bar.py[line:274] - INFO: epoch 001:   1791 / 6313 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=223.6, ups=0.88, wpb=255.5, bsz=100, num_updates=1790, lr=4.96465e-05, gnorm=0.903, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2099
2023-03-14 20:07:23 - progress_bar.py[line:274] - INFO: epoch 001:   1801 / 6313 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=223.1, ups=0.88, wpb=255, bsz=100, num_updates=1800, lr=4.96298e-05, gnorm=0.885, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2110
2023-03-14 20:07:34 - progress_bar.py[line:274] - INFO: epoch 001:   1811 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=223, ups=0.88, wpb=254.3, bsz=100, num_updates=1810, lr=4.96132e-05, gnorm=0.926, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2122
2023-03-14 20:07:45 - progress_bar.py[line:274] - INFO: epoch 001:   1821 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.3, ups=0.87, wpb=250.9, bsz=100, num_updates=1820, lr=4.95965e-05, gnorm=0.917, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2133
2023-03-14 20:07:57 - progress_bar.py[line:274] - INFO: epoch 001:   1831 / 6313 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.8, ups=0.86, wpb=253.6, bsz=100, num_updates=1830, lr=4.95798e-05, gnorm=0.797, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2145
2023-03-14 20:08:09 - progress_bar.py[line:274] - INFO: epoch 001:   1841 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=221.9, ups=0.87, wpb=253.7, bsz=100, num_updates=1840, lr=4.95631e-05, gnorm=0.853, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2156
2023-03-14 20:08:20 - progress_bar.py[line:274] - INFO: epoch 001:   1851 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.7, ups=0.86, wpb=255.7, bsz=100, num_updates=1850, lr=4.95465e-05, gnorm=0.866, clip=20, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=2168
2023-03-14 20:08:33 - progress_bar.py[line:274] - INFO: epoch 001:   1861 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=206.1, ups=0.81, wpb=253.8, bsz=100, num_updates=1860, lr=4.95298e-05, gnorm=0.866, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2180
2023-03-14 20:08:44 - progress_bar.py[line:274] - INFO: epoch 001:   1871 / 6313 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.7, ups=0.87, wpb=252.2, bsz=100, num_updates=1870, lr=4.95131e-05, gnorm=0.944, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2192
2023-03-14 20:08:56 - progress_bar.py[line:274] - INFO: epoch 001:   1881 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.2, ups=0.87, wpb=254.5, bsz=100, num_updates=1880, lr=4.94964e-05, gnorm=1.005, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2203
2023-03-14 20:09:07 - progress_bar.py[line:274] - INFO: epoch 001:   1891 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=223.2, ups=0.87, wpb=255.2, bsz=100, num_updates=1890, lr=4.94798e-05, gnorm=0.803, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2215
2023-03-14 20:09:19 - progress_bar.py[line:274] - INFO: epoch 001:   1901 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.2, ups=0.87, wpb=254.3, bsz=100, num_updates=1900, lr=4.94631e-05, gnorm=0.803, clip=0, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=2226
2023-03-14 20:09:30 - progress_bar.py[line:274] - INFO: epoch 001:   1911 / 6313 loss=0.652, loss_v1=0, loss_v2=0, nll_loss=0.448, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=212.5, ups=0.84, wpb=251.5, bsz=100, num_updates=1910, lr=4.94464e-05, gnorm=0.865, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2238
2023-03-14 20:09:42 - progress_bar.py[line:274] - INFO: epoch 001:   1921 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.6, ups=0.86, wpb=253.1, bsz=100, num_updates=1920, lr=4.94298e-05, gnorm=0.819, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2250
2023-03-14 20:09:54 - progress_bar.py[line:274] - INFO: epoch 001:   1931 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220, ups=0.86, wpb=254.4, bsz=100, num_updates=1930, lr=4.94131e-05, gnorm=0.935, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2261
2023-03-14 20:10:05 - progress_bar.py[line:274] - INFO: epoch 001:   1941 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.6, ups=0.87, wpb=253.3, bsz=100, num_updates=1940, lr=4.93964e-05, gnorm=0.903, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2273
2023-03-14 20:10:17 - progress_bar.py[line:274] - INFO: epoch 001:   1951 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.7, ups=0.86, wpb=253, bsz=100, num_updates=1950, lr=4.93797e-05, gnorm=0.784, clip=10, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=2285
2023-03-14 20:10:28 - progress_bar.py[line:274] - INFO: epoch 001:   1961 / 6313 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=220.2, ups=0.87, wpb=251.9, bsz=100, num_updates=1960, lr=4.93631e-05, gnorm=0.839, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2296
2023-03-14 20:10:40 - progress_bar.py[line:274] - INFO: epoch 001:   1971 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.9, ups=0.86, wpb=253.1, bsz=100, num_updates=1970, lr=4.93464e-05, gnorm=0.843, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2308
2023-03-14 20:10:52 - progress_bar.py[line:274] - INFO: epoch 001:   1981 / 6313 loss=0.666, loss_v1=0, loss_v2=0, nll_loss=0.463, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.38, wps=213.7, ups=0.85, wpb=250.7, bsz=100, num_updates=1980, lr=4.93297e-05, gnorm=0.947, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2319
2023-03-14 20:11:03 - progress_bar.py[line:274] - INFO: epoch 001:   1991 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220.2, ups=0.87, wpb=253.8, bsz=100, num_updates=1990, lr=4.9313e-05, gnorm=0.858, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2331
2023-03-14 20:11:15 - progress_bar.py[line:274] - INFO: epoch 001:   2001 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219, ups=0.87, wpb=253, bsz=100, num_updates=2000, lr=4.92964e-05, gnorm=0.935, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2343
2023-03-14 20:11:26 - progress_bar.py[line:274] - INFO: epoch 001:   2011 / 6313 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.4, ups=0.86, wpb=254.3, bsz=100, num_updates=2010, lr=4.92797e-05, gnorm=0.776, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2354
2023-03-14 20:11:38 - progress_bar.py[line:274] - INFO: epoch 001:   2021 / 6313 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.431, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=217.6, ups=0.86, wpb=252.5, bsz=100, num_updates=2020, lr=4.9263e-05, gnorm=0.931, clip=30, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2366
2023-03-14 20:11:50 - progress_bar.py[line:274] - INFO: epoch 001:   2031 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=221.3, ups=0.86, wpb=256.2, bsz=100, num_updates=2030, lr=4.92463e-05, gnorm=0.946, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2377
2023-03-14 20:12:01 - progress_bar.py[line:274] - INFO: epoch 001:   2041 / 6313 loss=0.642, loss_v1=0, loss_v2=0, nll_loss=0.44, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.36, wps=218.3, ups=0.86, wpb=252.5, bsz=100, num_updates=2040, lr=4.92297e-05, gnorm=0.879, clip=30, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=2389
2023-03-14 20:12:13 - progress_bar.py[line:274] - INFO: epoch 001:   2051 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.4, ups=0.86, wpb=253.8, bsz=100, num_updates=2050, lr=4.9213e-05, gnorm=0.883, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2401
2023-03-14 20:12:18 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 20:12:25 - progress_bar.py[line:274] - INFO: epoch 001:   2062 / 6313 loss=0.647, loss_v1=0, loss_v2=0, nll_loss=0.437, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=203.2, ups=0.81, wpb=250.5, bsz=100, num_updates=2060, lr=4.91963e-05, gnorm=0.968, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2413
2023-03-14 20:12:36 - progress_bar.py[line:274] - INFO: epoch 001:   2072 / 6313 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=223.3, ups=0.88, wpb=254.5, bsz=100, num_updates=2070, lr=4.91796e-05, gnorm=0.877, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2424
2023-03-14 20:12:48 - progress_bar.py[line:274] - INFO: epoch 001:   2082 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=223, ups=0.87, wpb=255.5, bsz=100, num_updates=2080, lr=4.9163e-05, gnorm=0.819, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2436
2023-03-14 20:13:00 - progress_bar.py[line:274] - INFO: epoch 001:   2092 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.5, ups=0.86, wpb=253.8, bsz=100, num_updates=2090, lr=4.91463e-05, gnorm=0.811, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2447
2023-03-14 20:13:11 - progress_bar.py[line:274] - INFO: epoch 001:   2102 / 6313 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=215.2, ups=0.85, wpb=252.3, bsz=100, num_updates=2100, lr=4.91296e-05, gnorm=0.896, clip=30, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=2459
2023-03-14 20:13:23 - progress_bar.py[line:274] - INFO: epoch 001:   2112 / 6313 loss=0.633, loss_v1=0, loss_v2=0, nll_loss=0.428, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=222.2, ups=0.88, wpb=253.4, bsz=100, num_updates=2110, lr=4.91129e-05, gnorm=0.995, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=2471
2023-03-14 20:13:34 - progress_bar.py[line:274] - INFO: epoch 001:   2122 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.3, ups=0.86, wpb=253.8, bsz=100, num_updates=2120, lr=4.90963e-05, gnorm=0.948, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2482
2023-03-14 20:13:46 - progress_bar.py[line:274] - INFO: epoch 001:   2132 / 6313 loss=0.636, loss_v1=0, loss_v2=0, nll_loss=0.436, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=219.6, ups=0.87, wpb=253.8, bsz=100, num_updates=2130, lr=4.90796e-05, gnorm=0.807, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2494
2023-03-14 20:13:57 - progress_bar.py[line:274] - INFO: epoch 001:   2142 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.433, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.5, ups=0.87, wpb=252.4, bsz=100, num_updates=2140, lr=4.90629e-05, gnorm=0.874, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2505
2023-03-14 20:14:09 - progress_bar.py[line:274] - INFO: epoch 001:   2152 / 6313 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=220.8, ups=0.88, wpb=252.1, bsz=100, num_updates=2150, lr=4.90463e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=2517
2023-03-14 20:14:20 - progress_bar.py[line:274] - INFO: epoch 001:   2162 / 6313 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.422, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=221.4, ups=0.87, wpb=255.4, bsz=100, num_updates=2160, lr=4.90296e-05, gnorm=0.8, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2528
2023-03-14 20:14:32 - progress_bar.py[line:274] - INFO: epoch 001:   2172 / 6313 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=221.6, ups=0.88, wpb=253.3, bsz=100, num_updates=2170, lr=4.90129e-05, gnorm=0.806, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2540
2023-03-14 20:14:43 - progress_bar.py[line:274] - INFO: epoch 001:   2182 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=223.2, ups=0.87, wpb=255.6, bsz=100, num_updates=2180, lr=4.89962e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2551
2023-03-14 20:14:55 - progress_bar.py[line:274] - INFO: epoch 001:   2192 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.6, ups=0.87, wpb=251.2, bsz=100, num_updates=2190, lr=4.89796e-05, gnorm=0.939, clip=20, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=2563
2023-03-14 20:15:06 - progress_bar.py[line:274] - INFO: epoch 001:   2202 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=222.4, ups=0.88, wpb=252.2, bsz=100, num_updates=2200, lr=4.89629e-05, gnorm=0.791, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2574
2023-03-14 20:15:18 - progress_bar.py[line:274] - INFO: epoch 001:   2212 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.434, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=216.6, ups=0.87, wpb=250.2, bsz=100, num_updates=2210, lr=4.89462e-05, gnorm=0.894, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2586
2023-03-14 20:15:29 - progress_bar.py[line:274] - INFO: epoch 001:   2222 / 6313 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.2, ups=0.86, wpb=254, bsz=100, num_updates=2220, lr=4.89295e-05, gnorm=0.861, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2597
2023-03-14 20:15:41 - progress_bar.py[line:274] - INFO: epoch 001:   2232 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.6, ups=0.86, wpb=252.5, bsz=100, num_updates=2230, lr=4.89129e-05, gnorm=0.768, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2609
2023-03-14 20:15:53 - progress_bar.py[line:274] - INFO: epoch 001:   2242 / 6313 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.427, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218, ups=0.86, wpb=252.3, bsz=100, num_updates=2240, lr=4.88962e-05, gnorm=0.856, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2620
2023-03-14 20:16:04 - progress_bar.py[line:274] - INFO: epoch 001:   2252 / 6313 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.9, ups=0.87, wpb=250.5, bsz=100, num_updates=2250, lr=4.88795e-05, gnorm=0.975, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=2632
2023-03-14 20:16:16 - progress_bar.py[line:274] - INFO: epoch 001:   2262 / 6313 loss=0.632, loss_v1=0, loss_v2=0, nll_loss=0.429, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=217.1, ups=0.86, wpb=251.4, bsz=100, num_updates=2260, lr=4.88628e-05, gnorm=0.826, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2644
2023-03-14 20:16:27 - progress_bar.py[line:274] - INFO: epoch 001:   2272 / 6313 loss=0.634, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=216.5, ups=0.87, wpb=250.2, bsz=100, num_updates=2270, lr=4.88462e-05, gnorm=0.982, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2655
2023-03-14 20:16:39 - progress_bar.py[line:274] - INFO: epoch 001:   2282 / 6313 loss=0.635, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219, ups=0.87, wpb=252.6, bsz=100, num_updates=2280, lr=4.88295e-05, gnorm=0.836, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=2667
2023-03-14 20:16:50 - progress_bar.py[line:274] - INFO: epoch 001:   2292 / 6313 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.8, ups=0.86, wpb=255.4, bsz=100, num_updates=2290, lr=4.88128e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=12, gb_free=11.2, ema_decay=0.9999, wall=2678
2023-03-14 20:17:02 - progress_bar.py[line:274] - INFO: epoch 001:   2302 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=256.7, nsentences=100, sample_size=256.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221.3, ups=0.86, wpb=256.7, bsz=100, num_updates=2300, lr=4.87961e-05, gnorm=0.828, clip=20, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=2690
2023-03-14 20:17:14 - progress_bar.py[line:274] - INFO: epoch 001:   2312 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.5, ups=0.86, wpb=254.5, bsz=100, num_updates=2310, lr=4.87795e-05, gnorm=0.685, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=2701
2023-03-14 20:17:25 - progress_bar.py[line:274] - INFO: epoch 001:   2322 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.4, ups=0.86, wpb=253.9, bsz=100, num_updates=2320, lr=4.87628e-05, gnorm=0.746, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2713
2023-03-14 20:17:37 - progress_bar.py[line:274] - INFO: epoch 001:   2332 / 6313 loss=0.637, loss_v1=0, loss_v2=0, nll_loss=0.435, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.35, wps=218.5, ups=0.87, wpb=252.6, bsz=100, num_updates=2330, lr=4.87461e-05, gnorm=0.778, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2725
2023-03-14 20:17:48 - progress_bar.py[line:274] - INFO: epoch 001:   2342 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.8, ups=0.86, wpb=252.5, bsz=100, num_updates=2340, lr=4.87294e-05, gnorm=0.853, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2736
2023-03-14 20:18:00 - progress_bar.py[line:274] - INFO: epoch 001:   2352 / 6313 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=216.5, ups=0.86, wpb=252.9, bsz=100, num_updates=2350, lr=4.87128e-05, gnorm=0.883, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2748
2023-03-14 20:18:12 - progress_bar.py[line:274] - INFO: epoch 001:   2362 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.419, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.6, ups=0.86, wpb=254.8, bsz=100, num_updates=2360, lr=4.86961e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2760
2023-03-14 20:18:23 - progress_bar.py[line:274] - INFO: epoch 001:   2372 / 6313 loss=0.623, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.8, ups=0.87, wpb=251.6, bsz=100, num_updates=2370, lr=4.86794e-05, gnorm=0.833, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2771
2023-03-14 20:18:35 - progress_bar.py[line:274] - INFO: epoch 001:   2382 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=221, ups=0.86, wpb=255.6, bsz=100, num_updates=2380, lr=4.86628e-05, gnorm=0.812, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2783
2023-03-14 20:18:46 - progress_bar.py[line:274] - INFO: epoch 001:   2392 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=256.8, nsentences=100, sample_size=256.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222, ups=0.86, wpb=256.8, bsz=100, num_updates=2390, lr=4.86461e-05, gnorm=0.734, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2794
2023-03-14 20:18:58 - progress_bar.py[line:274] - INFO: epoch 001:   2402 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=222, ups=0.87, wpb=256.4, bsz=100, num_updates=2400, lr=4.86294e-05, gnorm=0.751, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2806
2023-03-14 20:19:10 - progress_bar.py[line:274] - INFO: epoch 001:   2412 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.3, ups=0.87, wpb=253.4, bsz=100, num_updates=2410, lr=4.86127e-05, gnorm=0.739, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=2817
2023-03-14 20:19:21 - progress_bar.py[line:274] - INFO: epoch 001:   2422 / 6313 loss=0.624, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.2, ups=0.86, wpb=251.2, bsz=100, num_updates=2420, lr=4.85961e-05, gnorm=0.791, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2829
2023-03-14 20:19:33 - progress_bar.py[line:274] - INFO: epoch 001:   2432 / 6313 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.4, ups=0.86, wpb=250.8, bsz=100, num_updates=2430, lr=4.85794e-05, gnorm=0.812, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=2841
2023-03-14 20:19:44 - progress_bar.py[line:274] - INFO: epoch 001:   2442 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.6, ups=0.86, wpb=251.3, bsz=100, num_updates=2440, lr=4.85627e-05, gnorm=0.789, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=2852
2023-03-14 20:19:56 - progress_bar.py[line:274] - INFO: epoch 001:   2452 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=222.6, ups=0.88, wpb=254.1, bsz=100, num_updates=2450, lr=4.8546e-05, gnorm=0.76, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2864
2023-03-14 20:20:07 - progress_bar.py[line:274] - INFO: epoch 001:   2462 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.4, ups=0.86, wpb=252.6, bsz=100, num_updates=2460, lr=4.85294e-05, gnorm=0.842, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2875
2023-03-14 20:20:19 - progress_bar.py[line:274] - INFO: epoch 001:   2472 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.8, ups=0.86, wpb=252.6, bsz=100, num_updates=2470, lr=4.85127e-05, gnorm=0.779, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2887
2023-03-14 20:20:31 - progress_bar.py[line:274] - INFO: epoch 001:   2482 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.416, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=220, ups=0.87, wpb=254, bsz=100, num_updates=2480, lr=4.8496e-05, gnorm=0.888, clip=30, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=2898
2023-03-14 20:20:42 - progress_bar.py[line:274] - INFO: epoch 001:   2492 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219, ups=0.86, wpb=253.3, bsz=100, num_updates=2490, lr=4.84793e-05, gnorm=0.928, clip=30, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2910
2023-03-14 20:20:54 - progress_bar.py[line:274] - INFO: epoch 001:   2502 / 6313 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.3, ups=0.86, wpb=252.5, bsz=100, num_updates=2500, lr=4.84627e-05, gnorm=0.757, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=2922
2023-03-14 20:21:05 - progress_bar.py[line:274] - INFO: epoch 001:   2512 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.4, ups=0.86, wpb=255.9, bsz=100, num_updates=2510, lr=4.8446e-05, gnorm=0.716, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=2933
2023-03-14 20:21:17 - progress_bar.py[line:274] - INFO: epoch 001:   2522 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=224, ups=0.88, wpb=255.9, bsz=100, num_updates=2520, lr=4.84293e-05, gnorm=0.8, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=2945
2023-03-14 20:21:28 - progress_bar.py[line:274] - INFO: epoch 001:   2532 / 6313 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=222.2, ups=0.88, wpb=253.1, bsz=100, num_updates=2530, lr=4.84126e-05, gnorm=0.757, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=2956
2023-03-14 20:21:40 - progress_bar.py[line:274] - INFO: epoch 001:   2542 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.5, ups=0.86, wpb=254, bsz=100, num_updates=2540, lr=4.8396e-05, gnorm=0.784, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=2968
2023-03-14 20:21:51 - progress_bar.py[line:274] - INFO: epoch 001:   2552 / 6313 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.9, ups=0.86, wpb=252.8, bsz=100, num_updates=2550, lr=4.83793e-05, gnorm=0.804, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=2979
2023-03-14 20:22:03 - progress_bar.py[line:274] - INFO: epoch 001:   2562 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.7, ups=0.87, wpb=252.4, bsz=100, num_updates=2560, lr=4.83626e-05, gnorm=0.77, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=2991
2023-03-14 20:22:14 - progress_bar.py[line:274] - INFO: epoch 001:   2572 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.1, ups=0.87, wpb=251.9, bsz=100, num_updates=2570, lr=4.83459e-05, gnorm=0.859, clip=20, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3002
2023-03-14 20:22:24 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 20:22:27 - progress_bar.py[line:274] - INFO: epoch 001:   2583 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=200, ups=0.79, wpb=253.8, bsz=100, num_updates=2580, lr=4.83293e-05, gnorm=0.744, clip=0, loss_scale=512, train_wall=13, gb_free=10.1, ema_decay=0.9999, wall=3015
2023-03-14 20:22:39 - progress_bar.py[line:274] - INFO: epoch 001:   2593 / 6313 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=216.5, ups=0.86, wpb=252.5, bsz=100, num_updates=2590, lr=4.83126e-05, gnorm=0.814, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3027
2023-03-14 20:22:50 - progress_bar.py[line:274] - INFO: epoch 001:   2603 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.5, ups=0.86, wpb=254, bsz=100, num_updates=2600, lr=4.82959e-05, gnorm=0.796, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3038
2023-03-14 20:23:02 - progress_bar.py[line:274] - INFO: epoch 001:   2613 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.1, ups=0.86, wpb=251.2, bsz=100, num_updates=2610, lr=4.82793e-05, gnorm=0.834, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3050
2023-03-14 20:23:14 - progress_bar.py[line:274] - INFO: epoch 001:   2623 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=221.6, ups=0.87, wpb=255.8, bsz=100, num_updates=2620, lr=4.82626e-05, gnorm=0.871, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=3061
2023-03-14 20:23:25 - progress_bar.py[line:274] - INFO: epoch 001:   2633 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221.4, ups=0.86, wpb=256.1, bsz=100, num_updates=2630, lr=4.82459e-05, gnorm=0.83, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3073
2023-03-14 20:23:36 - progress_bar.py[line:274] - INFO: epoch 001:   2643 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=225.4, ups=0.89, wpb=253.9, bsz=100, num_updates=2640, lr=4.82292e-05, gnorm=0.858, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3084
2023-03-14 20:23:48 - progress_bar.py[line:274] - INFO: epoch 001:   2653 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.4, ups=0.87, wpb=255.6, bsz=100, num_updates=2650, lr=4.82126e-05, gnorm=0.819, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3096
2023-03-14 20:23:59 - progress_bar.py[line:274] - INFO: epoch 001:   2663 / 6313 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.9, ups=0.87, wpb=251.8, bsz=100, num_updates=2660, lr=4.81959e-05, gnorm=0.822, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3107
2023-03-14 20:24:11 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 256.0
2023-03-14 20:24:12 - progress_bar.py[line:274] - INFO: epoch 001:   2674 / 6313 loss=0.626, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=199.8, ups=0.79, wpb=251.5, bsz=100, num_updates=2670, lr=4.81792e-05, gnorm=0.876, clip=20, loss_scale=256, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=3120
2023-03-14 20:24:23 - progress_bar.py[line:274] - INFO: epoch 001:   2684 / 6313 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=225.1, ups=0.89, wpb=254, bsz=100, num_updates=2680, lr=4.81625e-05, gnorm=0.774, clip=10, loss_scale=256, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=3131
2023-03-14 20:24:35 - progress_bar.py[line:274] - INFO: epoch 001:   2694 / 6313 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.415, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.8, ups=0.87, wpb=252.6, bsz=100, num_updates=2690, lr=4.81459e-05, gnorm=0.849, clip=20, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3143
2023-03-14 20:24:47 - progress_bar.py[line:274] - INFO: epoch 001:   2704 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=257.9, nsentences=100, sample_size=257.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.7, ups=0.86, wpb=257.9, bsz=100, num_updates=2700, lr=4.81292e-05, gnorm=0.949, clip=40, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3154
2023-03-14 20:24:58 - progress_bar.py[line:274] - INFO: epoch 001:   2714 / 6313 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.8, ups=0.87, wpb=252.2, bsz=100, num_updates=2710, lr=4.81125e-05, gnorm=0.834, clip=20, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3166
2023-03-14 20:25:09 - progress_bar.py[line:274] - INFO: epoch 001:   2724 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=223, ups=0.88, wpb=252.4, bsz=100, num_updates=2720, lr=4.80958e-05, gnorm=0.84, clip=10, loss_scale=256, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=3177
2023-03-14 20:25:21 - progress_bar.py[line:274] - INFO: epoch 001:   2734 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.3, ups=0.86, wpb=252.7, bsz=100, num_updates=2730, lr=4.80792e-05, gnorm=0.814, clip=10, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3189
2023-03-14 20:25:32 - progress_bar.py[line:274] - INFO: epoch 001:   2744 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.6, ups=0.88, wpb=252, bsz=100, num_updates=2740, lr=4.80625e-05, gnorm=0.731, clip=0, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3200
2023-03-14 20:25:44 - progress_bar.py[line:274] - INFO: epoch 001:   2754 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.5, ups=0.87, wpb=252, bsz=100, num_updates=2750, lr=4.80458e-05, gnorm=0.845, clip=20, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3212
2023-03-14 20:25:55 - progress_bar.py[line:274] - INFO: epoch 001:   2764 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.7, ups=0.87, wpb=252.2, bsz=100, num_updates=2760, lr=4.80291e-05, gnorm=0.838, clip=0, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3223
2023-03-14 20:26:07 - progress_bar.py[line:274] - INFO: epoch 001:   2774 / 6313 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.8, ups=0.86, wpb=251.9, bsz=100, num_updates=2770, lr=4.80125e-05, gnorm=0.763, clip=10, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3235
2023-03-14 20:26:19 - progress_bar.py[line:274] - INFO: epoch 001:   2784 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218, ups=0.86, wpb=253, bsz=100, num_updates=2780, lr=4.79958e-05, gnorm=0.773, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3246
2023-03-14 20:26:30 - progress_bar.py[line:274] - INFO: epoch 001:   2794 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.3, ups=0.88, wpb=251.4, bsz=100, num_updates=2790, lr=4.79791e-05, gnorm=0.87, clip=30, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3258
2023-03-14 20:26:41 - progress_bar.py[line:274] - INFO: epoch 001:   2804 / 6313 loss=0.631, loss_v1=0, loss_v2=0, nll_loss=0.426, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=224.1, ups=0.89, wpb=253, bsz=100, num_updates=2800, lr=4.79625e-05, gnorm=0.894, clip=20, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3269
2023-03-14 20:26:53 - progress_bar.py[line:274] - INFO: epoch 001:   2814 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=220.1, ups=0.87, wpb=254, bsz=100, num_updates=2810, lr=4.79458e-05, gnorm=0.807, clip=0, loss_scale=256, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=3281
2023-03-14 20:27:05 - progress_bar.py[line:274] - INFO: epoch 001:   2824 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=215.6, ups=0.85, wpb=252.6, bsz=100, num_updates=2820, lr=4.79291e-05, gnorm=0.793, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3292
2023-03-14 20:27:16 - progress_bar.py[line:274] - INFO: epoch 001:   2834 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221.8, ups=0.87, wpb=254.2, bsz=100, num_updates=2830, lr=4.79124e-05, gnorm=0.863, clip=10, loss_scale=256, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=3304
2023-03-14 20:27:28 - progress_bar.py[line:274] - INFO: epoch 001:   2844 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.1, ups=0.86, wpb=253.2, bsz=100, num_updates=2840, lr=4.78958e-05, gnorm=0.804, clip=20, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3316
2023-03-14 20:27:39 - progress_bar.py[line:274] - INFO: epoch 001:   2854 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221.1, ups=0.86, wpb=255.8, bsz=100, num_updates=2850, lr=4.78791e-05, gnorm=0.778, clip=20, loss_scale=256, train_wall=12, gb_free=10, ema_decay=0.9999, wall=3327
2023-03-14 20:27:51 - progress_bar.py[line:274] - INFO: epoch 001:   2864 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.5, ups=0.86, wpb=254.1, bsz=100, num_updates=2860, lr=4.78624e-05, gnorm=0.768, clip=0, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3339
2023-03-14 20:28:02 - progress_bar.py[line:274] - INFO: epoch 001:   2874 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.8, ups=0.86, wpb=254.4, bsz=100, num_updates=2870, lr=4.78457e-05, gnorm=0.945, clip=40, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3350
2023-03-14 20:28:14 - progress_bar.py[line:274] - INFO: epoch 001:   2884 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.8, ups=0.87, wpb=253.4, bsz=100, num_updates=2880, lr=4.78291e-05, gnorm=0.949, clip=40, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3362
2023-03-14 20:28:26 - progress_bar.py[line:274] - INFO: epoch 001:   2894 / 6313 loss=0.616, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.1, ups=0.86, wpb=253.8, bsz=100, num_updates=2890, lr=4.78124e-05, gnorm=0.814, clip=10, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=3373
2023-03-14 20:28:37 - progress_bar.py[line:274] - INFO: epoch 001:   2904 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=224.2, ups=0.88, wpb=255.8, bsz=100, num_updates=2900, lr=4.77957e-05, gnorm=0.857, clip=20, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3385
2023-03-14 20:28:49 - progress_bar.py[line:274] - INFO: epoch 001:   2914 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.9, ups=0.87, wpb=252.9, bsz=100, num_updates=2910, lr=4.7779e-05, gnorm=0.903, clip=30, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3396
2023-03-14 20:29:00 - progress_bar.py[line:274] - INFO: epoch 001:   2924 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.7, ups=0.86, wpb=253.2, bsz=100, num_updates=2920, lr=4.77624e-05, gnorm=0.868, clip=10, loss_scale=256, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=3408
2023-03-14 20:29:12 - progress_bar.py[line:274] - INFO: epoch 001:   2934 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.412, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=221.3, ups=0.88, wpb=252.4, bsz=100, num_updates=2930, lr=4.77457e-05, gnorm=0.996, clip=40, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3419
2023-03-14 20:29:23 - progress_bar.py[line:274] - INFO: epoch 001:   2944 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.425, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.4, ups=0.87, wpb=252.7, bsz=100, num_updates=2940, lr=4.7729e-05, gnorm=0.911, clip=10, loss_scale=256, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3431
2023-03-14 20:29:35 - progress_bar.py[line:274] - INFO: epoch 001:   2954 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.8, ups=0.87, wpb=251.2, bsz=100, num_updates=2950, lr=4.77123e-05, gnorm=0.758, clip=0, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3442
2023-03-14 20:29:46 - progress_bar.py[line:274] - INFO: epoch 001:   2964 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.418, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.5, ups=0.86, wpb=252.9, bsz=100, num_updates=2960, lr=4.76957e-05, gnorm=0.746, clip=10, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3454
2023-03-14 20:29:58 - progress_bar.py[line:274] - INFO: epoch 001:   2974 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=249.4, nsentences=100, sample_size=249.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=216, ups=0.87, wpb=249.4, bsz=100, num_updates=2970, lr=4.7679e-05, gnorm=0.853, clip=20, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3466
2023-03-14 20:30:09 - progress_bar.py[line:274] - INFO: epoch 001:   2984 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.7, ups=0.86, wpb=254.2, bsz=100, num_updates=2980, lr=4.76623e-05, gnorm=0.792, clip=20, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3477
2023-03-14 20:30:21 - progress_bar.py[line:274] - INFO: epoch 001:   2994 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.9, ups=0.86, wpb=253.7, bsz=100, num_updates=2990, lr=4.76456e-05, gnorm=0.745, clip=10, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3489
2023-03-14 20:30:32 - progress_bar.py[line:274] - INFO: epoch 001:   3004 / 6313 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.2, ups=0.87, wpb=251.7, bsz=100, num_updates=3000, lr=4.7629e-05, gnorm=0.774, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3500
2023-03-14 20:30:44 - progress_bar.py[line:274] - INFO: epoch 001:   3014 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=224.3, ups=0.88, wpb=256.1, bsz=100, num_updates=3010, lr=4.76123e-05, gnorm=0.739, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3512
2023-03-14 20:30:55 - progress_bar.py[line:274] - INFO: epoch 001:   3024 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.2, ups=0.87, wpb=253.1, bsz=100, num_updates=3020, lr=4.75956e-05, gnorm=0.818, clip=20, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3523
2023-03-14 20:31:07 - progress_bar.py[line:274] - INFO: epoch 001:   3034 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.3, ups=0.86, wpb=255, bsz=100, num_updates=3030, lr=4.7579e-05, gnorm=0.731, clip=10, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3535
2023-03-14 20:31:19 - progress_bar.py[line:274] - INFO: epoch 001:   3044 / 6313 loss=0.618, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.8, ups=0.87, wpb=252.9, bsz=100, num_updates=3040, lr=4.75623e-05, gnorm=0.769, clip=0, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3546
2023-03-14 20:31:30 - progress_bar.py[line:274] - INFO: epoch 001:   3054 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.7, ups=0.87, wpb=251.4, bsz=100, num_updates=3050, lr=4.75456e-05, gnorm=0.838, clip=30, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3558
2023-03-14 20:31:42 - progress_bar.py[line:274] - INFO: epoch 001:   3064 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.9, ups=0.86, wpb=253.3, bsz=100, num_updates=3060, lr=4.75289e-05, gnorm=0.837, clip=20, loss_scale=256, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3570
2023-03-14 20:31:53 - progress_bar.py[line:274] - INFO: epoch 001:   3074 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.411, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=222.8, ups=0.87, wpb=254.6, bsz=100, num_updates=3070, lr=4.75123e-05, gnorm=0.819, clip=10, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3581
2023-03-14 20:32:05 - progress_bar.py[line:274] - INFO: epoch 001:   3084 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.1, ups=0.86, wpb=251.4, bsz=100, num_updates=3080, lr=4.74956e-05, gnorm=0.833, clip=0, loss_scale=256, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3593
2023-03-14 20:32:16 - progress_bar.py[line:274] - INFO: epoch 001:   3094 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.7, ups=0.86, wpb=254.6, bsz=100, num_updates=3090, lr=4.74789e-05, gnorm=0.763, clip=0, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3604
2023-03-14 20:32:28 - progress_bar.py[line:274] - INFO: epoch 001:   3104 / 6313 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.421, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=212.4, ups=0.84, wpb=252.3, bsz=100, num_updates=3100, lr=4.74622e-05, gnorm=0.858, clip=20, loss_scale=256, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3616
2023-03-14 20:32:40 - progress_bar.py[line:274] - INFO: epoch 001:   3114 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.9, ups=0.86, wpb=253.8, bsz=100, num_updates=3110, lr=4.74456e-05, gnorm=0.781, clip=10, loss_scale=256, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3628
2023-03-14 20:32:51 - progress_bar.py[line:274] - INFO: epoch 001:   3124 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.9, ups=0.86, wpb=252.4, bsz=100, num_updates=3120, lr=4.74289e-05, gnorm=0.867, clip=20, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3639
2023-03-14 20:33:03 - progress_bar.py[line:274] - INFO: epoch 001:   3134 / 6313 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.8, ups=0.87, wpb=250.4, bsz=100, num_updates=3130, lr=4.74122e-05, gnorm=0.8, clip=10, loss_scale=256, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=3651
2023-03-14 20:33:14 - progress_bar.py[line:274] - INFO: epoch 001:   3144 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=256.7, nsentences=100, sample_size=256.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.7, ups=0.86, wpb=256.7, bsz=100, num_updates=3140, lr=4.73955e-05, gnorm=0.732, clip=0, loss_scale=256, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=3662
2023-03-14 20:33:26 - progress_bar.py[line:274] - INFO: epoch 001:   3154 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221.2, ups=0.88, wpb=252.7, bsz=100, num_updates=3150, lr=4.73789e-05, gnorm=0.789, clip=10, loss_scale=256, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3674
2023-03-14 20:33:37 - progress_bar.py[line:274] - INFO: epoch 001:   3164 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=223.5, ups=0.88, wpb=255.1, bsz=100, num_updates=3160, lr=4.73622e-05, gnorm=0.736, clip=0, loss_scale=256, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3685
2023-03-14 20:33:49 - progress_bar.py[line:274] - INFO: epoch 001:   3174 / 6313 loss=0.619, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=221.9, ups=0.88, wpb=253.4, bsz=100, num_updates=3170, lr=4.73455e-05, gnorm=0.728, clip=0, loss_scale=256, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=3697
2023-03-14 20:34:00 - progress_bar.py[line:274] - INFO: epoch 001:   3184 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219, ups=0.86, wpb=253.5, bsz=100, num_updates=3180, lr=4.73288e-05, gnorm=0.692, clip=0, loss_scale=256, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3708
2023-03-14 20:34:12 - progress_bar.py[line:274] - INFO: epoch 001:   3194 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.6, ups=0.86, wpb=255.1, bsz=100, num_updates=3190, lr=4.73122e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3720
2023-03-14 20:34:23 - progress_bar.py[line:274] - INFO: epoch 001:   3204 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.1, ups=0.87, wpb=252, bsz=100, num_updates=3200, lr=4.72955e-05, gnorm=0.714, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3731
2023-03-14 20:34:35 - progress_bar.py[line:274] - INFO: epoch 001:   3214 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.9, ups=0.86, wpb=253.1, bsz=100, num_updates=3210, lr=4.72788e-05, gnorm=0.761, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3743
2023-03-14 20:34:47 - progress_bar.py[line:274] - INFO: epoch 001:   3224 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.8, ups=0.86, wpb=253.3, bsz=100, num_updates=3220, lr=4.72621e-05, gnorm=0.67, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3754
2023-03-14 20:34:58 - progress_bar.py[line:274] - INFO: epoch 001:   3234 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.2, ups=0.87, wpb=254.7, bsz=100, num_updates=3230, lr=4.72455e-05, gnorm=0.749, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3766
2023-03-14 20:35:10 - progress_bar.py[line:274] - INFO: epoch 001:   3244 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.6, ups=0.87, wpb=253.9, bsz=100, num_updates=3240, lr=4.72288e-05, gnorm=0.697, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3778
2023-03-14 20:35:21 - progress_bar.py[line:274] - INFO: epoch 001:   3254 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221.6, ups=0.87, wpb=253.5, bsz=100, num_updates=3250, lr=4.72121e-05, gnorm=0.794, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3789
2023-03-14 20:35:33 - progress_bar.py[line:274] - INFO: epoch 001:   3264 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.4, ups=0.86, wpb=254, bsz=100, num_updates=3260, lr=4.71955e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3801
2023-03-14 20:35:44 - progress_bar.py[line:274] - INFO: epoch 001:   3274 / 6313 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.7, ups=0.87, wpb=252.8, bsz=100, num_updates=3270, lr=4.71788e-05, gnorm=0.81, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3812
2023-03-14 20:35:56 - progress_bar.py[line:274] - INFO: epoch 001:   3284 / 6313 loss=0.629, loss_v1=0, loss_v2=0, nll_loss=0.42, ntokens=248.1, nsentences=100, sample_size=248.1, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218.2, ups=0.88, wpb=248.1, bsz=100, num_updates=3280, lr=4.71621e-05, gnorm=0.746, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=3824
2023-03-14 20:36:07 - progress_bar.py[line:274] - INFO: epoch 001:   3294 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.1, ups=0.86, wpb=255, bsz=100, num_updates=3290, lr=4.71454e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3835
2023-03-14 20:36:19 - progress_bar.py[line:274] - INFO: epoch 001:   3304 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.1, ups=0.85, wpb=252.9, bsz=100, num_updates=3300, lr=4.71288e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3847
2023-03-14 20:36:31 - progress_bar.py[line:274] - INFO: epoch 001:   3314 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217, ups=0.85, wpb=253.8, bsz=100, num_updates=3310, lr=4.71121e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=3858
2023-03-14 20:36:42 - progress_bar.py[line:274] - INFO: epoch 001:   3324 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.4, ups=0.87, wpb=253.2, bsz=100, num_updates=3320, lr=4.70954e-05, gnorm=0.728, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3870
2023-03-14 20:36:54 - progress_bar.py[line:274] - INFO: epoch 001:   3334 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.5, ups=0.87, wpb=251.2, bsz=100, num_updates=3330, lr=4.70787e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=3882
2023-03-14 20:37:05 - progress_bar.py[line:274] - INFO: epoch 001:   3344 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.8, ups=0.86, wpb=253.4, bsz=100, num_updates=3340, lr=4.70621e-05, gnorm=0.779, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3893
2023-03-14 20:37:17 - progress_bar.py[line:274] - INFO: epoch 001:   3354 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.7, ups=0.86, wpb=253.6, bsz=100, num_updates=3350, lr=4.70454e-05, gnorm=0.806, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3905
2023-03-14 20:37:28 - progress_bar.py[line:274] - INFO: epoch 001:   3364 / 6313 loss=0.622, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=221.6, ups=0.88, wpb=253, bsz=100, num_updates=3360, lr=4.70287e-05, gnorm=0.826, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=3916
2023-03-14 20:37:40 - progress_bar.py[line:274] - INFO: epoch 001:   3374 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.9, ups=0.86, wpb=255.5, bsz=100, num_updates=3370, lr=4.7012e-05, gnorm=0.819, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=3928
2023-03-14 20:37:51 - progress_bar.py[line:274] - INFO: epoch 001:   3384 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.9, ups=0.86, wpb=255.9, bsz=100, num_updates=3380, lr=4.69954e-05, gnorm=0.736, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3939
2023-03-14 20:38:03 - progress_bar.py[line:274] - INFO: epoch 001:   3394 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.4, ups=0.86, wpb=252.6, bsz=100, num_updates=3390, lr=4.69787e-05, gnorm=0.747, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=3951
2023-03-14 20:38:15 - progress_bar.py[line:274] - INFO: epoch 001:   3404 / 6313 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.7, ups=0.86, wpb=252.5, bsz=100, num_updates=3400, lr=4.6962e-05, gnorm=0.7, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=3962
2023-03-14 20:38:26 - progress_bar.py[line:274] - INFO: epoch 001:   3414 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.6, ups=0.86, wpb=254.2, bsz=100, num_updates=3410, lr=4.69453e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=3974
2023-03-14 20:38:38 - progress_bar.py[line:274] - INFO: epoch 001:   3424 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=249.8, nsentences=100, sample_size=249.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=213.8, ups=0.86, wpb=249.8, bsz=100, num_updates=3420, lr=4.69287e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=3986
2023-03-14 20:38:50 - progress_bar.py[line:274] - INFO: epoch 001:   3434 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.6, ups=0.86, wpb=255.6, bsz=100, num_updates=3430, lr=4.6912e-05, gnorm=0.804, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=3997
2023-03-14 20:39:01 - progress_bar.py[line:274] - INFO: epoch 001:   3444 / 6313 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=214.4, ups=0.85, wpb=251.3, bsz=100, num_updates=3440, lr=4.68953e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4009
2023-03-14 20:39:13 - progress_bar.py[line:274] - INFO: epoch 001:   3454 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.1, ups=0.88, wpb=250.3, bsz=100, num_updates=3450, lr=4.68786e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4021
2023-03-14 20:39:24 - progress_bar.py[line:274] - INFO: epoch 001:   3464 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=248.3, nsentences=100, sample_size=248.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=214.3, ups=0.86, wpb=248.3, bsz=100, num_updates=3460, lr=4.6862e-05, gnorm=0.771, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4032
2023-03-14 20:39:36 - progress_bar.py[line:274] - INFO: epoch 001:   3474 / 6313 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.8, ups=0.87, wpb=252.6, bsz=100, num_updates=3470, lr=4.68453e-05, gnorm=0.758, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4044
2023-03-14 20:39:47 - progress_bar.py[line:274] - INFO: epoch 001:   3484 / 6313 loss=0.612, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.2, ups=0.86, wpb=253.2, bsz=100, num_updates=3480, lr=4.68286e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4055
2023-03-14 20:39:59 - progress_bar.py[line:274] - INFO: epoch 001:   3494 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.423, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=216, ups=0.86, wpb=251.2, bsz=100, num_updates=3490, lr=4.6812e-05, gnorm=0.702, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4067
2023-03-14 20:40:11 - progress_bar.py[line:274] - INFO: epoch 001:   3504 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.1, ups=0.86, wpb=255.2, bsz=100, num_updates=3500, lr=4.67953e-05, gnorm=0.754, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4078
2023-03-14 20:40:22 - progress_bar.py[line:274] - INFO: epoch 001:   3514 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.2, ups=0.87, wpb=252.8, bsz=100, num_updates=3510, lr=4.67786e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4090
2023-03-14 20:40:34 - progress_bar.py[line:274] - INFO: epoch 001:   3524 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.403, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.8, ups=0.87, wpb=255, bsz=100, num_updates=3520, lr=4.67619e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=4102
2023-03-14 20:40:45 - progress_bar.py[line:274] - INFO: epoch 001:   3534 / 6313 loss=0.627, loss_v1=0, loss_v2=0, nll_loss=0.424, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=218, ups=0.86, wpb=252.8, bsz=100, num_updates=3530, lr=4.67453e-05, gnorm=0.757, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=4113
2023-03-14 20:40:57 - progress_bar.py[line:274] - INFO: epoch 001:   3544 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.6, ups=0.86, wpb=253, bsz=100, num_updates=3540, lr=4.67286e-05, gnorm=0.694, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4125
2023-03-14 20:41:08 - progress_bar.py[line:274] - INFO: epoch 001:   3554 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221.3, ups=0.87, wpb=254.8, bsz=100, num_updates=3550, lr=4.67119e-05, gnorm=0.685, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4136
2023-03-14 20:41:20 - progress_bar.py[line:274] - INFO: epoch 001:   3564 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.1, ups=0.86, wpb=254.4, bsz=100, num_updates=3560, lr=4.66952e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4148
2023-03-14 20:41:32 - progress_bar.py[line:274] - INFO: epoch 001:   3574 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.9, ups=0.88, wpb=251.3, bsz=100, num_updates=3570, lr=4.66786e-05, gnorm=0.749, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=4159
2023-03-14 20:41:43 - progress_bar.py[line:274] - INFO: epoch 001:   3584 / 6313 loss=0.625, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=217.9, ups=0.86, wpb=252.2, bsz=100, num_updates=3580, lr=4.66619e-05, gnorm=0.752, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4171
2023-03-14 20:41:55 - progress_bar.py[line:274] - INFO: epoch 001:   3594 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.1, ups=0.86, wpb=253.4, bsz=100, num_updates=3590, lr=4.66452e-05, gnorm=0.751, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=4183
2023-03-14 20:42:06 - progress_bar.py[line:274] - INFO: epoch 001:   3604 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.8, ups=0.86, wpb=252.9, bsz=100, num_updates=3600, lr=4.66285e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4194
2023-03-14 20:42:18 - progress_bar.py[line:274] - INFO: epoch 001:   3614 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=224.5, ups=0.88, wpb=253.9, bsz=100, num_updates=3610, lr=4.66119e-05, gnorm=0.715, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=4205
2023-03-14 20:42:29 - progress_bar.py[line:274] - INFO: epoch 001:   3624 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219, ups=0.86, wpb=253.7, bsz=100, num_updates=3620, lr=4.65952e-05, gnorm=0.816, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=4217
2023-03-14 20:42:41 - progress_bar.py[line:274] - INFO: epoch 001:   3634 / 6313 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.4, ups=0.87, wpb=252.7, bsz=100, num_updates=3630, lr=4.65785e-05, gnorm=0.785, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=4228
2023-03-14 20:42:52 - progress_bar.py[line:274] - INFO: epoch 001:   3644 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220, ups=0.86, wpb=255.1, bsz=100, num_updates=3640, lr=4.65618e-05, gnorm=0.75, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=4240
2023-03-14 20:43:04 - progress_bar.py[line:274] - INFO: epoch 001:   3654 / 6313 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221.1, ups=0.87, wpb=253.7, bsz=100, num_updates=3650, lr=4.65452e-05, gnorm=0.805, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=4252
2023-03-14 20:43:15 - progress_bar.py[line:274] - INFO: epoch 001:   3664 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.2, ups=0.86, wpb=252.8, bsz=100, num_updates=3660, lr=4.65285e-05, gnorm=0.796, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4263
2023-03-14 20:43:27 - progress_bar.py[line:274] - INFO: epoch 001:   3674 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.5, ups=0.85, wpb=254.1, bsz=100, num_updates=3670, lr=4.65118e-05, gnorm=0.839, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4275
2023-03-14 20:43:39 - progress_bar.py[line:274] - INFO: epoch 001:   3684 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.1, ups=0.86, wpb=253.3, bsz=100, num_updates=3680, lr=4.64951e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4287
2023-03-14 20:43:50 - progress_bar.py[line:274] - INFO: epoch 001:   3694 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.5, ups=0.87, wpb=252.2, bsz=100, num_updates=3690, lr=4.64785e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4298
2023-03-14 20:44:02 - progress_bar.py[line:274] - INFO: epoch 001:   3704 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.413, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=219.8, ups=0.87, wpb=253.1, bsz=100, num_updates=3700, lr=4.64618e-05, gnorm=0.787, clip=10, loss_scale=1024, train_wall=11, gb_free=10, ema_decay=0.9999, wall=4310
2023-03-14 20:44:13 - progress_bar.py[line:274] - INFO: epoch 001:   3714 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221, ups=0.86, wpb=256.2, bsz=100, num_updates=3710, lr=4.64451e-05, gnorm=0.712, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4321
2023-03-14 20:44:15 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 20:44:26 - progress_bar.py[line:274] - INFO: epoch 001:   3725 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=201.1, ups=0.79, wpb=253.7, bsz=100, num_updates=3720, lr=4.64285e-05, gnorm=0.817, clip=0, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=4334
2023-03-14 20:44:38 - progress_bar.py[line:274] - INFO: epoch 001:   3735 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.5, ups=0.86, wpb=252.6, bsz=100, num_updates=3730, lr=4.64118e-05, gnorm=0.775, clip=0, loss_scale=512, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=4346
2023-03-14 20:44:49 - progress_bar.py[line:274] - INFO: epoch 001:   3745 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.3, ups=0.86, wpb=254.5, bsz=100, num_updates=3740, lr=4.63951e-05, gnorm=0.72, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4357
2023-03-14 20:45:01 - progress_bar.py[line:274] - INFO: epoch 001:   3755 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.2, ups=0.86, wpb=254.3, bsz=100, num_updates=3750, lr=4.63784e-05, gnorm=0.718, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4369
2023-03-14 20:45:12 - progress_bar.py[line:274] - INFO: epoch 001:   3765 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=223.4, ups=0.88, wpb=254.6, bsz=100, num_updates=3760, lr=4.63618e-05, gnorm=0.739, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4380
2023-03-14 20:45:24 - progress_bar.py[line:274] - INFO: epoch 001:   3775 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.404, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220.9, ups=0.87, wpb=252.5, bsz=100, num_updates=3770, lr=4.63451e-05, gnorm=0.741, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4392
2023-03-14 20:45:35 - progress_bar.py[line:274] - INFO: epoch 001:   3785 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=226.7, ups=0.89, wpb=253.8, bsz=100, num_updates=3780, lr=4.63284e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=4403
2023-03-14 20:45:47 - progress_bar.py[line:274] - INFO: epoch 001:   3795 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.8, ups=0.85, wpb=255.5, bsz=100, num_updates=3790, lr=4.63117e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=4415
2023-03-14 20:45:59 - progress_bar.py[line:274] - INFO: epoch 001:   3805 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.1, ups=0.85, wpb=254.5, bsz=100, num_updates=3800, lr=4.62951e-05, gnorm=0.846, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4426
2023-03-14 20:46:10 - progress_bar.py[line:274] - INFO: epoch 001:   3815 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.1, ups=0.86, wpb=250.2, bsz=100, num_updates=3810, lr=4.62784e-05, gnorm=0.754, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4438
2023-03-14 20:46:22 - progress_bar.py[line:274] - INFO: epoch 001:   3825 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.1, ups=0.87, wpb=252.1, bsz=100, num_updates=3820, lr=4.62617e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4449
2023-03-14 20:46:33 - progress_bar.py[line:274] - INFO: epoch 001:   3835 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.9, ups=0.86, wpb=250.7, bsz=100, num_updates=3830, lr=4.6245e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4461
2023-03-14 20:46:45 - progress_bar.py[line:274] - INFO: epoch 001:   3845 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.3, ups=0.88, wpb=253.5, bsz=100, num_updates=3840, lr=4.62284e-05, gnorm=0.689, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4472
2023-03-14 20:46:56 - progress_bar.py[line:274] - INFO: epoch 001:   3855 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.2, ups=0.86, wpb=253.4, bsz=100, num_updates=3850, lr=4.62117e-05, gnorm=0.655, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4484
2023-03-14 20:47:08 - progress_bar.py[line:274] - INFO: epoch 001:   3865 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.4, ups=0.86, wpb=252.7, bsz=100, num_updates=3860, lr=4.6195e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4496
2023-03-14 20:47:19 - progress_bar.py[line:274] - INFO: epoch 001:   3875 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=222.3, ups=0.88, wpb=253.4, bsz=100, num_updates=3870, lr=4.61783e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4507
2023-03-14 20:47:31 - progress_bar.py[line:274] - INFO: epoch 001:   3885 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.4, ups=0.87, wpb=252.6, bsz=100, num_updates=3880, lr=4.61617e-05, gnorm=0.698, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=4519
2023-03-14 20:47:42 - progress_bar.py[line:274] - INFO: epoch 001:   3895 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.2, ups=0.86, wpb=253.9, bsz=100, num_updates=3890, lr=4.6145e-05, gnorm=0.693, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=4530
2023-03-14 20:47:54 - progress_bar.py[line:274] - INFO: epoch 001:   3905 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.2, ups=0.87, wpb=254.6, bsz=100, num_updates=3900, lr=4.61283e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=4542
2023-03-14 20:48:05 - progress_bar.py[line:274] - INFO: epoch 001:   3915 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.1, ups=0.86, wpb=255.4, bsz=100, num_updates=3910, lr=4.61116e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4553
2023-03-14 20:48:17 - progress_bar.py[line:274] - INFO: epoch 001:   3925 / 6313 loss=0.615, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=222.1, ups=0.87, wpb=255.2, bsz=100, num_updates=3920, lr=4.6095e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4565
2023-03-14 20:48:28 - progress_bar.py[line:274] - INFO: epoch 001:   3935 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.7, ups=0.87, wpb=252.7, bsz=100, num_updates=3930, lr=4.60783e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4576
2023-03-14 20:48:40 - progress_bar.py[line:274] - INFO: epoch 001:   3945 / 6313 loss=0.62, loss_v1=0, loss_v2=0, nll_loss=0.407, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=215.6, ups=0.86, wpb=250.1, bsz=100, num_updates=3940, lr=4.60616e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4588
2023-03-14 20:48:52 - progress_bar.py[line:274] - INFO: epoch 001:   3955 / 6313 loss=0.599, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=221.1, ups=0.87, wpb=252.9, bsz=100, num_updates=3950, lr=4.6045e-05, gnorm=0.755, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4599
2023-03-14 20:49:03 - progress_bar.py[line:274] - INFO: epoch 001:   3965 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=225.4, ups=0.89, wpb=254.7, bsz=100, num_updates=3960, lr=4.60283e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=4611
2023-03-14 20:49:14 - progress_bar.py[line:274] - INFO: epoch 001:   3975 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=248.3, nsentences=100, sample_size=248.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=215.4, ups=0.87, wpb=248.3, bsz=100, num_updates=3970, lr=4.60116e-05, gnorm=0.798, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4622
2023-03-14 20:49:26 - progress_bar.py[line:274] - INFO: epoch 001:   3985 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221, ups=0.86, wpb=256.3, bsz=100, num_updates=3980, lr=4.59949e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4634
2023-03-14 20:49:37 - progress_bar.py[line:274] - INFO: epoch 001:   3995 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.9, ups=0.88, wpb=253.4, bsz=100, num_updates=3990, lr=4.59783e-05, gnorm=0.804, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=4645
2023-03-14 20:49:49 - progress_bar.py[line:274] - INFO: epoch 001:   4005 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.3, ups=0.88, wpb=253.8, bsz=100, num_updates=4000, lr=4.59616e-05, gnorm=0.783, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=4657
2023-03-14 20:50:00 - progress_bar.py[line:274] - INFO: epoch 001:   4015 / 6313 loss=0.621, loss_v1=0, loss_v2=0, nll_loss=0.417, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.34, wps=219.6, ups=0.87, wpb=253.5, bsz=100, num_updates=4010, lr=4.59449e-05, gnorm=0.829, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4668
2023-03-14 20:50:12 - progress_bar.py[line:274] - INFO: epoch 001:   4025 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.9, ups=0.86, wpb=253.4, bsz=100, num_updates=4020, lr=4.59282e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=4680
2023-03-14 20:50:24 - progress_bar.py[line:274] - INFO: epoch 001:   4035 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.8, ups=0.86, wpb=254.3, bsz=100, num_updates=4030, lr=4.59116e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4692
2023-03-14 20:50:36 - progress_bar.py[line:274] - INFO: epoch 001:   4045 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=215.8, ups=0.86, wpb=252.2, bsz=100, num_updates=4040, lr=4.58949e-05, gnorm=0.772, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4703
2023-03-14 20:50:47 - progress_bar.py[line:274] - INFO: epoch 001:   4055 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.8, ups=0.86, wpb=253.7, bsz=100, num_updates=4050, lr=4.58782e-05, gnorm=0.728, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4715
2023-03-14 20:50:59 - progress_bar.py[line:274] - INFO: epoch 001:   4065 / 6313 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.5, ups=0.87, wpb=253.7, bsz=100, num_updates=4060, lr=4.58615e-05, gnorm=0.8, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4727
2023-03-14 20:51:10 - progress_bar.py[line:274] - INFO: epoch 001:   4075 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.1, ups=0.87, wpb=251.5, bsz=100, num_updates=4070, lr=4.58449e-05, gnorm=0.769, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4738
2023-03-14 20:51:22 - progress_bar.py[line:274] - INFO: epoch 001:   4085 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.9, ups=0.87, wpb=253.9, bsz=100, num_updates=4080, lr=4.58282e-05, gnorm=0.728, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=4750
2023-03-14 20:51:33 - progress_bar.py[line:274] - INFO: epoch 001:   4095 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.8, ups=0.87, wpb=252.8, bsz=100, num_updates=4090, lr=4.58115e-05, gnorm=0.731, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4761
2023-03-14 20:51:45 - progress_bar.py[line:274] - INFO: epoch 001:   4105 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.9, ups=0.88, wpb=251.1, bsz=100, num_updates=4100, lr=4.57948e-05, gnorm=0.755, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=4773
2023-03-14 20:51:56 - progress_bar.py[line:274] - INFO: epoch 001:   4115 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.1, ups=0.86, wpb=253, bsz=100, num_updates=4110, lr=4.57782e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4784
2023-03-14 20:52:08 - progress_bar.py[line:274] - INFO: epoch 001:   4125 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.4, ups=0.86, wpb=255.1, bsz=100, num_updates=4120, lr=4.57615e-05, gnorm=0.745, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4796
2023-03-14 20:52:20 - progress_bar.py[line:274] - INFO: epoch 001:   4135 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.4, ups=0.86, wpb=254.8, bsz=100, num_updates=4130, lr=4.57448e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=4807
2023-03-14 20:52:31 - progress_bar.py[line:274] - INFO: epoch 001:   4145 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.5, ups=0.86, wpb=256.2, bsz=100, num_updates=4140, lr=4.57281e-05, gnorm=0.738, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4819
2023-03-14 20:52:43 - progress_bar.py[line:274] - INFO: epoch 001:   4155 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=214.4, ups=0.85, wpb=253.1, bsz=100, num_updates=4150, lr=4.57115e-05, gnorm=0.733, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4831
2023-03-14 20:52:55 - progress_bar.py[line:274] - INFO: epoch 001:   4165 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.1, ups=0.85, wpb=254.2, bsz=100, num_updates=4160, lr=4.56948e-05, gnorm=0.759, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4843
2023-03-14 20:53:06 - progress_bar.py[line:274] - INFO: epoch 001:   4175 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.7, ups=0.86, wpb=254.2, bsz=100, num_updates=4170, lr=4.56781e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=4854
2023-03-14 20:53:18 - progress_bar.py[line:274] - INFO: epoch 001:   4185 / 6313 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.402, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.7, ups=0.86, wpb=252, bsz=100, num_updates=4180, lr=4.56615e-05, gnorm=0.781, clip=10, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=4866
2023-03-14 20:53:30 - progress_bar.py[line:274] - INFO: epoch 001:   4195 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.394, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.9, ups=0.88, wpb=251.1, bsz=100, num_updates=4190, lr=4.56448e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=4877
2023-03-14 20:53:41 - progress_bar.py[line:274] - INFO: epoch 001:   4205 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.8, ups=0.86, wpb=253.4, bsz=100, num_updates=4200, lr=4.56281e-05, gnorm=0.745, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=4889
2023-03-14 20:53:53 - progress_bar.py[line:274] - INFO: epoch 001:   4215 / 6313 loss=0.61, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.9, ups=0.86, wpb=253.2, bsz=100, num_updates=4210, lr=4.56114e-05, gnorm=0.747, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=4901
2023-03-14 20:54:04 - progress_bar.py[line:274] - INFO: epoch 001:   4225 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.3, ups=0.86, wpb=254.2, bsz=100, num_updates=4220, lr=4.55948e-05, gnorm=0.747, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=4912
2023-03-14 20:54:16 - progress_bar.py[line:274] - INFO: epoch 001:   4235 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.6, ups=0.87, wpb=255.5, bsz=100, num_updates=4230, lr=4.55781e-05, gnorm=0.663, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=4924
2023-03-14 20:54:27 - progress_bar.py[line:274] - INFO: epoch 001:   4245 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.8, ups=0.86, wpb=252.9, bsz=100, num_updates=4240, lr=4.55614e-05, gnorm=0.66, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4935
2023-03-14 20:54:39 - progress_bar.py[line:274] - INFO: epoch 001:   4255 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.5, ups=0.86, wpb=254.1, bsz=100, num_updates=4250, lr=4.55447e-05, gnorm=0.792, clip=10, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=4947
2023-03-14 20:54:51 - progress_bar.py[line:274] - INFO: epoch 001:   4265 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=222.6, ups=0.87, wpb=254.8, bsz=100, num_updates=4260, lr=4.55281e-05, gnorm=0.686, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=4958
2023-03-14 20:55:02 - progress_bar.py[line:274] - INFO: epoch 001:   4275 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=214.1, ups=0.85, wpb=252.4, bsz=100, num_updates=4270, lr=4.55114e-05, gnorm=0.688, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4970
2023-03-14 20:55:14 - progress_bar.py[line:274] - INFO: epoch 001:   4285 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.9, ups=0.85, wpb=253.1, bsz=100, num_updates=4280, lr=4.54947e-05, gnorm=0.735, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=4982
2023-03-14 20:55:26 - progress_bar.py[line:274] - INFO: epoch 001:   4295 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218, ups=0.86, wpb=252.1, bsz=100, num_updates=4290, lr=4.5478e-05, gnorm=0.799, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=4994
2023-03-14 20:55:37 - progress_bar.py[line:274] - INFO: epoch 001:   4305 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=220, ups=0.86, wpb=255.7, bsz=100, num_updates=4300, lr=4.54614e-05, gnorm=0.785, clip=20, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5005
2023-03-14 20:55:42 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 20:55:50 - progress_bar.py[line:274] - INFO: epoch 001:   4316 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=202.2, ups=0.79, wpb=254.5, bsz=100, num_updates=4310, lr=4.54447e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=5018
2023-03-14 20:56:02 - progress_bar.py[line:274] - INFO: epoch 001:   4326 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.6, ups=0.87, wpb=253.6, bsz=100, num_updates=4320, lr=4.5428e-05, gnorm=0.79, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5029
2023-03-14 20:56:13 - progress_bar.py[line:274] - INFO: epoch 001:   4336 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219, ups=0.86, wpb=253.5, bsz=100, num_updates=4330, lr=4.54113e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5041
2023-03-14 20:56:25 - progress_bar.py[line:274] - INFO: epoch 001:   4346 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.1, ups=0.86, wpb=254.4, bsz=100, num_updates=4340, lr=4.53947e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5053
2023-03-14 20:56:36 - progress_bar.py[line:274] - INFO: epoch 001:   4356 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218, ups=0.87, wpb=251.5, bsz=100, num_updates=4350, lr=4.5378e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5064
2023-03-14 20:56:48 - progress_bar.py[line:274] - INFO: epoch 001:   4366 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.9, ups=0.86, wpb=253.3, bsz=100, num_updates=4360, lr=4.53613e-05, gnorm=0.694, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5076
2023-03-14 20:56:59 - progress_bar.py[line:274] - INFO: epoch 001:   4376 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.3, ups=0.88, wpb=251.7, bsz=100, num_updates=4370, lr=4.53446e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5087
2023-03-14 20:57:11 - progress_bar.py[line:274] - INFO: epoch 001:   4386 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.4, ups=0.87, wpb=253.3, bsz=100, num_updates=4380, lr=4.5328e-05, gnorm=0.654, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5099
2023-03-14 20:57:22 - progress_bar.py[line:274] - INFO: epoch 001:   4396 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.3, ups=0.87, wpb=252.2, bsz=100, num_updates=4390, lr=4.53113e-05, gnorm=0.707, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=5110
2023-03-14 20:57:35 - progress_bar.py[line:274] - INFO: epoch 001:   4406 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=209.3, ups=0.83, wpb=253.5, bsz=100, num_updates=4400, lr=4.52946e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=5122
2023-03-14 20:57:46 - progress_bar.py[line:274] - INFO: epoch 001:   4416 / 6313 loss=0.608, loss_v1=0, loss_v2=0, nll_loss=0.399, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=218.4, ups=0.86, wpb=252.5, bsz=100, num_updates=4410, lr=4.5278e-05, gnorm=0.73, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5134
2023-03-14 20:57:58 - progress_bar.py[line:274] - INFO: epoch 001:   4426 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.2, ups=0.86, wpb=253.1, bsz=100, num_updates=4420, lr=4.52613e-05, gnorm=0.723, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5146
2023-03-14 20:58:09 - progress_bar.py[line:274] - INFO: epoch 001:   4436 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.9, ups=0.86, wpb=252.8, bsz=100, num_updates=4430, lr=4.52446e-05, gnorm=0.723, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5157
2023-03-14 20:58:21 - progress_bar.py[line:274] - INFO: epoch 001:   4446 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.7, ups=0.88, wpb=253.6, bsz=100, num_updates=4440, lr=4.52279e-05, gnorm=0.702, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5169
2023-03-14 20:58:32 - progress_bar.py[line:274] - INFO: epoch 001:   4456 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.2, ups=0.87, wpb=253, bsz=100, num_updates=4450, lr=4.52113e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=5180
2023-03-14 20:58:44 - progress_bar.py[line:274] - INFO: epoch 001:   4466 / 6313 loss=0.595, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.8, ups=0.86, wpb=250.6, bsz=100, num_updates=4460, lr=4.51946e-05, gnorm=0.698, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5192
2023-03-14 20:58:55 - progress_bar.py[line:274] - INFO: epoch 001:   4476 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.1, ups=0.86, wpb=253.1, bsz=100, num_updates=4470, lr=4.51779e-05, gnorm=0.697, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=5203
2023-03-14 20:59:07 - progress_bar.py[line:274] - INFO: epoch 001:   4486 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.8, ups=0.86, wpb=251.8, bsz=100, num_updates=4480, lr=4.51612e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5215
2023-03-14 20:59:19 - progress_bar.py[line:274] - INFO: epoch 001:   4496 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.389, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.8, ups=0.86, wpb=253.8, bsz=100, num_updates=4490, lr=4.51446e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5226
2023-03-14 20:59:30 - progress_bar.py[line:274] - INFO: epoch 001:   4506 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218, ups=0.86, wpb=254.4, bsz=100, num_updates=4500, lr=4.51279e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5238
2023-03-14 20:59:42 - progress_bar.py[line:274] - INFO: epoch 001:   4516 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.5, ups=0.86, wpb=252.1, bsz=100, num_updates=4510, lr=4.51112e-05, gnorm=0.715, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5250
2023-03-14 20:59:53 - progress_bar.py[line:274] - INFO: epoch 001:   4526 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.3, ups=0.87, wpb=252.2, bsz=100, num_updates=4520, lr=4.50945e-05, gnorm=0.751, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5261
2023-03-14 21:00:05 - progress_bar.py[line:274] - INFO: epoch 001:   4536 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.9, ups=0.86, wpb=255.9, bsz=100, num_updates=4530, lr=4.50779e-05, gnorm=0.66, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5273
2023-03-14 21:00:17 - progress_bar.py[line:274] - INFO: epoch 001:   4546 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.9, ups=0.87, wpb=254.3, bsz=100, num_updates=4540, lr=4.50612e-05, gnorm=0.764, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5284
2023-03-14 21:00:28 - progress_bar.py[line:274] - INFO: epoch 001:   4556 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.2, ups=0.86, wpb=253.9, bsz=100, num_updates=4550, lr=4.50445e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=12, gb_free=9.6, ema_decay=0.9999, wall=5296
2023-03-14 21:00:40 - progress_bar.py[line:274] - INFO: epoch 001:   4566 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.9, ups=0.86, wpb=253.6, bsz=100, num_updates=4560, lr=4.50278e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=5308
2023-03-14 21:00:51 - progress_bar.py[line:274] - INFO: epoch 001:   4576 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.8, ups=0.86, wpb=254.5, bsz=100, num_updates=4570, lr=4.50112e-05, gnorm=0.702, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5319
2023-03-14 21:01:03 - progress_bar.py[line:274] - INFO: epoch 001:   4586 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.6, ups=0.86, wpb=254.1, bsz=100, num_updates=4580, lr=4.49945e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5331
2023-03-14 21:01:14 - progress_bar.py[line:274] - INFO: epoch 001:   4596 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.9, ups=0.87, wpb=251.6, bsz=100, num_updates=4590, lr=4.49778e-05, gnorm=0.739, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5342
2023-03-14 21:01:26 - progress_bar.py[line:274] - INFO: epoch 001:   4606 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.9, ups=0.87, wpb=252.7, bsz=100, num_updates=4600, lr=4.49611e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=5354
2023-03-14 21:01:37 - progress_bar.py[line:274] - INFO: epoch 001:   4616 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221, ups=0.88, wpb=252.4, bsz=100, num_updates=4610, lr=4.49445e-05, gnorm=0.716, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5365
2023-03-14 21:01:49 - progress_bar.py[line:274] - INFO: epoch 001:   4626 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.8, ups=0.87, wpb=252.8, bsz=100, num_updates=4620, lr=4.49278e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=5377
2023-03-14 21:02:00 - progress_bar.py[line:274] - INFO: epoch 001:   4636 / 6313 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.6, ups=0.86, wpb=254.1, bsz=100, num_updates=4630, lr=4.49111e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=5388
2023-03-14 21:02:12 - progress_bar.py[line:274] - INFO: epoch 001:   4646 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.2, ups=0.87, wpb=254, bsz=100, num_updates=4640, lr=4.48945e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5400
2023-03-14 21:02:23 - progress_bar.py[line:274] - INFO: epoch 001:   4656 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.1, ups=0.88, wpb=253.5, bsz=100, num_updates=4650, lr=4.48778e-05, gnorm=0.712, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5411
2023-03-14 21:02:35 - progress_bar.py[line:274] - INFO: epoch 001:   4666 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.1, ups=0.86, wpb=253.7, bsz=100, num_updates=4660, lr=4.48611e-05, gnorm=0.711, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5423
2023-03-14 21:02:47 - progress_bar.py[line:274] - INFO: epoch 001:   4676 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.9, ups=0.86, wpb=254.5, bsz=100, num_updates=4670, lr=4.48444e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5434
2023-03-14 21:02:58 - progress_bar.py[line:274] - INFO: epoch 001:   4686 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.2, ups=0.86, wpb=254.6, bsz=100, num_updates=4680, lr=4.48278e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=12, gb_free=9.5, ema_decay=0.9999, wall=5446
2023-03-14 21:03:10 - progress_bar.py[line:274] - INFO: epoch 001:   4696 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=223.2, ups=0.88, wpb=254, bsz=100, num_updates=4690, lr=4.48111e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5458
2023-03-14 21:03:21 - progress_bar.py[line:274] - INFO: epoch 001:   4706 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=221.2, ups=0.88, wpb=250.6, bsz=100, num_updates=4700, lr=4.47944e-05, gnorm=0.714, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=5469
2023-03-14 21:03:33 - progress_bar.py[line:274] - INFO: epoch 001:   4716 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.9, ups=0.86, wpb=254.7, bsz=100, num_updates=4710, lr=4.47777e-05, gnorm=0.761, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5480
2023-03-14 21:03:44 - progress_bar.py[line:274] - INFO: epoch 001:   4726 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.9, ups=0.86, wpb=255, bsz=100, num_updates=4720, lr=4.47611e-05, gnorm=0.648, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5492
2023-03-14 21:03:56 - progress_bar.py[line:274] - INFO: epoch 001:   4736 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.1, ups=0.86, wpb=251.1, bsz=100, num_updates=4730, lr=4.47444e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5504
2023-03-14 21:04:07 - progress_bar.py[line:274] - INFO: epoch 001:   4746 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=256.6, nsentences=100, sample_size=256.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.1, ups=0.87, wpb=256.6, bsz=100, num_updates=4740, lr=4.47277e-05, gnorm=0.751, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5515
2023-03-14 21:04:19 - progress_bar.py[line:274] - INFO: epoch 001:   4756 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=221.5, ups=0.88, wpb=252.7, bsz=100, num_updates=4750, lr=4.4711e-05, gnorm=0.767, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=5527
2023-03-14 21:04:31 - progress_bar.py[line:274] - INFO: epoch 001:   4766 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.2, ups=0.85, wpb=255.9, bsz=100, num_updates=4760, lr=4.46944e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5538
2023-03-14 21:04:42 - progress_bar.py[line:274] - INFO: epoch 001:   4776 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.2, ups=0.86, wpb=254.1, bsz=100, num_updates=4770, lr=4.46777e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5550
2023-03-14 21:04:54 - progress_bar.py[line:274] - INFO: epoch 001:   4786 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=221.3, ups=0.87, wpb=253.1, bsz=100, num_updates=4780, lr=4.4661e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=5561
2023-03-14 21:05:05 - progress_bar.py[line:274] - INFO: epoch 001:   4796 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.7, ups=0.86, wpb=256, bsz=100, num_updates=4790, lr=4.46443e-05, gnorm=0.716, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5573
2023-03-14 21:05:17 - progress_bar.py[line:274] - INFO: epoch 001:   4806 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=224.4, ups=0.88, wpb=256.1, bsz=100, num_updates=4800, lr=4.46277e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=5584
2023-03-14 21:05:28 - progress_bar.py[line:274] - INFO: epoch 001:   4816 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=223.8, ups=0.87, wpb=256.3, bsz=100, num_updates=4810, lr=4.4611e-05, gnorm=0.753, clip=10, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=5596
2023-03-14 21:05:40 - progress_bar.py[line:274] - INFO: epoch 001:   4826 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.39, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.1, ups=0.86, wpb=252.9, bsz=100, num_updates=4820, lr=4.45943e-05, gnorm=0.663, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5608
2023-03-14 21:05:50 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 21:05:52 - progress_bar.py[line:274] - INFO: epoch 001:   4837 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=200.4, ups=0.79, wpb=252.3, bsz=100, num_updates=4830, lr=4.45777e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=5620
2023-03-14 21:06:04 - progress_bar.py[line:274] - INFO: epoch 001:   4847 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=222.4, ups=0.87, wpb=254.4, bsz=100, num_updates=4840, lr=4.4561e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=5632
2023-03-14 21:06:15 - progress_bar.py[line:274] - INFO: epoch 001:   4857 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=221.6, ups=0.87, wpb=253.9, bsz=100, num_updates=4850, lr=4.45443e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=5643
2023-03-14 21:06:27 - progress_bar.py[line:274] - INFO: epoch 001:   4867 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.7, ups=0.86, wpb=253.9, bsz=100, num_updates=4860, lr=4.45276e-05, gnorm=0.753, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5655
2023-03-14 21:06:39 - progress_bar.py[line:274] - INFO: epoch 001:   4877 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217, ups=0.86, wpb=252.1, bsz=100, num_updates=4870, lr=4.4511e-05, gnorm=0.706, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5666
2023-03-14 21:06:50 - progress_bar.py[line:274] - INFO: epoch 001:   4887 / 6313 loss=0.613, loss_v1=0, loss_v2=0, nll_loss=0.406, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217, ups=0.86, wpb=251.8, bsz=100, num_updates=4880, lr=4.44943e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=5678
2023-03-14 21:07:02 - progress_bar.py[line:274] - INFO: epoch 001:   4897 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.2, ups=0.86, wpb=253.9, bsz=100, num_updates=4890, lr=4.44776e-05, gnorm=0.683, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5690
2023-03-14 21:07:13 - progress_bar.py[line:274] - INFO: epoch 001:   4907 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=222.3, ups=0.87, wpb=255.1, bsz=100, num_updates=4900, lr=4.44609e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=5701
2023-03-14 21:07:25 - progress_bar.py[line:274] - INFO: epoch 001:   4917 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=224.8, ups=0.88, wpb=254.8, bsz=100, num_updates=4910, lr=4.44443e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5712
2023-03-14 21:07:36 - progress_bar.py[line:274] - INFO: epoch 001:   4927 / 6313 loss=0.607, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.6, ups=0.86, wpb=250.6, bsz=100, num_updates=4920, lr=4.44276e-05, gnorm=0.786, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5724
2023-03-14 21:07:48 - progress_bar.py[line:274] - INFO: epoch 001:   4937 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.8, ups=0.86, wpb=252.5, bsz=100, num_updates=4930, lr=4.44109e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5736
2023-03-14 21:07:59 - progress_bar.py[line:274] - INFO: epoch 001:   4947 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=225.4, ups=0.89, wpb=254.5, bsz=100, num_updates=4940, lr=4.43942e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5747
2023-03-14 21:08:11 - progress_bar.py[line:274] - INFO: epoch 001:   4957 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.6, ups=0.87, wpb=254.8, bsz=100, num_updates=4950, lr=4.43776e-05, gnorm=0.679, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5759
2023-03-14 21:08:22 - progress_bar.py[line:274] - INFO: epoch 001:   4967 / 6313 loss=0.617, loss_v1=0, loss_v2=0, nll_loss=0.414, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=218.5, ups=0.86, wpb=252.8, bsz=100, num_updates=4960, lr=4.43609e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5770
2023-03-14 21:08:34 - progress_bar.py[line:274] - INFO: epoch 001:   4977 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218, ups=0.86, wpb=252.5, bsz=100, num_updates=4970, lr=4.43442e-05, gnorm=0.724, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=5782
2023-03-14 21:08:45 - progress_bar.py[line:274] - INFO: epoch 001:   4987 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=219.9, ups=0.86, wpb=254.6, bsz=100, num_updates=4980, lr=4.43275e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=5793
2023-03-14 21:08:57 - progress_bar.py[line:274] - INFO: epoch 001:   4997 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=214.7, ups=0.85, wpb=252.9, bsz=100, num_updates=4990, lr=4.43109e-05, gnorm=0.633, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=5805
2023-03-14 21:09:09 - progress_bar.py[line:274] - INFO: epoch 001:   5007 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.4, ups=0.86, wpb=256.4, bsz=100, num_updates=5000, lr=4.42942e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5817
2023-03-14 21:09:21 - progress_bar.py[line:274] - INFO: epoch 001:   5017 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.3, ups=0.86, wpb=254.4, bsz=100, num_updates=5010, lr=4.42775e-05, gnorm=0.69, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5828
2023-03-14 21:09:32 - progress_bar.py[line:274] - INFO: epoch 001:   5027 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.9, ups=0.86, wpb=254.3, bsz=100, num_updates=5020, lr=4.42608e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5840
2023-03-14 21:09:44 - progress_bar.py[line:274] - INFO: epoch 001:   5037 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.3, ups=0.87, wpb=254.6, bsz=100, num_updates=5030, lr=4.42442e-05, gnorm=0.778, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=5851
2023-03-14 21:09:55 - progress_bar.py[line:274] - INFO: epoch 001:   5047 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.408, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=217.2, ups=0.86, wpb=251.9, bsz=100, num_updates=5040, lr=4.42275e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5863
2023-03-14 21:10:07 - progress_bar.py[line:274] - INFO: epoch 001:   5057 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.1, ups=0.87, wpb=253, bsz=100, num_updates=5050, lr=4.42108e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5875
2023-03-14 21:10:19 - progress_bar.py[line:274] - INFO: epoch 001:   5067 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.9, ups=0.86, wpb=255.4, bsz=100, num_updates=5060, lr=4.41942e-05, gnorm=0.786, clip=20, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5886
2023-03-14 21:10:30 - progress_bar.py[line:274] - INFO: epoch 001:   5077 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222, ups=0.87, wpb=253.9, bsz=100, num_updates=5070, lr=4.41775e-05, gnorm=0.752, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=5898
2023-03-14 21:10:42 - progress_bar.py[line:274] - INFO: epoch 001:   5087 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220, ups=0.86, wpb=254.9, bsz=100, num_updates=5080, lr=4.41608e-05, gnorm=0.713, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=5910
2023-03-14 21:10:53 - progress_bar.py[line:274] - INFO: epoch 001:   5097 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=223.7, ups=0.88, wpb=255.2, bsz=100, num_updates=5090, lr=4.41441e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5921
2023-03-14 21:11:05 - progress_bar.py[line:274] - INFO: epoch 001:   5107 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.7, ups=0.86, wpb=254.8, bsz=100, num_updates=5100, lr=4.41275e-05, gnorm=0.616, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=5933
2023-03-14 21:11:17 - progress_bar.py[line:274] - INFO: epoch 001:   5117 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.1, ups=0.86, wpb=252.5, bsz=100, num_updates=5110, lr=4.41108e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=5944
2023-03-14 21:11:28 - progress_bar.py[line:274] - INFO: epoch 001:   5127 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.1, ups=0.85, wpb=254.3, bsz=100, num_updates=5120, lr=4.40941e-05, gnorm=0.69, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=5956
2023-03-14 21:11:40 - progress_bar.py[line:274] - INFO: epoch 001:   5137 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=212.9, ups=0.85, wpb=251.4, bsz=100, num_updates=5130, lr=4.40774e-05, gnorm=0.724, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=5968
2023-03-14 21:11:52 - progress_bar.py[line:274] - INFO: epoch 001:   5147 / 6313 loss=0.604, loss_v1=0, loss_v2=0, nll_loss=0.4, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=217.8, ups=0.86, wpb=252.1, bsz=100, num_updates=5140, lr=4.40608e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=5980
2023-03-14 21:12:03 - progress_bar.py[line:274] - INFO: epoch 001:   5157 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.4, ups=0.88, wpb=250.6, bsz=100, num_updates=5150, lr=4.40441e-05, gnorm=0.713, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=5991
2023-03-14 21:12:15 - progress_bar.py[line:274] - INFO: epoch 001:   5167 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=225.9, ups=0.89, wpb=254.5, bsz=100, num_updates=5160, lr=4.40274e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6002
2023-03-14 21:12:26 - progress_bar.py[line:274] - INFO: epoch 001:   5177 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=226, ups=0.89, wpb=255.3, bsz=100, num_updates=5170, lr=4.40107e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6014
2023-03-14 21:12:37 - progress_bar.py[line:274] - INFO: epoch 001:   5187 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218, ups=0.87, wpb=251.6, bsz=100, num_updates=5180, lr=4.39941e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=6025
2023-03-14 21:12:49 - progress_bar.py[line:274] - INFO: epoch 001:   5197 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=222.1, ups=0.87, wpb=253.9, bsz=100, num_updates=5190, lr=4.39774e-05, gnorm=0.599, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6037
2023-03-14 21:13:01 - progress_bar.py[line:274] - INFO: epoch 001:   5207 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.397, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.7, ups=0.86, wpb=252, bsz=100, num_updates=5200, lr=4.39607e-05, gnorm=0.651, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6048
2023-03-14 21:13:12 - progress_bar.py[line:274] - INFO: epoch 001:   5217 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.1, ups=0.86, wpb=254.2, bsz=100, num_updates=5210, lr=4.3944e-05, gnorm=0.704, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6060
2023-03-14 21:13:23 - progress_bar.py[line:274] - INFO: epoch 001:   5227 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=257.2, nsentences=100, sample_size=257.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=229.8, ups=0.89, wpb=257.2, bsz=100, num_updates=5220, lr=4.39274e-05, gnorm=0.626, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6071
2023-03-14 21:13:35 - progress_bar.py[line:274] - INFO: epoch 001:   5237 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.5, ups=0.86, wpb=252.5, bsz=100, num_updates=5230, lr=4.39107e-05, gnorm=0.68, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6083
2023-03-14 21:13:47 - progress_bar.py[line:274] - INFO: epoch 001:   5247 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.405, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=215.5, ups=0.85, wpb=254.3, bsz=100, num_updates=5240, lr=4.3894e-05, gnorm=0.758, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6095
2023-03-14 21:13:58 - progress_bar.py[line:274] - INFO: epoch 001:   5257 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.3, ups=0.86, wpb=254.8, bsz=100, num_updates=5250, lr=4.38773e-05, gnorm=0.671, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6106
2023-03-14 21:14:10 - progress_bar.py[line:274] - INFO: epoch 001:   5267 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.4, ups=0.87, wpb=252.4, bsz=100, num_updates=5260, lr=4.38607e-05, gnorm=0.65, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6118
2023-03-14 21:14:22 - progress_bar.py[line:274] - INFO: epoch 001:   5277 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.3, ups=0.86, wpb=253.2, bsz=100, num_updates=5270, lr=4.3844e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6129
2023-03-14 21:14:33 - progress_bar.py[line:274] - INFO: epoch 001:   5287 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.5, ups=0.86, wpb=254, bsz=100, num_updates=5280, lr=4.38273e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6141
2023-03-14 21:14:45 - progress_bar.py[line:274] - INFO: epoch 001:   5297 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=223.2, ups=0.87, wpb=255.7, bsz=100, num_updates=5290, lr=4.38107e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6152
2023-03-14 21:14:56 - progress_bar.py[line:274] - INFO: epoch 001:   5307 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=257.6, nsentences=100, sample_size=257.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=225.9, ups=0.88, wpb=257.6, bsz=100, num_updates=5300, lr=4.3794e-05, gnorm=0.615, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6164
2023-03-14 21:15:08 - progress_bar.py[line:274] - INFO: epoch 001:   5317 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219, ups=0.86, wpb=253.9, bsz=100, num_updates=5310, lr=4.37773e-05, gnorm=0.694, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=6176
2023-03-14 21:15:19 - progress_bar.py[line:274] - INFO: epoch 001:   5327 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.6, ups=0.87, wpb=252, bsz=100, num_updates=5320, lr=4.37606e-05, gnorm=0.628, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6187
2023-03-14 21:15:31 - progress_bar.py[line:274] - INFO: epoch 001:   5337 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=225.6, ups=0.89, wpb=254.4, bsz=100, num_updates=5330, lr=4.3744e-05, gnorm=0.624, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=6198
2023-03-14 21:15:42 - progress_bar.py[line:274] - INFO: epoch 001:   5347 / 6313 loss=0.606, loss_v1=0, loss_v2=0, nll_loss=0.398, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.3, ups=0.86, wpb=251, bsz=100, num_updates=5340, lr=4.37273e-05, gnorm=0.715, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6210
2023-03-14 21:15:54 - progress_bar.py[line:274] - INFO: epoch 001:   5357 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218, ups=0.86, wpb=254.6, bsz=100, num_updates=5350, lr=4.37106e-05, gnorm=0.629, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6222
2023-03-14 21:16:05 - progress_bar.py[line:274] - INFO: epoch 001:   5367 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219, ups=0.86, wpb=253.5, bsz=100, num_updates=5360, lr=4.36939e-05, gnorm=0.776, clip=20, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6233
2023-03-14 21:16:17 - progress_bar.py[line:274] - INFO: epoch 001:   5377 / 6313 loss=0.614, loss_v1=0, loss_v2=0, nll_loss=0.41, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.33, wps=214.5, ups=0.85, wpb=251.4, bsz=100, num_updates=5370, lr=4.36773e-05, gnorm=0.668, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6245
2023-03-14 21:16:29 - progress_bar.py[line:274] - INFO: epoch 001:   5387 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220, ups=0.87, wpb=251.5, bsz=100, num_updates=5380, lr=4.36606e-05, gnorm=0.719, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6256
2023-03-14 21:16:40 - progress_bar.py[line:274] - INFO: epoch 001:   5397 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.8, ups=0.86, wpb=253.1, bsz=100, num_updates=5390, lr=4.36439e-05, gnorm=0.685, clip=0, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6268
2023-03-14 21:16:52 - progress_bar.py[line:274] - INFO: epoch 001:   5407 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.3, ups=0.86, wpb=253.9, bsz=100, num_updates=5400, lr=4.36272e-05, gnorm=0.67, clip=0, loss_scale=1024, train_wall=12, gb_free=10, ema_decay=0.9999, wall=6280
2023-03-14 21:16:54 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 21:17:04 - progress_bar.py[line:274] - INFO: epoch 001:   5418 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.395, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=200.6, ups=0.79, wpb=252.9, bsz=100, num_updates=5410, lr=4.36106e-05, gnorm=0.803, clip=10, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=6292
2023-03-14 21:17:16 - progress_bar.py[line:274] - INFO: epoch 001:   5428 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.8, ups=0.87, wpb=252.6, bsz=100, num_updates=5420, lr=4.35939e-05, gnorm=0.671, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6304
2023-03-14 21:17:28 - progress_bar.py[line:274] - INFO: epoch 001:   5438 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.3, ups=0.86, wpb=253.9, bsz=100, num_updates=5430, lr=4.35772e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6315
2023-03-14 21:17:39 - progress_bar.py[line:274] - INFO: epoch 001:   5448 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=223, ups=0.88, wpb=254.6, bsz=100, num_updates=5440, lr=4.35605e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6327
2023-03-14 21:17:51 - progress_bar.py[line:274] - INFO: epoch 001:   5458 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.7, ups=0.86, wpb=251.9, bsz=100, num_updates=5450, lr=4.35439e-05, gnorm=0.694, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6339
2023-03-14 21:18:02 - progress_bar.py[line:274] - INFO: epoch 001:   5468 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.4, ups=0.86, wpb=255.2, bsz=100, num_updates=5460, lr=4.35272e-05, gnorm=0.683, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6350
2023-03-14 21:18:14 - progress_bar.py[line:274] - INFO: epoch 001:   5478 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221.1, ups=0.87, wpb=253.2, bsz=100, num_updates=5470, lr=4.35105e-05, gnorm=0.651, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6362
2023-03-14 21:18:26 - progress_bar.py[line:274] - INFO: epoch 001:   5488 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=213.7, ups=0.85, wpb=250.4, bsz=100, num_updates=5480, lr=4.34938e-05, gnorm=0.782, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6374
2023-03-14 21:18:38 - progress_bar.py[line:274] - INFO: epoch 001:   5498 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.3, ups=0.86, wpb=253.8, bsz=100, num_updates=5490, lr=4.34772e-05, gnorm=0.763, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6385
2023-03-14 21:18:49 - progress_bar.py[line:274] - INFO: epoch 001:   5508 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223.4, ups=0.87, wpb=256.3, bsz=100, num_updates=5500, lr=4.34605e-05, gnorm=0.716, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6397
2023-03-14 21:19:01 - progress_bar.py[line:274] - INFO: epoch 001:   5518 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.9, ups=0.86, wpb=253.8, bsz=100, num_updates=5510, lr=4.34438e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6408
2023-03-14 21:19:12 - progress_bar.py[line:274] - INFO: epoch 001:   5528 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=250, nsentences=100, sample_size=250, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.1, ups=0.86, wpb=250, bsz=100, num_updates=5520, lr=4.34272e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6420
2023-03-14 21:19:24 - progress_bar.py[line:274] - INFO: epoch 001:   5538 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221.1, ups=0.87, wpb=253.7, bsz=100, num_updates=5530, lr=4.34105e-05, gnorm=0.633, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6431
2023-03-14 21:19:35 - progress_bar.py[line:274] - INFO: epoch 001:   5548 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.3, ups=0.86, wpb=252.8, bsz=100, num_updates=5540, lr=4.33938e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6443
2023-03-14 21:19:47 - progress_bar.py[line:274] - INFO: epoch 001:   5558 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=223.1, ups=0.88, wpb=254.9, bsz=100, num_updates=5550, lr=4.33771e-05, gnorm=0.634, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6454
2023-03-14 21:19:58 - progress_bar.py[line:274] - INFO: epoch 001:   5568 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218, ups=0.86, wpb=252.3, bsz=100, num_updates=5560, lr=4.33605e-05, gnorm=0.626, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6466
2023-03-14 21:20:10 - progress_bar.py[line:274] - INFO: epoch 001:   5578 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.8, ups=0.86, wpb=256.5, bsz=100, num_updates=5570, lr=4.33438e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6478
2023-03-14 21:20:21 - progress_bar.py[line:274] - INFO: epoch 001:   5588 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.8, ups=0.86, wpb=254.2, bsz=100, num_updates=5580, lr=4.33271e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6489
2023-03-14 21:20:33 - progress_bar.py[line:274] - INFO: epoch 001:   5598 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218, ups=0.86, wpb=253.5, bsz=100, num_updates=5590, lr=4.33104e-05, gnorm=0.761, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6501
2023-03-14 21:20:45 - progress_bar.py[line:274] - INFO: epoch 001:   5608 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=214.1, ups=0.85, wpb=251, bsz=100, num_updates=5600, lr=4.32938e-05, gnorm=0.737, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6513
2023-03-14 21:20:56 - progress_bar.py[line:274] - INFO: epoch 001:   5618 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.8, ups=0.86, wpb=252.2, bsz=100, num_updates=5610, lr=4.32771e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6524
2023-03-14 21:21:08 - progress_bar.py[line:274] - INFO: epoch 001:   5628 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=223, ups=0.88, wpb=254.4, bsz=100, num_updates=5620, lr=4.32604e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6536
2023-03-14 21:21:19 - progress_bar.py[line:274] - INFO: epoch 001:   5638 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.6, ups=0.87, wpb=250.4, bsz=100, num_updates=5630, lr=4.32437e-05, gnorm=0.715, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=6547
2023-03-14 21:21:31 - progress_bar.py[line:274] - INFO: epoch 001:   5648 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.1, ups=0.86, wpb=255.9, bsz=100, num_updates=5640, lr=4.32271e-05, gnorm=0.697, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6559
2023-03-14 21:21:43 - progress_bar.py[line:274] - INFO: epoch 001:   5658 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.9, ups=0.86, wpb=255.8, bsz=100, num_updates=5650, lr=4.32104e-05, gnorm=0.665, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6570
2023-03-14 21:21:54 - progress_bar.py[line:274] - INFO: epoch 001:   5668 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=226.1, ups=0.89, wpb=255.1, bsz=100, num_updates=5660, lr=4.31937e-05, gnorm=0.682, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6582
2023-03-14 21:22:05 - progress_bar.py[line:274] - INFO: epoch 001:   5678 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.5, ups=0.87, wpb=251.3, bsz=100, num_updates=5670, lr=4.3177e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=6593
2023-03-14 21:22:17 - progress_bar.py[line:274] - INFO: epoch 001:   5688 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.7, ups=0.87, wpb=254.8, bsz=100, num_updates=5680, lr=4.31604e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6605
2023-03-14 21:22:28 - progress_bar.py[line:274] - INFO: epoch 001:   5698 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221.4, ups=0.87, wpb=253.2, bsz=100, num_updates=5690, lr=4.31437e-05, gnorm=0.69, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=6616
2023-03-14 21:22:40 - progress_bar.py[line:274] - INFO: epoch 001:   5708 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.8, ups=0.87, wpb=252.8, bsz=100, num_updates=5700, lr=4.3127e-05, gnorm=0.63, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6628
2023-03-14 21:22:52 - progress_bar.py[line:274] - INFO: epoch 001:   5718 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.1, ups=0.86, wpb=253.8, bsz=100, num_updates=5710, lr=4.31103e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6639
2023-03-14 21:23:03 - progress_bar.py[line:274] - INFO: epoch 001:   5728 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.4, ups=0.86, wpb=253.9, bsz=100, num_updates=5720, lr=4.30937e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6651
2023-03-14 21:23:15 - progress_bar.py[line:274] - INFO: epoch 001:   5738 / 6313 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.9, ups=0.86, wpb=254, bsz=100, num_updates=5730, lr=4.3077e-05, gnorm=0.712, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6663
2023-03-14 21:23:26 - progress_bar.py[line:274] - INFO: epoch 001:   5748 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=222.5, ups=0.88, wpb=254.2, bsz=100, num_updates=5740, lr=4.30603e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=6674
2023-03-14 21:23:38 - progress_bar.py[line:274] - INFO: epoch 001:   5758 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219, ups=0.87, wpb=252.8, bsz=100, num_updates=5750, lr=4.30437e-05, gnorm=0.775, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=6686
2023-03-14 21:23:50 - progress_bar.py[line:274] - INFO: epoch 001:   5768 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.3, ups=0.86, wpb=254.3, bsz=100, num_updates=5760, lr=4.3027e-05, gnorm=0.723, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6697
2023-03-14 21:24:01 - progress_bar.py[line:274] - INFO: epoch 001:   5778 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.2, ups=0.86, wpb=253.8, bsz=100, num_updates=5770, lr=4.30103e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=6709
2023-03-14 21:24:13 - progress_bar.py[line:274] - INFO: epoch 001:   5788 / 6313 loss=0.6, loss_v1=0, loss_v2=0, nll_loss=0.391, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=221.2, ups=0.87, wpb=253.6, bsz=100, num_updates=5780, lr=4.29936e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6721
2023-03-14 21:24:24 - progress_bar.py[line:274] - INFO: epoch 001:   5798 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.8, ups=0.86, wpb=255.3, bsz=100, num_updates=5790, lr=4.2977e-05, gnorm=0.567, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6732
2023-03-14 21:24:36 - progress_bar.py[line:274] - INFO: epoch 001:   5808 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.9, ups=0.86, wpb=253.1, bsz=100, num_updates=5800, lr=4.29603e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6744
2023-03-14 21:24:48 - progress_bar.py[line:274] - INFO: epoch 001:   5818 / 6313 loss=0.605, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.2, ups=0.86, wpb=250.5, bsz=100, num_updates=5810, lr=4.29436e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6755
2023-03-14 21:24:59 - progress_bar.py[line:274] - INFO: epoch 001:   5828 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.1, ups=0.87, wpb=252.1, bsz=100, num_updates=5820, lr=4.29269e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=6767
2023-03-14 21:25:11 - progress_bar.py[line:274] - INFO: epoch 001:   5838 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.4, ups=0.86, wpb=255.3, bsz=100, num_updates=5830, lr=4.29103e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6779
2023-03-14 21:25:23 - progress_bar.py[line:274] - INFO: epoch 001:   5848 / 6313 loss=0.609, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=214.3, ups=0.86, wpb=250.3, bsz=100, num_updates=5840, lr=4.28936e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6790
2023-03-14 21:25:34 - progress_bar.py[line:274] - INFO: epoch 001:   5858 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.8, ups=0.86, wpb=253.6, bsz=100, num_updates=5850, lr=4.28769e-05, gnorm=0.694, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6802
2023-03-14 21:25:46 - progress_bar.py[line:274] - INFO: epoch 001:   5868 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.2, ups=0.86, wpb=253.6, bsz=100, num_updates=5860, lr=4.28602e-05, gnorm=0.765, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6814
2023-03-14 21:25:57 - progress_bar.py[line:274] - INFO: epoch 001:   5878 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.3, ups=0.86, wpb=252.5, bsz=100, num_updates=5870, lr=4.28436e-05, gnorm=0.68, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6825
2023-03-14 21:26:09 - progress_bar.py[line:274] - INFO: epoch 001:   5888 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.2, ups=0.86, wpb=253.8, bsz=100, num_updates=5880, lr=4.28269e-05, gnorm=0.783, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6837
2023-03-14 21:26:20 - progress_bar.py[line:274] - INFO: epoch 001:   5898 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.2, ups=0.86, wpb=253.2, bsz=100, num_updates=5890, lr=4.28102e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=6848
2023-03-14 21:26:32 - progress_bar.py[line:274] - INFO: epoch 001:   5908 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.3, ups=0.87, wpb=251.9, bsz=100, num_updates=5900, lr=4.27935e-05, gnorm=0.69, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=6860
2023-03-14 21:26:44 - progress_bar.py[line:274] - INFO: epoch 001:   5918 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.5, ups=0.86, wpb=253.9, bsz=100, num_updates=5910, lr=4.27769e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6871
2023-03-14 21:26:55 - progress_bar.py[line:274] - INFO: epoch 001:   5928 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.387, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.7, ups=0.87, wpb=253.8, bsz=100, num_updates=5920, lr=4.27602e-05, gnorm=0.629, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=6883
2023-03-14 21:27:01 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 21:27:08 - progress_bar.py[line:274] - INFO: epoch 001:   5939 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=199.8, ups=0.79, wpb=252.1, bsz=100, num_updates=5930, lr=4.27435e-05, gnorm=0.598, clip=0, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=6896
2023-03-14 21:27:19 - progress_bar.py[line:274] - INFO: epoch 001:   5949 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=222.6, ups=0.88, wpb=253.8, bsz=100, num_updates=5940, lr=4.27268e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6907
2023-03-14 21:27:31 - progress_bar.py[line:274] - INFO: epoch 001:   5959 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.3, ups=0.86, wpb=254.1, bsz=100, num_updates=5950, lr=4.27102e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6919
2023-03-14 21:27:42 - progress_bar.py[line:274] - INFO: epoch 001:   5969 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217, ups=0.86, wpb=252, bsz=100, num_updates=5960, lr=4.26935e-05, gnorm=0.692, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=6930
2023-03-14 21:27:54 - progress_bar.py[line:274] - INFO: epoch 001:   5979 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.7, ups=0.86, wpb=255.1, bsz=100, num_updates=5970, lr=4.26768e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=6942
2023-03-14 21:28:06 - progress_bar.py[line:274] - INFO: epoch 001:   5989 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.8, ups=0.86, wpb=255.7, bsz=100, num_updates=5980, lr=4.26602e-05, gnorm=0.625, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=6953
2023-03-14 21:28:17 - progress_bar.py[line:274] - INFO: epoch 001:   5999 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.8, ups=0.86, wpb=255.7, bsz=100, num_updates=5990, lr=4.26435e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6965
2023-03-14 21:28:29 - progress_bar.py[line:274] - INFO: epoch 001:   6009 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.7, ups=0.87, wpb=254.7, bsz=100, num_updates=6000, lr=4.26268e-05, gnorm=0.687, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=6977
2023-03-14 21:28:40 - progress_bar.py[line:274] - INFO: epoch 001:   6019 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=218.6, ups=0.86, wpb=253.2, bsz=100, num_updates=6010, lr=4.26101e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=6988
2023-03-14 21:28:52 - progress_bar.py[line:274] - INFO: epoch 001:   6029 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.5, ups=0.86, wpb=254.4, bsz=100, num_updates=6020, lr=4.25935e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7000
2023-03-14 21:29:04 - progress_bar.py[line:274] - INFO: epoch 001:   6039 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.6, ups=0.86, wpb=255.2, bsz=100, num_updates=6030, lr=4.25768e-05, gnorm=0.655, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=7011
2023-03-14 21:29:15 - progress_bar.py[line:274] - INFO: epoch 001:   6049 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.9, ups=0.86, wpb=254.3, bsz=100, num_updates=6040, lr=4.25601e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=7023
2023-03-14 21:29:27 - progress_bar.py[line:274] - INFO: epoch 001:   6059 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.4, ups=0.86, wpb=253.9, bsz=100, num_updates=6050, lr=4.25434e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=7035
2023-03-14 21:29:38 - progress_bar.py[line:274] - INFO: epoch 001:   6069 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.3, ups=0.86, wpb=252.5, bsz=100, num_updates=6060, lr=4.25268e-05, gnorm=0.606, clip=0, loss_scale=512, train_wall=12, gb_free=11.2, ema_decay=0.9999, wall=7046
2023-03-14 21:29:50 - progress_bar.py[line:274] - INFO: epoch 001:   6079 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216, ups=0.86, wpb=251.7, bsz=100, num_updates=6070, lr=4.25101e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=7058
2023-03-14 21:30:02 - progress_bar.py[line:274] - INFO: epoch 001:   6089 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.3, ups=0.87, wpb=253, bsz=100, num_updates=6080, lr=4.24934e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=7069
2023-03-14 21:30:13 - progress_bar.py[line:274] - INFO: epoch 001:   6099 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.5, ups=0.85, wpb=255.7, bsz=100, num_updates=6090, lr=4.24767e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=7081
2023-03-14 21:30:25 - progress_bar.py[line:274] - INFO: epoch 001:   6109 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=249.9, nsentences=100, sample_size=249.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216, ups=0.86, wpb=249.9, bsz=100, num_updates=6100, lr=4.24601e-05, gnorm=0.629, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=7093
2023-03-14 21:30:36 - progress_bar.py[line:274] - INFO: epoch 001:   6119 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.4, ups=0.86, wpb=252.5, bsz=100, num_updates=6110, lr=4.24434e-05, gnorm=0.692, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=7104
2023-03-14 21:30:48 - progress_bar.py[line:274] - INFO: epoch 001:   6129 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.5, ups=0.86, wpb=251.7, bsz=100, num_updates=6120, lr=4.24267e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7116
2023-03-14 21:31:00 - progress_bar.py[line:274] - INFO: epoch 001:   6139 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.383, ntokens=257.4, nsentences=100, sample_size=257.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.1, ups=0.86, wpb=257.4, bsz=100, num_updates=6130, lr=4.241e-05, gnorm=0.715, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7127
2023-03-14 21:31:11 - progress_bar.py[line:274] - INFO: epoch 001:   6149 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.1, ups=0.86, wpb=251.4, bsz=100, num_updates=6140, lr=4.23934e-05, gnorm=0.747, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7139
2023-03-14 21:31:23 - progress_bar.py[line:274] - INFO: epoch 001:   6159 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.9, ups=0.86, wpb=253.7, bsz=100, num_updates=6150, lr=4.23767e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=7151
2023-03-14 21:31:34 - progress_bar.py[line:274] - INFO: epoch 001:   6169 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.9, ups=0.87, wpb=251.8, bsz=100, num_updates=6160, lr=4.236e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=7162
2023-03-14 21:31:46 - progress_bar.py[line:274] - INFO: epoch 001:   6179 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.5, ups=0.87, wpb=252.4, bsz=100, num_updates=6170, lr=4.23433e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=7174
2023-03-14 21:31:57 - progress_bar.py[line:274] - INFO: epoch 001:   6189 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.3, ups=0.86, wpb=253.7, bsz=100, num_updates=6180, lr=4.23267e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=7185
2023-03-14 21:32:09 - progress_bar.py[line:274] - INFO: epoch 001:   6199 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.8, ups=0.86, wpb=254.2, bsz=100, num_updates=6190, lr=4.231e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7197
2023-03-14 21:32:21 - progress_bar.py[line:274] - INFO: epoch 001:   6209 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.3, ups=0.86, wpb=255.2, bsz=100, num_updates=6200, lr=4.22933e-05, gnorm=0.676, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=7209
2023-03-14 21:32:32 - progress_bar.py[line:274] - INFO: epoch 001:   6219 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=215.6, ups=0.85, wpb=252.5, bsz=100, num_updates=6210, lr=4.22767e-05, gnorm=0.712, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7220
2023-03-14 21:32:44 - progress_bar.py[line:274] - INFO: epoch 001:   6229 / 6313 loss=0.597, loss_v1=0, loss_v2=0, nll_loss=0.386, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217.7, ups=0.86, wpb=251.7, bsz=100, num_updates=6220, lr=4.226e-05, gnorm=0.752, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7232
2023-03-14 21:32:56 - progress_bar.py[line:274] - INFO: epoch 001:   6239 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.382, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219, ups=0.86, wpb=253.7, bsz=100, num_updates=6230, lr=4.22433e-05, gnorm=0.791, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7243
2023-03-14 21:33:07 - progress_bar.py[line:274] - INFO: epoch 001:   6249 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.7, ups=0.86, wpb=253.2, bsz=100, num_updates=6240, lr=4.22266e-05, gnorm=0.714, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7255
2023-03-14 21:33:19 - progress_bar.py[line:274] - INFO: epoch 001:   6259 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219, ups=0.87, wpb=253.1, bsz=100, num_updates=6250, lr=4.221e-05, gnorm=0.731, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=7267
2023-03-14 21:33:30 - progress_bar.py[line:274] - INFO: epoch 001:   6269 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.9, ups=0.87, wpb=254, bsz=100, num_updates=6260, lr=4.21933e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=7278
2023-03-14 21:33:42 - progress_bar.py[line:274] - INFO: epoch 001:   6279 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.7, ups=0.88, wpb=254.3, bsz=100, num_updates=6270, lr=4.21766e-05, gnorm=0.66, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=7290
2023-03-14 21:33:53 - progress_bar.py[line:274] - INFO: epoch 001:   6289 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.9, ups=0.86, wpb=250.4, bsz=100, num_updates=6280, lr=4.21599e-05, gnorm=0.758, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=7301
2023-03-14 21:34:05 - progress_bar.py[line:274] - INFO: epoch 001:   6299 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223.6, ups=0.89, wpb=252.6, bsz=100, num_updates=6290, lr=4.21433e-05, gnorm=0.661, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=7313
2023-03-14 21:34:16 - progress_bar.py[line:274] - INFO: epoch 001:   6309 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.5, ups=0.86, wpb=252.1, bsz=100, num_updates=6300, lr=4.21266e-05, gnorm=0.655, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=7324
2023-03-14 21:34:21 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-03-14 21:34:21 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-03-14 21:34:23 - train.py[line:549] - INFO: 0 / 7052
2023-03-14 21:34:23 - train.py[line:551] - INFO: load:1.31 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-03-14 21:37:04 - train.py[line:549] - INFO: 200 / 7052
2023-03-14 21:37:04 - train.py[line:551] - INFO: load:1.33 valid_run:161.16 task_valid:150.22 collect_output:9.82
2023-03-14 21:39:37 - train.py[line:549] - INFO: 400 / 7052
2023-03-14 21:39:37 - train.py[line:551] - INFO: load:1.36 valid_run:314.84 task_valid:297.60 collect_output:15.03
2023-03-14 21:42:13 - train.py[line:549] - INFO: 600 / 7052
2023-03-14 21:42:13 - train.py[line:551] - INFO: load:1.38 valid_run:470.69 task_valid:442.88 collect_output:24.54
2023-03-14 21:44:52 - train.py[line:549] - INFO: 800 / 7052
2023-03-14 21:44:52 - train.py[line:551] - INFO: load:1.40 valid_run:629.68 task_valid:588.44 collect_output:36.92
2023-03-14 21:47:31 - train.py[line:549] - INFO: 1000 / 7052
2023-03-14 21:47:31 - train.py[line:551] - INFO: load:1.43 valid_run:787.99 task_valid:736.09 collect_output:46.50
2023-03-14 21:50:07 - train.py[line:549] - INFO: 1200 / 7052
2023-03-14 21:50:07 - train.py[line:551] - INFO: load:1.45 valid_run:944.42 task_valid:884.22 collect_output:53.75
2023-03-14 21:52:45 - train.py[line:549] - INFO: 1400 / 7052
2023-03-14 21:52:45 - train.py[line:551] - INFO: load:1.48 valid_run:1102.00 task_valid:1034.75 collect_output:59.72
2023-03-14 21:55:20 - train.py[line:549] - INFO: 1600 / 7052
2023-03-14 21:55:20 - train.py[line:551] - INFO: load:1.50 valid_run:1257.17 task_valid:1183.72 collect_output:64.86
2023-03-14 21:57:57 - train.py[line:549] - INFO: 1800 / 7052
2023-03-14 21:57:57 - train.py[line:551] - INFO: load:1.52 valid_run:1414.19 task_valid:1334.61 collect_output:69.93
2023-03-14 22:00:33 - train.py[line:549] - INFO: 2000 / 7052
2023-03-14 22:00:33 - train.py[line:551] - INFO: load:1.55 valid_run:1569.72 task_valid:1483.95 collect_output:75.04
2023-03-14 22:03:08 - train.py[line:549] - INFO: 2200 / 7052
2023-03-14 22:03:08 - train.py[line:551] - INFO: load:1.57 valid_run:1725.04 task_valid:1633.44 collect_output:79.82
2023-03-14 22:05:43 - train.py[line:549] - INFO: 2400 / 7052
2023-03-14 22:05:43 - train.py[line:551] - INFO: load:1.59 valid_run:1879.70 task_valid:1779.55 collect_output:87.30
2023-03-14 22:08:18 - train.py[line:549] - INFO: 2600 / 7052
2023-03-14 22:08:18 - train.py[line:551] - INFO: load:1.62 valid_run:2034.48 task_valid:1927.02 collect_output:93.53
2023-03-14 22:10:54 - train.py[line:549] - INFO: 2800 / 7052
2023-03-14 22:10:54 - train.py[line:551] - INFO: load:1.64 valid_run:2190.59 task_valid:2073.85 collect_output:101.78
2023-03-14 22:13:29 - train.py[line:549] - INFO: 3000 / 7052
2023-03-14 22:13:29 - train.py[line:551] - INFO: load:1.67 valid_run:2345.38 task_valid:2220.60 collect_output:108.73
2023-03-14 22:16:05 - train.py[line:549] - INFO: 3200 / 7052
2023-03-14 22:16:05 - train.py[line:551] - INFO: load:1.69 valid_run:2501.47 task_valid:2369.95 collect_output:114.43
2023-03-14 22:18:40 - train.py[line:549] - INFO: 3400 / 7052
2023-03-14 22:18:40 - train.py[line:551] - INFO: load:1.71 valid_run:2656.51 task_valid:2517.03 collect_output:121.35
2023-03-14 22:21:15 - train.py[line:549] - INFO: 3600 / 7052
2023-03-14 22:21:15 - train.py[line:551] - INFO: load:1.74 valid_run:2811.01 task_valid:2664.00 collect_output:127.82
2023-03-14 22:23:52 - train.py[line:549] - INFO: 3800 / 7052
2023-03-14 22:23:52 - train.py[line:551] - INFO: load:1.76 valid_run:2968.26 task_valid:2814.72 collect_output:133.30
2023-03-14 22:26:24 - train.py[line:549] - INFO: 4000 / 7052
2023-03-14 22:26:24 - train.py[line:551] - INFO: load:1.79 valid_run:3120.74 task_valid:2959.89 collect_output:139.53
2023-03-14 22:29:00 - train.py[line:549] - INFO: 4200 / 7052
2023-03-14 22:29:00 - train.py[line:551] - INFO: load:1.81 valid_run:3275.94 task_valid:3105.15 collect_output:148.39
2023-03-14 22:31:34 - train.py[line:549] - INFO: 4400 / 7052
2023-03-14 22:31:34 - train.py[line:551] - INFO: load:1.83 valid_run:3430.26 task_valid:3255.20 collect_output:151.56
2023-03-14 22:34:10 - train.py[line:549] - INFO: 4600 / 7052
2023-03-14 22:34:10 - train.py[line:551] - INFO: load:1.86 valid_run:3585.96 task_valid:3401.50 collect_output:159.90
2023-03-14 22:36:45 - train.py[line:549] - INFO: 4800 / 7052
2023-03-14 22:36:45 - train.py[line:551] - INFO: load:1.88 valid_run:3741.31 task_valid:3546.24 collect_output:169.42
2023-03-14 22:39:21 - train.py[line:549] - INFO: 5000 / 7052
2023-03-14 22:39:21 - train.py[line:551] - INFO: load:1.90 valid_run:3896.84 task_valid:3693.32 collect_output:176.80
2023-03-14 22:41:58 - train.py[line:549] - INFO: 5200 / 7052
2023-03-14 22:41:58 - train.py[line:551] - INFO: load:1.92 valid_run:4053.74 task_valid:3844.86 collect_output:181.10
2023-03-14 22:44:33 - train.py[line:549] - INFO: 5400 / 7052
2023-03-14 22:44:33 - train.py[line:551] - INFO: load:1.95 valid_run:4209.17 task_valid:3989.80 collect_output:190.49
2023-03-14 22:47:09 - train.py[line:549] - INFO: 5600 / 7052
2023-03-14 22:47:09 - train.py[line:551] - INFO: load:1.97 valid_run:4364.42 task_valid:4134.84 collect_output:199.64
2023-03-14 22:49:44 - train.py[line:549] - INFO: 5800 / 7052
2023-03-14 22:49:44 - train.py[line:551] - INFO: load:2.00 valid_run:4520.24 task_valid:4281.21 collect_output:208.01
2023-03-14 22:52:20 - train.py[line:549] - INFO: 6000 / 7052
2023-03-14 22:52:20 - train.py[line:551] - INFO: load:2.02 valid_run:4675.76 task_valid:4428.87 collect_output:214.75
2023-03-14 22:54:57 - train.py[line:549] - INFO: 6200 / 7052
2023-03-14 22:54:57 - train.py[line:551] - INFO: load:2.05 valid_run:4832.50 task_valid:4575.93 collect_output:223.39
2023-03-14 22:57:34 - train.py[line:549] - INFO: 6400 / 7052
2023-03-14 22:57:34 - train.py[line:551] - INFO: load:2.07 valid_run:4989.19 task_valid:4721.20 collect_output:233.72
2023-03-14 23:00:11 - train.py[line:549] - INFO: 6600 / 7052
2023-03-14 23:00:11 - train.py[line:551] - INFO: load:2.09 valid_run:5146.08 task_valid:4872.26 collect_output:238.46
2023-03-14 23:02:47 - train.py[line:549] - INFO: 6800 / 7052
2023-03-14 23:02:47 - train.py[line:551] - INFO: load:2.12 valid_run:5302.77 task_valid:5021.40 collect_output:244.95
2023-03-14 23:05:22 - train.py[line:549] - INFO: 7000 / 7052
2023-03-14 23:05:22 - train.py[line:551] - INFO: load:2.14 valid_run:5457.74 task_valid:5171.53 collect_output:248.71

====================================================================================================
SGG eval:     R @ 50: 0.6089;     R @ 100: 0.6362;     R @ 500: 0.6526;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1422;    mR @ 100: 0.1625;    mR @ 500: 0.1889;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0576) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3355) (attached to:0.0000) (behind:0.3017) (belonging to:0.0000) (between:0.0000) (carrying:0.6845) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7293) (holding:0.2636) (in:0.3117) (in front of:0.1176) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4010) (of:0.4101) (on:0.9187) (on back of:0.0000) (over:0.0556) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.4167) (says:0.0000) (sitting on:0.2131) (standing on:0.0000) (to:0.0000) (under:0.2130) (using:1.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.3333) (wearing:0.9846) (wears:0.0000) (with:0.0880) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6089;     R @ 100: 0.6362;     R @ 500: 0.6526;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1422;    mR @ 100: 0.1625;    mR @ 500: 0.1889;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0576) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3355) (attached to:0.0000) (behind:0.3017) (belonging to:0.0000) (between:0.0000) (carrying:0.6845) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7293) (holding:0.2636) (in:0.3117) (in front of:0.1176) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4010) (of:0.4101) (on:0.9187) (on back of:0.0000) (over:0.0556) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.4167) (says:0.0000) (sitting on:0.2131) (standing on:0.0000) (to:0.0000) (under:0.2130) (using:1.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.3333) (wearing:0.9846) (wears:0.0000) (with:0.0880) 
--------------------------------------------------------
====================================================================================================

2023-03-14 23:06:24 - train.py[line:487] - INFO: 0.6361565199279681

====================================================================================================
SGG eval:     R @ 50: 0.6089;     R @ 100: 0.6362;     R @ 500: 0.6526;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1422;    mR @ 100: 0.1625;    mR @ 500: 0.1889;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0576) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3355) (attached to:0.0000) (behind:0.3017) (belonging to:0.0000) (between:0.0000) (carrying:0.6845) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7293) (holding:0.2636) (in:0.3117) (in front of:0.1176) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4010) (of:0.4101) (on:0.9187) (on back of:0.0000) (over:0.0556) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.4167) (says:0.0000) (sitting on:0.2131) (standing on:0.0000) (to:0.0000) (under:0.2130) (using:1.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.3333) (wearing:0.9846) (wears:0.0000) (with:0.0880) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6089;     R @ 100: 0.6362;     R @ 500: 0.6526;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1422;    mR @ 100: 0.1625;    mR @ 500: 0.1889;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0576) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3355) (attached to:0.0000) (behind:0.3017) (belonging to:0.0000) (between:0.0000) (carrying:0.6845) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7293) (holding:0.2636) (in:0.3117) (in front of:0.1176) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4010) (of:0.4101) (on:0.9187) (on back of:0.0000) (over:0.0556) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.4167) (says:0.0000) (sitting on:0.2131) (standing on:0.0000) (to:0.0000) (under:0.2130) (using:1.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.3333) (wearing:0.9846) (wears:0.0000) (with:0.0880) 
--------------------------------------------------------
====================================================================================================

2023-03-14 23:06:25 - train.py[line:578] - INFO: logits:torch.Size([282060, 51]) sample_ids:torch.Size([282060])
2023-03-14 23:06:25 - progress_bar.py[line:282] - INFO: epoch 001 | valid on 'valid' subset | loss 0.313 | loss_v1 0 | loss_v2 0 | nll_loss 0.121 | ntokens 119.044 | nsentences 39.997 | sample_size 119.044 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.636157 | ppl 1.09 | vqa_score 0.4967 | wps 152.1 | wpb 119 | bsz 40 | num_updates 6304
2023-03-14 23:06:25 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 1 @ 6304 updates
2023-03-14 23:06:25 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint1.pt

====================================================================================================
SGG eval:     R @ 50: 0.6089;     R @ 100: 0.6362;     R @ 500: 0.6526;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.1422;    mR @ 100: 0.1625;    mR @ 500: 0.1889;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0576) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3355) (attached to:0.0000) (behind:0.3017) (belonging to:0.0000) (between:0.0000) (carrying:0.6845) (covered in:0.0000) (covering:0.0000) (eating:0.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0385) (has:0.7293) (holding:0.2636) (in:0.3117) (in front of:0.1176) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.4010) (of:0.4101) (on:0.9187) (on back of:0.0000) (over:0.0556) (painted on:0.0000) (parked on:0.0000) (part of:0.0000) (playing:0.0000) (riding:0.4167) (says:0.0000) (sitting on:0.2131) (standing on:0.0000) (to:0.0000) (under:0.2130) (using:1.0000) (walking in:0.0000) (walking on:0.0000) (watching:0.3333) (wearing:0.9846) (wears:0.0000) (with:0.0880) 
--------------------------------------------------------
====================================================================================================

file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv slice_id 1 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv slice_id 3 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv slice_id 4 row count 126256 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv slice_id 2 row count 126257 total row count 631284
2023-03-14 23:06:34 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint1.pt
2023-03-14 23:06:40 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint1.pt (epoch 1 @ 6304 updates, score 0.6361565199279681) (writing took 15.841196255758405 seconds)
2023-03-14 23:06:40 - train.py[line:339] - INFO: end of epoch 1 (average epoch stats below)
2023-03-14 23:06:40 - progress_bar.py[line:282] - INFO: epoch 001 | loss 0.625 | loss_v1 0 | loss_v2 0 | nll_loss 0.419 | ntokens 253.355 | nsentences 99.997 | sample_size 253.355 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.34 | wps 124.3 | ups 0.49 | wpb 253.4 | bsz 100 | num_updates 6304 | lr 4.21199e-05 | gnorm 0.982 | clip 25.9 | loss_scale 512 | train_wall 7273 | gb_free 14.3 | ema_decay 0.9999 | wall 12868
2023-03-14 23:06:40 - trainer.py[line:694] - INFO: loading train data for epoch 2
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv slice_id 0 row count 126257 total row count 631284
2023-03-14 23:06:41 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-03-14 23:06:41 - trainer.py[line:758] - INFO: begin training epoch 2
2023-03-14 23:06:41 - train.py[line:312] - INFO: Start iterating over samples
2023-03-14 23:06:50 - progress_bar.py[line:274] - INFO: epoch 002:      6 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=250.7, nsentences=98.4, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=0.5, ups=0, wpb=250.7, bsz=98.4, num_updates=6310, lr=4.21099e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=12878
2023-03-14 23:07:02 - progress_bar.py[line:274] - INFO: epoch 002:     16 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.7, ups=0.86, wpb=252.3, bsz=100, num_updates=6320, lr=4.20932e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=12890
2023-03-14 23:07:14 - progress_bar.py[line:274] - INFO: epoch 002:     26 / 6313 loss=0.602, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=249.8, nsentences=100, sample_size=249.8, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=213.6, ups=0.86, wpb=249.8, bsz=100, num_updates=6330, lr=4.20766e-05, gnorm=0.692, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=12901
2023-03-14 23:07:25 - progress_bar.py[line:274] - INFO: epoch 002:     36 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.8, ups=0.86, wpb=255.6, bsz=100, num_updates=6340, lr=4.20599e-05, gnorm=0.687, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=12913
2023-03-14 23:07:37 - progress_bar.py[line:274] - INFO: epoch 002:     46 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.7, ups=0.88, wpb=251.5, bsz=100, num_updates=6350, lr=4.20432e-05, gnorm=0.7, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=12924
2023-03-14 23:07:48 - progress_bar.py[line:274] - INFO: epoch 002:     56 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221, ups=0.86, wpb=255.6, bsz=100, num_updates=6360, lr=4.20265e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=12936
2023-03-14 23:08:00 - progress_bar.py[line:274] - INFO: epoch 002:     66 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215, ups=0.86, wpb=250.4, bsz=100, num_updates=6370, lr=4.20099e-05, gnorm=0.788, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=12948
2023-03-14 23:08:11 - progress_bar.py[line:274] - INFO: epoch 002:     76 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.5, ups=0.87, wpb=253.8, bsz=100, num_updates=6380, lr=4.19932e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=12959
2023-03-14 23:08:23 - progress_bar.py[line:274] - INFO: epoch 002:     86 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217, ups=0.86, wpb=253.1, bsz=100, num_updates=6390, lr=4.19765e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=12971
2023-03-14 23:08:35 - progress_bar.py[line:274] - INFO: epoch 002:     96 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.7, ups=0.86, wpb=253.3, bsz=100, num_updates=6400, lr=4.19598e-05, gnorm=0.659, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=12983
2023-03-14 23:08:46 - progress_bar.py[line:274] - INFO: epoch 002:    106 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.2, ups=0.86, wpb=251.9, bsz=100, num_updates=6410, lr=4.19432e-05, gnorm=0.723, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=12994
2023-03-14 23:08:58 - progress_bar.py[line:274] - INFO: epoch 002:    116 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.4, ups=0.86, wpb=253.5, bsz=100, num_updates=6420, lr=4.19265e-05, gnorm=0.661, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13006
2023-03-14 23:09:09 - progress_bar.py[line:274] - INFO: epoch 002:    126 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.7, ups=0.86, wpb=254.6, bsz=100, num_updates=6430, lr=4.19098e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13017
2023-03-14 23:09:21 - progress_bar.py[line:274] - INFO: epoch 002:    136 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.8, ups=0.86, wpb=254.4, bsz=100, num_updates=6440, lr=4.18932e-05, gnorm=0.735, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13029
2023-03-14 23:09:33 - progress_bar.py[line:274] - INFO: epoch 002:    146 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.3, ups=0.86, wpb=253.8, bsz=100, num_updates=6450, lr=4.18765e-05, gnorm=0.694, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13041
2023-03-14 23:09:44 - progress_bar.py[line:274] - INFO: epoch 002:    156 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=224, ups=0.87, wpb=256.3, bsz=100, num_updates=6460, lr=4.18598e-05, gnorm=0.678, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13052
2023-03-14 23:09:56 - progress_bar.py[line:274] - INFO: epoch 002:    166 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.8, ups=0.86, wpb=254.1, bsz=100, num_updates=6470, lr=4.18431e-05, gnorm=0.704, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13064
2023-03-14 23:10:07 - progress_bar.py[line:274] - INFO: epoch 002:    176 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.1, ups=0.86, wpb=251.9, bsz=100, num_updates=6480, lr=4.18265e-05, gnorm=0.743, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=13075
2023-03-14 23:10:19 - progress_bar.py[line:274] - INFO: epoch 002:    186 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.5, ups=0.86, wpb=255.1, bsz=100, num_updates=6490, lr=4.18098e-05, gnorm=0.693, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=13087
2023-03-14 23:10:31 - progress_bar.py[line:274] - INFO: epoch 002:    196 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.7, ups=0.86, wpb=251.2, bsz=100, num_updates=6500, lr=4.17931e-05, gnorm=0.736, clip=10, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=13099
2023-03-14 23:10:42 - progress_bar.py[line:274] - INFO: epoch 002:    206 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219, ups=0.86, wpb=254.7, bsz=100, num_updates=6510, lr=4.17764e-05, gnorm=0.695, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13110
2023-03-14 23:10:50 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 23:10:55 - progress_bar.py[line:274] - INFO: epoch 002:    217 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=202, ups=0.79, wpb=254.7, bsz=100, num_updates=6520, lr=4.17598e-05, gnorm=0.783, clip=10, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=13123
2023-03-14 23:11:07 - progress_bar.py[line:274] - INFO: epoch 002:    227 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.2, ups=0.87, wpb=253.3, bsz=100, num_updates=6530, lr=4.17431e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13134
2023-03-14 23:11:18 - progress_bar.py[line:274] - INFO: epoch 002:    237 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.7, ups=0.87, wpb=254.1, bsz=100, num_updates=6540, lr=4.17264e-05, gnorm=0.746, clip=10, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=13146
2023-03-14 23:11:30 - progress_bar.py[line:274] - INFO: epoch 002:    247 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.4, ups=0.86, wpb=252.9, bsz=100, num_updates=6550, lr=4.17097e-05, gnorm=0.723, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13158
2023-03-14 23:11:41 - progress_bar.py[line:274] - INFO: epoch 002:    257 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.3, ups=0.86, wpb=253.6, bsz=100, num_updates=6560, lr=4.16931e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13169
2023-03-14 23:11:53 - progress_bar.py[line:274] - INFO: epoch 002:    267 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.6, ups=0.87, wpb=252.8, bsz=100, num_updates=6570, lr=4.16764e-05, gnorm=0.698, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13181
2023-03-14 23:12:04 - progress_bar.py[line:274] - INFO: epoch 002:    277 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.3, ups=0.86, wpb=252.3, bsz=100, num_updates=6580, lr=4.16597e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13192
2023-03-14 23:12:16 - progress_bar.py[line:274] - INFO: epoch 002:    287 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.5, ups=0.86, wpb=253, bsz=100, num_updates=6590, lr=4.1643e-05, gnorm=0.713, clip=10, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=13204
2023-03-14 23:12:28 - progress_bar.py[line:274] - INFO: epoch 002:    297 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.6, ups=0.85, wpb=253.5, bsz=100, num_updates=6600, lr=4.16264e-05, gnorm=0.771, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13216
2023-03-14 23:12:39 - progress_bar.py[line:274] - INFO: epoch 002:    307 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=214.7, ups=0.85, wpb=251.2, bsz=100, num_updates=6610, lr=4.16097e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13227
2023-03-14 23:12:51 - progress_bar.py[line:274] - INFO: epoch 002:    317 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.1, ups=0.86, wpb=252.7, bsz=100, num_updates=6620, lr=4.1593e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13239
2023-03-14 23:13:03 - progress_bar.py[line:274] - INFO: epoch 002:    327 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.7, ups=0.86, wpb=254.8, bsz=100, num_updates=6630, lr=4.15763e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=13251
2023-03-14 23:13:14 - progress_bar.py[line:274] - INFO: epoch 002:    337 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=220.2, ups=0.86, wpb=254.8, bsz=100, num_updates=6640, lr=4.15597e-05, gnorm=0.845, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13262
2023-03-14 23:13:26 - progress_bar.py[line:274] - INFO: epoch 002:    347 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.7, ups=0.86, wpb=252.5, bsz=100, num_updates=6650, lr=4.1543e-05, gnorm=0.75, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=13274
2023-03-14 23:13:38 - progress_bar.py[line:274] - INFO: epoch 002:    357 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=214.1, ups=0.85, wpb=252.7, bsz=100, num_updates=6660, lr=4.15263e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13286
2023-03-14 23:13:49 - progress_bar.py[line:274] - INFO: epoch 002:    367 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.1, ups=0.87, wpb=252.1, bsz=100, num_updates=6670, lr=4.15097e-05, gnorm=0.753, clip=10, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=13297
2023-03-14 23:14:01 - progress_bar.py[line:274] - INFO: epoch 002:    377 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.1, ups=0.86, wpb=253.9, bsz=100, num_updates=6680, lr=4.1493e-05, gnorm=0.745, clip=0, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=13309
2023-03-14 23:14:12 - progress_bar.py[line:274] - INFO: epoch 002:    387 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.6, ups=0.87, wpb=252.3, bsz=100, num_updates=6690, lr=4.14763e-05, gnorm=0.691, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=13320
2023-03-14 23:14:24 - progress_bar.py[line:274] - INFO: epoch 002:    397 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.1, ups=0.86, wpb=254.7, bsz=100, num_updates=6700, lr=4.14596e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13332
2023-03-14 23:14:35 - progress_bar.py[line:274] - INFO: epoch 002:    407 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.5, ups=0.87, wpb=252.7, bsz=100, num_updates=6710, lr=4.1443e-05, gnorm=0.628, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=13343
2023-03-14 23:14:47 - progress_bar.py[line:274] - INFO: epoch 002:    417 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.4, ups=0.87, wpb=254, bsz=100, num_updates=6720, lr=4.14263e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=13355
2023-03-14 23:14:59 - progress_bar.py[line:274] - INFO: epoch 002:    427 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=212.8, ups=0.84, wpb=252.9, bsz=100, num_updates=6730, lr=4.14096e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=13367
2023-03-14 23:15:11 - progress_bar.py[line:274] - INFO: epoch 002:    437 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.3, ups=0.86, wpb=252.8, bsz=100, num_updates=6740, lr=4.13929e-05, gnorm=0.815, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13378
2023-03-14 23:15:22 - progress_bar.py[line:274] - INFO: epoch 002:    447 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.7, ups=0.86, wpb=252.4, bsz=100, num_updates=6750, lr=4.13763e-05, gnorm=0.749, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=13390
2023-03-14 23:15:34 - progress_bar.py[line:274] - INFO: epoch 002:    457 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.1, ups=0.86, wpb=250.8, bsz=100, num_updates=6760, lr=4.13596e-05, gnorm=0.715, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13402
2023-03-14 23:15:45 - progress_bar.py[line:274] - INFO: epoch 002:    467 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.7, ups=0.86, wpb=254.4, bsz=100, num_updates=6770, lr=4.13429e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=13413
2023-03-14 23:15:57 - progress_bar.py[line:274] - INFO: epoch 002:    477 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=249.5, nsentences=100, sample_size=249.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=214.8, ups=0.86, wpb=249.5, bsz=100, num_updates=6780, lr=4.13262e-05, gnorm=0.734, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=13425
2023-03-14 23:16:09 - progress_bar.py[line:274] - INFO: epoch 002:    487 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.2, ups=0.87, wpb=254.1, bsz=100, num_updates=6790, lr=4.13096e-05, gnorm=0.877, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=13436
2023-03-14 23:16:20 - progress_bar.py[line:274] - INFO: epoch 002:    497 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=223.2, ups=0.88, wpb=252.6, bsz=100, num_updates=6800, lr=4.12929e-05, gnorm=0.879, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13448
2023-03-14 23:16:31 - progress_bar.py[line:274] - INFO: epoch 002:    507 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.2, ups=0.87, wpb=251.8, bsz=100, num_updates=6810, lr=4.12762e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13459
2023-03-14 23:16:43 - progress_bar.py[line:274] - INFO: epoch 002:    517 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.7, ups=0.86, wpb=251.4, bsz=100, num_updates=6820, lr=4.12595e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13471
2023-03-14 23:16:55 - progress_bar.py[line:274] - INFO: epoch 002:    527 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.6, ups=0.86, wpb=255.1, bsz=100, num_updates=6830, lr=4.12429e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=13482
2023-03-14 23:17:06 - progress_bar.py[line:274] - INFO: epoch 002:    537 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.6, ups=0.86, wpb=252.2, bsz=100, num_updates=6840, lr=4.12262e-05, gnorm=0.711, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13494
2023-03-14 23:17:18 - progress_bar.py[line:274] - INFO: epoch 002:    547 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.1, ups=0.85, wpb=254.5, bsz=100, num_updates=6850, lr=4.12095e-05, gnorm=0.755, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13506
2023-03-14 23:17:30 - progress_bar.py[line:274] - INFO: epoch 002:    557 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.7, ups=0.86, wpb=253.8, bsz=100, num_updates=6860, lr=4.11929e-05, gnorm=0.639, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13517
2023-03-14 23:17:41 - progress_bar.py[line:274] - INFO: epoch 002:    567 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.4, ups=0.86, wpb=251.6, bsz=100, num_updates=6870, lr=4.11762e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13529
2023-03-14 23:17:53 - progress_bar.py[line:274] - INFO: epoch 002:    577 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219, ups=0.86, wpb=253.6, bsz=100, num_updates=6880, lr=4.11595e-05, gnorm=0.637, clip=0, loss_scale=512, train_wall=12, gb_free=11.3, ema_decay=0.9999, wall=13541
2023-03-14 23:18:04 - progress_bar.py[line:274] - INFO: epoch 002:    587 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.4, ups=0.86, wpb=256.3, bsz=100, num_updates=6890, lr=4.11428e-05, gnorm=0.665, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=13552
2023-03-14 23:18:16 - progress_bar.py[line:274] - INFO: epoch 002:    597 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.9, ups=0.86, wpb=251.2, bsz=100, num_updates=6900, lr=4.11262e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13564
2023-03-14 23:18:28 - progress_bar.py[line:274] - INFO: epoch 002:    607 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=249.7, nsentences=100, sample_size=249.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.9, ups=0.86, wpb=249.7, bsz=100, num_updates=6910, lr=4.11095e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13576
2023-03-14 23:18:39 - progress_bar.py[line:274] - INFO: epoch 002:    617 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.1, ups=0.86, wpb=253, bsz=100, num_updates=6920, lr=4.10928e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=13587
2023-03-14 23:18:51 - progress_bar.py[line:274] - INFO: epoch 002:    627 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.8, ups=0.86, wpb=254.7, bsz=100, num_updates=6930, lr=4.10761e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=13599
2023-03-14 23:19:03 - progress_bar.py[line:274] - INFO: epoch 002:    637 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.6, ups=0.86, wpb=254.3, bsz=100, num_updates=6940, lr=4.10595e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13610
2023-03-14 23:19:14 - progress_bar.py[line:274] - INFO: epoch 002:    647 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.6, ups=0.86, wpb=251.5, bsz=100, num_updates=6950, lr=4.10428e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=13622
2023-03-14 23:19:26 - progress_bar.py[line:274] - INFO: epoch 002:    657 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.1, ups=0.86, wpb=252.5, bsz=100, num_updates=6960, lr=4.10261e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=13634
2023-03-14 23:19:38 - progress_bar.py[line:274] - INFO: epoch 002:    667 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.8, ups=0.85, wpb=253.9, bsz=100, num_updates=6970, lr=4.10094e-05, gnorm=0.765, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13646
2023-03-14 23:19:49 - progress_bar.py[line:274] - INFO: epoch 002:    677 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.9, ups=0.87, wpb=251.7, bsz=100, num_updates=6980, lr=4.09928e-05, gnorm=0.687, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13657
2023-03-14 23:20:01 - progress_bar.py[line:274] - INFO: epoch 002:    687 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.3, ups=0.86, wpb=252.2, bsz=100, num_updates=6990, lr=4.09761e-05, gnorm=0.679, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=13669
2023-03-14 23:20:12 - progress_bar.py[line:274] - INFO: epoch 002:    697 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.7, ups=0.86, wpb=254.4, bsz=100, num_updates=7000, lr=4.09594e-05, gnorm=0.72, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13680
2023-03-14 23:20:24 - progress_bar.py[line:274] - INFO: epoch 002:    707 / 6313 loss=0.59, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=214.7, ups=0.86, wpb=250.2, bsz=100, num_updates=7010, lr=4.09427e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=13692
2023-03-14 23:20:36 - progress_bar.py[line:274] - INFO: epoch 002:    717 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.2, ups=0.85, wpb=251.8, bsz=100, num_updates=7020, lr=4.09261e-05, gnorm=0.782, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=13704
2023-03-14 23:20:47 - progress_bar.py[line:274] - INFO: epoch 002:    727 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.8, ups=0.87, wpb=254.5, bsz=100, num_updates=7030, lr=4.09094e-05, gnorm=0.756, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=13715
2023-03-14 23:20:49 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 23:21:00 - progress_bar.py[line:274] - INFO: epoch 002:    738 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=200.9, ups=0.79, wpb=254.2, bsz=100, num_updates=7040, lr=4.08927e-05, gnorm=0.772, clip=10, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=13728
2023-03-14 23:21:12 - progress_bar.py[line:274] - INFO: epoch 002:    748 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.4, ups=0.86, wpb=254.6, bsz=100, num_updates=7050, lr=4.0876e-05, gnorm=0.722, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13739
2023-03-14 23:21:23 - progress_bar.py[line:274] - INFO: epoch 002:    758 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.2, ups=0.86, wpb=255.2, bsz=100, num_updates=7060, lr=4.08594e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=13751
2023-03-14 23:21:35 - progress_bar.py[line:274] - INFO: epoch 002:    768 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.9, ups=0.86, wpb=252.1, bsz=100, num_updates=7070, lr=4.08427e-05, gnorm=0.746, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=13763
2023-03-14 23:21:46 - progress_bar.py[line:274] - INFO: epoch 002:    778 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.3, ups=0.86, wpb=252, bsz=100, num_updates=7080, lr=4.0826e-05, gnorm=0.765, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=13774
2023-03-14 23:21:58 - progress_bar.py[line:274] - INFO: epoch 002:    788 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=213.9, ups=0.84, wpb=254.5, bsz=100, num_updates=7090, lr=4.08094e-05, gnorm=0.697, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=13786
2023-03-14 23:22:10 - progress_bar.py[line:274] - INFO: epoch 002:    798 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.1, ups=0.86, wpb=252.5, bsz=100, num_updates=7100, lr=4.07927e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13798
2023-03-14 23:22:22 - progress_bar.py[line:274] - INFO: epoch 002:    808 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.8, ups=0.86, wpb=252.2, bsz=100, num_updates=7110, lr=4.0776e-05, gnorm=0.769, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13810
2023-03-14 23:22:33 - progress_bar.py[line:274] - INFO: epoch 002:    818 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.9, ups=0.86, wpb=253.4, bsz=100, num_updates=7120, lr=4.07593e-05, gnorm=0.767, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13821
2023-03-14 23:22:45 - progress_bar.py[line:274] - INFO: epoch 002:    828 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.3, ups=0.86, wpb=253.3, bsz=100, num_updates=7130, lr=4.07427e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13833
2023-03-14 23:22:57 - progress_bar.py[line:274] - INFO: epoch 002:    838 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.1, ups=0.87, wpb=253.2, bsz=100, num_updates=7140, lr=4.0726e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13844
2023-03-14 23:23:08 - progress_bar.py[line:274] - INFO: epoch 002:    848 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.1, ups=0.86, wpb=256, bsz=100, num_updates=7150, lr=4.07093e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13856
2023-03-14 23:23:20 - progress_bar.py[line:274] - INFO: epoch 002:    858 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218, ups=0.86, wpb=253.8, bsz=100, num_updates=7160, lr=4.06926e-05, gnorm=0.654, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13868
2023-03-14 23:23:31 - progress_bar.py[line:274] - INFO: epoch 002:    868 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.3, ups=0.86, wpb=255.5, bsz=100, num_updates=7170, lr=4.0676e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13879
2023-03-14 23:23:43 - progress_bar.py[line:274] - INFO: epoch 002:    878 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.3, ups=0.86, wpb=255.7, bsz=100, num_updates=7180, lr=4.06593e-05, gnorm=0.68, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13891
2023-03-14 23:23:55 - progress_bar.py[line:274] - INFO: epoch 002:    888 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.9, ups=0.86, wpb=252.3, bsz=100, num_updates=7190, lr=4.06426e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13903
2023-03-14 23:24:06 - progress_bar.py[line:274] - INFO: epoch 002:    898 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.388, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=220.2, ups=0.87, wpb=252, bsz=100, num_updates=7200, lr=4.06259e-05, gnorm=0.753, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=13914
2023-03-14 23:24:18 - progress_bar.py[line:274] - INFO: epoch 002:    908 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.3, ups=0.86, wpb=252.4, bsz=100, num_updates=7210, lr=4.06093e-05, gnorm=0.697, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=13926
2023-03-14 23:24:29 - progress_bar.py[line:274] - INFO: epoch 002:    918 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218, ups=0.86, wpb=253.4, bsz=100, num_updates=7220, lr=4.05926e-05, gnorm=0.709, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13937
2023-03-14 23:24:41 - progress_bar.py[line:274] - INFO: epoch 002:    928 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.5, ups=0.86, wpb=255.2, bsz=100, num_updates=7230, lr=4.05759e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=13949
2023-03-14 23:24:53 - progress_bar.py[line:274] - INFO: epoch 002:    938 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.8, ups=0.87, wpb=250.7, bsz=100, num_updates=7240, lr=4.05592e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=13960
2023-03-14 23:25:04 - progress_bar.py[line:274] - INFO: epoch 002:    948 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218, ups=0.86, wpb=253.5, bsz=100, num_updates=7250, lr=4.05426e-05, gnorm=0.626, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=13972
2023-03-14 23:25:16 - progress_bar.py[line:274] - INFO: epoch 002:    958 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.4, ups=0.87, wpb=251.8, bsz=100, num_updates=7260, lr=4.05259e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=13984
2023-03-14 23:25:28 - progress_bar.py[line:274] - INFO: epoch 002:    968 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214.1, ups=0.85, wpb=252.9, bsz=100, num_updates=7270, lr=4.05092e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=13995
2023-03-14 23:25:39 - progress_bar.py[line:274] - INFO: epoch 002:    978 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.2, ups=0.86, wpb=254.6, bsz=100, num_updates=7280, lr=4.04925e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14007
2023-03-14 23:25:51 - progress_bar.py[line:274] - INFO: epoch 002:    988 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.2, ups=0.87, wpb=251, bsz=100, num_updates=7290, lr=4.04759e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14018
2023-03-14 23:26:02 - progress_bar.py[line:274] - INFO: epoch 002:    998 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.1, ups=0.86, wpb=252.2, bsz=100, num_updates=7300, lr=4.04592e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=14030
2023-03-14 23:26:14 - progress_bar.py[line:274] - INFO: epoch 002:   1008 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.3, ups=0.86, wpb=253.9, bsz=100, num_updates=7310, lr=4.04425e-05, gnorm=0.753, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14042
2023-03-14 23:26:25 - progress_bar.py[line:274] - INFO: epoch 002:   1018 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.1, ups=0.87, wpb=252.8, bsz=100, num_updates=7320, lr=4.04259e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14053
2023-03-14 23:26:37 - progress_bar.py[line:274] - INFO: epoch 002:   1028 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.3, ups=0.85, wpb=253.2, bsz=100, num_updates=7330, lr=4.04092e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=14065
2023-03-14 23:26:49 - progress_bar.py[line:274] - INFO: epoch 002:   1038 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217, ups=0.86, wpb=251.8, bsz=100, num_updates=7340, lr=4.03925e-05, gnorm=0.911, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14077
2023-03-14 23:27:01 - progress_bar.py[line:274] - INFO: epoch 002:   1048 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.392, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=213.5, ups=0.85, wpb=250.1, bsz=100, num_updates=7350, lr=4.03758e-05, gnorm=0.698, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14088
2023-03-14 23:27:12 - progress_bar.py[line:274] - INFO: epoch 002:   1058 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.2, ups=0.87, wpb=252.8, bsz=100, num_updates=7360, lr=4.03592e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14100
2023-03-14 23:27:24 - progress_bar.py[line:274] - INFO: epoch 002:   1068 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.2, ups=0.86, wpb=254.9, bsz=100, num_updates=7370, lr=4.03425e-05, gnorm=0.753, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=14112
2023-03-14 23:27:35 - progress_bar.py[line:274] - INFO: epoch 002:   1078 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=222, ups=0.87, wpb=254.7, bsz=100, num_updates=7380, lr=4.03258e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14123
2023-03-14 23:27:47 - progress_bar.py[line:274] - INFO: epoch 002:   1088 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.1, ups=0.85, wpb=256.1, bsz=100, num_updates=7390, lr=4.03091e-05, gnorm=0.623, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14135
2023-03-14 23:27:58 - progress_bar.py[line:274] - INFO: epoch 002:   1098 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.6, ups=0.87, wpb=253.4, bsz=100, num_updates=7400, lr=4.02925e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14146
2023-03-14 23:28:10 - progress_bar.py[line:274] - INFO: epoch 002:   1108 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.3, ups=0.87, wpb=250.3, bsz=100, num_updates=7410, lr=4.02758e-05, gnorm=0.772, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=14158
2023-03-14 23:28:22 - progress_bar.py[line:274] - INFO: epoch 002:   1118 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.2, ups=0.86, wpb=253.5, bsz=100, num_updates=7420, lr=4.02591e-05, gnorm=0.704, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=14169
2023-03-14 23:28:33 - progress_bar.py[line:274] - INFO: epoch 002:   1128 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.5, ups=0.86, wpb=254.2, bsz=100, num_updates=7430, lr=4.02424e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14181
2023-03-14 23:28:44 - progress_bar.py[line:274] - INFO: epoch 002:   1138 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223.8, ups=0.89, wpb=252.6, bsz=100, num_updates=7440, lr=4.02258e-05, gnorm=0.617, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=14192
2023-03-14 23:28:56 - progress_bar.py[line:274] - INFO: epoch 002:   1148 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.7, ups=0.85, wpb=252.3, bsz=100, num_updates=7450, lr=4.02091e-05, gnorm=0.729, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=14204
2023-03-14 23:29:08 - progress_bar.py[line:274] - INFO: epoch 002:   1158 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.4, ups=0.86, wpb=253.7, bsz=100, num_updates=7460, lr=4.01924e-05, gnorm=0.621, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=14216
2023-03-14 23:29:20 - progress_bar.py[line:274] - INFO: epoch 002:   1168 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.2, ups=0.85, wpb=254.7, bsz=100, num_updates=7470, lr=4.01757e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14227
2023-03-14 23:29:31 - progress_bar.py[line:274] - INFO: epoch 002:   1178 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=222.3, ups=0.87, wpb=255.1, bsz=100, num_updates=7480, lr=4.01591e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14239
2023-03-14 23:29:43 - progress_bar.py[line:274] - INFO: epoch 002:   1188 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=257.2, nsentences=100, sample_size=257.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.9, ups=0.86, wpb=257.2, bsz=100, num_updates=7490, lr=4.01424e-05, gnorm=0.621, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=14250
2023-03-14 23:29:54 - progress_bar.py[line:274] - INFO: epoch 002:   1198 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.8, ups=0.86, wpb=255.3, bsz=100, num_updates=7500, lr=4.01257e-05, gnorm=0.58, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14262
2023-03-14 23:30:06 - progress_bar.py[line:274] - INFO: epoch 002:   1208 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.3, ups=0.85, wpb=253.7, bsz=100, num_updates=7510, lr=4.0109e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=14274
2023-03-14 23:30:17 - progress_bar.py[line:274] - INFO: epoch 002:   1218 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223, ups=0.87, wpb=254.9, bsz=100, num_updates=7520, lr=4.00924e-05, gnorm=0.745, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=14285
2023-03-14 23:30:29 - progress_bar.py[line:274] - INFO: epoch 002:   1228 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221, ups=0.86, wpb=256.2, bsz=100, num_updates=7530, lr=4.00757e-05, gnorm=0.615, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14297
2023-03-14 23:30:41 - progress_bar.py[line:274] - INFO: epoch 002:   1238 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.7, ups=0.86, wpb=255.3, bsz=100, num_updates=7540, lr=4.0059e-05, gnorm=0.671, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=14309
2023-03-14 23:30:52 - progress_bar.py[line:274] - INFO: epoch 002:   1248 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.1, ups=0.86, wpb=253.3, bsz=100, num_updates=7550, lr=4.00424e-05, gnorm=0.625, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14320
2023-03-14 23:31:04 - progress_bar.py[line:274] - INFO: epoch 002:   1258 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.4, ups=0.86, wpb=253.5, bsz=100, num_updates=7560, lr=4.00257e-05, gnorm=0.747, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14332
2023-03-14 23:31:16 - progress_bar.py[line:274] - INFO: epoch 002:   1268 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.7, ups=0.85, wpb=253, bsz=100, num_updates=7570, lr=4.0009e-05, gnorm=0.646, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14343
2023-03-14 23:31:27 - progress_bar.py[line:274] - INFO: epoch 002:   1278 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.1, ups=0.86, wpb=251.3, bsz=100, num_updates=7580, lr=3.99923e-05, gnorm=0.667, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=14355
2023-03-14 23:31:39 - progress_bar.py[line:274] - INFO: epoch 002:   1288 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.9, ups=0.86, wpb=253.5, bsz=100, num_updates=7590, lr=3.99757e-05, gnorm=0.698, clip=0, loss_scale=1024, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=14367
2023-03-14 23:31:51 - progress_bar.py[line:274] - INFO: epoch 002:   1298 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.4, ups=0.86, wpb=254.7, bsz=100, num_updates=7600, lr=3.9959e-05, gnorm=0.634, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14378
2023-03-14 23:32:02 - progress_bar.py[line:274] - INFO: epoch 002:   1308 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219, ups=0.86, wpb=253.8, bsz=100, num_updates=7610, lr=3.99423e-05, gnorm=0.699, clip=10, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=14390
2023-03-14 23:32:14 - progress_bar.py[line:274] - INFO: epoch 002:   1318 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.6, ups=0.86, wpb=251.7, bsz=100, num_updates=7620, lr=3.99256e-05, gnorm=0.648, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=14402
2023-03-14 23:32:25 - progress_bar.py[line:274] - INFO: epoch 002:   1328 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.2, ups=0.85, wpb=251.9, bsz=100, num_updates=7630, lr=3.9909e-05, gnorm=0.63, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=14413
2023-03-14 23:32:37 - progress_bar.py[line:274] - INFO: epoch 002:   1338 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.5, ups=0.87, wpb=252.6, bsz=100, num_updates=7640, lr=3.98923e-05, gnorm=0.697, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14425
2023-03-14 23:32:49 - progress_bar.py[line:274] - INFO: epoch 002:   1348 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.9, ups=0.86, wpb=251, bsz=100, num_updates=7650, lr=3.98756e-05, gnorm=0.706, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14437
2023-03-14 23:33:00 - progress_bar.py[line:274] - INFO: epoch 002:   1358 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223.3, ups=0.88, wpb=253.7, bsz=100, num_updates=7660, lr=3.98589e-05, gnorm=0.731, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=14448
2023-03-14 23:33:12 - progress_bar.py[line:274] - INFO: epoch 002:   1368 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.9, ups=0.87, wpb=254, bsz=100, num_updates=7670, lr=3.98423e-05, gnorm=0.64, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14459
2023-03-14 23:33:23 - progress_bar.py[line:274] - INFO: epoch 002:   1378 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=224.6, ups=0.88, wpb=255.6, bsz=100, num_updates=7680, lr=3.98256e-05, gnorm=0.673, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=14471
2023-03-14 23:33:35 - progress_bar.py[line:274] - INFO: epoch 002:   1388 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.1, ups=0.85, wpb=252.7, bsz=100, num_updates=7690, lr=3.98089e-05, gnorm=0.739, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=14483
2023-03-14 23:33:46 - progress_bar.py[line:274] - INFO: epoch 002:   1398 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.7, ups=0.87, wpb=256, bsz=100, num_updates=7700, lr=3.97922e-05, gnorm=0.703, clip=0, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=14494
2023-03-14 23:33:58 - progress_bar.py[line:274] - INFO: epoch 002:   1408 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.3, ups=0.86, wpb=254.5, bsz=100, num_updates=7710, lr=3.97756e-05, gnorm=0.632, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14506
2023-03-14 23:34:10 - progress_bar.py[line:274] - INFO: epoch 002:   1418 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.1, ups=0.86, wpb=251.5, bsz=100, num_updates=7720, lr=3.97589e-05, gnorm=0.684, clip=0, loss_scale=1024, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=14517
2023-03-14 23:34:11 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 23:34:22 - progress_bar.py[line:274] - INFO: epoch 002:   1429 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=204.1, ups=0.8, wpb=255.4, bsz=100, num_updates=7730, lr=3.97422e-05, gnorm=0.628, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=14530
2023-03-14 23:34:34 - progress_bar.py[line:274] - INFO: epoch 002:   1439 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.381, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.8, ups=0.86, wpb=252.2, bsz=100, num_updates=7740, lr=3.97255e-05, gnorm=0.702, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14542
2023-03-14 23:34:45 - progress_bar.py[line:274] - INFO: epoch 002:   1449 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.2, ups=0.85, wpb=253.7, bsz=100, num_updates=7750, lr=3.97089e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=14553
2023-03-14 23:34:57 - progress_bar.py[line:274] - INFO: epoch 002:   1459 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.2, ups=0.86, wpb=254.3, bsz=100, num_updates=7760, lr=3.96922e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=14565
2023-03-14 23:35:09 - progress_bar.py[line:274] - INFO: epoch 002:   1469 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219, ups=0.87, wpb=252.8, bsz=100, num_updates=7770, lr=3.96755e-05, gnorm=0.748, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14577
2023-03-14 23:35:20 - progress_bar.py[line:274] - INFO: epoch 002:   1479 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.8, ups=0.86, wpb=253.2, bsz=100, num_updates=7780, lr=3.96589e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14588
2023-03-14 23:35:32 - progress_bar.py[line:274] - INFO: epoch 002:   1489 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.4, ups=0.86, wpb=251.8, bsz=100, num_updates=7790, lr=3.96422e-05, gnorm=0.732, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=14600
2023-03-14 23:35:44 - progress_bar.py[line:274] - INFO: epoch 002:   1499 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.3, ups=0.86, wpb=253.8, bsz=100, num_updates=7800, lr=3.96255e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14611
2023-03-14 23:35:55 - progress_bar.py[line:274] - INFO: epoch 002:   1509 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.6, ups=0.85, wpb=254.5, bsz=100, num_updates=7810, lr=3.96088e-05, gnorm=0.773, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=14623
2023-03-14 23:36:07 - progress_bar.py[line:274] - INFO: epoch 002:   1519 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.7, ups=0.86, wpb=252.3, bsz=100, num_updates=7820, lr=3.95922e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=14635
2023-03-14 23:36:19 - progress_bar.py[line:274] - INFO: epoch 002:   1529 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.5, ups=0.86, wpb=254.8, bsz=100, num_updates=7830, lr=3.95755e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=14646
2023-03-14 23:36:30 - progress_bar.py[line:274] - INFO: epoch 002:   1539 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.2, ups=0.87, wpb=254.5, bsz=100, num_updates=7840, lr=3.95588e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14658
2023-03-14 23:36:42 - progress_bar.py[line:274] - INFO: epoch 002:   1549 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.3, ups=0.86, wpb=250.3, bsz=100, num_updates=7850, lr=3.95421e-05, gnorm=0.69, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14669
2023-03-14 23:36:53 - progress_bar.py[line:274] - INFO: epoch 002:   1559 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.3, ups=0.86, wpb=253.1, bsz=100, num_updates=7860, lr=3.95255e-05, gnorm=0.78, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=14681
2023-03-14 23:37:05 - progress_bar.py[line:274] - INFO: epoch 002:   1569 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.3, ups=0.87, wpb=252.4, bsz=100, num_updates=7870, lr=3.95088e-05, gnorm=0.63, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=14693
2023-03-14 23:37:16 - progress_bar.py[line:274] - INFO: epoch 002:   1579 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.4, ups=0.86, wpb=254.6, bsz=100, num_updates=7880, lr=3.94921e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=14704
2023-03-14 23:37:28 - progress_bar.py[line:274] - INFO: epoch 002:   1589 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.5, ups=0.86, wpb=252.4, bsz=100, num_updates=7890, lr=3.94754e-05, gnorm=0.748, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=14716
2023-03-14 23:37:39 - progress_bar.py[line:274] - INFO: epoch 002:   1599 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221.3, ups=0.87, wpb=254.3, bsz=100, num_updates=7900, lr=3.94588e-05, gnorm=0.695, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=14727
2023-03-14 23:37:51 - progress_bar.py[line:274] - INFO: epoch 002:   1609 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.5, ups=0.86, wpb=255.1, bsz=100, num_updates=7910, lr=3.94421e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14739
2023-03-14 23:38:03 - progress_bar.py[line:274] - INFO: epoch 002:   1619 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.4, ups=0.86, wpb=251.8, bsz=100, num_updates=7920, lr=3.94254e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=12, gb_free=9.6, ema_decay=0.9999, wall=14751
2023-03-14 23:38:15 - progress_bar.py[line:274] - INFO: epoch 002:   1629 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.3, ups=0.85, wpb=255.5, bsz=100, num_updates=7930, lr=3.94087e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=14762
2023-03-14 23:38:26 - progress_bar.py[line:274] - INFO: epoch 002:   1639 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.3, ups=0.86, wpb=255.1, bsz=100, num_updates=7940, lr=3.93921e-05, gnorm=0.645, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=14774
2023-03-14 23:38:38 - progress_bar.py[line:274] - INFO: epoch 002:   1649 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=213.7, ups=0.84, wpb=254.5, bsz=100, num_updates=7950, lr=3.93754e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=14786
2023-03-14 23:38:50 - progress_bar.py[line:274] - INFO: epoch 002:   1659 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.8, ups=0.87, wpb=253.8, bsz=100, num_updates=7960, lr=3.93587e-05, gnorm=0.633, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=14798
2023-03-14 23:39:01 - progress_bar.py[line:274] - INFO: epoch 002:   1669 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217, ups=0.86, wpb=252, bsz=100, num_updates=7970, lr=3.9342e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14809
2023-03-14 23:39:13 - progress_bar.py[line:274] - INFO: epoch 002:   1679 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.5, ups=0.86, wpb=254, bsz=100, num_updates=7980, lr=3.93254e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=14821
2023-03-14 23:39:25 - progress_bar.py[line:274] - INFO: epoch 002:   1689 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.1, ups=0.86, wpb=251.9, bsz=100, num_updates=7990, lr=3.93087e-05, gnorm=0.591, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=14832
2023-03-14 23:39:36 - progress_bar.py[line:274] - INFO: epoch 002:   1699 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=224.5, ups=0.88, wpb=255, bsz=100, num_updates=8000, lr=3.9292e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=14844
2023-03-14 23:39:47 - progress_bar.py[line:274] - INFO: epoch 002:   1709 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.4, ups=0.87, wpb=251.8, bsz=100, num_updates=8010, lr=3.92754e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=14855
2023-03-14 23:39:59 - progress_bar.py[line:274] - INFO: epoch 002:   1719 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.6, ups=0.86, wpb=254.6, bsz=100, num_updates=8020, lr=3.92587e-05, gnorm=0.638, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=14867
2023-03-14 23:40:11 - progress_bar.py[line:274] - INFO: epoch 002:   1729 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.9, ups=0.86, wpb=253.5, bsz=100, num_updates=8030, lr=3.9242e-05, gnorm=0.639, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14879
2023-03-14 23:40:22 - progress_bar.py[line:274] - INFO: epoch 002:   1739 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.8, ups=0.86, wpb=253.4, bsz=100, num_updates=8040, lr=3.92253e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14890
2023-03-14 23:40:34 - progress_bar.py[line:274] - INFO: epoch 002:   1749 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.2, ups=0.86, wpb=254.7, bsz=100, num_updates=8050, lr=3.92087e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14902
2023-03-14 23:40:45 - progress_bar.py[line:274] - INFO: epoch 002:   1759 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=258, nsentences=100, sample_size=258, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223.1, ups=0.86, wpb=258, bsz=100, num_updates=8060, lr=3.9192e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=14913
2023-03-14 23:40:57 - progress_bar.py[line:274] - INFO: epoch 002:   1769 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.2, ups=0.86, wpb=253.1, bsz=100, num_updates=8070, lr=3.91753e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14925
2023-03-14 23:41:09 - progress_bar.py[line:274] - INFO: epoch 002:   1779 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.3, ups=0.86, wpb=250.5, bsz=100, num_updates=8080, lr=3.91586e-05, gnorm=0.623, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=14937
2023-03-14 23:41:20 - progress_bar.py[line:274] - INFO: epoch 002:   1789 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.4, ups=0.86, wpb=253.5, bsz=100, num_updates=8090, lr=3.9142e-05, gnorm=0.612, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14948
2023-03-14 23:41:32 - progress_bar.py[line:274] - INFO: epoch 002:   1799 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219, ups=0.86, wpb=253.7, bsz=100, num_updates=8100, lr=3.91253e-05, gnorm=0.712, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=14960
2023-03-14 23:41:44 - progress_bar.py[line:274] - INFO: epoch 002:   1809 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.8, ups=0.85, wpb=255.1, bsz=100, num_updates=8110, lr=3.91086e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=14971
2023-03-14 23:41:55 - progress_bar.py[line:274] - INFO: epoch 002:   1819 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217, ups=0.86, wpb=251.8, bsz=100, num_updates=8120, lr=3.90919e-05, gnorm=0.713, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=14983
2023-03-14 23:42:07 - progress_bar.py[line:274] - INFO: epoch 002:   1829 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.7, ups=0.86, wpb=254, bsz=100, num_updates=8130, lr=3.90753e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=14995
2023-03-14 23:42:18 - progress_bar.py[line:274] - INFO: epoch 002:   1839 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.5, ups=0.86, wpb=252.4, bsz=100, num_updates=8140, lr=3.90586e-05, gnorm=0.679, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15006
2023-03-14 23:42:30 - progress_bar.py[line:274] - INFO: epoch 002:   1849 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219, ups=0.87, wpb=251.1, bsz=100, num_updates=8150, lr=3.90419e-05, gnorm=0.667, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15018
2023-03-14 23:42:41 - progress_bar.py[line:274] - INFO: epoch 002:   1859 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.8, ups=0.87, wpb=251.9, bsz=100, num_updates=8160, lr=3.90252e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=15029
2023-03-14 23:42:53 - progress_bar.py[line:274] - INFO: epoch 002:   1869 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=222.4, ups=0.88, wpb=251.6, bsz=100, num_updates=8170, lr=3.90086e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15041
2023-03-14 23:43:04 - progress_bar.py[line:274] - INFO: epoch 002:   1879 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.9, ups=0.86, wpb=254.5, bsz=100, num_updates=8180, lr=3.89919e-05, gnorm=0.7, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=15052
2023-03-14 23:43:16 - progress_bar.py[line:274] - INFO: epoch 002:   1889 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.3, ups=0.86, wpb=254.3, bsz=100, num_updates=8190, lr=3.89752e-05, gnorm=0.68, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=15064
2023-03-14 23:43:28 - progress_bar.py[line:274] - INFO: epoch 002:   1899 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.3, ups=0.86, wpb=253.8, bsz=100, num_updates=8200, lr=3.89585e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=15075
2023-03-14 23:43:39 - progress_bar.py[line:274] - INFO: epoch 002:   1909 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.7, ups=0.86, wpb=252.3, bsz=100, num_updates=8210, lr=3.89419e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15087
2023-03-14 23:43:51 - progress_bar.py[line:274] - INFO: epoch 002:   1919 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.2, ups=0.86, wpb=252.3, bsz=100, num_updates=8220, lr=3.89252e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15099
2023-03-14 23:44:02 - progress_bar.py[line:274] - INFO: epoch 002:   1929 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.3, ups=0.87, wpb=252.9, bsz=100, num_updates=8230, lr=3.89085e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15110
2023-03-14 23:44:14 - progress_bar.py[line:274] - INFO: epoch 002:   1939 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.2, ups=0.86, wpb=254.5, bsz=100, num_updates=8240, lr=3.88919e-05, gnorm=0.663, clip=0, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=15122
2023-03-14 23:44:26 - progress_bar.py[line:274] - INFO: epoch 002:   1949 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218, ups=0.86, wpb=252.9, bsz=100, num_updates=8250, lr=3.88752e-05, gnorm=0.686, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15133
2023-03-14 23:44:37 - progress_bar.py[line:274] - INFO: epoch 002:   1959 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.8, ups=0.86, wpb=251.4, bsz=100, num_updates=8260, lr=3.88585e-05, gnorm=0.66, clip=0, loss_scale=1024, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=15145
2023-03-14 23:44:49 - progress_bar.py[line:274] - INFO: epoch 002:   1969 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=257.9, nsentences=100, sample_size=257.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=226.2, ups=0.88, wpb=257.9, bsz=100, num_updates=8270, lr=3.88418e-05, gnorm=0.666, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15157
2023-03-14 23:45:00 - progress_bar.py[line:274] - INFO: epoch 002:   1979 / 6313 loss=0.603, loss_v1=0, loss_v2=0, nll_loss=0.396, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=216.7, ups=0.86, wpb=251.1, bsz=100, num_updates=8280, lr=3.88252e-05, gnorm=0.661, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=15168
2023-03-14 23:45:12 - progress_bar.py[line:274] - INFO: epoch 002:   1989 / 6313 loss=0.593, loss_v1=0, loss_v2=0, nll_loss=0.384, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=217, ups=0.86, wpb=251.3, bsz=100, num_updates=8290, lr=3.88085e-05, gnorm=0.69, clip=0, loss_scale=1024, train_wall=12, gb_free=11, ema_decay=0.9999, wall=15180
2023-03-14 23:45:24 - progress_bar.py[line:274] - INFO: epoch 002:   1999 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.1, ups=0.85, wpb=255.5, bsz=100, num_updates=8300, lr=3.87918e-05, gnorm=0.654, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=15192
2023-03-14 23:45:35 - progress_bar.py[line:274] - INFO: epoch 002:   2009 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.5, ups=0.86, wpb=253.3, bsz=100, num_updates=8310, lr=3.87751e-05, gnorm=0.697, clip=0, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=15203
2023-03-14 23:45:41 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 23:45:48 - progress_bar.py[line:274] - INFO: epoch 002:   2020 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=203.3, ups=0.8, wpb=253.5, bsz=100, num_updates=8320, lr=3.87585e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15216
2023-03-14 23:45:59 - progress_bar.py[line:274] - INFO: epoch 002:   2030 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.5, ups=0.86, wpb=255.2, bsz=100, num_updates=8330, lr=3.87418e-05, gnorm=0.687, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=15227
2023-03-14 23:46:11 - progress_bar.py[line:274] - INFO: epoch 002:   2040 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.9, ups=0.86, wpb=254.5, bsz=100, num_updates=8340, lr=3.87251e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=15239
2023-03-14 23:46:22 - progress_bar.py[line:274] - INFO: epoch 002:   2050 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=224.4, ups=0.89, wpb=252, bsz=100, num_updates=8350, lr=3.87084e-05, gnorm=0.685, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15250
2023-03-14 23:46:34 - progress_bar.py[line:274] - INFO: epoch 002:   2060 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.1, ups=0.87, wpb=251.4, bsz=100, num_updates=8360, lr=3.86918e-05, gnorm=0.654, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15262
2023-03-14 23:46:45 - progress_bar.py[line:274] - INFO: epoch 002:   2070 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.6, ups=0.86, wpb=252.7, bsz=100, num_updates=8370, lr=3.86751e-05, gnorm=0.749, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=15273
2023-03-14 23:46:57 - progress_bar.py[line:274] - INFO: epoch 002:   2080 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.1, ups=0.86, wpb=252.6, bsz=100, num_updates=8380, lr=3.86584e-05, gnorm=0.684, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=15285
2023-03-14 23:47:09 - progress_bar.py[line:274] - INFO: epoch 002:   2090 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.8, ups=0.86, wpb=252, bsz=100, num_updates=8390, lr=3.86417e-05, gnorm=0.692, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15297
2023-03-14 23:47:20 - progress_bar.py[line:274] - INFO: epoch 002:   2100 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.5, ups=0.86, wpb=254.5, bsz=100, num_updates=8400, lr=3.86251e-05, gnorm=0.624, clip=0, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=15308
2023-03-14 23:47:32 - progress_bar.py[line:274] - INFO: epoch 002:   2110 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.6, ups=0.86, wpb=250.8, bsz=100, num_updates=8410, lr=3.86084e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=15320
2023-03-14 23:47:44 - progress_bar.py[line:274] - INFO: epoch 002:   2120 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.3, ups=0.86, wpb=256.5, bsz=100, num_updates=8420, lr=3.85917e-05, gnorm=0.639, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=15332
2023-03-14 23:47:55 - progress_bar.py[line:274] - INFO: epoch 002:   2130 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.6, ups=0.86, wpb=256.4, bsz=100, num_updates=8430, lr=3.8575e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=15343
2023-03-14 23:48:07 - progress_bar.py[line:274] - INFO: epoch 002:   2140 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.3, ups=0.84, wpb=255.3, bsz=100, num_updates=8440, lr=3.85584e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15355
2023-03-14 23:48:19 - progress_bar.py[line:274] - INFO: epoch 002:   2150 / 6313 loss=0.601, loss_v1=0, loss_v2=0, nll_loss=0.393, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=219.5, ups=0.87, wpb=252.5, bsz=100, num_updates=8450, lr=3.85417e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15367
2023-03-14 23:48:30 - progress_bar.py[line:274] - INFO: epoch 002:   2160 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.6, ups=0.86, wpb=253.6, bsz=100, num_updates=8460, lr=3.8525e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=15378
2023-03-14 23:48:42 - progress_bar.py[line:274] - INFO: epoch 002:   2170 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.1, ups=0.86, wpb=255.4, bsz=100, num_updates=8470, lr=3.85084e-05, gnorm=0.59, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15390
2023-03-14 23:48:54 - progress_bar.py[line:274] - INFO: epoch 002:   2180 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219, ups=0.86, wpb=255.5, bsz=100, num_updates=8480, lr=3.84917e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=15402
2023-03-14 23:49:05 - progress_bar.py[line:274] - INFO: epoch 002:   2190 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216, ups=0.86, wpb=250.5, bsz=100, num_updates=8490, lr=3.8475e-05, gnorm=0.692, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15413
2023-03-14 23:49:17 - progress_bar.py[line:274] - INFO: epoch 002:   2200 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.5, ups=0.86, wpb=254.1, bsz=100, num_updates=8500, lr=3.84583e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15425
2023-03-14 23:49:29 - progress_bar.py[line:274] - INFO: epoch 002:   2210 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.3, ups=0.86, wpb=253, bsz=100, num_updates=8510, lr=3.84417e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15436
2023-03-14 23:49:40 - progress_bar.py[line:274] - INFO: epoch 002:   2220 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.8, ups=0.86, wpb=253.8, bsz=100, num_updates=8520, lr=3.8425e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=15448
2023-03-14 23:49:52 - progress_bar.py[line:274] - INFO: epoch 002:   2230 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.8, ups=0.86, wpb=251.9, bsz=100, num_updates=8530, lr=3.84083e-05, gnorm=0.598, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=15460
2023-03-14 23:50:04 - progress_bar.py[line:274] - INFO: epoch 002:   2240 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=214.3, ups=0.85, wpb=251.6, bsz=100, num_updates=8540, lr=3.83916e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15471
2023-03-14 23:50:15 - progress_bar.py[line:274] - INFO: epoch 002:   2250 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.9, ups=0.86, wpb=252.3, bsz=100, num_updates=8550, lr=3.8375e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15483
2023-03-14 23:50:27 - progress_bar.py[line:274] - INFO: epoch 002:   2260 / 6313 loss=0.596, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=249.8, nsentences=100, sample_size=249.8, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.9, ups=0.87, wpb=249.8, bsz=100, num_updates=8560, lr=3.83583e-05, gnorm=0.732, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15495
2023-03-14 23:50:38 - progress_bar.py[line:274] - INFO: epoch 002:   2270 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.7, ups=0.86, wpb=253.7, bsz=100, num_updates=8570, lr=3.83416e-05, gnorm=0.748, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15506
2023-03-14 23:50:50 - progress_bar.py[line:274] - INFO: epoch 002:   2280 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.9, ups=0.86, wpb=251.8, bsz=100, num_updates=8580, lr=3.83249e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=15518
2023-03-14 23:51:01 - progress_bar.py[line:274] - INFO: epoch 002:   2290 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.3, ups=0.86, wpb=253.1, bsz=100, num_updates=8590, lr=3.83083e-05, gnorm=0.69, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15529
2023-03-14 23:51:13 - progress_bar.py[line:274] - INFO: epoch 002:   2300 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=212.5, ups=0.85, wpb=250.4, bsz=100, num_updates=8600, lr=3.82916e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15541
2023-03-14 23:51:25 - progress_bar.py[line:274] - INFO: epoch 002:   2310 / 6313 loss=0.611, loss_v1=0, loss_v2=0, nll_loss=0.401, ntokens=248.8, nsentences=100, sample_size=248.8, sample_size_v1=0, sample_size_v2=0, ppl=1.32, wps=213.7, ups=0.86, wpb=248.8, bsz=100, num_updates=8610, lr=3.82749e-05, gnorm=0.752, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15553
2023-03-14 23:51:37 - progress_bar.py[line:274] - INFO: epoch 002:   2320 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.1, ups=0.86, wpb=252.7, bsz=100, num_updates=8620, lr=3.82582e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=15564
2023-03-14 23:51:48 - progress_bar.py[line:274] - INFO: epoch 002:   2330 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.3, ups=0.86, wpb=253.3, bsz=100, num_updates=8630, lr=3.82416e-05, gnorm=0.708, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15576
2023-03-14 23:52:00 - progress_bar.py[line:274] - INFO: epoch 002:   2340 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.3, ups=0.87, wpb=252.7, bsz=100, num_updates=8640, lr=3.82249e-05, gnorm=0.75, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=15587
2023-03-14 23:52:11 - progress_bar.py[line:274] - INFO: epoch 002:   2350 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217, ups=0.86, wpb=251.4, bsz=100, num_updates=8650, lr=3.82082e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15599
2023-03-14 23:52:23 - progress_bar.py[line:274] - INFO: epoch 002:   2360 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.8, ups=0.85, wpb=252.6, bsz=100, num_updates=8660, lr=3.81915e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15611
2023-03-14 23:52:35 - progress_bar.py[line:274] - INFO: epoch 002:   2370 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.7, ups=0.86, wpb=255, bsz=100, num_updates=8670, lr=3.81749e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=15622
2023-03-14 23:52:46 - progress_bar.py[line:274] - INFO: epoch 002:   2380 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.7, ups=0.86, wpb=251.6, bsz=100, num_updates=8680, lr=3.81582e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15634
2023-03-14 23:52:58 - progress_bar.py[line:274] - INFO: epoch 002:   2390 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.4, ups=0.87, wpb=251.7, bsz=100, num_updates=8690, lr=3.81415e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=15645
2023-03-14 23:53:09 - progress_bar.py[line:274] - INFO: epoch 002:   2400 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.3, ups=0.86, wpb=254.5, bsz=100, num_updates=8700, lr=3.81249e-05, gnorm=0.623, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=15657
2023-03-14 23:53:21 - progress_bar.py[line:274] - INFO: epoch 002:   2410 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.3, ups=0.86, wpb=251.5, bsz=100, num_updates=8710, lr=3.81082e-05, gnorm=0.729, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=15669
2023-03-14 23:53:33 - progress_bar.py[line:274] - INFO: epoch 002:   2420 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.3, ups=0.86, wpb=254.9, bsz=100, num_updates=8720, lr=3.80915e-05, gnorm=0.732, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=15680
2023-03-14 23:53:44 - progress_bar.py[line:274] - INFO: epoch 002:   2430 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.4, ups=0.86, wpb=252.3, bsz=100, num_updates=8730, lr=3.80748e-05, gnorm=0.746, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=15692
2023-03-14 23:53:56 - progress_bar.py[line:274] - INFO: epoch 002:   2440 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.5, ups=0.86, wpb=254.1, bsz=100, num_updates=8740, lr=3.80582e-05, gnorm=0.616, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=15704
2023-03-14 23:54:07 - progress_bar.py[line:274] - INFO: epoch 002:   2450 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.7, ups=0.86, wpb=256, bsz=100, num_updates=8750, lr=3.80415e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15715
2023-03-14 23:54:19 - progress_bar.py[line:274] - INFO: epoch 002:   2460 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.5, ups=0.86, wpb=250.3, bsz=100, num_updates=8760, lr=3.80248e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15727
2023-03-14 23:54:31 - progress_bar.py[line:274] - INFO: epoch 002:   2470 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217, ups=0.86, wpb=253, bsz=100, num_updates=8770, lr=3.80081e-05, gnorm=0.648, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=15739
2023-03-14 23:54:42 - progress_bar.py[line:274] - INFO: epoch 002:   2480 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.2, ups=0.85, wpb=255, bsz=100, num_updates=8780, lr=3.79915e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=15750
2023-03-14 23:54:54 - progress_bar.py[line:274] - INFO: epoch 002:   2490 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.1, ups=0.86, wpb=252, bsz=100, num_updates=8790, lr=3.79748e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=15762
2023-03-14 23:55:06 - progress_bar.py[line:274] - INFO: epoch 002:   2500 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.1, ups=0.87, wpb=252.7, bsz=100, num_updates=8800, lr=3.79581e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=15773
2023-03-14 23:55:17 - progress_bar.py[line:274] - INFO: epoch 002:   2510 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.5, ups=0.87, wpb=251.2, bsz=100, num_updates=8810, lr=3.79414e-05, gnorm=0.716, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15785
2023-03-14 23:55:29 - progress_bar.py[line:274] - INFO: epoch 002:   2520 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.5, ups=0.86, wpb=255, bsz=100, num_updates=8820, lr=3.79248e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15797
2023-03-14 23:55:40 - progress_bar.py[line:274] - INFO: epoch 002:   2530 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.9, ups=0.86, wpb=255.5, bsz=100, num_updates=8830, lr=3.79081e-05, gnorm=0.704, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15808
2023-03-14 23:55:46 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-14 23:55:53 - progress_bar.py[line:274] - INFO: epoch 002:   2541 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=199.6, ups=0.79, wpb=253.8, bsz=100, num_updates=8840, lr=3.78914e-05, gnorm=0.603, clip=0, loss_scale=512, train_wall=13, gb_free=10.1, ema_decay=0.9999, wall=15821
2023-03-14 23:56:05 - progress_bar.py[line:274] - INFO: epoch 002:   2551 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.7, ups=0.86, wpb=254.3, bsz=100, num_updates=8850, lr=3.78747e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=15833
2023-03-14 23:56:16 - progress_bar.py[line:274] - INFO: epoch 002:   2561 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.8, ups=0.87, wpb=252.6, bsz=100, num_updates=8860, lr=3.78581e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15844
2023-03-14 23:56:28 - progress_bar.py[line:274] - INFO: epoch 002:   2571 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.9, ups=0.87, wpb=250.4, bsz=100, num_updates=8870, lr=3.78414e-05, gnorm=0.718, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15856
2023-03-14 23:56:39 - progress_bar.py[line:274] - INFO: epoch 002:   2581 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.9, ups=0.86, wpb=254.1, bsz=100, num_updates=8880, lr=3.78247e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=15867
2023-03-14 23:56:51 - progress_bar.py[line:274] - INFO: epoch 002:   2591 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.5, ups=0.86, wpb=254.9, bsz=100, num_updates=8890, lr=3.78081e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=15879
2023-03-14 23:57:03 - progress_bar.py[line:274] - INFO: epoch 002:   2601 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.1, ups=0.85, wpb=254.6, bsz=100, num_updates=8900, lr=3.77914e-05, gnorm=0.624, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=15891
2023-03-14 23:57:14 - progress_bar.py[line:274] - INFO: epoch 002:   2611 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=216.6, ups=0.86, wpb=251.3, bsz=100, num_updates=8910, lr=3.77747e-05, gnorm=0.698, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=15902
2023-03-14 23:57:26 - progress_bar.py[line:274] - INFO: epoch 002:   2621 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.7, ups=0.87, wpb=251.7, bsz=100, num_updates=8920, lr=3.7758e-05, gnorm=0.712, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=15914
2023-03-14 23:57:38 - progress_bar.py[line:274] - INFO: epoch 002:   2631 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.8, ups=0.86, wpb=252.1, bsz=100, num_updates=8930, lr=3.77414e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=15925
2023-03-14 23:57:49 - progress_bar.py[line:274] - INFO: epoch 002:   2641 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.4, ups=0.87, wpb=255.7, bsz=100, num_updates=8940, lr=3.77247e-05, gnorm=0.618, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=15937
2023-03-14 23:58:01 - progress_bar.py[line:274] - INFO: epoch 002:   2651 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219, ups=0.86, wpb=253.7, bsz=100, num_updates=8950, lr=3.7708e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=15949
2023-03-14 23:58:12 - progress_bar.py[line:274] - INFO: epoch 002:   2661 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.2, ups=0.86, wpb=253.3, bsz=100, num_updates=8960, lr=3.76913e-05, gnorm=0.665, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=15960
2023-03-14 23:58:24 - progress_bar.py[line:274] - INFO: epoch 002:   2671 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.9, ups=0.86, wpb=254.7, bsz=100, num_updates=8970, lr=3.76747e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=15972
2023-03-14 23:58:35 - progress_bar.py[line:274] - INFO: epoch 002:   2681 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=223.2, ups=0.87, wpb=255.5, bsz=100, num_updates=8980, lr=3.7658e-05, gnorm=0.619, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=15983
2023-03-14 23:58:47 - progress_bar.py[line:274] - INFO: epoch 002:   2691 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=223, ups=0.88, wpb=254.6, bsz=100, num_updates=8990, lr=3.76413e-05, gnorm=0.651, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=15995
2023-03-14 23:58:59 - progress_bar.py[line:274] - INFO: epoch 002:   2701 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.4, ups=0.86, wpb=253, bsz=100, num_updates=9000, lr=3.76246e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=16006
2023-03-14 23:59:10 - progress_bar.py[line:274] - INFO: epoch 002:   2711 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.7, ups=0.86, wpb=253.3, bsz=100, num_updates=9010, lr=3.7608e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16018
2023-03-14 23:59:22 - progress_bar.py[line:274] - INFO: epoch 002:   2721 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.4, ups=0.86, wpb=252.7, bsz=100, num_updates=9020, lr=3.75913e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=16030
2023-03-14 23:59:33 - progress_bar.py[line:274] - INFO: epoch 002:   2731 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221.4, ups=0.86, wpb=256, bsz=100, num_updates=9030, lr=3.75746e-05, gnorm=0.671, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=16041
2023-03-14 23:59:45 - progress_bar.py[line:274] - INFO: epoch 002:   2741 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.4, ups=0.86, wpb=252, bsz=100, num_updates=9040, lr=3.75579e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16053
2023-03-14 23:59:57 - progress_bar.py[line:274] - INFO: epoch 002:   2751 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.8, ups=0.87, wpb=256.3, bsz=100, num_updates=9050, lr=3.75413e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=16064
2023-03-15 00:00:08 - progress_bar.py[line:274] - INFO: epoch 002:   2761 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.8, ups=0.87, wpb=252.9, bsz=100, num_updates=9060, lr=3.75246e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=16076
2023-03-15 00:00:20 - progress_bar.py[line:274] - INFO: epoch 002:   2771 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.1, ups=0.87, wpb=253.8, bsz=100, num_updates=9070, lr=3.75079e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=16087
2023-03-15 00:00:31 - progress_bar.py[line:274] - INFO: epoch 002:   2781 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.8, ups=0.86, wpb=252.2, bsz=100, num_updates=9080, lr=3.74912e-05, gnorm=0.603, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16099
2023-03-15 00:00:43 - progress_bar.py[line:274] - INFO: epoch 002:   2791 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.6, ups=0.86, wpb=252.1, bsz=100, num_updates=9090, lr=3.74746e-05, gnorm=0.646, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16111
2023-03-15 00:00:54 - progress_bar.py[line:274] - INFO: epoch 002:   2801 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.6, ups=0.86, wpb=252.7, bsz=100, num_updates=9100, lr=3.74579e-05, gnorm=0.691, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16122
2023-03-15 00:01:06 - progress_bar.py[line:274] - INFO: epoch 002:   2811 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=256.7, nsentences=100, sample_size=256.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.7, ups=0.86, wpb=256.7, bsz=100, num_updates=9110, lr=3.74412e-05, gnorm=0.635, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=16134
2023-03-15 00:01:18 - progress_bar.py[line:274] - INFO: epoch 002:   2821 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=249.4, nsentences=100, sample_size=249.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.2, ups=0.88, wpb=249.4, bsz=100, num_updates=9120, lr=3.74246e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=16146
2023-03-15 00:01:29 - progress_bar.py[line:274] - INFO: epoch 002:   2831 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.7, ups=0.87, wpb=252.6, bsz=100, num_updates=9130, lr=3.74079e-05, gnorm=0.624, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=16157
2023-03-15 00:01:41 - progress_bar.py[line:274] - INFO: epoch 002:   2841 / 6313 loss=0.589, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=218.7, ups=0.87, wpb=251.7, bsz=100, num_updates=9140, lr=3.73912e-05, gnorm=0.747, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=16169
2023-03-15 00:01:53 - progress_bar.py[line:274] - INFO: epoch 002:   2851 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.376, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=215.2, ups=0.86, wpb=250.7, bsz=100, num_updates=9150, lr=3.73745e-05, gnorm=0.684, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=16180
2023-03-15 00:02:04 - progress_bar.py[line:274] - INFO: epoch 002:   2861 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=222.9, ups=0.88, wpb=254.4, bsz=100, num_updates=9160, lr=3.73579e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=16192
2023-03-15 00:02:16 - progress_bar.py[line:274] - INFO: epoch 002:   2871 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.2, ups=0.86, wpb=253, bsz=100, num_updates=9170, lr=3.73412e-05, gnorm=0.66, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=16203
2023-03-15 00:02:27 - progress_bar.py[line:274] - INFO: epoch 002:   2881 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.3, ups=0.86, wpb=253, bsz=100, num_updates=9180, lr=3.73245e-05, gnorm=0.598, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16215
2023-03-15 00:02:39 - progress_bar.py[line:274] - INFO: epoch 002:   2891 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.8, ups=0.86, wpb=250.3, bsz=100, num_updates=9190, lr=3.73078e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=16227
2023-03-15 00:02:51 - progress_bar.py[line:274] - INFO: epoch 002:   2901 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.7, ups=0.86, wpb=254.2, bsz=100, num_updates=9200, lr=3.72912e-05, gnorm=0.686, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16238
2023-03-15 00:03:02 - progress_bar.py[line:274] - INFO: epoch 002:   2911 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.9, ups=0.86, wpb=253.1, bsz=100, num_updates=9210, lr=3.72745e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16250
2023-03-15 00:03:14 - progress_bar.py[line:274] - INFO: epoch 002:   2921 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.8, ups=0.86, wpb=254.2, bsz=100, num_updates=9220, lr=3.72578e-05, gnorm=0.629, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16262
2023-03-15 00:03:25 - progress_bar.py[line:274] - INFO: epoch 002:   2931 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.1, ups=0.86, wpb=251.5, bsz=100, num_updates=9230, lr=3.72411e-05, gnorm=0.611, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16273
2023-03-15 00:03:37 - progress_bar.py[line:274] - INFO: epoch 002:   2941 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.8, ups=0.87, wpb=255.2, bsz=100, num_updates=9240, lr=3.72245e-05, gnorm=0.73, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=16285
2023-03-15 00:03:48 - progress_bar.py[line:274] - INFO: epoch 002:   2951 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.9, ups=0.86, wpb=251.2, bsz=100, num_updates=9250, lr=3.72078e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=16296
2023-03-15 00:04:00 - progress_bar.py[line:274] - INFO: epoch 002:   2961 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.8, ups=0.86, wpb=252.9, bsz=100, num_updates=9260, lr=3.71911e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=16308
2023-03-15 00:04:12 - progress_bar.py[line:274] - INFO: epoch 002:   2971 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217, ups=0.85, wpb=254.2, bsz=100, num_updates=9270, lr=3.71744e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=16320
2023-03-15 00:04:23 - progress_bar.py[line:274] - INFO: epoch 002:   2981 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=224.3, ups=0.87, wpb=256.5, bsz=100, num_updates=9280, lr=3.71578e-05, gnorm=0.601, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=16331
2023-03-15 00:04:35 - progress_bar.py[line:274] - INFO: epoch 002:   2991 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.6, ups=0.85, wpb=254.1, bsz=100, num_updates=9290, lr=3.71411e-05, gnorm=0.72, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=16343
2023-03-15 00:04:47 - progress_bar.py[line:274] - INFO: epoch 002:   3001 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=222.5, ups=0.87, wpb=255.3, bsz=100, num_updates=9300, lr=3.71244e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=16354
2023-03-15 00:04:58 - progress_bar.py[line:274] - INFO: epoch 002:   3011 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.1, ups=0.86, wpb=254.1, bsz=100, num_updates=9310, lr=3.71077e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16366
2023-03-15 00:05:10 - progress_bar.py[line:274] - INFO: epoch 002:   3021 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=226.2, ups=0.88, wpb=255.9, bsz=100, num_updates=9320, lr=3.70911e-05, gnorm=0.689, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=16377
2023-03-15 00:05:21 - progress_bar.py[line:274] - INFO: epoch 002:   3031 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=257.2, nsentences=100, sample_size=257.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=224.8, ups=0.87, wpb=257.2, bsz=100, num_updates=9330, lr=3.70744e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=16389
2023-03-15 00:05:32 - progress_bar.py[line:274] - INFO: epoch 002:   3041 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=224, ups=0.89, wpb=252.7, bsz=100, num_updates=9340, lr=3.70577e-05, gnorm=0.637, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=16400
2023-03-15 00:05:44 - progress_bar.py[line:274] - INFO: epoch 002:   3051 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221.4, ups=0.87, wpb=253, bsz=100, num_updates=9350, lr=3.70411e-05, gnorm=0.693, clip=0, loss_scale=1024, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=16412
2023-03-15 00:05:54 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 00:05:56 - progress_bar.py[line:274] - INFO: epoch 002:   3062 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=203.6, ups=0.8, wpb=256.1, bsz=100, num_updates=9360, lr=3.70244e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=13, gb_free=9.7, ema_decay=0.9999, wall=16424
2023-03-15 00:06:08 - progress_bar.py[line:274] - INFO: epoch 002:   3072 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.3, ups=0.86, wpb=255.2, bsz=100, num_updates=9370, lr=3.70077e-05, gnorm=0.686, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16436
2023-03-15 00:06:19 - progress_bar.py[line:274] - INFO: epoch 002:   3082 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.8, ups=0.88, wpb=253.5, bsz=100, num_updates=9380, lr=3.6991e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=16447
2023-03-15 00:06:31 - progress_bar.py[line:274] - INFO: epoch 002:   3092 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.2, ups=0.86, wpb=252.5, bsz=100, num_updates=9390, lr=3.69744e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=16459
2023-03-15 00:06:42 - progress_bar.py[line:274] - INFO: epoch 002:   3102 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.9, ups=0.86, wpb=253.9, bsz=100, num_updates=9400, lr=3.69577e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=16470
2023-03-15 00:06:54 - progress_bar.py[line:274] - INFO: epoch 002:   3112 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.1, ups=0.85, wpb=253.8, bsz=100, num_updates=9410, lr=3.6941e-05, gnorm=0.601, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=16482
2023-03-15 00:07:06 - progress_bar.py[line:274] - INFO: epoch 002:   3122 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.9, ups=0.87, wpb=253.6, bsz=100, num_updates=9420, lr=3.69243e-05, gnorm=0.697, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=16494
2023-03-15 00:07:17 - progress_bar.py[line:274] - INFO: epoch 002:   3132 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.8, ups=0.87, wpb=256.1, bsz=100, num_updates=9430, lr=3.69077e-05, gnorm=0.593, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=16505
2023-03-15 00:07:29 - progress_bar.py[line:274] - INFO: epoch 002:   3142 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=257.8, nsentences=100, sample_size=257.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=224.8, ups=0.87, wpb=257.8, bsz=100, num_updates=9440, lr=3.6891e-05, gnorm=0.628, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=16517
2023-03-15 00:07:40 - progress_bar.py[line:274] - INFO: epoch 002:   3152 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.6, ups=0.86, wpb=251.4, bsz=100, num_updates=9450, lr=3.68743e-05, gnorm=0.673, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=16528
2023-03-15 00:07:52 - progress_bar.py[line:274] - INFO: epoch 002:   3162 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.9, ups=0.87, wpb=252.8, bsz=100, num_updates=9460, lr=3.68576e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=16540
2023-03-15 00:08:04 - progress_bar.py[line:274] - INFO: epoch 002:   3172 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.7, ups=0.86, wpb=254.1, bsz=100, num_updates=9470, lr=3.6841e-05, gnorm=0.611, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16551
2023-03-15 00:08:15 - progress_bar.py[line:274] - INFO: epoch 002:   3182 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.9, ups=0.86, wpb=253.4, bsz=100, num_updates=9480, lr=3.68243e-05, gnorm=0.806, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=16563
2023-03-15 00:08:27 - progress_bar.py[line:274] - INFO: epoch 002:   3192 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.7, ups=0.87, wpb=252.6, bsz=100, num_updates=9490, lr=3.68076e-05, gnorm=0.698, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=16574
2023-03-15 00:08:38 - progress_bar.py[line:274] - INFO: epoch 002:   3202 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219, ups=0.86, wpb=253.5, bsz=100, num_updates=9500, lr=3.67909e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=16586
2023-03-15 00:08:50 - progress_bar.py[line:274] - INFO: epoch 002:   3212 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.6, ups=0.86, wpb=252.8, bsz=100, num_updates=9510, lr=3.67743e-05, gnorm=0.74, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16598
2023-03-15 00:09:01 - progress_bar.py[line:274] - INFO: epoch 002:   3222 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.5, ups=0.86, wpb=254.2, bsz=100, num_updates=9520, lr=3.67576e-05, gnorm=0.735, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=16609
2023-03-15 00:09:13 - progress_bar.py[line:274] - INFO: epoch 002:   3232 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.9, ups=0.86, wpb=255.7, bsz=100, num_updates=9530, lr=3.67409e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=16621
2023-03-15 00:09:25 - progress_bar.py[line:274] - INFO: epoch 002:   3242 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.7, ups=0.86, wpb=255.3, bsz=100, num_updates=9540, lr=3.67242e-05, gnorm=0.633, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=16633
2023-03-15 00:09:36 - progress_bar.py[line:274] - INFO: epoch 002:   3252 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218, ups=0.86, wpb=252.1, bsz=100, num_updates=9550, lr=3.67076e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=16644
2023-03-15 00:09:48 - progress_bar.py[line:274] - INFO: epoch 002:   3262 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.8, ups=0.86, wpb=252.7, bsz=100, num_updates=9560, lr=3.66909e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=16656
2023-03-15 00:09:59 - progress_bar.py[line:274] - INFO: epoch 002:   3272 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.4, ups=0.87, wpb=251.3, bsz=100, num_updates=9570, lr=3.66742e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=16667
2023-03-15 00:10:11 - progress_bar.py[line:274] - INFO: epoch 002:   3282 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218, ups=0.86, wpb=252.8, bsz=100, num_updates=9580, lr=3.66576e-05, gnorm=0.792, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=16679
2023-03-15 00:10:23 - progress_bar.py[line:274] - INFO: epoch 002:   3292 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.38, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.8, ups=0.87, wpb=253.9, bsz=100, num_updates=9590, lr=3.66409e-05, gnorm=0.744, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=16690
2023-03-15 00:10:34 - progress_bar.py[line:274] - INFO: epoch 002:   3302 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217, ups=0.86, wpb=251.8, bsz=100, num_updates=9600, lr=3.66242e-05, gnorm=0.661, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=16702
2023-03-15 00:10:46 - progress_bar.py[line:274] - INFO: epoch 002:   3312 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.2, ups=0.87, wpb=252.8, bsz=100, num_updates=9610, lr=3.66075e-05, gnorm=0.613, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=16713
2023-03-15 00:10:57 - progress_bar.py[line:274] - INFO: epoch 002:   3322 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221.6, ups=0.87, wpb=253.3, bsz=100, num_updates=9620, lr=3.65909e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=16725
2023-03-15 00:11:09 - progress_bar.py[line:274] - INFO: epoch 002:   3332 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218, ups=0.86, wpb=254.7, bsz=100, num_updates=9630, lr=3.65742e-05, gnorm=0.601, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=16737
2023-03-15 00:11:21 - progress_bar.py[line:274] - INFO: epoch 002:   3342 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.367, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.1, ups=0.86, wpb=252.3, bsz=100, num_updates=9640, lr=3.65575e-05, gnorm=0.69, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=16748
2023-03-15 00:11:33 - progress_bar.py[line:274] - INFO: epoch 002:   3352 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.5, ups=0.86, wpb=252.2, bsz=100, num_updates=9650, lr=3.65408e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16760
2023-03-15 00:11:44 - progress_bar.py[line:274] - INFO: epoch 002:   3362 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.9, ups=0.86, wpb=252.7, bsz=100, num_updates=9660, lr=3.65242e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16772
2023-03-15 00:11:56 - progress_bar.py[line:274] - INFO: epoch 002:   3372 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.3, ups=0.86, wpb=253, bsz=100, num_updates=9670, lr=3.65075e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16784
2023-03-15 00:12:07 - progress_bar.py[line:274] - INFO: epoch 002:   3382 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.6, ups=0.86, wpb=252.3, bsz=100, num_updates=9680, lr=3.64908e-05, gnorm=0.733, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=16795
2023-03-15 00:12:19 - progress_bar.py[line:274] - INFO: epoch 002:   3392 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.5, ups=0.86, wpb=253.3, bsz=100, num_updates=9690, lr=3.64741e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16807
2023-03-15 00:12:31 - progress_bar.py[line:274] - INFO: epoch 002:   3402 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.9, ups=0.86, wpb=252.5, bsz=100, num_updates=9700, lr=3.64575e-05, gnorm=0.697, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=16818
2023-03-15 00:12:42 - progress_bar.py[line:274] - INFO: epoch 002:   3412 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.9, ups=0.87, wpb=252.7, bsz=100, num_updates=9710, lr=3.64408e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=16830
2023-03-15 00:12:54 - progress_bar.py[line:274] - INFO: epoch 002:   3422 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.5, ups=0.87, wpb=251.4, bsz=100, num_updates=9720, lr=3.64241e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=16841
2023-03-15 00:13:05 - progress_bar.py[line:274] - INFO: epoch 002:   3432 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.4, ups=0.86, wpb=254, bsz=100, num_updates=9730, lr=3.64074e-05, gnorm=0.694, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=16853
2023-03-15 00:13:17 - progress_bar.py[line:274] - INFO: epoch 002:   3442 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=213.4, ups=0.84, wpb=252.9, bsz=100, num_updates=9740, lr=3.63908e-05, gnorm=0.853, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=16865
2023-03-15 00:13:29 - progress_bar.py[line:274] - INFO: epoch 002:   3452 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.4, ups=0.86, wpb=251, bsz=100, num_updates=9750, lr=3.63741e-05, gnorm=0.672, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=16877
2023-03-15 00:13:40 - progress_bar.py[line:274] - INFO: epoch 002:   3462 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.8, ups=0.86, wpb=253.3, bsz=100, num_updates=9760, lr=3.63574e-05, gnorm=0.629, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16888
2023-03-15 00:13:52 - progress_bar.py[line:274] - INFO: epoch 002:   3472 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222, ups=0.87, wpb=253.8, bsz=100, num_updates=9770, lr=3.63407e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=16900
2023-03-15 00:14:03 - progress_bar.py[line:274] - INFO: epoch 002:   3482 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.8, ups=0.86, wpb=253.4, bsz=100, num_updates=9780, lr=3.63241e-05, gnorm=0.739, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=16911
2023-03-15 00:14:15 - progress_bar.py[line:274] - INFO: epoch 002:   3492 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.5, ups=0.86, wpb=252, bsz=100, num_updates=9790, lr=3.63074e-05, gnorm=0.665, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16923
2023-03-15 00:14:27 - progress_bar.py[line:274] - INFO: epoch 002:   3502 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.4, ups=0.86, wpb=251.9, bsz=100, num_updates=9800, lr=3.62907e-05, gnorm=0.695, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=16934
2023-03-15 00:14:38 - progress_bar.py[line:274] - INFO: epoch 002:   3512 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221, ups=0.86, wpb=255.6, bsz=100, num_updates=9810, lr=3.62741e-05, gnorm=0.702, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16946
2023-03-15 00:14:50 - progress_bar.py[line:274] - INFO: epoch 002:   3522 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.2, ups=0.86, wpb=256.5, bsz=100, num_updates=9820, lr=3.62574e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=16958
2023-03-15 00:15:01 - progress_bar.py[line:274] - INFO: epoch 002:   3532 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.5, ups=0.86, wpb=251.5, bsz=100, num_updates=9830, lr=3.62407e-05, gnorm=0.747, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=16969
2023-03-15 00:15:13 - progress_bar.py[line:274] - INFO: epoch 002:   3542 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.3, ups=0.86, wpb=254.3, bsz=100, num_updates=9840, lr=3.6224e-05, gnorm=0.67, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=16981
2023-03-15 00:15:24 - progress_bar.py[line:274] - INFO: epoch 002:   3552 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221.7, ups=0.88, wpb=253.2, bsz=100, num_updates=9850, lr=3.62074e-05, gnorm=0.613, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=16992
2023-03-15 00:15:36 - progress_bar.py[line:274] - INFO: epoch 002:   3562 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.7, ups=0.87, wpb=250.2, bsz=100, num_updates=9860, lr=3.61907e-05, gnorm=0.656, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17004
2023-03-15 00:15:48 - progress_bar.py[line:274] - INFO: epoch 002:   3572 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.4, ups=0.87, wpb=254.6, bsz=100, num_updates=9870, lr=3.6174e-05, gnorm=0.685, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=17015
2023-03-15 00:15:59 - progress_bar.py[line:274] - INFO: epoch 002:   3582 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.4, ups=0.87, wpb=254.6, bsz=100, num_updates=9880, lr=3.61573e-05, gnorm=0.66, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=17027
2023-03-15 00:16:00 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 00:16:12 - progress_bar.py[line:274] - INFO: epoch 002:   3593 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=204.8, ups=0.8, wpb=256.4, bsz=100, num_updates=9890, lr=3.61407e-05, gnorm=0.779, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17039
2023-03-15 00:16:23 - progress_bar.py[line:274] - INFO: epoch 002:   3603 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.6, ups=0.86, wpb=251.4, bsz=100, num_updates=9900, lr=3.6124e-05, gnorm=0.828, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17051
2023-03-15 00:16:35 - progress_bar.py[line:274] - INFO: epoch 002:   3613 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.7, ups=0.87, wpb=251.7, bsz=100, num_updates=9910, lr=3.61073e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=17063
2023-03-15 00:16:46 - progress_bar.py[line:274] - INFO: epoch 002:   3623 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=224.5, ups=0.88, wpb=254.6, bsz=100, num_updates=9920, lr=3.60906e-05, gnorm=0.618, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=17074
2023-03-15 00:16:58 - progress_bar.py[line:274] - INFO: epoch 002:   3633 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.1, ups=0.86, wpb=252.4, bsz=100, num_updates=9930, lr=3.6074e-05, gnorm=0.721, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17086
2023-03-15 00:17:09 - progress_bar.py[line:274] - INFO: epoch 002:   3643 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=257.4, nsentences=100, sample_size=257.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=224.5, ups=0.87, wpb=257.4, bsz=100, num_updates=9940, lr=3.60573e-05, gnorm=0.693, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=17097
2023-03-15 00:17:21 - progress_bar.py[line:274] - INFO: epoch 002:   3653 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=222.8, ups=0.87, wpb=255.9, bsz=100, num_updates=9950, lr=3.60406e-05, gnorm=0.725, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=17108
2023-03-15 00:17:32 - progress_bar.py[line:274] - INFO: epoch 002:   3663 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.7, ups=0.87, wpb=251.4, bsz=100, num_updates=9960, lr=3.60239e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=17120
2023-03-15 00:17:44 - progress_bar.py[line:274] - INFO: epoch 002:   3673 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=225.9, ups=0.88, wpb=255.4, bsz=100, num_updates=9970, lr=3.60073e-05, gnorm=0.654, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=17131
2023-03-15 00:17:55 - progress_bar.py[line:274] - INFO: epoch 002:   3683 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.366, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221.2, ups=0.86, wpb=255.7, bsz=100, num_updates=9980, lr=3.59906e-05, gnorm=0.73, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=17143
2023-03-15 00:18:07 - progress_bar.py[line:274] - INFO: epoch 002:   3693 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.2, ups=0.86, wpb=255.8, bsz=100, num_updates=9990, lr=3.59739e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=17155
2023-03-15 00:18:19 - progress_bar.py[line:274] - INFO: epoch 002:   3703 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.3, ups=0.86, wpb=251.4, bsz=100, num_updates=10000, lr=3.59572e-05, gnorm=0.744, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=17166
2023-03-15 00:18:30 - progress_bar.py[line:274] - INFO: epoch 002:   3713 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.6, ups=0.85, wpb=253.4, bsz=100, num_updates=10010, lr=3.59406e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=17178
2023-03-15 00:18:42 - progress_bar.py[line:274] - INFO: epoch 002:   3723 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219, ups=0.86, wpb=253.7, bsz=100, num_updates=10020, lr=3.59239e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17190
2023-03-15 00:18:53 - progress_bar.py[line:274] - INFO: epoch 002:   3733 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=222.7, ups=0.87, wpb=254.8, bsz=100, num_updates=10030, lr=3.59072e-05, gnorm=0.677, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=17201
2023-03-15 00:19:05 - progress_bar.py[line:274] - INFO: epoch 002:   3743 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.9, ups=0.86, wpb=253.9, bsz=100, num_updates=10040, lr=3.58906e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17213
2023-03-15 00:19:17 - progress_bar.py[line:274] - INFO: epoch 002:   3753 / 6313 loss=0.587, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=213.7, ups=0.85, wpb=250.2, bsz=100, num_updates=10050, lr=3.58739e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17224
2023-03-15 00:19:28 - progress_bar.py[line:274] - INFO: epoch 002:   3763 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220, ups=0.86, wpb=254.8, bsz=100, num_updates=10060, lr=3.58572e-05, gnorm=0.734, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17236
2023-03-15 00:19:40 - progress_bar.py[line:274] - INFO: epoch 002:   3773 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.6, ups=0.87, wpb=253.8, bsz=100, num_updates=10070, lr=3.58405e-05, gnorm=0.781, clip=10, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=17248
2023-03-15 00:19:52 - progress_bar.py[line:274] - INFO: epoch 002:   3783 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.2, ups=0.87, wpb=252.1, bsz=100, num_updates=10080, lr=3.58239e-05, gnorm=0.728, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=17259
2023-03-15 00:20:03 - progress_bar.py[line:274] - INFO: epoch 002:   3793 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218, ups=0.86, wpb=252.7, bsz=100, num_updates=10090, lr=3.58072e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=17271
2023-03-15 00:20:15 - progress_bar.py[line:274] - INFO: epoch 002:   3803 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.9, ups=0.86, wpb=255.2, bsz=100, num_updates=10100, lr=3.57905e-05, gnorm=0.746, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17283
2023-03-15 00:20:27 - progress_bar.py[line:274] - INFO: epoch 002:   3813 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.8, ups=0.85, wpb=252.5, bsz=100, num_updates=10110, lr=3.57738e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=17294
2023-03-15 00:20:38 - progress_bar.py[line:274] - INFO: epoch 002:   3823 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=224.1, ups=0.88, wpb=253.5, bsz=100, num_updates=10120, lr=3.57572e-05, gnorm=0.61, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=17306
2023-03-15 00:20:49 - progress_bar.py[line:274] - INFO: epoch 002:   3833 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220, ups=0.86, wpb=254.8, bsz=100, num_updates=10130, lr=3.57405e-05, gnorm=0.659, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=17317
2023-03-15 00:21:01 - progress_bar.py[line:274] - INFO: epoch 002:   3843 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=224.8, ups=0.88, wpb=256.4, bsz=100, num_updates=10140, lr=3.57238e-05, gnorm=0.583, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=17329
2023-03-15 00:21:12 - progress_bar.py[line:274] - INFO: epoch 002:   3853 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.3, ups=0.86, wpb=254.8, bsz=100, num_updates=10150, lr=3.57071e-05, gnorm=0.653, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=17340
2023-03-15 00:21:24 - progress_bar.py[line:274] - INFO: epoch 002:   3863 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.9, ups=0.87, wpb=253.6, bsz=100, num_updates=10160, lr=3.56905e-05, gnorm=0.59, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=17352
2023-03-15 00:21:35 - progress_bar.py[line:274] - INFO: epoch 002:   3873 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221.8, ups=0.87, wpb=253.7, bsz=100, num_updates=10170, lr=3.56738e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=17363
2023-03-15 00:21:47 - progress_bar.py[line:274] - INFO: epoch 002:   3883 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.4, ups=0.87, wpb=254.4, bsz=100, num_updates=10180, lr=3.56571e-05, gnorm=0.66, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=17375
2023-03-15 00:21:59 - progress_bar.py[line:274] - INFO: epoch 002:   3893 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.4, ups=0.86, wpb=253, bsz=100, num_updates=10190, lr=3.56404e-05, gnorm=0.59, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17386
2023-03-15 00:22:10 - progress_bar.py[line:274] - INFO: epoch 002:   3903 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.5, ups=0.86, wpb=253, bsz=100, num_updates=10200, lr=3.56238e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17398
2023-03-15 00:22:22 - progress_bar.py[line:274] - INFO: epoch 002:   3913 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.9, ups=0.87, wpb=251.7, bsz=100, num_updates=10210, lr=3.56071e-05, gnorm=0.796, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=17410
2023-03-15 00:22:33 - progress_bar.py[line:274] - INFO: epoch 002:   3923 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.6, ups=0.87, wpb=251.4, bsz=100, num_updates=10220, lr=3.55904e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=17421
2023-03-15 00:22:45 - progress_bar.py[line:274] - INFO: epoch 002:   3933 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218, ups=0.85, wpb=255.5, bsz=100, num_updates=10230, lr=3.55737e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17433
2023-03-15 00:22:57 - progress_bar.py[line:274] - INFO: epoch 002:   3943 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.1, ups=0.86, wpb=253.9, bsz=100, num_updates=10240, lr=3.55571e-05, gnorm=0.619, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=17444
2023-03-15 00:23:08 - progress_bar.py[line:274] - INFO: epoch 002:   3953 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.1, ups=0.85, wpb=250.7, bsz=100, num_updates=10250, lr=3.55404e-05, gnorm=0.648, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17456
2023-03-15 00:23:20 - progress_bar.py[line:274] - INFO: epoch 002:   3963 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.9, ups=0.87, wpb=252.9, bsz=100, num_updates=10260, lr=3.55237e-05, gnorm=0.62, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=17468
2023-03-15 00:23:32 - progress_bar.py[line:274] - INFO: epoch 002:   3973 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.6, ups=0.86, wpb=255, bsz=100, num_updates=10270, lr=3.55071e-05, gnorm=0.57, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17479
2023-03-15 00:23:43 - progress_bar.py[line:274] - INFO: epoch 002:   3983 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=249, nsentences=100, sample_size=249, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.1, ups=0.86, wpb=249, bsz=100, num_updates=10280, lr=3.54904e-05, gnorm=0.724, clip=0, loss_scale=512, train_wall=12, gb_free=11.2, ema_decay=0.9999, wall=17491
2023-03-15 00:23:55 - progress_bar.py[line:274] - INFO: epoch 002:   3993 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.4, ups=0.85, wpb=253.2, bsz=100, num_updates=10290, lr=3.54737e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=17503
2023-03-15 00:24:06 - progress_bar.py[line:274] - INFO: epoch 002:   4003 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.5, ups=0.86, wpb=251.7, bsz=100, num_updates=10300, lr=3.5457e-05, gnorm=0.685, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17514
2023-03-15 00:24:18 - progress_bar.py[line:274] - INFO: epoch 002:   4013 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.6, ups=0.86, wpb=255.2, bsz=100, num_updates=10310, lr=3.54404e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17526
2023-03-15 00:24:30 - progress_bar.py[line:274] - INFO: epoch 002:   4023 / 6313 loss=0.577, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.2, ups=0.87, wpb=252.2, bsz=100, num_updates=10320, lr=3.54237e-05, gnorm=0.648, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17537
2023-03-15 00:24:41 - progress_bar.py[line:274] - INFO: epoch 002:   4033 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221.8, ups=0.88, wpb=253.1, bsz=100, num_updates=10330, lr=3.5407e-05, gnorm=0.571, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=17549
2023-03-15 00:24:53 - progress_bar.py[line:274] - INFO: epoch 002:   4043 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220, ups=0.87, wpb=254.2, bsz=100, num_updates=10340, lr=3.53903e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17560
2023-03-15 00:25:04 - progress_bar.py[line:274] - INFO: epoch 002:   4053 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.2, ups=0.87, wpb=251.3, bsz=100, num_updates=10350, lr=3.53737e-05, gnorm=0.696, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=17572
2023-03-15 00:25:16 - progress_bar.py[line:274] - INFO: epoch 002:   4063 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=249.8, nsentences=100, sample_size=249.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=213.1, ups=0.85, wpb=249.8, bsz=100, num_updates=10360, lr=3.5357e-05, gnorm=0.619, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=17584
2023-03-15 00:25:27 - progress_bar.py[line:274] - INFO: epoch 002:   4073 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=223.2, ups=0.87, wpb=255.4, bsz=100, num_updates=10370, lr=3.53403e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=17595
2023-03-15 00:25:39 - progress_bar.py[line:274] - INFO: epoch 002:   4083 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=256.9, nsentences=100, sample_size=256.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.9, ups=0.85, wpb=256.9, bsz=100, num_updates=10380, lr=3.53236e-05, gnorm=0.635, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17607
2023-03-15 00:25:51 - progress_bar.py[line:274] - INFO: epoch 002:   4093 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.5, ups=0.86, wpb=255.5, bsz=100, num_updates=10390, lr=3.5307e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17618
2023-03-15 00:26:02 - progress_bar.py[line:274] - INFO: epoch 002:   4103 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.8, ups=0.86, wpb=253.7, bsz=100, num_updates=10400, lr=3.52903e-05, gnorm=0.633, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17630
2023-03-15 00:26:14 - progress_bar.py[line:274] - INFO: epoch 002:   4113 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.6, ups=0.85, wpb=253.6, bsz=100, num_updates=10410, lr=3.52736e-05, gnorm=0.674, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=17642
2023-03-15 00:26:25 - progress_bar.py[line:274] - INFO: epoch 002:   4123 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.8, ups=0.86, wpb=251.3, bsz=100, num_updates=10420, lr=3.52569e-05, gnorm=0.667, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17653
2023-03-15 00:26:37 - progress_bar.py[line:274] - INFO: epoch 002:   4133 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.6, ups=0.86, wpb=251.8, bsz=100, num_updates=10430, lr=3.52403e-05, gnorm=0.699, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=17665
2023-03-15 00:26:49 - progress_bar.py[line:274] - INFO: epoch 002:   4143 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.9, ups=0.87, wpb=253.9, bsz=100, num_updates=10440, lr=3.52236e-05, gnorm=0.629, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=17676
2023-03-15 00:27:00 - progress_bar.py[line:274] - INFO: epoch 002:   4153 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.2, ups=0.86, wpb=251.4, bsz=100, num_updates=10450, lr=3.52069e-05, gnorm=0.703, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17688
2023-03-15 00:27:12 - progress_bar.py[line:274] - INFO: epoch 002:   4163 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.9, ups=0.86, wpb=251.4, bsz=100, num_updates=10460, lr=3.51902e-05, gnorm=0.621, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=17700
2023-03-15 00:27:21 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 00:27:24 - progress_bar.py[line:274] - INFO: epoch 002:   4174 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=200.6, ups=0.79, wpb=253.1, bsz=100, num_updates=10470, lr=3.51736e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=13, gb_free=10.3, ema_decay=0.9999, wall=17712
2023-03-15 00:27:36 - progress_bar.py[line:274] - INFO: epoch 002:   4184 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.8, ups=0.86, wpb=254.1, bsz=100, num_updates=10480, lr=3.51569e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17724
2023-03-15 00:27:48 - progress_bar.py[line:274] - INFO: epoch 002:   4194 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.6, ups=0.86, wpb=253.7, bsz=100, num_updates=10490, lr=3.51402e-05, gnorm=0.698, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17735
2023-03-15 00:27:59 - progress_bar.py[line:274] - INFO: epoch 002:   4204 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.4, ups=0.86, wpb=254.2, bsz=100, num_updates=10500, lr=3.51236e-05, gnorm=0.7, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=17747
2023-03-15 00:28:11 - progress_bar.py[line:274] - INFO: epoch 002:   4214 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.7, ups=0.86, wpb=252.1, bsz=100, num_updates=10510, lr=3.51069e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=17759
2023-03-15 00:28:23 - progress_bar.py[line:274] - INFO: epoch 002:   4224 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.5, ups=0.86, wpb=253.7, bsz=100, num_updates=10520, lr=3.50902e-05, gnorm=0.626, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17770
2023-03-15 00:28:34 - progress_bar.py[line:274] - INFO: epoch 002:   4234 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.3, ups=0.86, wpb=254.3, bsz=100, num_updates=10530, lr=3.50735e-05, gnorm=0.61, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=17782
2023-03-15 00:28:46 - progress_bar.py[line:274] - INFO: epoch 002:   4244 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=256.9, nsentences=100, sample_size=256.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.6, ups=0.86, wpb=256.9, bsz=100, num_updates=10540, lr=3.50569e-05, gnorm=0.637, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17794
2023-03-15 00:28:57 - progress_bar.py[line:274] - INFO: epoch 002:   4254 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.5, ups=0.86, wpb=252.5, bsz=100, num_updates=10550, lr=3.50402e-05, gnorm=0.825, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=17805
2023-03-15 00:29:09 - progress_bar.py[line:274] - INFO: epoch 002:   4264 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=222.9, ups=0.88, wpb=254.6, bsz=100, num_updates=10560, lr=3.50235e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=17817
2023-03-15 00:29:20 - progress_bar.py[line:274] - INFO: epoch 002:   4274 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.6, ups=0.86, wpb=252.6, bsz=100, num_updates=10570, lr=3.50068e-05, gnorm=0.701, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=17828
2023-03-15 00:29:32 - progress_bar.py[line:274] - INFO: epoch 002:   4284 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.5, ups=0.86, wpb=255.6, bsz=100, num_updates=10580, lr=3.49902e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17840
2023-03-15 00:29:44 - progress_bar.py[line:274] - INFO: epoch 002:   4294 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.7, ups=0.86, wpb=251.7, bsz=100, num_updates=10590, lr=3.49735e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=17851
2023-03-15 00:29:55 - progress_bar.py[line:274] - INFO: epoch 002:   4304 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.7, ups=0.85, wpb=253.6, bsz=100, num_updates=10600, lr=3.49568e-05, gnorm=0.616, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17863
2023-03-15 00:30:07 - progress_bar.py[line:274] - INFO: epoch 002:   4314 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.2, ups=0.87, wpb=252.2, bsz=100, num_updates=10610, lr=3.49401e-05, gnorm=0.772, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=17875
2023-03-15 00:30:19 - progress_bar.py[line:274] - INFO: epoch 002:   4324 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.8, ups=0.86, wpb=253.1, bsz=100, num_updates=10620, lr=3.49235e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17886
2023-03-15 00:30:30 - progress_bar.py[line:274] - INFO: epoch 002:   4334 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.3, ups=0.86, wpb=252.1, bsz=100, num_updates=10630, lr=3.49068e-05, gnorm=0.724, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17898
2023-03-15 00:30:42 - progress_bar.py[line:274] - INFO: epoch 002:   4344 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.9, ups=0.86, wpb=251.4, bsz=100, num_updates=10640, lr=3.48901e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17910
2023-03-15 00:30:53 - progress_bar.py[line:274] - INFO: epoch 002:   4354 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.1, ups=0.85, wpb=256.4, bsz=100, num_updates=10650, lr=3.48734e-05, gnorm=0.683, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=17921
2023-03-15 00:31:05 - progress_bar.py[line:274] - INFO: epoch 002:   4364 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.378, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217.7, ups=0.87, wpb=250.9, bsz=100, num_updates=10660, lr=3.48568e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=17933
2023-03-15 00:31:16 - progress_bar.py[line:274] - INFO: epoch 002:   4374 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=222.1, ups=0.88, wpb=253.5, bsz=100, num_updates=10670, lr=3.48401e-05, gnorm=0.621, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=17944
2023-03-15 00:31:28 - progress_bar.py[line:274] - INFO: epoch 002:   4384 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.3, ups=0.86, wpb=255, bsz=100, num_updates=10680, lr=3.48234e-05, gnorm=0.633, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=17956
2023-03-15 00:31:40 - progress_bar.py[line:274] - INFO: epoch 002:   4394 / 6313 loss=0.584, loss_v1=0, loss_v2=0, nll_loss=0.372, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.9, ups=0.86, wpb=250.9, bsz=100, num_updates=10690, lr=3.48067e-05, gnorm=0.651, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=17967
2023-03-15 00:31:51 - progress_bar.py[line:274] - INFO: epoch 002:   4404 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.6, ups=0.86, wpb=256.5, bsz=100, num_updates=10700, lr=3.47901e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=17979
2023-03-15 00:32:03 - progress_bar.py[line:274] - INFO: epoch 002:   4414 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.1, ups=0.87, wpb=252.5, bsz=100, num_updates=10710, lr=3.47734e-05, gnorm=0.617, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=17991
2023-03-15 00:32:14 - progress_bar.py[line:274] - INFO: epoch 002:   4424 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.9, ups=0.86, wpb=253.7, bsz=100, num_updates=10720, lr=3.47567e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18002
2023-03-15 00:32:26 - progress_bar.py[line:274] - INFO: epoch 002:   4434 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.8, ups=0.86, wpb=254.6, bsz=100, num_updates=10730, lr=3.47401e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18014
2023-03-15 00:32:38 - progress_bar.py[line:274] - INFO: epoch 002:   4444 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.7, ups=0.85, wpb=252.4, bsz=100, num_updates=10740, lr=3.47234e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=18026
2023-03-15 00:32:49 - progress_bar.py[line:274] - INFO: epoch 002:   4454 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.9, ups=0.86, wpb=252.4, bsz=100, num_updates=10750, lr=3.47067e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18037
2023-03-15 00:33:01 - progress_bar.py[line:274] - INFO: epoch 002:   4464 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.3, ups=0.86, wpb=253.6, bsz=100, num_updates=10760, lr=3.469e-05, gnorm=0.708, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18049
2023-03-15 00:33:13 - progress_bar.py[line:274] - INFO: epoch 002:   4474 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.371, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.5, ups=0.86, wpb=252.6, bsz=100, num_updates=10770, lr=3.46734e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18060
2023-03-15 00:33:24 - progress_bar.py[line:274] - INFO: epoch 002:   4484 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.6, ups=0.87, wpb=254.2, bsz=100, num_updates=10780, lr=3.46567e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=18072
2023-03-15 00:33:36 - progress_bar.py[line:274] - INFO: epoch 002:   4494 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.2, ups=0.86, wpb=254.3, bsz=100, num_updates=10790, lr=3.464e-05, gnorm=0.58, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=18083
2023-03-15 00:33:47 - progress_bar.py[line:274] - INFO: epoch 002:   4504 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221, ups=0.86, wpb=255.9, bsz=100, num_updates=10800, lr=3.46233e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18095
2023-03-15 00:33:59 - progress_bar.py[line:274] - INFO: epoch 002:   4514 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.3, ups=0.86, wpb=254.1, bsz=100, num_updates=10810, lr=3.46067e-05, gnorm=0.612, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=18107
2023-03-15 00:34:10 - progress_bar.py[line:274] - INFO: epoch 002:   4524 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.3, ups=0.87, wpb=255.8, bsz=100, num_updates=10820, lr=3.459e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=18118
2023-03-15 00:34:22 - progress_bar.py[line:274] - INFO: epoch 002:   4534 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.9, ups=0.87, wpb=252.2, bsz=100, num_updates=10830, lr=3.45733e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18130
2023-03-15 00:34:33 - progress_bar.py[line:274] - INFO: epoch 002:   4544 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.3, ups=0.86, wpb=253.6, bsz=100, num_updates=10840, lr=3.45566e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18141
2023-03-15 00:34:45 - progress_bar.py[line:274] - INFO: epoch 002:   4554 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221, ups=0.86, wpb=256.3, bsz=100, num_updates=10850, lr=3.454e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18153
2023-03-15 00:34:57 - progress_bar.py[line:274] - INFO: epoch 002:   4564 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.2, ups=0.85, wpb=252.8, bsz=100, num_updates=10860, lr=3.45233e-05, gnorm=0.649, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18165
2023-03-15 00:35:08 - progress_bar.py[line:274] - INFO: epoch 002:   4574 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.373, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218, ups=0.86, wpb=252.4, bsz=100, num_updates=10870, lr=3.45066e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18176
2023-03-15 00:35:20 - progress_bar.py[line:274] - INFO: epoch 002:   4584 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.3, ups=0.86, wpb=254.6, bsz=100, num_updates=10880, lr=3.44899e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18188
2023-03-15 00:35:32 - progress_bar.py[line:274] - INFO: epoch 002:   4594 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.6, ups=0.87, wpb=252.7, bsz=100, num_updates=10890, lr=3.44733e-05, gnorm=0.602, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=18199
2023-03-15 00:35:43 - progress_bar.py[line:274] - INFO: epoch 002:   4604 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218, ups=0.85, wpb=255.7, bsz=100, num_updates=10900, lr=3.44566e-05, gnorm=0.623, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18211
2023-03-15 00:35:55 - progress_bar.py[line:274] - INFO: epoch 002:   4614 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.3, ups=0.87, wpb=253.7, bsz=100, num_updates=10910, lr=3.44399e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18223
2023-03-15 00:36:07 - progress_bar.py[line:274] - INFO: epoch 002:   4624 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.4, ups=0.87, wpb=252.2, bsz=100, num_updates=10920, lr=3.44233e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=18235
2023-03-15 00:36:18 - progress_bar.py[line:274] - INFO: epoch 002:   4634 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.8, ups=0.86, wpb=253.9, bsz=100, num_updates=10930, lr=3.44066e-05, gnorm=0.685, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18246
2023-03-15 00:36:30 - progress_bar.py[line:274] - INFO: epoch 002:   4644 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=249.7, nsentences=100, sample_size=249.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.1, ups=0.86, wpb=249.7, bsz=100, num_updates=10940, lr=3.43899e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=18258
2023-03-15 00:36:41 - progress_bar.py[line:274] - INFO: epoch 002:   4654 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.9, ups=0.86, wpb=253.1, bsz=100, num_updates=10950, lr=3.43732e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=18269
2023-03-15 00:36:53 - progress_bar.py[line:274] - INFO: epoch 002:   4664 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216.2, ups=0.86, wpb=252.4, bsz=100, num_updates=10960, lr=3.43566e-05, gnorm=0.818, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18281
2023-03-15 00:37:05 - progress_bar.py[line:274] - INFO: epoch 002:   4674 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.3, ups=0.86, wpb=253.4, bsz=100, num_updates=10970, lr=3.43399e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18293
2023-03-15 00:37:17 - progress_bar.py[line:274] - INFO: epoch 002:   4684 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.3, ups=0.85, wpb=255.6, bsz=100, num_updates=10980, lr=3.43232e-05, gnorm=0.672, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18304
2023-03-15 00:37:28 - progress_bar.py[line:274] - INFO: epoch 002:   4694 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.6, ups=0.86, wpb=252.4, bsz=100, num_updates=10990, lr=3.43065e-05, gnorm=0.655, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=18316
2023-03-15 00:37:40 - progress_bar.py[line:274] - INFO: epoch 002:   4704 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.6, ups=0.86, wpb=253.6, bsz=100, num_updates=11000, lr=3.42899e-05, gnorm=0.645, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=18328
2023-03-15 00:37:51 - progress_bar.py[line:274] - INFO: epoch 002:   4714 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=222.3, ups=0.87, wpb=255, bsz=100, num_updates=11010, lr=3.42732e-05, gnorm=0.723, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=18339
2023-03-15 00:38:03 - progress_bar.py[line:274] - INFO: epoch 002:   4724 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.5, ups=0.86, wpb=254.5, bsz=100, num_updates=11020, lr=3.42565e-05, gnorm=0.647, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18351
2023-03-15 00:38:14 - progress_bar.py[line:274] - INFO: epoch 002:   4734 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.7, ups=0.86, wpb=254.1, bsz=100, num_updates=11030, lr=3.42398e-05, gnorm=0.59, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18362
2023-03-15 00:38:26 - progress_bar.py[line:274] - INFO: epoch 002:   4744 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.3, ups=0.86, wpb=255.2, bsz=100, num_updates=11040, lr=3.42232e-05, gnorm=0.655, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18374
2023-03-15 00:38:38 - progress_bar.py[line:274] - INFO: epoch 002:   4754 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.9, ups=0.86, wpb=253.3, bsz=100, num_updates=11050, lr=3.42065e-05, gnorm=0.714, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18385
2023-03-15 00:38:49 - progress_bar.py[line:274] - INFO: epoch 002:   4764 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.6, ups=0.86, wpb=255.7, bsz=100, num_updates=11060, lr=3.41898e-05, gnorm=0.613, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18397
2023-03-15 00:39:01 - progress_bar.py[line:274] - INFO: epoch 002:   4774 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.2, ups=0.86, wpb=253.3, bsz=100, num_updates=11070, lr=3.41731e-05, gnorm=0.668, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=18409
2023-03-15 00:39:13 - progress_bar.py[line:274] - INFO: epoch 002:   4784 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.9, ups=0.86, wpb=251.8, bsz=100, num_updates=11080, lr=3.41565e-05, gnorm=0.759, clip=10, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=18420
2023-03-15 00:39:24 - progress_bar.py[line:274] - INFO: epoch 002:   4794 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=256.6, nsentences=100, sample_size=256.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221.4, ups=0.86, wpb=256.6, bsz=100, num_updates=11090, lr=3.41398e-05, gnorm=0.703, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=18432
2023-03-15 00:39:36 - progress_bar.py[line:274] - INFO: epoch 002:   4804 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.6, ups=0.85, wpb=253.5, bsz=100, num_updates=11100, lr=3.41231e-05, gnorm=0.618, clip=0, loss_scale=1024, train_wall=12, gb_free=9.8, ema_decay=0.9999, wall=18444
2023-03-15 00:39:47 - progress_bar.py[line:274] - INFO: epoch 002:   4814 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.7, ups=0.86, wpb=253.1, bsz=100, num_updates=11110, lr=3.41064e-05, gnorm=0.63, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18455
2023-03-15 00:39:59 - progress_bar.py[line:274] - INFO: epoch 002:   4824 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=215.8, ups=0.86, wpb=251, bsz=100, num_updates=11120, lr=3.40898e-05, gnorm=0.747, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18467
2023-03-15 00:40:11 - progress_bar.py[line:274] - INFO: epoch 002:   4834 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.3, ups=0.87, wpb=252.1, bsz=100, num_updates=11130, lr=3.40731e-05, gnorm=0.634, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=18478
2023-03-15 00:40:20 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 00:40:23 - progress_bar.py[line:274] - INFO: epoch 002:   4845 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=199.2, ups=0.79, wpb=253.7, bsz=100, num_updates=11140, lr=3.40564e-05, gnorm=0.57, clip=0, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=18491
2023-03-15 00:40:35 - progress_bar.py[line:274] - INFO: epoch 002:   4855 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.2, ups=0.86, wpb=251.5, bsz=100, num_updates=11150, lr=3.40398e-05, gnorm=0.63, clip=0, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=18503
2023-03-15 00:40:46 - progress_bar.py[line:274] - INFO: epoch 002:   4865 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.7, ups=0.86, wpb=251.5, bsz=100, num_updates=11160, lr=3.40231e-05, gnorm=0.658, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=18514
2023-03-15 00:40:58 - progress_bar.py[line:274] - INFO: epoch 002:   4875 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.1, ups=0.86, wpb=252.7, bsz=100, num_updates=11170, lr=3.40064e-05, gnorm=0.587, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18526
2023-03-15 00:41:10 - progress_bar.py[line:274] - INFO: epoch 002:   4885 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.2, ups=0.86, wpb=253.1, bsz=100, num_updates=11180, lr=3.39897e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=18537
2023-03-15 00:41:21 - progress_bar.py[line:274] - INFO: epoch 002:   4895 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220, ups=0.86, wpb=254.6, bsz=100, num_updates=11190, lr=3.39731e-05, gnorm=0.671, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18549
2023-03-15 00:41:33 - progress_bar.py[line:274] - INFO: epoch 002:   4905 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=214.5, ups=0.86, wpb=250.8, bsz=100, num_updates=11200, lr=3.39564e-05, gnorm=0.804, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18561
2023-03-15 00:41:45 - progress_bar.py[line:274] - INFO: epoch 002:   4915 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=218.4, ups=0.86, wpb=253.2, bsz=100, num_updates=11210, lr=3.39397e-05, gnorm=0.647, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18572
2023-03-15 00:41:56 - progress_bar.py[line:274] - INFO: epoch 002:   4925 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220, ups=0.87, wpb=252.9, bsz=100, num_updates=11220, lr=3.3923e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=18584
2023-03-15 00:42:07 - progress_bar.py[line:274] - INFO: epoch 002:   4935 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=223.1, ups=0.88, wpb=254.4, bsz=100, num_updates=11230, lr=3.39064e-05, gnorm=0.662, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=18595
2023-03-15 00:42:19 - progress_bar.py[line:274] - INFO: epoch 002:   4945 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.6, ups=0.86, wpb=253.1, bsz=100, num_updates=11240, lr=3.38897e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18607
2023-03-15 00:42:31 - progress_bar.py[line:274] - INFO: epoch 002:   4955 / 6313 loss=0.594, loss_v1=0, loss_v2=0, nll_loss=0.377, ntokens=249.5, nsentences=100, sample_size=249.5, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=219.3, ups=0.88, wpb=249.5, bsz=100, num_updates=11250, lr=3.3873e-05, gnorm=0.772, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18619
2023-03-15 00:42:42 - progress_bar.py[line:274] - INFO: epoch 002:   4965 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.1, ups=0.86, wpb=253.9, bsz=100, num_updates=11260, lr=3.38563e-05, gnorm=0.627, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=18630
2023-03-15 00:42:54 - progress_bar.py[line:274] - INFO: epoch 002:   4975 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.2, ups=0.86, wpb=252.6, bsz=100, num_updates=11270, lr=3.38397e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18642
2023-03-15 00:43:05 - progress_bar.py[line:274] - INFO: epoch 002:   4985 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.8, ups=0.87, wpb=255.2, bsz=100, num_updates=11280, lr=3.3823e-05, gnorm=0.621, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=18653
2023-03-15 00:43:17 - progress_bar.py[line:274] - INFO: epoch 002:   4995 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223.1, ups=0.87, wpb=255.1, bsz=100, num_updates=11290, lr=3.38063e-05, gnorm=0.708, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18665
2023-03-15 00:43:28 - progress_bar.py[line:274] - INFO: epoch 002:   5005 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.8, ups=0.87, wpb=253.4, bsz=100, num_updates=11300, lr=3.37896e-05, gnorm=0.61, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=18676
2023-03-15 00:43:40 - progress_bar.py[line:274] - INFO: epoch 002:   5015 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=226.4, ups=0.89, wpb=255.4, bsz=100, num_updates=11310, lr=3.3773e-05, gnorm=0.603, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18688
2023-03-15 00:43:51 - progress_bar.py[line:274] - INFO: epoch 002:   5025 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215, ups=0.86, wpb=251.4, bsz=100, num_updates=11320, lr=3.37563e-05, gnorm=0.581, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18699
2023-03-15 00:44:03 - progress_bar.py[line:274] - INFO: epoch 002:   5035 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217, ups=0.86, wpb=251.6, bsz=100, num_updates=11330, lr=3.37396e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=18711
2023-03-15 00:44:15 - progress_bar.py[line:274] - INFO: epoch 002:   5045 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.8, ups=0.86, wpb=251, bsz=100, num_updates=11340, lr=3.37229e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=18722
2023-03-15 00:44:26 - progress_bar.py[line:274] - INFO: epoch 002:   5055 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.4, ups=0.87, wpb=252.4, bsz=100, num_updates=11350, lr=3.37063e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18734
2023-03-15 00:44:38 - progress_bar.py[line:274] - INFO: epoch 002:   5065 / 6313 loss=0.583, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=249.6, nsentences=100, sample_size=249.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.4, ups=0.86, wpb=249.6, bsz=100, num_updates=11360, lr=3.36896e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18746
2023-03-15 00:44:49 - progress_bar.py[line:274] - INFO: epoch 002:   5075 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=256.8, nsentences=100, sample_size=256.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=227.7, ups=0.89, wpb=256.8, bsz=100, num_updates=11370, lr=3.36729e-05, gnorm=0.665, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18757
2023-03-15 00:45:01 - progress_bar.py[line:274] - INFO: epoch 002:   5085 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.1, ups=0.87, wpb=254.3, bsz=100, num_updates=11380, lr=3.36563e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=18768
2023-03-15 00:45:12 - progress_bar.py[line:274] - INFO: epoch 002:   5095 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=226.9, ups=0.9, wpb=252.9, bsz=100, num_updates=11390, lr=3.36396e-05, gnorm=0.742, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=18780
2023-03-15 00:45:23 - progress_bar.py[line:274] - INFO: epoch 002:   5105 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.3, ups=0.86, wpb=253.6, bsz=100, num_updates=11400, lr=3.36229e-05, gnorm=0.616, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=18791
2023-03-15 00:45:35 - progress_bar.py[line:274] - INFO: epoch 002:   5115 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.4, ups=0.86, wpb=254, bsz=100, num_updates=11410, lr=3.36062e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18803
2023-03-15 00:45:47 - progress_bar.py[line:274] - INFO: epoch 002:   5125 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.4, ups=0.86, wpb=255.2, bsz=100, num_updates=11420, lr=3.35896e-05, gnorm=0.607, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18814
2023-03-15 00:45:58 - progress_bar.py[line:274] - INFO: epoch 002:   5135 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.4, ups=0.86, wpb=251.7, bsz=100, num_updates=11430, lr=3.35729e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=18826
2023-03-15 00:46:10 - progress_bar.py[line:274] - INFO: epoch 002:   5145 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.3, ups=0.86, wpb=252.7, bsz=100, num_updates=11440, lr=3.35562e-05, gnorm=0.616, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18838
2023-03-15 00:46:21 - progress_bar.py[line:274] - INFO: epoch 002:   5155 / 6313 loss=0.591, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=248.6, nsentences=100, sample_size=248.6, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=214.2, ups=0.86, wpb=248.6, bsz=100, num_updates=11450, lr=3.35395e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=18849
2023-03-15 00:46:33 - progress_bar.py[line:274] - INFO: epoch 002:   5165 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.6, ups=0.88, wpb=251.9, bsz=100, num_updates=11460, lr=3.35229e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=18861
2023-03-15 00:46:44 - progress_bar.py[line:274] - INFO: epoch 002:   5175 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.7, ups=0.87, wpb=254.3, bsz=100, num_updates=11470, lr=3.35062e-05, gnorm=0.621, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=18872
2023-03-15 00:46:56 - progress_bar.py[line:274] - INFO: epoch 002:   5185 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220, ups=0.87, wpb=251.6, bsz=100, num_updates=11480, lr=3.34895e-05, gnorm=0.671, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=18884
2023-03-15 00:47:07 - progress_bar.py[line:274] - INFO: epoch 002:   5195 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220, ups=0.86, wpb=254.9, bsz=100, num_updates=11490, lr=3.34728e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=18895
2023-03-15 00:47:19 - progress_bar.py[line:274] - INFO: epoch 002:   5205 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.3, ups=0.85, wpb=254.2, bsz=100, num_updates=11500, lr=3.34562e-05, gnorm=0.697, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=18907
2023-03-15 00:47:31 - progress_bar.py[line:274] - INFO: epoch 002:   5215 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.8, ups=0.86, wpb=252.6, bsz=100, num_updates=11510, lr=3.34395e-05, gnorm=0.713, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=18919
2023-03-15 00:47:42 - progress_bar.py[line:274] - INFO: epoch 002:   5225 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.5, ups=0.86, wpb=255.2, bsz=100, num_updates=11520, lr=3.34228e-05, gnorm=0.643, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=18930
2023-03-15 00:47:54 - progress_bar.py[line:274] - INFO: epoch 002:   5235 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.7, ups=0.86, wpb=252.4, bsz=100, num_updates=11530, lr=3.34061e-05, gnorm=0.709, clip=20, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=18942
2023-03-15 00:48:05 - progress_bar.py[line:274] - INFO: epoch 002:   5245 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.3, ups=0.86, wpb=255.4, bsz=100, num_updates=11540, lr=3.33895e-05, gnorm=0.616, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18953
2023-03-15 00:48:17 - progress_bar.py[line:274] - INFO: epoch 002:   5255 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.7, ups=0.86, wpb=254, bsz=100, num_updates=11550, lr=3.33728e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=18965
2023-03-15 00:48:29 - progress_bar.py[line:274] - INFO: epoch 002:   5265 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.2, ups=0.86, wpb=252.1, bsz=100, num_updates=11560, lr=3.33561e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18977
2023-03-15 00:48:40 - progress_bar.py[line:274] - INFO: epoch 002:   5275 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.1, ups=0.86, wpb=254.7, bsz=100, num_updates=11570, lr=3.33394e-05, gnorm=0.586, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=18988
2023-03-15 00:48:52 - progress_bar.py[line:274] - INFO: epoch 002:   5285 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.2, ups=0.86, wpb=252.4, bsz=100, num_updates=11580, lr=3.33228e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19000
2023-03-15 00:49:04 - progress_bar.py[line:274] - INFO: epoch 002:   5295 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221, ups=0.86, wpb=255.7, bsz=100, num_updates=11590, lr=3.33061e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=19011
2023-03-15 00:49:15 - progress_bar.py[line:274] - INFO: epoch 002:   5305 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.3, ups=0.86, wpb=252.9, bsz=100, num_updates=11600, lr=3.32894e-05, gnorm=0.634, clip=0, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=19023
2023-03-15 00:49:27 - progress_bar.py[line:274] - INFO: epoch 002:   5315 / 6313 loss=0.588, loss_v1=0, loss_v2=0, nll_loss=0.37, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.1, ups=0.86, wpb=250.2, bsz=100, num_updates=11610, lr=3.32728e-05, gnorm=0.701, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19035
2023-03-15 00:49:38 - progress_bar.py[line:274] - INFO: epoch 002:   5325 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.1, ups=0.86, wpb=254, bsz=100, num_updates=11620, lr=3.32561e-05, gnorm=0.575, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19046
2023-03-15 00:49:50 - progress_bar.py[line:274] - INFO: epoch 002:   5335 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.9, ups=0.86, wpb=254.5, bsz=100, num_updates=11630, lr=3.32394e-05, gnorm=0.69, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=19058
2023-03-15 00:50:01 - progress_bar.py[line:274] - INFO: epoch 002:   5345 / 6313 loss=0.586, loss_v1=0, loss_v2=0, nll_loss=0.375, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=217, ups=0.86, wpb=251.4, bsz=100, num_updates=11640, lr=3.32227e-05, gnorm=0.696, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19069
2023-03-15 00:50:13 - progress_bar.py[line:274] - INFO: epoch 002:   5355 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.4, ups=0.86, wpb=253.9, bsz=100, num_updates=11650, lr=3.32061e-05, gnorm=0.663, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19081
2023-03-15 00:50:25 - progress_bar.py[line:274] - INFO: epoch 002:   5365 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.8, ups=0.86, wpb=254, bsz=100, num_updates=11660, lr=3.31894e-05, gnorm=0.621, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19093
2023-03-15 00:50:28 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 00:50:37 - progress_bar.py[line:274] - INFO: epoch 002:   5376 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=200.9, ups=0.79, wpb=253, bsz=100, num_updates=11670, lr=3.31727e-05, gnorm=0.642, clip=0, loss_scale=512, train_wall=13, gb_free=10.3, ema_decay=0.9999, wall=19105
2023-03-15 00:50:49 - progress_bar.py[line:274] - INFO: epoch 002:   5386 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.2, ups=0.86, wpb=254.5, bsz=100, num_updates=11680, lr=3.3156e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=19117
2023-03-15 00:51:01 - progress_bar.py[line:274] - INFO: epoch 002:   5396 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=213.8, ups=0.85, wpb=250.7, bsz=100, num_updates=11690, lr=3.31394e-05, gnorm=0.67, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19128
2023-03-15 00:51:12 - progress_bar.py[line:274] - INFO: epoch 002:   5406 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.1, ups=0.86, wpb=256.4, bsz=100, num_updates=11700, lr=3.31227e-05, gnorm=0.659, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19140
2023-03-15 00:51:24 - progress_bar.py[line:274] - INFO: epoch 002:   5416 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.6, ups=0.87, wpb=253.8, bsz=100, num_updates=11710, lr=3.3106e-05, gnorm=0.716, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19152
2023-03-15 00:51:35 - progress_bar.py[line:274] - INFO: epoch 002:   5426 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=256.6, nsentences=100, sample_size=256.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=224.8, ups=0.88, wpb=256.6, bsz=100, num_updates=11720, lr=3.30893e-05, gnorm=0.626, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19163
2023-03-15 00:51:47 - progress_bar.py[line:274] - INFO: epoch 002:   5436 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.3, ups=0.86, wpb=252.8, bsz=100, num_updates=11730, lr=3.30727e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19175
2023-03-15 00:51:58 - progress_bar.py[line:274] - INFO: epoch 002:   5446 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=223.2, ups=0.87, wpb=255.4, bsz=100, num_updates=11740, lr=3.3056e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19186
2023-03-15 00:52:10 - progress_bar.py[line:274] - INFO: epoch 002:   5456 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.368, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.8, ups=0.86, wpb=252.3, bsz=100, num_updates=11750, lr=3.30393e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19198
2023-03-15 00:52:21 - progress_bar.py[line:274] - INFO: epoch 002:   5466 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.8, ups=0.87, wpb=251.4, bsz=100, num_updates=11760, lr=3.30226e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19209
2023-03-15 00:52:33 - progress_bar.py[line:274] - INFO: epoch 002:   5476 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217.7, ups=0.86, wpb=252.6, bsz=100, num_updates=11770, lr=3.3006e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19221
2023-03-15 00:52:44 - progress_bar.py[line:274] - INFO: epoch 002:   5486 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.9, ups=0.86, wpb=255.7, bsz=100, num_updates=11780, lr=3.29893e-05, gnorm=0.665, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19232
2023-03-15 00:52:56 - progress_bar.py[line:274] - INFO: epoch 002:   5496 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.5, ups=0.87, wpb=252.1, bsz=100, num_updates=11790, lr=3.29726e-05, gnorm=0.628, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19244
2023-03-15 00:53:07 - progress_bar.py[line:274] - INFO: epoch 002:   5506 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.6, ups=0.86, wpb=252.1, bsz=100, num_updates=11800, lr=3.29559e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19255
2023-03-15 00:53:19 - progress_bar.py[line:274] - INFO: epoch 002:   5516 / 6313 loss=0.585, loss_v1=0, loss_v2=0, nll_loss=0.374, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=213.7, ups=0.85, wpb=251.9, bsz=100, num_updates=11810, lr=3.29393e-05, gnorm=0.659, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19267
2023-03-15 00:53:31 - progress_bar.py[line:274] - INFO: epoch 002:   5526 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.1, ups=0.87, wpb=251.7, bsz=100, num_updates=11820, lr=3.29226e-05, gnorm=0.65, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19279
2023-03-15 00:53:42 - progress_bar.py[line:274] - INFO: epoch 002:   5536 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.5, ups=0.87, wpb=253.4, bsz=100, num_updates=11830, lr=3.29059e-05, gnorm=0.665, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=19290
2023-03-15 00:53:54 - progress_bar.py[line:274] - INFO: epoch 002:   5546 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.1, ups=0.87, wpb=251.6, bsz=100, num_updates=11840, lr=3.28893e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19301
2023-03-15 00:54:05 - progress_bar.py[line:274] - INFO: epoch 002:   5556 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219, ups=0.86, wpb=253.4, bsz=100, num_updates=11850, lr=3.28726e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19313
2023-03-15 00:54:17 - progress_bar.py[line:274] - INFO: epoch 002:   5566 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.7, ups=0.86, wpb=252, bsz=100, num_updates=11860, lr=3.28559e-05, gnorm=0.648, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19325
2023-03-15 00:54:28 - progress_bar.py[line:274] - INFO: epoch 002:   5576 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.5, ups=0.87, wpb=252.5, bsz=100, num_updates=11870, lr=3.28392e-05, gnorm=0.743, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19336
2023-03-15 00:54:40 - progress_bar.py[line:274] - INFO: epoch 002:   5586 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.1, ups=0.86, wpb=254.3, bsz=100, num_updates=11880, lr=3.28226e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=19348
2023-03-15 00:54:52 - progress_bar.py[line:274] - INFO: epoch 002:   5596 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.7, ups=0.86, wpb=250.9, bsz=100, num_updates=11890, lr=3.28059e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19359
2023-03-15 00:55:03 - progress_bar.py[line:274] - INFO: epoch 002:   5606 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.8, ups=0.86, wpb=253.4, bsz=100, num_updates=11900, lr=3.27892e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19371
2023-03-15 00:55:15 - progress_bar.py[line:274] - INFO: epoch 002:   5616 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.356, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.5, ups=0.86, wpb=252.9, bsz=100, num_updates=11910, lr=3.27725e-05, gnorm=0.655, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=19383
2023-03-15 00:55:26 - progress_bar.py[line:274] - INFO: epoch 002:   5626 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.6, ups=0.87, wpb=252.1, bsz=100, num_updates=11920, lr=3.27559e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19394
2023-03-15 00:55:38 - progress_bar.py[line:274] - INFO: epoch 002:   5636 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.5, ups=0.85, wpb=254.5, bsz=100, num_updates=11930, lr=3.27392e-05, gnorm=0.625, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19406
2023-03-15 00:55:50 - progress_bar.py[line:274] - INFO: epoch 002:   5646 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.2, ups=0.86, wpb=255.8, bsz=100, num_updates=11940, lr=3.27225e-05, gnorm=0.709, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19417
2023-03-15 00:56:01 - progress_bar.py[line:274] - INFO: epoch 002:   5656 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217, ups=0.86, wpb=251.5, bsz=100, num_updates=11950, lr=3.27058e-05, gnorm=0.64, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=19429
2023-03-15 00:56:13 - progress_bar.py[line:274] - INFO: epoch 002:   5666 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=222.3, ups=0.88, wpb=254, bsz=100, num_updates=11960, lr=3.26892e-05, gnorm=0.633, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19441
2023-03-15 00:56:24 - progress_bar.py[line:274] - INFO: epoch 002:   5676 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.2, ups=0.86, wpb=252.8, bsz=100, num_updates=11970, lr=3.26725e-05, gnorm=0.582, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19452
2023-03-15 00:56:36 - progress_bar.py[line:274] - INFO: epoch 002:   5686 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=256.6, nsentences=100, sample_size=256.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.2, ups=0.86, wpb=256.6, bsz=100, num_updates=11980, lr=3.26558e-05, gnorm=0.647, clip=10, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=19464
2023-03-15 00:56:47 - progress_bar.py[line:274] - INFO: epoch 002:   5696 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=258, nsentences=100, sample_size=258, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=223, ups=0.86, wpb=258, bsz=100, num_updates=11990, lr=3.26391e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19475
2023-03-15 00:56:59 - progress_bar.py[line:274] - INFO: epoch 002:   5706 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.4, ups=0.87, wpb=253.6, bsz=100, num_updates=12000, lr=3.26225e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=19487
2023-03-15 00:57:10 - progress_bar.py[line:274] - INFO: epoch 002:   5716 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221.6, ups=0.88, wpb=252.9, bsz=100, num_updates=12010, lr=3.26058e-05, gnorm=0.691, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=19498
2023-03-15 00:57:22 - progress_bar.py[line:274] - INFO: epoch 002:   5726 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.8, ups=0.87, wpb=253.9, bsz=100, num_updates=12020, lr=3.25891e-05, gnorm=0.717, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19510
2023-03-15 00:57:33 - progress_bar.py[line:274] - INFO: epoch 002:   5736 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220, ups=0.86, wpb=254.4, bsz=100, num_updates=12030, lr=3.25724e-05, gnorm=0.652, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=19521
2023-03-15 00:57:45 - progress_bar.py[line:274] - INFO: epoch 002:   5746 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.1, ups=0.86, wpb=254.1, bsz=100, num_updates=12040, lr=3.25558e-05, gnorm=0.635, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=19533
2023-03-15 00:57:57 - progress_bar.py[line:274] - INFO: epoch 002:   5756 / 6313 loss=0.578, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=214.7, ups=0.85, wpb=253.1, bsz=100, num_updates=12050, lr=3.25391e-05, gnorm=0.692, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=19545
2023-03-15 00:58:08 - progress_bar.py[line:274] - INFO: epoch 002:   5766 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222.8, ups=0.88, wpb=254.5, bsz=100, num_updates=12060, lr=3.25224e-05, gnorm=0.632, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19556
2023-03-15 00:58:20 - progress_bar.py[line:274] - INFO: epoch 002:   5776 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220, ups=0.86, wpb=254.9, bsz=100, num_updates=12070, lr=3.25058e-05, gnorm=0.706, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19568
2023-03-15 00:58:31 - progress_bar.py[line:274] - INFO: epoch 002:   5786 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219, ups=0.87, wpb=250.5, bsz=100, num_updates=12080, lr=3.24891e-05, gnorm=0.614, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19579
2023-03-15 00:58:43 - progress_bar.py[line:274] - INFO: epoch 002:   5796 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222.5, ups=0.88, wpb=254.2, bsz=100, num_updates=12090, lr=3.24724e-05, gnorm=0.591, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19591
2023-03-15 00:58:54 - progress_bar.py[line:274] - INFO: epoch 002:   5806 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222.4, ups=0.88, wpb=253.6, bsz=100, num_updates=12100, lr=3.24557e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19602
2023-03-15 00:59:06 - progress_bar.py[line:274] - INFO: epoch 002:   5816 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.3, ups=0.85, wpb=252, bsz=100, num_updates=12110, lr=3.24391e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=19614
2023-03-15 00:59:17 - progress_bar.py[line:274] - INFO: epoch 002:   5826 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.8, ups=0.87, wpb=253.6, bsz=100, num_updates=12120, lr=3.24224e-05, gnorm=0.637, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19625
2023-03-15 00:59:29 - progress_bar.py[line:274] - INFO: epoch 002:   5836 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.1, ups=0.86, wpb=252.1, bsz=100, num_updates=12130, lr=3.24057e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=19637
2023-03-15 00:59:41 - progress_bar.py[line:274] - INFO: epoch 002:   5846 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.6, ups=0.86, wpb=252.4, bsz=100, num_updates=12140, lr=3.2389e-05, gnorm=0.627, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19649
2023-03-15 00:59:52 - progress_bar.py[line:274] - INFO: epoch 002:   5856 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.6, ups=0.86, wpb=254.1, bsz=100, num_updates=12150, lr=3.23724e-05, gnorm=0.659, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19660
2023-03-15 01:00:04 - progress_bar.py[line:274] - INFO: epoch 002:   5866 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223, ups=0.88, wpb=252.3, bsz=100, num_updates=12160, lr=3.23557e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=19671
2023-03-15 01:00:15 - progress_bar.py[line:274] - INFO: epoch 002:   5876 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218, ups=0.86, wpb=252.3, bsz=100, num_updates=12170, lr=3.2339e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19683
2023-03-15 01:00:27 - progress_bar.py[line:274] - INFO: epoch 002:   5886 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220, ups=0.87, wpb=254.2, bsz=100, num_updates=12180, lr=3.23223e-05, gnorm=0.724, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19695
2023-03-15 01:00:38 - progress_bar.py[line:274] - INFO: epoch 002:   5896 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.4, ups=0.86, wpb=251.5, bsz=100, num_updates=12190, lr=3.23057e-05, gnorm=0.718, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19706
2023-03-15 01:00:49 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 01:00:51 - progress_bar.py[line:274] - INFO: epoch 002:   5907 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=202.7, ups=0.8, wpb=253.1, bsz=100, num_updates=12200, lr=3.2289e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=19719
2023-03-15 01:01:02 - progress_bar.py[line:274] - INFO: epoch 002:   5917 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=224.1, ups=0.89, wpb=252.8, bsz=100, num_updates=12210, lr=3.22723e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19730
2023-03-15 01:01:14 - progress_bar.py[line:274] - INFO: epoch 002:   5927 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=223.2, ups=0.88, wpb=254.7, bsz=100, num_updates=12220, lr=3.22556e-05, gnorm=0.618, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=19741
2023-03-15 01:01:25 - progress_bar.py[line:274] - INFO: epoch 002:   5937 / 6313 loss=0.581, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=214.4, ups=0.86, wpb=250.5, bsz=100, num_updates=12230, lr=3.2239e-05, gnorm=0.719, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=19753
2023-03-15 01:01:37 - progress_bar.py[line:274] - INFO: epoch 002:   5947 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=224.4, ups=0.88, wpb=255.8, bsz=100, num_updates=12240, lr=3.22223e-05, gnorm=0.625, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19765
2023-03-15 01:01:48 - progress_bar.py[line:274] - INFO: epoch 002:   5957 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.363, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=220.4, ups=0.86, wpb=255.3, bsz=100, num_updates=12250, lr=3.22056e-05, gnorm=0.69, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=19776
2023-03-15 01:02:00 - progress_bar.py[line:274] - INFO: epoch 002:   5967 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=257.1, nsentences=100, sample_size=257.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.6, ups=0.86, wpb=257.1, bsz=100, num_updates=12260, lr=3.21889e-05, gnorm=0.58, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=19788
2023-03-15 01:02:11 - progress_bar.py[line:274] - INFO: epoch 002:   5977 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=221.1, ups=0.87, wpb=253.6, bsz=100, num_updates=12270, lr=3.21723e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=19799
2023-03-15 01:02:23 - progress_bar.py[line:274] - INFO: epoch 002:   5987 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.8, ups=0.88, wpb=252.2, bsz=100, num_updates=12280, lr=3.21556e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19811
2023-03-15 01:02:34 - progress_bar.py[line:274] - INFO: epoch 002:   5997 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.369, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.7, ups=0.85, wpb=252.6, bsz=100, num_updates=12290, lr=3.21389e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19822
2023-03-15 01:02:46 - progress_bar.py[line:274] - INFO: epoch 002:   6007 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.3, ups=0.86, wpb=251.7, bsz=100, num_updates=12300, lr=3.21223e-05, gnorm=0.637, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=19834
2023-03-15 01:02:58 - progress_bar.py[line:274] - INFO: epoch 002:   6017 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.2, ups=0.86, wpb=252.3, bsz=100, num_updates=12310, lr=3.21056e-05, gnorm=0.715, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19846
2023-03-15 01:03:09 - progress_bar.py[line:274] - INFO: epoch 002:   6027 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221, ups=0.87, wpb=253.2, bsz=100, num_updates=12320, lr=3.20889e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=19857
2023-03-15 01:03:21 - progress_bar.py[line:274] - INFO: epoch 002:   6037 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.2, ups=0.86, wpb=251.4, bsz=100, num_updates=12330, lr=3.20722e-05, gnorm=0.641, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=19869
2023-03-15 01:03:32 - progress_bar.py[line:274] - INFO: epoch 002:   6047 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.8, ups=0.86, wpb=253.1, bsz=100, num_updates=12340, lr=3.20556e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19880
2023-03-15 01:03:44 - progress_bar.py[line:274] - INFO: epoch 002:   6057 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.7, ups=0.86, wpb=252.6, bsz=100, num_updates=12350, lr=3.20389e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19892
2023-03-15 01:03:55 - progress_bar.py[line:274] - INFO: epoch 002:   6067 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219, ups=0.86, wpb=253.5, bsz=100, num_updates=12360, lr=3.20222e-05, gnorm=0.613, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19903
2023-03-15 01:04:07 - progress_bar.py[line:274] - INFO: epoch 002:   6077 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222.1, ups=0.88, wpb=253.6, bsz=100, num_updates=12370, lr=3.20055e-05, gnorm=0.623, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=19915
2023-03-15 01:04:18 - progress_bar.py[line:274] - INFO: epoch 002:   6087 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.9, ups=0.86, wpb=253.5, bsz=100, num_updates=12380, lr=3.19889e-05, gnorm=0.67, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=19926
2023-03-15 01:04:30 - progress_bar.py[line:274] - INFO: epoch 002:   6097 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=256.7, nsentences=100, sample_size=256.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222, ups=0.86, wpb=256.7, bsz=100, num_updates=12390, lr=3.19722e-05, gnorm=0.628, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19938
2023-03-15 01:04:42 - progress_bar.py[line:274] - INFO: epoch 002:   6107 / 6313 loss=0.574, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218, ups=0.86, wpb=252.5, bsz=100, num_updates=12400, lr=3.19555e-05, gnorm=0.661, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=19949
2023-03-15 01:04:53 - progress_bar.py[line:274] - INFO: epoch 002:   6117 / 6313 loss=0.598, loss_v1=0, loss_v2=0, nll_loss=0.385, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.31, wps=214.2, ups=0.86, wpb=250.4, bsz=100, num_updates=12410, lr=3.19388e-05, gnorm=0.724, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19961
2023-03-15 01:05:05 - progress_bar.py[line:274] - INFO: epoch 002:   6127 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=219.4, ups=0.86, wpb=254.3, bsz=100, num_updates=12420, lr=3.19222e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=19973
2023-03-15 01:05:17 - progress_bar.py[line:274] - INFO: epoch 002:   6137 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.2, ups=0.85, wpb=255.9, bsz=100, num_updates=12430, lr=3.19055e-05, gnorm=0.68, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19984
2023-03-15 01:05:28 - progress_bar.py[line:274] - INFO: epoch 002:   6147 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=257.7, nsentences=100, sample_size=257.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.5, ups=0.86, wpb=257.7, bsz=100, num_updates=12440, lr=3.18888e-05, gnorm=0.651, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=19996
2023-03-15 01:05:40 - progress_bar.py[line:274] - INFO: epoch 002:   6157 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222.6, ups=0.87, wpb=254.9, bsz=100, num_updates=12450, lr=3.18721e-05, gnorm=0.636, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=20008
2023-03-15 01:05:51 - progress_bar.py[line:274] - INFO: epoch 002:   6167 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=222.5, ups=0.87, wpb=254.7, bsz=100, num_updates=12460, lr=3.18555e-05, gnorm=0.745, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20019
2023-03-15 01:06:03 - progress_bar.py[line:274] - INFO: epoch 002:   6177 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.3, ups=0.86, wpb=255.1, bsz=100, num_updates=12470, lr=3.18388e-05, gnorm=0.717, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=20031
2023-03-15 01:06:14 - progress_bar.py[line:274] - INFO: epoch 002:   6187 / 6313 loss=0.592, loss_v1=0, loss_v2=0, nll_loss=0.379, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.3, wps=216, ups=0.86, wpb=250.7, bsz=100, num_updates=12480, lr=3.18221e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=20042
2023-03-15 01:06:26 - progress_bar.py[line:274] - INFO: epoch 002:   6197 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=257.2, nsentences=100, sample_size=257.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.6, ups=0.86, wpb=257.2, bsz=100, num_updates=12490, lr=3.18054e-05, gnorm=0.604, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=20054
2023-03-15 01:06:38 - progress_bar.py[line:274] - INFO: epoch 002:   6207 / 6313 loss=0.58, loss_v1=0, loss_v2=0, nll_loss=0.365, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=216.3, ups=0.86, wpb=250.6, bsz=100, num_updates=12500, lr=3.17888e-05, gnorm=0.695, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=20065
2023-03-15 01:06:49 - progress_bar.py[line:274] - INFO: epoch 002:   6217 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.5, ups=0.86, wpb=256.5, bsz=100, num_updates=12510, lr=3.17721e-05, gnorm=0.6, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=20077
2023-03-15 01:07:01 - progress_bar.py[line:274] - INFO: epoch 002:   6227 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=221.6, ups=0.87, wpb=253.3, bsz=100, num_updates=12520, lr=3.17554e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=20089
2023-03-15 01:07:12 - progress_bar.py[line:274] - INFO: epoch 002:   6237 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=249.6, nsentences=100, sample_size=249.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=214.8, ups=0.86, wpb=249.6, bsz=100, num_updates=12530, lr=3.17388e-05, gnorm=0.73, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=20100
2023-03-15 01:07:24 - progress_bar.py[line:274] - INFO: epoch 002:   6247 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.358, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.8, ups=0.87, wpb=252.7, bsz=100, num_updates=12540, lr=3.17221e-05, gnorm=0.656, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=20112
2023-03-15 01:07:36 - progress_bar.py[line:274] - INFO: epoch 002:   6257 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.9, ups=0.85, wpb=255, bsz=100, num_updates=12550, lr=3.17054e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=20123
2023-03-15 01:07:47 - progress_bar.py[line:274] - INFO: epoch 002:   6267 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.5, ups=0.86, wpb=255.2, bsz=100, num_updates=12560, lr=3.16887e-05, gnorm=0.639, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=20135
2023-03-15 01:07:59 - progress_bar.py[line:274] - INFO: epoch 002:   6277 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.9, ups=0.87, wpb=252.7, bsz=100, num_updates=12570, lr=3.16721e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=20147
2023-03-15 01:08:10 - progress_bar.py[line:274] - INFO: epoch 002:   6287 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=223.9, ups=0.88, wpb=255.8, bsz=100, num_updates=12580, lr=3.16554e-05, gnorm=0.639, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=20158
2023-03-15 01:08:22 - progress_bar.py[line:274] - INFO: epoch 002:   6297 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.6, ups=0.86, wpb=255.4, bsz=100, num_updates=12590, lr=3.16387e-05, gnorm=0.648, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=20170
2023-03-15 01:08:33 - progress_bar.py[line:274] - INFO: epoch 002:   6307 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.6, ups=0.86, wpb=253.4, bsz=100, num_updates=12600, lr=3.1622e-05, gnorm=0.69, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=20181
2023-03-15 01:08:40 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-03-15 01:08:42 - train.py[line:549] - INFO: 0 / 7052
2023-03-15 01:08:42 - train.py[line:551] - INFO: load:1.71 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-03-15 01:11:22 - train.py[line:549] - INFO: 200 / 7052
2023-03-15 01:11:22 - train.py[line:551] - INFO: load:1.73 valid_run:159.27 task_valid:148.00 collect_output:10.16
2023-03-15 01:13:56 - train.py[line:549] - INFO: 400 / 7052
2023-03-15 01:13:56 - train.py[line:551] - INFO: load:1.75 valid_run:313.98 task_valid:295.72 collect_output:16.06
2023-03-15 01:16:31 - train.py[line:549] - INFO: 600 / 7052
2023-03-15 01:16:31 - train.py[line:551] - INFO: load:1.78 valid_run:468.92 task_valid:441.64 collect_output:23.99
2023-03-15 01:19:10 - train.py[line:549] - INFO: 800 / 7052
2023-03-15 01:19:10 - train.py[line:551] - INFO: load:1.80 valid_run:628.00 task_valid:587.16 collect_output:36.48
2023-03-15 01:21:49 - train.py[line:549] - INFO: 1000 / 7052
2023-03-15 01:21:49 - train.py[line:551] - INFO: load:1.82 valid_run:786.58 task_valid:734.77 collect_output:46.37
2023-03-15 01:24:25 - train.py[line:549] - INFO: 1200 / 7052
2023-03-15 01:24:25 - train.py[line:551] - INFO: load:1.85 valid_run:942.57 task_valid:882.98 collect_output:53.10
2023-03-15 01:27:03 - train.py[line:549] - INFO: 1400 / 7052
2023-03-15 01:27:03 - train.py[line:551] - INFO: load:1.87 valid_run:1100.02 task_valid:1033.46 collect_output:58.99
2023-03-15 01:29:38 - train.py[line:549] - INFO: 1600 / 7052
2023-03-15 01:29:38 - train.py[line:551] - INFO: load:1.89 valid_run:1255.40 task_valid:1182.23 collect_output:64.54
2023-03-15 01:32:17 - train.py[line:549] - INFO: 1800 / 7052
2023-03-15 01:32:17 - train.py[line:551] - INFO: load:1.92 valid_run:1414.49 task_valid:1332.80 collect_output:72.01
2023-03-15 01:34:53 - train.py[line:549] - INFO: 2000 / 7052
2023-03-15 01:34:53 - train.py[line:551] - INFO: load:1.94 valid_run:1570.65 task_valid:1481.87 collect_output:78.08
2023-03-15 01:37:29 - train.py[line:549] - INFO: 2200 / 7052
2023-03-15 01:37:29 - train.py[line:551] - INFO: load:1.96 valid_run:1725.87 task_valid:1631.75 collect_output:82.34
2023-03-15 01:40:04 - train.py[line:549] - INFO: 2400 / 7052
2023-03-15 01:40:04 - train.py[line:551] - INFO: load:1.98 valid_run:1880.78 task_valid:1777.70 collect_output:90.25
2023-03-15 01:42:40 - train.py[line:549] - INFO: 2600 / 7052
2023-03-15 01:42:40 - train.py[line:551] - INFO: load:2.01 valid_run:2036.97 task_valid:1925.11 collect_output:97.96
2023-03-15 01:45:16 - train.py[line:549] - INFO: 2800 / 7052
2023-03-15 01:45:16 - train.py[line:551] - INFO: load:2.03 valid_run:2193.44 task_valid:2072.26 collect_output:106.21
2023-03-15 01:47:51 - train.py[line:549] - INFO: 3000 / 7052
2023-03-15 01:47:51 - train.py[line:551] - INFO: load:2.05 valid_run:2347.74 task_valid:2218.68 collect_output:113.02
2023-03-15 01:50:28 - train.py[line:549] - INFO: 3200 / 7052
2023-03-15 01:50:28 - train.py[line:551] - INFO: load:2.07 valid_run:2504.91 task_valid:2368.29 collect_output:119.51
2023-03-15 01:53:03 - train.py[line:549] - INFO: 3400 / 7052
2023-03-15 01:53:03 - train.py[line:551] - INFO: load:2.10 valid_run:2659.87 task_valid:2514.95 collect_output:126.76
2023-03-15 01:55:38 - train.py[line:549] - INFO: 3600 / 7052
2023-03-15 01:55:38 - train.py[line:551] - INFO: load:2.12 valid_run:2814.75 task_valid:2661.88 collect_output:133.65
2023-03-15 01:58:15 - train.py[line:549] - INFO: 3800 / 7052
2023-03-15 01:58:15 - train.py[line:551] - INFO: load:2.14 valid_run:2971.42 task_valid:2812.09 collect_output:139.04
2023-03-15 02:00:49 - train.py[line:549] - INFO: 4000 / 7052
2023-03-15 02:00:49 - train.py[line:551] - INFO: load:2.16 valid_run:3125.39 task_valid:2957.34 collect_output:146.68
2023-03-15 02:03:24 - train.py[line:549] - INFO: 4200 / 7052
2023-03-15 02:03:24 - train.py[line:551] - INFO: load:2.19 valid_run:3280.96 task_valid:3102.47 collect_output:156.07
2023-03-15 02:05:59 - train.py[line:549] - INFO: 4400 / 7052
2023-03-15 02:05:59 - train.py[line:551] - INFO: load:2.21 valid_run:3435.72 task_valid:3252.40 collect_output:159.84
2023-03-15 02:08:35 - train.py[line:549] - INFO: 4600 / 7052
2023-03-15 02:08:35 - train.py[line:551] - INFO: load:2.23 valid_run:3591.38 task_valid:3398.10 collect_output:168.75
2023-03-15 02:11:11 - train.py[line:549] - INFO: 4800 / 7052
2023-03-15 02:11:11 - train.py[line:551] - INFO: load:2.25 valid_run:3747.45 task_valid:3542.41 collect_output:179.45
2023-03-15 02:13:47 - train.py[line:549] - INFO: 5000 / 7052
2023-03-15 02:13:47 - train.py[line:551] - INFO: load:2.28 valid_run:3903.48 task_valid:3689.55 collect_output:187.27
2023-03-15 02:16:24 - train.py[line:549] - INFO: 5200 / 7052
2023-03-15 02:16:24 - train.py[line:551] - INFO: load:2.30 valid_run:4060.19 task_valid:3840.96 collect_output:191.53
2023-03-15 02:19:00 - train.py[line:549] - INFO: 5400 / 7052
2023-03-15 02:19:00 - train.py[line:551] - INFO: load:2.33 valid_run:4216.21 task_valid:3985.31 collect_output:202.16
2023-03-15 02:21:35 - train.py[line:549] - INFO: 5600 / 7052
2023-03-15 02:21:35 - train.py[line:551] - INFO: load:2.35 valid_run:4371.51 task_valid:4130.18 collect_output:211.51
2023-03-15 02:24:12 - train.py[line:549] - INFO: 5800 / 7052
2023-03-15 02:24:12 - train.py[line:551] - INFO: load:2.37 valid_run:4527.72 task_valid:4276.67 collect_output:220.17
2023-03-15 02:26:47 - train.py[line:549] - INFO: 6000 / 7052
2023-03-15 02:26:47 - train.py[line:551] - INFO: load:2.40 valid_run:4683.23 task_valid:4423.84 collect_output:227.48
2023-03-15 02:29:24 - train.py[line:549] - INFO: 6200 / 7052
2023-03-15 02:29:24 - train.py[line:551] - INFO: load:2.42 valid_run:4840.07 task_valid:4570.85 collect_output:236.26
2023-03-15 02:32:01 - train.py[line:549] - INFO: 6400 / 7052
2023-03-15 02:32:01 - train.py[line:551] - INFO: load:2.44 valid_run:4997.34 task_valid:4715.90 collect_output:247.40
2023-03-15 02:34:38 - train.py[line:549] - INFO: 6600 / 7052
2023-03-15 02:34:38 - train.py[line:551] - INFO: load:2.47 valid_run:5153.91 task_valid:4866.62 collect_output:252.23
2023-03-15 02:37:15 - train.py[line:549] - INFO: 6800 / 7052
2023-03-15 02:37:15 - train.py[line:551] - INFO: load:2.49 valid_run:5310.51 task_valid:5015.51 collect_output:258.90
2023-03-15 02:39:51 - train.py[line:549] - INFO: 7000 / 7052
2023-03-15 02:39:51 - train.py[line:551] - INFO: load:2.51 valid_run:5466.73 task_valid:5165.54 collect_output:264.04

====================================================================================================
SGG eval:     R @ 50: 0.6438;     R @ 100: 0.6623;     R @ 500: 0.6683;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2156;    mR @ 100: 0.2305;    mR @ 500: 0.2363;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0791) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3526) (attached to:0.0000) (behind:0.3667) (belonging to:0.0000) (between:0.0000) (carrying:0.7560) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0577) (has:0.7479) (holding:0.3256) (in:0.3438) (in front of:0.1884) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5446) (of:0.4213) (on:0.9026) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.0476) (part of:0.0000) (playing:0.0000) (riding:0.6667) (says:0.0000) (sitting on:0.3497) (standing on:0.0000) (to:0.0000) (under:0.3796) (using:1.0000) (walking in:0.0000) (walking on:0.4225) (watching:0.3333) (wearing:0.9879) (wears:0.0000) (with:0.1659) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6438;     R @ 100: 0.6623;     R @ 500: 0.6683;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2156;    mR @ 100: 0.2305;    mR @ 500: 0.2363;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0791) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3526) (attached to:0.0000) (behind:0.3667) (belonging to:0.0000) (between:0.0000) (carrying:0.7560) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0577) (has:0.7479) (holding:0.3256) (in:0.3438) (in front of:0.1884) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5446) (of:0.4213) (on:0.9026) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.0476) (part of:0.0000) (playing:0.0000) (riding:0.6667) (says:0.0000) (sitting on:0.3497) (standing on:0.0000) (to:0.0000) (under:0.3796) (using:1.0000) (walking in:0.0000) (walking on:0.4225) (watching:0.3333) (wearing:0.9879) (wears:0.0000) (with:0.1659) 
--------------------------------------------------------
====================================================================================================

2023-03-15 02:40:53 - train.py[line:487] - INFO: 0.6622944497936782

====================================================================================================
SGG eval:     R @ 50: 0.6438;     R @ 100: 0.6623;     R @ 500: 0.6683;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2156;    mR @ 100: 0.2305;    mR @ 500: 0.2363;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0791) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3526) (attached to:0.0000) (behind:0.3667) (belonging to:0.0000) (between:0.0000) (carrying:0.7560) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0577) (has:0.7479) (holding:0.3256) (in:0.3438) (in front of:0.1884) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5446) (of:0.4213) (on:0.9026) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.0476) (part of:0.0000) (playing:0.0000) (riding:0.6667) (says:0.0000) (sitting on:0.3497) (standing on:0.0000) (to:0.0000) (under:0.3796) (using:1.0000) (walking in:0.0000) (walking on:0.4225) (watching:0.3333) (wearing:0.9879) (wears:0.0000) (with:0.1659) 
--------------------------------------------------------
====================================================================================================

2023-03-15 02:40:53 - train.py[line:578] - INFO: logits:torch.Size([282060, 51]) sample_ids:torch.Size([282060])

====================================================================================================
SGG eval:     R @ 50: 0.6438;     R @ 100: 0.6623;     R @ 500: 0.6683;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2156;    mR @ 100: 0.2305;    mR @ 500: 0.2363;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0791) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3526) (attached to:0.0000) (behind:0.3667) (belonging to:0.0000) (between:0.0000) (carrying:0.7560) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0577) (has:0.7479) (holding:0.3256) (in:0.3438) (in front of:0.1884) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5446) (of:0.4213) (on:0.9026) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.0476) (part of:0.0000) (playing:0.0000) (riding:0.6667) (says:0.0000) (sitting on:0.3497) (standing on:0.0000) (to:0.0000) (under:0.3796) (using:1.0000) (walking in:0.0000) (walking on:0.4225) (watching:0.3333) (wearing:0.9879) (wears:0.0000) (with:0.1659) 
--------------------------------------------------------
====================================================================================================

2023-03-15 02:40:54 - progress_bar.py[line:282] - INFO: epoch 002 | valid on 'valid' subset | loss 0.309 | loss_v1 0 | loss_v2 0 | nll_loss 0.115 | ntokens 119.044 | nsentences 39.997 | sample_size 119.044 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.662294 | ppl 1.08 | vqa_score 0.6121 | wps 151.8 | wpb 119 | bsz 40 | num_updates 12606 | best_R@100 0.662294
2023-03-15 02:40:54 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 2 @ 12606 updates
2023-03-15 02:40:54 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint2.pt

====================================================================================================
SGG eval:     R @ 50: 0.6438;     R @ 100: 0.6623;     R @ 500: 0.6683;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2156;    mR @ 100: 0.2305;    mR @ 500: 0.2363;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.0791) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3526) (attached to:0.0000) (behind:0.3667) (belonging to:0.0000) (between:0.0000) (carrying:0.7560) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0577) (has:0.7479) (holding:0.3256) (in:0.3438) (in front of:0.1884) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5446) (of:0.4213) (on:0.9026) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.0476) (part of:0.0000) (playing:0.0000) (riding:0.6667) (says:0.0000) (sitting on:0.3497) (standing on:0.0000) (to:0.0000) (under:0.3796) (using:1.0000) (walking in:0.0000) (walking on:0.4225) (watching:0.3333) (wearing:0.9879) (wears:0.0000) (with:0.1659) 
--------------------------------------------------------
====================================================================================================

file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv slice_id 3 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv slice_id 2 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv slice_id 4 row count 126256 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv slice_id 1 row count 126257 total row count 631284
2023-03-15 02:41:03 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint2.pt
2023-03-15 02:41:10 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint2.pt (epoch 2 @ 12606 updates, score 0.6622944497936782) (writing took 16.454303162172437 seconds)
2023-03-15 02:41:10 - train.py[line:339] - INFO: end of epoch 2 (average epoch stats below)
2023-03-15 02:41:10 - progress_bar.py[line:282] - INFO: epoch 002 | loss 0.564 | loss_v1 0 | loss_v2 0 | nll_loss 0.351 | ntokens 253.356 | nsentences 99.997 | sample_size 253.356 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.28 | wps 124.1 | ups 0.49 | wpb 253.4 | bsz 100 | num_updates 12606 | lr 3.1612e-05 | gnorm 0.678 | clip 2.1 | loss_scale 512 | train_wall 7278 | gb_free 14.2 | ema_decay 0.9999 | wall 25738
2023-03-15 02:41:10 - trainer.py[line:694] - INFO: loading train data for epoch 3
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv slice_id 0 row count 126257 total row count 631284
2023-03-15 02:41:11 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-03-15 02:41:11 - trainer.py[line:758] - INFO: begin training epoch 3
2023-03-15 02:41:11 - train.py[line:312] - INFO: Start iterating over samples
2023-03-15 02:41:18 - progress_bar.py[line:274] - INFO: epoch 003:      4 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=247.4, nsentences=98.4, sample_size=247.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=0.4, ups=0, wpb=247.4, bsz=98.4, num_updates=12610, lr=3.16054e-05, gnorm=0.611, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=25746
2023-03-15 02:41:30 - progress_bar.py[line:274] - INFO: epoch 003:     14 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215.5, ups=0.86, wpb=251.6, bsz=100, num_updates=12620, lr=3.15887e-05, gnorm=0.645, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=25757
2023-03-15 02:41:41 - progress_bar.py[line:274] - INFO: epoch 003:     24 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.2, ups=0.87, wpb=255.7, bsz=100, num_updates=12630, lr=3.1572e-05, gnorm=0.631, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=25769
2023-03-15 02:41:53 - progress_bar.py[line:274] - INFO: epoch 003:     34 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.9, ups=0.86, wpb=252.3, bsz=100, num_updates=12640, lr=3.15553e-05, gnorm=0.619, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=25781
2023-03-15 02:42:04 - progress_bar.py[line:274] - INFO: epoch 003:     44 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215, ups=0.86, wpb=250.1, bsz=100, num_updates=12650, lr=3.15387e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=25792
2023-03-15 02:42:16 - progress_bar.py[line:274] - INFO: epoch 003:     54 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.4, ups=0.86, wpb=253.6, bsz=100, num_updates=12660, lr=3.1522e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=25804
2023-03-15 02:42:28 - progress_bar.py[line:274] - INFO: epoch 003:     64 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.3, ups=0.86, wpb=254.7, bsz=100, num_updates=12670, lr=3.15053e-05, gnorm=0.757, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=25816
2023-03-15 02:42:39 - progress_bar.py[line:274] - INFO: epoch 003:     74 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.5, ups=0.86, wpb=252.5, bsz=100, num_updates=12680, lr=3.14886e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=25827
2023-03-15 02:42:51 - progress_bar.py[line:274] - INFO: epoch 003:     84 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=250, nsentences=100, sample_size=250, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.4, ups=0.86, wpb=250, bsz=100, num_updates=12690, lr=3.1472e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=25839
2023-03-15 02:43:03 - progress_bar.py[line:274] - INFO: epoch 003:     94 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.7, ups=0.86, wpb=253.3, bsz=100, num_updates=12700, lr=3.14553e-05, gnorm=0.692, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=25851
2023-03-15 02:43:14 - progress_bar.py[line:274] - INFO: epoch 003:    104 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.6, ups=0.86, wpb=253.3, bsz=100, num_updates=12710, lr=3.14386e-05, gnorm=0.674, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=25862
2023-03-15 02:43:26 - progress_bar.py[line:274] - INFO: epoch 003:    114 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.1, ups=0.86, wpb=252.5, bsz=100, num_updates=12720, lr=3.14219e-05, gnorm=0.626, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=25874
2023-03-15 02:43:38 - progress_bar.py[line:274] - INFO: epoch 003:    124 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.7, ups=0.85, wpb=255.8, bsz=100, num_updates=12730, lr=3.14053e-05, gnorm=0.684, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=25886
2023-03-15 02:43:49 - progress_bar.py[line:274] - INFO: epoch 003:    134 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.6, ups=0.86, wpb=254.1, bsz=100, num_updates=12740, lr=3.13886e-05, gnorm=0.68, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=25897
2023-03-15 02:44:01 - progress_bar.py[line:274] - INFO: epoch 003:    144 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.4, ups=0.86, wpb=251.9, bsz=100, num_updates=12750, lr=3.13719e-05, gnorm=0.704, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=25909
2023-03-15 02:44:13 - progress_bar.py[line:274] - INFO: epoch 003:    154 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.2, ups=0.87, wpb=255.2, bsz=100, num_updates=12760, lr=3.13553e-05, gnorm=0.783, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=25920
2023-03-15 02:44:24 - progress_bar.py[line:274] - INFO: epoch 003:    164 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215.9, ups=0.85, wpb=254.4, bsz=100, num_updates=12770, lr=3.13386e-05, gnorm=0.663, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=25932
2023-03-15 02:44:36 - progress_bar.py[line:274] - INFO: epoch 003:    174 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.9, ups=0.86, wpb=252.7, bsz=100, num_updates=12780, lr=3.13219e-05, gnorm=0.653, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=25944
2023-03-15 02:44:48 - progress_bar.py[line:274] - INFO: epoch 003:    184 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.5, ups=0.87, wpb=252.5, bsz=100, num_updates=12790, lr=3.13052e-05, gnorm=0.792, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=25955
2023-03-15 02:44:59 - progress_bar.py[line:274] - INFO: epoch 003:    194 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214.8, ups=0.84, wpb=254.2, bsz=100, num_updates=12800, lr=3.12886e-05, gnorm=0.746, clip=10, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=25967
2023-03-15 02:45:11 - progress_bar.py[line:274] - INFO: epoch 003:    204 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.6, ups=0.86, wpb=254.2, bsz=100, num_updates=12810, lr=3.12719e-05, gnorm=0.649, clip=10, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=25979
2023-03-15 02:45:23 - progress_bar.py[line:274] - INFO: epoch 003:    214 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=256.8, nsentences=100, sample_size=256.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.3, ups=0.86, wpb=256.8, bsz=100, num_updates=12820, lr=3.12552e-05, gnorm=0.711, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=25991
2023-03-15 02:45:34 - progress_bar.py[line:274] - INFO: epoch 003:    224 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.8, ups=0.86, wpb=255.5, bsz=100, num_updates=12830, lr=3.12385e-05, gnorm=0.653, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26002
2023-03-15 02:45:42 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 02:45:47 - progress_bar.py[line:274] - INFO: epoch 003:    235 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=202.5, ups=0.8, wpb=253.5, bsz=100, num_updates=12840, lr=3.12219e-05, gnorm=0.708, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=26015
2023-03-15 02:45:58 - progress_bar.py[line:274] - INFO: epoch 003:    245 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=257.4, nsentences=100, sample_size=257.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=223.2, ups=0.87, wpb=257.4, bsz=100, num_updates=12850, lr=3.12052e-05, gnorm=0.716, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26026
2023-03-15 02:46:10 - progress_bar.py[line:274] - INFO: epoch 003:    255 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.3, ups=0.86, wpb=255, bsz=100, num_updates=12860, lr=3.11885e-05, gnorm=0.687, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26038
2023-03-15 02:46:22 - progress_bar.py[line:274] - INFO: epoch 003:    265 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.4, ups=0.87, wpb=255.2, bsz=100, num_updates=12870, lr=3.11718e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26049
2023-03-15 02:46:33 - progress_bar.py[line:274] - INFO: epoch 003:    275 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.3, ups=0.87, wpb=251.7, bsz=100, num_updates=12880, lr=3.11552e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=26061
2023-03-15 02:46:45 - progress_bar.py[line:274] - INFO: epoch 003:    285 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.4, ups=0.86, wpb=252.6, bsz=100, num_updates=12890, lr=3.11385e-05, gnorm=0.638, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=26073
2023-03-15 02:46:57 - progress_bar.py[line:274] - INFO: epoch 003:    295 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.7, ups=0.85, wpb=255.5, bsz=100, num_updates=12900, lr=3.11218e-05, gnorm=0.721, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26084
2023-03-15 02:47:08 - progress_bar.py[line:274] - INFO: epoch 003:    305 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.6, ups=0.86, wpb=251, bsz=100, num_updates=12910, lr=3.11051e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=26096
2023-03-15 02:47:20 - progress_bar.py[line:274] - INFO: epoch 003:    315 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.6, ups=0.86, wpb=253.2, bsz=100, num_updates=12920, lr=3.10885e-05, gnorm=0.729, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26108
2023-03-15 02:47:32 - progress_bar.py[line:274] - INFO: epoch 003:    325 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.4, ups=0.86, wpb=254.5, bsz=100, num_updates=12930, lr=3.10718e-05, gnorm=0.758, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=26119
2023-03-15 02:47:43 - progress_bar.py[line:274] - INFO: epoch 003:    335 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.1, ups=0.86, wpb=253.2, bsz=100, num_updates=12940, lr=3.10551e-05, gnorm=0.764, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=26131
2023-03-15 02:47:55 - progress_bar.py[line:274] - INFO: epoch 003:    345 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.8, ups=0.86, wpb=253, bsz=100, num_updates=12950, lr=3.10384e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=26143
2023-03-15 02:48:06 - progress_bar.py[line:274] - INFO: epoch 003:    355 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.3, ups=0.86, wpb=253.3, bsz=100, num_updates=12960, lr=3.10218e-05, gnorm=0.692, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=26154
2023-03-15 02:48:18 - progress_bar.py[line:274] - INFO: epoch 003:    365 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214.7, ups=0.85, wpb=252.9, bsz=100, num_updates=12970, lr=3.10051e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=26166
2023-03-15 02:48:30 - progress_bar.py[line:274] - INFO: epoch 003:    375 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=224.8, ups=0.88, wpb=255.4, bsz=100, num_updates=12980, lr=3.09884e-05, gnorm=0.683, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26177
2023-03-15 02:48:41 - progress_bar.py[line:274] - INFO: epoch 003:    385 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.359, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.1, ups=0.86, wpb=250.8, bsz=100, num_updates=12990, lr=3.09718e-05, gnorm=0.734, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26189
2023-03-15 02:48:53 - progress_bar.py[line:274] - INFO: epoch 003:    395 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.9, ups=0.86, wpb=255.1, bsz=100, num_updates=13000, lr=3.09551e-05, gnorm=0.733, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26201
2023-03-15 02:49:04 - progress_bar.py[line:274] - INFO: epoch 003:    405 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.1, ups=0.87, wpb=252.4, bsz=100, num_updates=13010, lr=3.09384e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26212
2023-03-15 02:49:16 - progress_bar.py[line:274] - INFO: epoch 003:    415 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.5, ups=0.85, wpb=253.8, bsz=100, num_updates=13020, lr=3.09217e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26224
2023-03-15 02:49:28 - progress_bar.py[line:274] - INFO: epoch 003:    425 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.7, ups=0.86, wpb=252.9, bsz=100, num_updates=13030, lr=3.09051e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=26236
2023-03-15 02:49:39 - progress_bar.py[line:274] - INFO: epoch 003:    435 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.8, ups=0.87, wpb=255.6, bsz=100, num_updates=13040, lr=3.08884e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26247
2023-03-15 02:49:51 - progress_bar.py[line:274] - INFO: epoch 003:    445 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.8, ups=0.86, wpb=252.9, bsz=100, num_updates=13050, lr=3.08717e-05, gnorm=0.727, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=26259
2023-03-15 02:50:03 - progress_bar.py[line:274] - INFO: epoch 003:    455 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.6, ups=0.87, wpb=255.4, bsz=100, num_updates=13060, lr=3.0855e-05, gnorm=0.752, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26270
2023-03-15 02:50:14 - progress_bar.py[line:274] - INFO: epoch 003:    465 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222.3, ups=0.87, wpb=255.2, bsz=100, num_updates=13070, lr=3.08384e-05, gnorm=0.687, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26282
2023-03-15 02:50:26 - progress_bar.py[line:274] - INFO: epoch 003:    475 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.8, ups=0.86, wpb=254.9, bsz=100, num_updates=13080, lr=3.08217e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26294
2023-03-15 02:50:37 - progress_bar.py[line:274] - INFO: epoch 003:    485 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.7, ups=0.85, wpb=254.9, bsz=100, num_updates=13090, lr=3.0805e-05, gnorm=0.743, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=26305
2023-03-15 02:50:49 - progress_bar.py[line:274] - INFO: epoch 003:    495 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.4, ups=0.86, wpb=251.4, bsz=100, num_updates=13100, lr=3.07883e-05, gnorm=0.753, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=26317
2023-03-15 02:51:01 - progress_bar.py[line:274] - INFO: epoch 003:    505 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.4, ups=0.86, wpb=252.7, bsz=100, num_updates=13110, lr=3.07717e-05, gnorm=0.785, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26329
2023-03-15 02:51:12 - progress_bar.py[line:274] - INFO: epoch 003:    515 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.4, ups=0.87, wpb=253.9, bsz=100, num_updates=13120, lr=3.0755e-05, gnorm=0.74, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26340
2023-03-15 02:51:24 - progress_bar.py[line:274] - INFO: epoch 003:    525 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.3, ups=0.85, wpb=254.4, bsz=100, num_updates=13130, lr=3.07383e-05, gnorm=0.748, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26352
2023-03-15 02:51:36 - progress_bar.py[line:274] - INFO: epoch 003:    535 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.8, ups=0.86, wpb=253.5, bsz=100, num_updates=13140, lr=3.07216e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=26363
2023-03-15 02:51:47 - progress_bar.py[line:274] - INFO: epoch 003:    545 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.4, ups=0.87, wpb=254.2, bsz=100, num_updates=13150, lr=3.0705e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26375
2023-03-15 02:51:59 - progress_bar.py[line:274] - INFO: epoch 003:    555 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.2, ups=0.86, wpb=255.7, bsz=100, num_updates=13160, lr=3.06883e-05, gnorm=0.733, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26387
2023-03-15 02:52:10 - progress_bar.py[line:274] - INFO: epoch 003:    565 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.2, ups=0.86, wpb=252.9, bsz=100, num_updates=13170, lr=3.06716e-05, gnorm=0.739, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26398
2023-03-15 02:52:22 - progress_bar.py[line:274] - INFO: epoch 003:    575 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.5, ups=0.87, wpb=251.3, bsz=100, num_updates=13180, lr=3.0655e-05, gnorm=0.758, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=26410
2023-03-15 02:52:34 - progress_bar.py[line:274] - INFO: epoch 003:    585 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.6, ups=0.86, wpb=253.1, bsz=100, num_updates=13190, lr=3.06383e-05, gnorm=0.777, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26422
2023-03-15 02:52:45 - progress_bar.py[line:274] - INFO: epoch 003:    595 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.6, ups=0.86, wpb=251.3, bsz=100, num_updates=13200, lr=3.06216e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=26433
2023-03-15 02:52:57 - progress_bar.py[line:274] - INFO: epoch 003:    605 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.352, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=214.8, ups=0.85, wpb=253.4, bsz=100, num_updates=13210, lr=3.06049e-05, gnorm=0.723, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26445
2023-03-15 02:53:09 - progress_bar.py[line:274] - INFO: epoch 003:    615 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.2, ups=0.86, wpb=252.1, bsz=100, num_updates=13220, lr=3.05883e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=26457
2023-03-15 02:53:20 - progress_bar.py[line:274] - INFO: epoch 003:    625 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218, ups=0.86, wpb=252.7, bsz=100, num_updates=13230, lr=3.05716e-05, gnorm=0.791, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=26468
2023-03-15 02:53:32 - progress_bar.py[line:274] - INFO: epoch 003:    635 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.4, ups=0.86, wpb=254.4, bsz=100, num_updates=13240, lr=3.05549e-05, gnorm=0.753, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=26480
2023-03-15 02:53:43 - progress_bar.py[line:274] - INFO: epoch 003:    645 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.1, ups=0.88, wpb=250.8, bsz=100, num_updates=13250, lr=3.05382e-05, gnorm=0.708, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=26491
2023-03-15 02:53:55 - progress_bar.py[line:274] - INFO: epoch 003:    655 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214, ups=0.85, wpb=251.8, bsz=100, num_updates=13260, lr=3.05216e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=12, gb_free=9.6, ema_decay=0.9999, wall=26503
2023-03-15 02:54:07 - progress_bar.py[line:274] - INFO: epoch 003:    665 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.6, ups=0.86, wpb=254.5, bsz=100, num_updates=13270, lr=3.05049e-05, gnorm=0.748, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=26515
2023-03-15 02:54:19 - progress_bar.py[line:274] - INFO: epoch 003:    675 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=212.9, ups=0.85, wpb=250.9, bsz=100, num_updates=13280, lr=3.04882e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=26527
2023-03-15 02:54:30 - progress_bar.py[line:274] - INFO: epoch 003:    685 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.2, ups=0.86, wpb=255.3, bsz=100, num_updates=13290, lr=3.04715e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26538
2023-03-15 02:54:42 - progress_bar.py[line:274] - INFO: epoch 003:    695 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.7, ups=0.86, wpb=253.7, bsz=100, num_updates=13300, lr=3.04549e-05, gnorm=0.712, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26550
2023-03-15 02:54:54 - progress_bar.py[line:274] - INFO: epoch 003:    705 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.4, ups=0.86, wpb=252.6, bsz=100, num_updates=13310, lr=3.04382e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26561
2023-03-15 02:55:05 - progress_bar.py[line:274] - INFO: epoch 003:    715 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.4, ups=0.86, wpb=253.6, bsz=100, num_updates=13320, lr=3.04215e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26573
2023-03-15 02:55:17 - progress_bar.py[line:274] - INFO: epoch 003:    725 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.9, ups=0.85, wpb=254, bsz=100, num_updates=13330, lr=3.04048e-05, gnorm=0.708, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=26585
2023-03-15 02:55:29 - progress_bar.py[line:274] - INFO: epoch 003:    735 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.7, ups=0.87, wpb=252, bsz=100, num_updates=13340, lr=3.03882e-05, gnorm=0.748, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26596
2023-03-15 02:55:40 - progress_bar.py[line:274] - INFO: epoch 003:    745 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216, ups=0.86, wpb=251.7, bsz=100, num_updates=13350, lr=3.03715e-05, gnorm=0.739, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26608
2023-03-15 02:55:52 - progress_bar.py[line:274] - INFO: epoch 003:    755 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.3, ups=0.87, wpb=254.8, bsz=100, num_updates=13360, lr=3.03548e-05, gnorm=0.81, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26620
2023-03-15 02:55:59 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 02:56:05 - progress_bar.py[line:274] - INFO: epoch 003:    766 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=197.4, ups=0.78, wpb=252.5, bsz=100, num_updates=13370, lr=3.03381e-05, gnorm=0.748, clip=0, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=26632
2023-03-15 02:56:16 - progress_bar.py[line:274] - INFO: epoch 003:    776 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=213, ups=0.85, wpb=251.9, bsz=100, num_updates=13380, lr=3.03215e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26644
2023-03-15 02:56:28 - progress_bar.py[line:274] - INFO: epoch 003:    786 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.7, ups=0.86, wpb=254.4, bsz=100, num_updates=13390, lr=3.03048e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=26656
2023-03-15 02:56:40 - progress_bar.py[line:274] - INFO: epoch 003:    796 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=213.8, ups=0.85, wpb=252.5, bsz=100, num_updates=13400, lr=3.02881e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26668
2023-03-15 02:56:51 - progress_bar.py[line:274] - INFO: epoch 003:    806 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220, ups=0.87, wpb=253.5, bsz=100, num_updates=13410, lr=3.02715e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26679
2023-03-15 02:57:03 - progress_bar.py[line:274] - INFO: epoch 003:    816 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.4, ups=0.85, wpb=253.2, bsz=100, num_updates=13420, lr=3.02548e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26691
2023-03-15 02:57:15 - progress_bar.py[line:274] - INFO: epoch 003:    826 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.8, ups=0.86, wpb=253.6, bsz=100, num_updates=13430, lr=3.02381e-05, gnorm=0.797, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=26702
2023-03-15 02:57:26 - progress_bar.py[line:274] - INFO: epoch 003:    836 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.9, ups=0.87, wpb=256.2, bsz=100, num_updates=13440, lr=3.02214e-05, gnorm=0.758, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=26714
2023-03-15 02:57:38 - progress_bar.py[line:274] - INFO: epoch 003:    846 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.6, ups=0.86, wpb=253.9, bsz=100, num_updates=13450, lr=3.02048e-05, gnorm=0.822, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26726
2023-03-15 02:57:49 - progress_bar.py[line:274] - INFO: epoch 003:    856 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.5, ups=0.86, wpb=254, bsz=100, num_updates=13460, lr=3.01881e-05, gnorm=0.748, clip=0, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=26737
2023-03-15 02:58:01 - progress_bar.py[line:274] - INFO: epoch 003:    866 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=223.9, ups=0.88, wpb=254.7, bsz=100, num_updates=13470, lr=3.01714e-05, gnorm=0.7, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=26749
2023-03-15 02:58:12 - progress_bar.py[line:274] - INFO: epoch 003:    876 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.2, ups=0.86, wpb=253.5, bsz=100, num_updates=13480, lr=3.01547e-05, gnorm=0.682, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26760
2023-03-15 02:58:24 - progress_bar.py[line:274] - INFO: epoch 003:    886 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.9, ups=0.85, wpb=252, bsz=100, num_updates=13490, lr=3.01381e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=26772
2023-03-15 02:58:36 - progress_bar.py[line:274] - INFO: epoch 003:    896 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214, ups=0.84, wpb=254.6, bsz=100, num_updates=13500, lr=3.01214e-05, gnorm=0.744, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=26784
2023-03-15 02:58:47 - progress_bar.py[line:274] - INFO: epoch 003:    906 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.8, ups=0.87, wpb=254.1, bsz=100, num_updates=13510, lr=3.01047e-05, gnorm=0.768, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26795
2023-03-15 02:58:59 - progress_bar.py[line:274] - INFO: epoch 003:    916 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.9, ups=0.87, wpb=255.7, bsz=100, num_updates=13520, lr=3.0088e-05, gnorm=0.668, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26807
2023-03-15 02:59:11 - progress_bar.py[line:274] - INFO: epoch 003:    926 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.3, ups=0.86, wpb=254.5, bsz=100, num_updates=13530, lr=3.00714e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26818
2023-03-15 02:59:22 - progress_bar.py[line:274] - INFO: epoch 003:    936 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.7, ups=0.86, wpb=253.1, bsz=100, num_updates=13540, lr=3.00547e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=26830
2023-03-15 02:59:34 - progress_bar.py[line:274] - INFO: epoch 003:    946 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.4, ups=0.86, wpb=254, bsz=100, num_updates=13550, lr=3.0038e-05, gnorm=0.659, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26842
2023-03-15 02:59:46 - progress_bar.py[line:274] - INFO: epoch 003:    956 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.4, ups=0.86, wpb=252.9, bsz=100, num_updates=13560, lr=3.00213e-05, gnorm=0.732, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26853
2023-03-15 02:59:57 - progress_bar.py[line:274] - INFO: epoch 003:    966 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.3, ups=0.87, wpb=254.7, bsz=100, num_updates=13570, lr=3.00047e-05, gnorm=0.712, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=26865
2023-03-15 03:00:09 - progress_bar.py[line:274] - INFO: epoch 003:    976 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.2, ups=0.87, wpb=250.4, bsz=100, num_updates=13580, lr=2.9988e-05, gnorm=0.727, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26876
2023-03-15 03:00:20 - progress_bar.py[line:274] - INFO: epoch 003:    986 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.9, ups=0.86, wpb=253.5, bsz=100, num_updates=13590, lr=2.99713e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26888
2023-03-15 03:00:32 - progress_bar.py[line:274] - INFO: epoch 003:    996 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.1, ups=0.87, wpb=254.1, bsz=100, num_updates=13600, lr=2.99546e-05, gnorm=0.796, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=26900
2023-03-15 03:00:43 - progress_bar.py[line:274] - INFO: epoch 003:   1006 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.7, ups=0.86, wpb=251.5, bsz=100, num_updates=13610, lr=2.9938e-05, gnorm=0.737, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=26911
2023-03-15 03:00:55 - progress_bar.py[line:274] - INFO: epoch 003:   1016 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.6, ups=0.86, wpb=252.3, bsz=100, num_updates=13620, lr=2.99213e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26923
2023-03-15 03:01:07 - progress_bar.py[line:274] - INFO: epoch 003:   1026 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.9, ups=0.86, wpb=251.5, bsz=100, num_updates=13630, lr=2.99046e-05, gnorm=0.801, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=26935
2023-03-15 03:01:18 - progress_bar.py[line:274] - INFO: epoch 003:   1036 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.6, ups=0.87, wpb=252.2, bsz=100, num_updates=13640, lr=2.9888e-05, gnorm=0.683, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=26946
2023-03-15 03:01:30 - progress_bar.py[line:274] - INFO: epoch 003:   1046 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.5, ups=0.86, wpb=252.4, bsz=100, num_updates=13650, lr=2.98713e-05, gnorm=0.759, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=26958
2023-03-15 03:01:41 - progress_bar.py[line:274] - INFO: epoch 003:   1056 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.6, ups=0.87, wpb=254.5, bsz=100, num_updates=13660, lr=2.98546e-05, gnorm=0.744, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=26969
2023-03-15 03:01:53 - progress_bar.py[line:274] - INFO: epoch 003:   1066 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.1, ups=0.86, wpb=252.2, bsz=100, num_updates=13670, lr=2.98379e-05, gnorm=0.749, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=26981
2023-03-15 03:02:05 - progress_bar.py[line:274] - INFO: epoch 003:   1076 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.1, ups=0.86, wpb=254.3, bsz=100, num_updates=13680, lr=2.98213e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=26992
2023-03-15 03:02:16 - progress_bar.py[line:274] - INFO: epoch 003:   1086 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.6, ups=0.88, wpb=252.6, bsz=100, num_updates=13690, lr=2.98046e-05, gnorm=0.758, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=27004
2023-03-15 03:02:28 - progress_bar.py[line:274] - INFO: epoch 003:   1096 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=213.8, ups=0.85, wpb=252.1, bsz=100, num_updates=13700, lr=2.97879e-05, gnorm=0.704, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27016
2023-03-15 03:02:39 - progress_bar.py[line:274] - INFO: epoch 003:   1106 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214.4, ups=0.86, wpb=250.1, bsz=100, num_updates=13710, lr=2.97712e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=27027
2023-03-15 03:02:51 - progress_bar.py[line:274] - INFO: epoch 003:   1116 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.8, ups=0.86, wpb=252.5, bsz=100, num_updates=13720, lr=2.97546e-05, gnorm=0.679, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=27039
2023-03-15 03:03:03 - progress_bar.py[line:274] - INFO: epoch 003:   1126 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217, ups=0.85, wpb=255.6, bsz=100, num_updates=13730, lr=2.97379e-05, gnorm=0.751, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=27051
2023-03-15 03:03:15 - progress_bar.py[line:274] - INFO: epoch 003:   1136 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=213.2, ups=0.84, wpb=254, bsz=100, num_updates=13740, lr=2.97212e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=27063
2023-03-15 03:03:26 - progress_bar.py[line:274] - INFO: epoch 003:   1146 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.9, ups=0.86, wpb=252, bsz=100, num_updates=13750, lr=2.97045e-05, gnorm=0.742, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=27074
2023-03-15 03:03:38 - progress_bar.py[line:274] - INFO: epoch 003:   1156 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.4, ups=0.86, wpb=252.5, bsz=100, num_updates=13760, lr=2.96879e-05, gnorm=0.808, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=27086
2023-03-15 03:03:50 - progress_bar.py[line:274] - INFO: epoch 003:   1166 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214.6, ups=0.85, wpb=252.4, bsz=100, num_updates=13770, lr=2.96712e-05, gnorm=0.683, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=27098
2023-03-15 03:04:02 - progress_bar.py[line:274] - INFO: epoch 003:   1176 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.3, ups=0.86, wpb=253.5, bsz=100, num_updates=13780, lr=2.96545e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=27110
2023-03-15 03:04:13 - progress_bar.py[line:274] - INFO: epoch 003:   1186 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.4, ups=0.86, wpb=252.4, bsz=100, num_updates=13790, lr=2.96378e-05, gnorm=0.748, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27121
2023-03-15 03:04:25 - progress_bar.py[line:274] - INFO: epoch 003:   1196 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216, ups=0.86, wpb=251.5, bsz=100, num_updates=13800, lr=2.96212e-05, gnorm=0.794, clip=10, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=27133
2023-03-15 03:04:37 - progress_bar.py[line:274] - INFO: epoch 003:   1206 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.4, ups=0.86, wpb=253.2, bsz=100, num_updates=13810, lr=2.96045e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=27144
2023-03-15 03:04:48 - progress_bar.py[line:274] - INFO: epoch 003:   1216 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215.8, ups=0.85, wpb=254.6, bsz=100, num_updates=13820, lr=2.95878e-05, gnorm=0.739, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27156
2023-03-15 03:05:00 - progress_bar.py[line:274] - INFO: epoch 003:   1226 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.2, ups=0.86, wpb=253.9, bsz=100, num_updates=13830, lr=2.95711e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=27168
2023-03-15 03:05:12 - progress_bar.py[line:274] - INFO: epoch 003:   1236 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.8, ups=0.87, wpb=254.1, bsz=100, num_updates=13840, lr=2.95545e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=27179
2023-03-15 03:05:23 - progress_bar.py[line:274] - INFO: epoch 003:   1246 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.3, ups=0.86, wpb=253.8, bsz=100, num_updates=13850, lr=2.95378e-05, gnorm=0.738, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=27191
2023-03-15 03:05:35 - progress_bar.py[line:274] - INFO: epoch 003:   1256 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217, ups=0.85, wpb=255, bsz=100, num_updates=13860, lr=2.95211e-05, gnorm=0.677, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=27203
2023-03-15 03:05:47 - progress_bar.py[line:274] - INFO: epoch 003:   1266 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.3, ups=0.86, wpb=254.7, bsz=100, num_updates=13870, lr=2.95045e-05, gnorm=0.72, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=27214
2023-03-15 03:05:58 - progress_bar.py[line:274] - INFO: epoch 003:   1276 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.9, ups=0.86, wpb=251.1, bsz=100, num_updates=13880, lr=2.94878e-05, gnorm=0.772, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=27226
2023-03-15 03:06:10 - progress_bar.py[line:274] - INFO: epoch 003:   1286 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=212.4, ups=0.85, wpb=250.4, bsz=100, num_updates=13890, lr=2.94711e-05, gnorm=0.76, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=27238
2023-03-15 03:06:22 - progress_bar.py[line:274] - INFO: epoch 003:   1296 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221, ups=0.87, wpb=254.1, bsz=100, num_updates=13900, lr=2.94544e-05, gnorm=0.742, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=27249
2023-03-15 03:06:33 - progress_bar.py[line:274] - INFO: epoch 003:   1306 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.3, ups=0.86, wpb=252.1, bsz=100, num_updates=13910, lr=2.94378e-05, gnorm=0.748, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=27261
2023-03-15 03:06:45 - progress_bar.py[line:274] - INFO: epoch 003:   1316 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.6, ups=0.86, wpb=253, bsz=100, num_updates=13920, lr=2.94211e-05, gnorm=0.757, clip=10, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=27273
2023-03-15 03:06:57 - progress_bar.py[line:274] - INFO: epoch 003:   1326 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.4, ups=0.86, wpb=256.3, bsz=100, num_updates=13930, lr=2.94044e-05, gnorm=0.749, clip=10, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=27284
2023-03-15 03:07:08 - progress_bar.py[line:274] - INFO: epoch 003:   1336 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.1, ups=0.85, wpb=253.4, bsz=100, num_updates=13940, lr=2.93877e-05, gnorm=0.765, clip=10, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=27296
2023-03-15 03:07:11 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 03:07:21 - progress_bar.py[line:274] - INFO: epoch 003:   1347 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=198.2, ups=0.79, wpb=252.5, bsz=100, num_updates=13950, lr=2.93711e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=27309
2023-03-15 03:07:33 - progress_bar.py[line:274] - INFO: epoch 003:   1357 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.7, ups=0.87, wpb=252.4, bsz=100, num_updates=13960, lr=2.93544e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=27320
2023-03-15 03:07:44 - progress_bar.py[line:274] - INFO: epoch 003:   1367 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=215.8, ups=0.85, wpb=254.3, bsz=100, num_updates=13970, lr=2.93377e-05, gnorm=0.695, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27332
2023-03-15 03:07:56 - progress_bar.py[line:274] - INFO: epoch 003:   1377 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=214.5, ups=0.85, wpb=251.9, bsz=100, num_updates=13980, lr=2.9321e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27344
2023-03-15 03:08:08 - progress_bar.py[line:274] - INFO: epoch 003:   1387 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.1, ups=0.87, wpb=253.2, bsz=100, num_updates=13990, lr=2.93044e-05, gnorm=0.728, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=27355
2023-03-15 03:08:19 - progress_bar.py[line:274] - INFO: epoch 003:   1397 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=256.8, nsentences=100, sample_size=256.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.8, ups=0.87, wpb=256.8, bsz=100, num_updates=14000, lr=2.92877e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=27367
2023-03-15 03:08:31 - progress_bar.py[line:274] - INFO: epoch 003:   1407 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.1, ups=0.86, wpb=252.5, bsz=100, num_updates=14010, lr=2.9271e-05, gnorm=0.753, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27379
2023-03-15 03:08:43 - progress_bar.py[line:274] - INFO: epoch 003:   1417 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.4, ups=0.86, wpb=253.7, bsz=100, num_updates=14020, lr=2.92543e-05, gnorm=0.734, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=27390
2023-03-15 03:08:54 - progress_bar.py[line:274] - INFO: epoch 003:   1427 / 6313 loss=0.57, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.9, ups=0.86, wpb=250.7, bsz=100, num_updates=14030, lr=2.92377e-05, gnorm=0.724, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=27402
2023-03-15 03:09:06 - progress_bar.py[line:274] - INFO: epoch 003:   1437 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=248.7, nsentences=100, sample_size=248.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.8, ups=0.86, wpb=248.7, bsz=100, num_updates=14040, lr=2.9221e-05, gnorm=0.714, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=27414
2023-03-15 03:09:17 - progress_bar.py[line:274] - INFO: epoch 003:   1447 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222.8, ups=0.88, wpb=252.1, bsz=100, num_updates=14050, lr=2.92043e-05, gnorm=0.742, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=27425
2023-03-15 03:09:29 - progress_bar.py[line:274] - INFO: epoch 003:   1457 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.6, ups=0.85, wpb=254.5, bsz=100, num_updates=14060, lr=2.91876e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27437
2023-03-15 03:09:40 - progress_bar.py[line:274] - INFO: epoch 003:   1467 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=250, nsentences=100, sample_size=250, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215.7, ups=0.86, wpb=250, bsz=100, num_updates=14070, lr=2.9171e-05, gnorm=0.862, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=27448
2023-03-15 03:09:52 - progress_bar.py[line:274] - INFO: epoch 003:   1477 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.8, ups=0.87, wpb=255.4, bsz=100, num_updates=14080, lr=2.91543e-05, gnorm=0.807, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=27460
2023-03-15 03:10:03 - progress_bar.py[line:274] - INFO: epoch 003:   1487 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.5, ups=0.86, wpb=255.2, bsz=100, num_updates=14090, lr=2.91376e-05, gnorm=0.75, clip=10, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=27471
2023-03-15 03:10:15 - progress_bar.py[line:274] - INFO: epoch 003:   1497 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=212.7, ups=0.85, wpb=250.7, bsz=100, num_updates=14100, lr=2.9121e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=27483
2023-03-15 03:10:27 - progress_bar.py[line:274] - INFO: epoch 003:   1507 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.1, ups=0.86, wpb=253.8, bsz=100, num_updates=14110, lr=2.91043e-05, gnorm=0.687, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27495
2023-03-15 03:10:39 - progress_bar.py[line:274] - INFO: epoch 003:   1517 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.8, ups=0.86, wpb=253.8, bsz=100, num_updates=14120, lr=2.90876e-05, gnorm=0.804, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27506
2023-03-15 03:10:50 - progress_bar.py[line:274] - INFO: epoch 003:   1527 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=212.7, ups=0.85, wpb=251.3, bsz=100, num_updates=14130, lr=2.90709e-05, gnorm=0.797, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=27518
2023-03-15 03:11:02 - progress_bar.py[line:274] - INFO: epoch 003:   1537 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.2, ups=0.86, wpb=252.7, bsz=100, num_updates=14140, lr=2.90543e-05, gnorm=0.876, clip=20, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=27530
2023-03-15 03:11:14 - progress_bar.py[line:274] - INFO: epoch 003:   1547 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.7, ups=0.86, wpb=255.3, bsz=100, num_updates=14150, lr=2.90376e-05, gnorm=0.808, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=27542
2023-03-15 03:11:25 - progress_bar.py[line:274] - INFO: epoch 003:   1557 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.4, ups=0.86, wpb=251.4, bsz=100, num_updates=14160, lr=2.90209e-05, gnorm=0.657, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=27553
2023-03-15 03:11:37 - progress_bar.py[line:274] - INFO: epoch 003:   1567 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.36, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.5, ups=0.86, wpb=252.7, bsz=100, num_updates=14170, lr=2.90042e-05, gnorm=0.795, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27565
2023-03-15 03:11:48 - progress_bar.py[line:274] - INFO: epoch 003:   1577 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.8, ups=0.87, wpb=252.8, bsz=100, num_updates=14180, lr=2.89876e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=27576
2023-03-15 03:12:00 - progress_bar.py[line:274] - INFO: epoch 003:   1587 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217, ups=0.86, wpb=252.4, bsz=100, num_updates=14190, lr=2.89709e-05, gnorm=0.759, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27588
2023-03-15 03:12:12 - progress_bar.py[line:274] - INFO: epoch 003:   1597 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218, ups=0.86, wpb=253.3, bsz=100, num_updates=14200, lr=2.89542e-05, gnorm=0.816, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=27600
2023-03-15 03:12:23 - progress_bar.py[line:274] - INFO: epoch 003:   1607 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.3, ups=0.86, wpb=255, bsz=100, num_updates=14210, lr=2.89375e-05, gnorm=0.818, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27611
2023-03-15 03:12:35 - progress_bar.py[line:274] - INFO: epoch 003:   1617 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.2, ups=0.86, wpb=253.1, bsz=100, num_updates=14220, lr=2.89209e-05, gnorm=0.663, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=27623
2023-03-15 03:12:47 - progress_bar.py[line:274] - INFO: epoch 003:   1627 / 6313 loss=0.575, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.2, ups=0.85, wpb=252.1, bsz=100, num_updates=14230, lr=2.89042e-05, gnorm=0.758, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27635
2023-03-15 03:12:58 - progress_bar.py[line:274] - INFO: epoch 003:   1637 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.2, ups=0.86, wpb=252.4, bsz=100, num_updates=14240, lr=2.88875e-05, gnorm=0.859, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=27646
2023-03-15 03:13:10 - progress_bar.py[line:274] - INFO: epoch 003:   1647 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214.7, ups=0.85, wpb=253.4, bsz=100, num_updates=14250, lr=2.88708e-05, gnorm=0.675, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27658
2023-03-15 03:13:22 - progress_bar.py[line:274] - INFO: epoch 003:   1657 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220, ups=0.87, wpb=253.2, bsz=100, num_updates=14260, lr=2.88542e-05, gnorm=0.684, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=27669
2023-03-15 03:13:33 - progress_bar.py[line:274] - INFO: epoch 003:   1667 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.9, ups=0.86, wpb=251.6, bsz=100, num_updates=14270, lr=2.88375e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27681
2023-03-15 03:13:45 - progress_bar.py[line:274] - INFO: epoch 003:   1677 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.9, ups=0.87, wpb=253.2, bsz=100, num_updates=14280, lr=2.88208e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=27693
2023-03-15 03:13:56 - progress_bar.py[line:274] - INFO: epoch 003:   1687 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.6, ups=0.87, wpb=254.6, bsz=100, num_updates=14290, lr=2.88041e-05, gnorm=0.692, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=27704
2023-03-15 03:14:08 - progress_bar.py[line:274] - INFO: epoch 003:   1697 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217, ups=0.85, wpb=254.2, bsz=100, num_updates=14300, lr=2.87875e-05, gnorm=0.72, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27716
2023-03-15 03:14:20 - progress_bar.py[line:274] - INFO: epoch 003:   1707 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.6, ups=0.86, wpb=252.2, bsz=100, num_updates=14310, lr=2.87708e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27728
2023-03-15 03:14:31 - progress_bar.py[line:274] - INFO: epoch 003:   1717 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.8, ups=0.86, wpb=253.4, bsz=100, num_updates=14320, lr=2.87541e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=27739
2023-03-15 03:14:43 - progress_bar.py[line:274] - INFO: epoch 003:   1727 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.8, ups=0.86, wpb=256, bsz=100, num_updates=14330, lr=2.87375e-05, gnorm=0.749, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=27751
2023-03-15 03:14:54 - progress_bar.py[line:274] - INFO: epoch 003:   1737 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=256.6, nsentences=100, sample_size=256.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=223.3, ups=0.87, wpb=256.6, bsz=100, num_updates=14340, lr=2.87208e-05, gnorm=0.755, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=27762
2023-03-15 03:15:06 - progress_bar.py[line:274] - INFO: epoch 003:   1747 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.3, ups=0.86, wpb=250.9, bsz=100, num_updates=14350, lr=2.87041e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=27774
2023-03-15 03:15:18 - progress_bar.py[line:274] - INFO: epoch 003:   1757 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.6, ups=0.86, wpb=253.2, bsz=100, num_updates=14360, lr=2.86874e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=27785
2023-03-15 03:15:29 - progress_bar.py[line:274] - INFO: epoch 003:   1767 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.6, ups=0.85, wpb=253.7, bsz=100, num_updates=14370, lr=2.86708e-05, gnorm=0.727, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=27797
2023-03-15 03:15:41 - progress_bar.py[line:274] - INFO: epoch 003:   1777 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216, ups=0.86, wpb=252.3, bsz=100, num_updates=14380, lr=2.86541e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=27809
2023-03-15 03:15:53 - progress_bar.py[line:274] - INFO: epoch 003:   1787 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.2, ups=0.86, wpb=251.4, bsz=100, num_updates=14390, lr=2.86374e-05, gnorm=0.685, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27821
2023-03-15 03:16:04 - progress_bar.py[line:274] - INFO: epoch 003:   1797 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.1, ups=0.87, wpb=251.7, bsz=100, num_updates=14400, lr=2.86207e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=27832
2023-03-15 03:16:16 - progress_bar.py[line:274] - INFO: epoch 003:   1807 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.9, ups=0.87, wpb=253.7, bsz=100, num_updates=14410, lr=2.86041e-05, gnorm=0.747, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=27844
2023-03-15 03:16:27 - progress_bar.py[line:274] - INFO: epoch 003:   1817 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.1, ups=0.86, wpb=254.7, bsz=100, num_updates=14420, lr=2.85874e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=27855
2023-03-15 03:16:39 - progress_bar.py[line:274] - INFO: epoch 003:   1827 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.5, ups=0.86, wpb=253.4, bsz=100, num_updates=14430, lr=2.85707e-05, gnorm=0.783, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=27867
2023-03-15 03:16:51 - progress_bar.py[line:274] - INFO: epoch 003:   1837 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.3, ups=0.86, wpb=254.8, bsz=100, num_updates=14440, lr=2.8554e-05, gnorm=0.756, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27878
2023-03-15 03:17:02 - progress_bar.py[line:274] - INFO: epoch 003:   1847 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.8, ups=0.86, wpb=255.7, bsz=100, num_updates=14450, lr=2.85374e-05, gnorm=0.715, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27890
2023-03-15 03:17:14 - progress_bar.py[line:274] - INFO: epoch 003:   1857 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.7, ups=0.86, wpb=250.8, bsz=100, num_updates=14460, lr=2.85207e-05, gnorm=0.761, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27902
2023-03-15 03:17:25 - progress_bar.py[line:274] - INFO: epoch 003:   1867 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.6, ups=0.87, wpb=253.1, bsz=100, num_updates=14470, lr=2.8504e-05, gnorm=0.854, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=27913
2023-03-15 03:17:37 - progress_bar.py[line:274] - INFO: epoch 003:   1877 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.3, ups=0.86, wpb=256.1, bsz=100, num_updates=14480, lr=2.84873e-05, gnorm=0.756, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27925
2023-03-15 03:17:49 - progress_bar.py[line:274] - INFO: epoch 003:   1887 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.4, ups=0.86, wpb=254.7, bsz=100, num_updates=14490, lr=2.84707e-05, gnorm=0.86, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=27937
2023-03-15 03:18:00 - progress_bar.py[line:274] - INFO: epoch 003:   1897 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.2, ups=0.86, wpb=251.1, bsz=100, num_updates=14500, lr=2.8454e-05, gnorm=0.781, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=27948
2023-03-15 03:18:12 - progress_bar.py[line:274] - INFO: epoch 003:   1907 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.7, ups=0.86, wpb=253.5, bsz=100, num_updates=14510, lr=2.84373e-05, gnorm=0.773, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=27960
2023-03-15 03:18:24 - progress_bar.py[line:274] - INFO: epoch 003:   1917 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214.3, ups=0.85, wpb=251.4, bsz=100, num_updates=14520, lr=2.84206e-05, gnorm=0.786, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=27972
2023-03-15 03:18:35 - progress_bar.py[line:274] - INFO: epoch 003:   1927 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.8, ups=0.87, wpb=252.8, bsz=100, num_updates=14530, lr=2.8404e-05, gnorm=0.686, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=27983
2023-03-15 03:18:47 - progress_bar.py[line:274] - INFO: epoch 003:   1937 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.2, ups=0.85, wpb=253.4, bsz=100, num_updates=14540, lr=2.83873e-05, gnorm=0.725, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=27995
2023-03-15 03:18:59 - progress_bar.py[line:274] - INFO: epoch 003:   1947 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.1, ups=0.87, wpb=254.8, bsz=100, num_updates=14550, lr=2.83706e-05, gnorm=0.706, clip=0, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=28006
2023-03-15 03:19:10 - progress_bar.py[line:274] - INFO: epoch 003:   1957 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.4, ups=0.86, wpb=252.8, bsz=100, num_updates=14560, lr=2.8354e-05, gnorm=0.728, clip=10, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=28018
2023-03-15 03:19:21 - progress_bar.py[line:274] - INFO: epoch 003:   1967 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=228.7, ups=0.9, wpb=252.7, bsz=100, num_updates=14570, lr=2.83373e-05, gnorm=0.747, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=28029
2023-03-15 03:19:33 - progress_bar.py[line:274] - INFO: epoch 003:   1977 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.6, ups=0.86, wpb=254.3, bsz=100, num_updates=14580, lr=2.83206e-05, gnorm=0.787, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=28041
2023-03-15 03:19:45 - progress_bar.py[line:274] - INFO: epoch 003:   1987 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.6, ups=0.86, wpb=254.6, bsz=100, num_updates=14590, lr=2.83039e-05, gnorm=0.77, clip=10, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=28052
2023-03-15 03:19:56 - progress_bar.py[line:274] - INFO: epoch 003:   1997 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219, ups=0.86, wpb=255.5, bsz=100, num_updates=14600, lr=2.82873e-05, gnorm=0.776, clip=10, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=28064
2023-03-15 03:20:08 - progress_bar.py[line:274] - INFO: epoch 003:   2007 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.2, ups=0.86, wpb=254.9, bsz=100, num_updates=14610, lr=2.82706e-05, gnorm=0.752, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=28076
2023-03-15 03:20:19 - progress_bar.py[line:274] - INFO: epoch 003:   2017 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.6, ups=0.87, wpb=255.7, bsz=100, num_updates=14620, lr=2.82539e-05, gnorm=0.682, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=28087
2023-03-15 03:20:31 - progress_bar.py[line:274] - INFO: epoch 003:   2027 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.7, ups=0.87, wpb=254.4, bsz=100, num_updates=14630, lr=2.82372e-05, gnorm=0.707, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28099
2023-03-15 03:20:42 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 03:20:43 - progress_bar.py[line:274] - INFO: epoch 003:   2038 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=203.2, ups=0.8, wpb=254.3, bsz=100, num_updates=14640, lr=2.82206e-05, gnorm=0.898, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=28111
2023-03-15 03:20:55 - progress_bar.py[line:274] - INFO: epoch 003:   2048 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.2, ups=0.87, wpb=254.6, bsz=100, num_updates=14650, lr=2.82039e-05, gnorm=0.766, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=28123
2023-03-15 03:21:07 - progress_bar.py[line:274] - INFO: epoch 003:   2058 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.6, ups=0.85, wpb=252.8, bsz=100, num_updates=14660, lr=2.81872e-05, gnorm=0.771, clip=20, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=28135
2023-03-15 03:21:18 - progress_bar.py[line:274] - INFO: epoch 003:   2068 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.9, ups=0.86, wpb=250.4, bsz=100, num_updates=14670, lr=2.81705e-05, gnorm=0.808, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28146
2023-03-15 03:21:30 - progress_bar.py[line:274] - INFO: epoch 003:   2078 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.7, ups=0.87, wpb=254.4, bsz=100, num_updates=14680, lr=2.81539e-05, gnorm=0.713, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=28158
2023-03-15 03:21:41 - progress_bar.py[line:274] - INFO: epoch 003:   2088 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=249.7, nsentences=100, sample_size=249.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.1, ups=0.86, wpb=249.7, bsz=100, num_updates=14690, lr=2.81372e-05, gnorm=0.792, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=28169
2023-03-15 03:21:53 - progress_bar.py[line:274] - INFO: epoch 003:   2098 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214.6, ups=0.86, wpb=250.5, bsz=100, num_updates=14700, lr=2.81205e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=28181
2023-03-15 03:22:05 - progress_bar.py[line:274] - INFO: epoch 003:   2108 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.4, ups=0.86, wpb=250.2, bsz=100, num_updates=14710, lr=2.81038e-05, gnorm=0.757, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=28193
2023-03-15 03:22:16 - progress_bar.py[line:274] - INFO: epoch 003:   2118 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=223.4, ups=0.87, wpb=256, bsz=100, num_updates=14720, lr=2.80872e-05, gnorm=0.821, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=28204
2023-03-15 03:22:28 - progress_bar.py[line:274] - INFO: epoch 003:   2128 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.6, ups=0.86, wpb=254.4, bsz=100, num_updates=14730, lr=2.80705e-05, gnorm=0.773, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=28216
2023-03-15 03:22:40 - progress_bar.py[line:274] - INFO: epoch 003:   2138 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.6, ups=0.86, wpb=251.8, bsz=100, num_updates=14740, lr=2.80538e-05, gnorm=0.783, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28227
2023-03-15 03:22:51 - progress_bar.py[line:274] - INFO: epoch 003:   2148 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.6, ups=0.86, wpb=252.6, bsz=100, num_updates=14750, lr=2.80371e-05, gnorm=0.757, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=28239
2023-03-15 03:23:03 - progress_bar.py[line:274] - INFO: epoch 003:   2158 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222.3, ups=0.87, wpb=256.5, bsz=100, num_updates=14760, lr=2.80205e-05, gnorm=0.747, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28251
2023-03-15 03:23:14 - progress_bar.py[line:274] - INFO: epoch 003:   2168 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.4, ups=0.86, wpb=253.7, bsz=100, num_updates=14770, lr=2.80038e-05, gnorm=0.777, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28262
2023-03-15 03:23:26 - progress_bar.py[line:274] - INFO: epoch 003:   2178 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.9, ups=0.87, wpb=250.8, bsz=100, num_updates=14780, lr=2.79871e-05, gnorm=0.736, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28274
2023-03-15 03:23:38 - progress_bar.py[line:274] - INFO: epoch 003:   2188 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.9, ups=0.87, wpb=253.5, bsz=100, num_updates=14790, lr=2.79705e-05, gnorm=0.669, clip=0, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=28286
2023-03-15 03:23:49 - progress_bar.py[line:274] - INFO: epoch 003:   2198 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.7, ups=0.86, wpb=252.6, bsz=100, num_updates=14800, lr=2.79538e-05, gnorm=0.647, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28297
2023-03-15 03:24:01 - progress_bar.py[line:274] - INFO: epoch 003:   2208 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218, ups=0.86, wpb=253.5, bsz=100, num_updates=14810, lr=2.79371e-05, gnorm=0.686, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=28309
2023-03-15 03:24:13 - progress_bar.py[line:274] - INFO: epoch 003:   2218 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.1, ups=0.86, wpb=254.9, bsz=100, num_updates=14820, lr=2.79204e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=28320
2023-03-15 03:24:24 - progress_bar.py[line:274] - INFO: epoch 003:   2228 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.4, ups=0.85, wpb=256.1, bsz=100, num_updates=14830, lr=2.79038e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=28332
2023-03-15 03:24:36 - progress_bar.py[line:274] - INFO: epoch 003:   2238 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.1, ups=0.87, wpb=252.2, bsz=100, num_updates=14840, lr=2.78871e-05, gnorm=0.807, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=28344
2023-03-15 03:24:47 - progress_bar.py[line:274] - INFO: epoch 003:   2248 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.4, ups=0.87, wpb=255.9, bsz=100, num_updates=14850, lr=2.78704e-05, gnorm=0.779, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=28355
2023-03-15 03:24:59 - progress_bar.py[line:274] - INFO: epoch 003:   2258 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.6, ups=0.86, wpb=251.4, bsz=100, num_updates=14860, lr=2.78537e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28367
2023-03-15 03:25:11 - progress_bar.py[line:274] - INFO: epoch 003:   2268 / 6313 loss=0.582, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=249.2, nsentences=100, sample_size=249.2, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=217, ups=0.87, wpb=249.2, bsz=100, num_updates=14870, lr=2.78371e-05, gnorm=0.861, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=28378
2023-03-15 03:25:22 - progress_bar.py[line:274] - INFO: epoch 003:   2278 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.6, ups=0.86, wpb=254.8, bsz=100, num_updates=14880, lr=2.78204e-05, gnorm=0.745, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=28390
2023-03-15 03:25:34 - progress_bar.py[line:274] - INFO: epoch 003:   2288 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.7, ups=0.86, wpb=255.4, bsz=100, num_updates=14890, lr=2.78037e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=28402
2023-03-15 03:25:45 - progress_bar.py[line:274] - INFO: epoch 003:   2298 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219, ups=0.86, wpb=254.8, bsz=100, num_updates=14900, lr=2.7787e-05, gnorm=0.715, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28413
2023-03-15 03:25:57 - progress_bar.py[line:274] - INFO: epoch 003:   2308 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=214.2, ups=0.85, wpb=252.4, bsz=100, num_updates=14910, lr=2.77704e-05, gnorm=0.712, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=28425
2023-03-15 03:26:09 - progress_bar.py[line:274] - INFO: epoch 003:   2318 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221, ups=0.87, wpb=253.9, bsz=100, num_updates=14920, lr=2.77537e-05, gnorm=0.708, clip=0, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=28437
2023-03-15 03:26:20 - progress_bar.py[line:274] - INFO: epoch 003:   2328 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.8, ups=0.87, wpb=252.5, bsz=100, num_updates=14930, lr=2.7737e-05, gnorm=0.775, clip=10, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=28448
2023-03-15 03:26:32 - progress_bar.py[line:274] - INFO: epoch 003:   2338 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.2, ups=0.86, wpb=251.9, bsz=100, num_updates=14940, lr=2.77203e-05, gnorm=0.79, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=28460
2023-03-15 03:26:44 - progress_bar.py[line:274] - INFO: epoch 003:   2348 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.5, ups=0.86, wpb=253.8, bsz=100, num_updates=14950, lr=2.77037e-05, gnorm=0.771, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=28471
2023-03-15 03:26:55 - progress_bar.py[line:274] - INFO: epoch 003:   2358 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.1, ups=0.87, wpb=253, bsz=100, num_updates=14960, lr=2.7687e-05, gnorm=0.78, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=28483
2023-03-15 03:27:06 - progress_bar.py[line:274] - INFO: epoch 003:   2368 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=224.7, ups=0.89, wpb=253.5, bsz=100, num_updates=14970, lr=2.76703e-05, gnorm=0.848, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=28494
2023-03-15 03:27:18 - progress_bar.py[line:274] - INFO: epoch 003:   2378 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.2, ups=0.86, wpb=254.8, bsz=100, num_updates=14980, lr=2.76536e-05, gnorm=0.801, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28506
2023-03-15 03:27:29 - progress_bar.py[line:274] - INFO: epoch 003:   2388 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=223.4, ups=0.89, wpb=252.4, bsz=100, num_updates=14990, lr=2.7637e-05, gnorm=0.828, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=28517
2023-03-15 03:27:41 - progress_bar.py[line:274] - INFO: epoch 003:   2398 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.1, ups=0.86, wpb=253.5, bsz=100, num_updates=15000, lr=2.76203e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=28529
2023-03-15 03:27:52 - progress_bar.py[line:274] - INFO: epoch 003:   2408 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.8, ups=0.87, wpb=253.8, bsz=100, num_updates=15010, lr=2.76036e-05, gnorm=0.733, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=28540
2023-03-15 03:28:04 - progress_bar.py[line:274] - INFO: epoch 003:   2418 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=215.2, ups=0.86, wpb=251.6, bsz=100, num_updates=15020, lr=2.7587e-05, gnorm=0.779, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=28552
2023-03-15 03:28:16 - progress_bar.py[line:274] - INFO: epoch 003:   2428 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216, ups=0.85, wpb=254.2, bsz=100, num_updates=15030, lr=2.75703e-05, gnorm=0.729, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28564
2023-03-15 03:28:27 - progress_bar.py[line:274] - INFO: epoch 003:   2438 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.4, ups=0.87, wpb=254.4, bsz=100, num_updates=15040, lr=2.75536e-05, gnorm=0.793, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=28575
2023-03-15 03:28:39 - progress_bar.py[line:274] - INFO: epoch 003:   2448 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219, ups=0.86, wpb=255.3, bsz=100, num_updates=15050, lr=2.75369e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=28587
2023-03-15 03:28:51 - progress_bar.py[line:274] - INFO: epoch 003:   2458 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218, ups=0.85, wpb=255.3, bsz=100, num_updates=15060, lr=2.75203e-05, gnorm=0.768, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28599
2023-03-15 03:29:03 - progress_bar.py[line:274] - INFO: epoch 003:   2468 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.3, ups=0.85, wpb=253.4, bsz=100, num_updates=15070, lr=2.75036e-05, gnorm=0.771, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=28610
2023-03-15 03:29:14 - progress_bar.py[line:274] - INFO: epoch 003:   2478 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.8, ups=0.86, wpb=252.9, bsz=100, num_updates=15080, lr=2.74869e-05, gnorm=0.771, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=28622
2023-03-15 03:29:26 - progress_bar.py[line:274] - INFO: epoch 003:   2488 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.2, ups=0.86, wpb=252, bsz=100, num_updates=15090, lr=2.74702e-05, gnorm=0.717, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28634
2023-03-15 03:29:38 - progress_bar.py[line:274] - INFO: epoch 003:   2498 / 6313 loss=0.576, loss_v1=0, loss_v2=0, nll_loss=0.362, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=213.1, ups=0.85, wpb=251.9, bsz=100, num_updates=15100, lr=2.74536e-05, gnorm=0.772, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=28646
2023-03-15 03:29:49 - progress_bar.py[line:274] - INFO: epoch 003:   2508 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.7, ups=0.86, wpb=251.9, bsz=100, num_updates=15110, lr=2.74369e-05, gnorm=0.747, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=28657
2023-03-15 03:30:01 - progress_bar.py[line:274] - INFO: epoch 003:   2518 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.2, ups=0.86, wpb=254, bsz=100, num_updates=15120, lr=2.74202e-05, gnorm=0.745, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=28669
2023-03-15 03:30:13 - progress_bar.py[line:274] - INFO: epoch 003:   2528 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.7, ups=0.86, wpb=252.1, bsz=100, num_updates=15130, lr=2.74035e-05, gnorm=0.74, clip=10, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=28680
2023-03-15 03:30:24 - progress_bar.py[line:274] - INFO: epoch 003:   2538 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.3, ups=0.86, wpb=254.2, bsz=100, num_updates=15140, lr=2.73869e-05, gnorm=0.76, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28692
2023-03-15 03:30:36 - progress_bar.py[line:274] - INFO: epoch 003:   2548 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.9, ups=0.85, wpb=256.3, bsz=100, num_updates=15150, lr=2.73702e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=28704
2023-03-15 03:30:48 - progress_bar.py[line:274] - INFO: epoch 003:   2558 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.8, ups=0.87, wpb=253.4, bsz=100, num_updates=15160, lr=2.73535e-05, gnorm=0.743, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=28715
2023-03-15 03:30:59 - progress_bar.py[line:274] - INFO: epoch 003:   2568 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.3, ups=0.87, wpb=253, bsz=100, num_updates=15170, lr=2.73368e-05, gnorm=0.838, clip=0, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=28727
2023-03-15 03:31:11 - progress_bar.py[line:274] - INFO: epoch 003:   2578 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.9, ups=0.86, wpb=256, bsz=100, num_updates=15180, lr=2.73202e-05, gnorm=0.755, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28739
2023-03-15 03:31:22 - progress_bar.py[line:274] - INFO: epoch 003:   2588 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219, ups=0.86, wpb=255.2, bsz=100, num_updates=15190, lr=2.73035e-05, gnorm=0.658, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=28750
2023-03-15 03:31:34 - progress_bar.py[line:274] - INFO: epoch 003:   2598 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.3, ups=0.86, wpb=255, bsz=100, num_updates=15200, lr=2.72868e-05, gnorm=0.706, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=28762
2023-03-15 03:31:42 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 03:31:47 - progress_bar.py[line:274] - INFO: epoch 003:   2609 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=198.8, ups=0.79, wpb=252, bsz=100, num_updates=15210, lr=2.72702e-05, gnorm=0.784, clip=0, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=28775
2023-03-15 03:31:58 - progress_bar.py[line:274] - INFO: epoch 003:   2619 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.8, ups=0.86, wpb=255.7, bsz=100, num_updates=15220, lr=2.72535e-05, gnorm=0.702, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28786
2023-03-15 03:32:10 - progress_bar.py[line:274] - INFO: epoch 003:   2629 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.3, ups=0.86, wpb=254.5, bsz=100, num_updates=15230, lr=2.72368e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=28798
2023-03-15 03:32:21 - progress_bar.py[line:274] - INFO: epoch 003:   2639 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.8, ups=0.88, wpb=252.4, bsz=100, num_updates=15240, lr=2.72201e-05, gnorm=0.749, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=28809
2023-03-15 03:32:33 - progress_bar.py[line:274] - INFO: epoch 003:   2649 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215.4, ups=0.86, wpb=250.8, bsz=100, num_updates=15250, lr=2.72035e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28821
2023-03-15 03:32:45 - progress_bar.py[line:274] - INFO: epoch 003:   2659 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.8, ups=0.86, wpb=253.5, bsz=100, num_updates=15260, lr=2.71868e-05, gnorm=0.695, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28833
2023-03-15 03:32:56 - progress_bar.py[line:274] - INFO: epoch 003:   2669 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.3, ups=0.86, wpb=254.6, bsz=100, num_updates=15270, lr=2.71701e-05, gnorm=0.695, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=28844
2023-03-15 03:33:08 - progress_bar.py[line:274] - INFO: epoch 003:   2679 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.2, ups=0.86, wpb=253.5, bsz=100, num_updates=15280, lr=2.71534e-05, gnorm=0.672, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28856
2023-03-15 03:33:20 - progress_bar.py[line:274] - INFO: epoch 003:   2689 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.5, ups=0.86, wpb=253.8, bsz=100, num_updates=15290, lr=2.71368e-05, gnorm=0.681, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=28867
2023-03-15 03:33:31 - progress_bar.py[line:274] - INFO: epoch 003:   2699 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.1, ups=0.86, wpb=251.5, bsz=100, num_updates=15300, lr=2.71201e-05, gnorm=0.72, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=28879
2023-03-15 03:33:43 - progress_bar.py[line:274] - INFO: epoch 003:   2709 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.5, ups=0.86, wpb=252.5, bsz=100, num_updates=15310, lr=2.71034e-05, gnorm=0.774, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=28891
2023-03-15 03:33:55 - progress_bar.py[line:274] - INFO: epoch 003:   2719 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.7, ups=0.85, wpb=254, bsz=100, num_updates=15320, lr=2.70867e-05, gnorm=0.691, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=28903
2023-03-15 03:34:06 - progress_bar.py[line:274] - INFO: epoch 003:   2729 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.3, ups=0.86, wpb=253.7, bsz=100, num_updates=15330, lr=2.70701e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=28914
2023-03-15 03:34:18 - progress_bar.py[line:274] - INFO: epoch 003:   2739 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.5, ups=0.86, wpb=253.9, bsz=100, num_updates=15340, lr=2.70534e-05, gnorm=0.811, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28926
2023-03-15 03:34:30 - progress_bar.py[line:274] - INFO: epoch 003:   2749 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.7, ups=0.86, wpb=254.4, bsz=100, num_updates=15350, lr=2.70367e-05, gnorm=0.753, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=28938
2023-03-15 03:34:41 - progress_bar.py[line:274] - INFO: epoch 003:   2759 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.7, ups=0.86, wpb=253.4, bsz=100, num_updates=15360, lr=2.702e-05, gnorm=0.729, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=28949
2023-03-15 03:34:53 - progress_bar.py[line:274] - INFO: epoch 003:   2769 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.8, ups=0.86, wpb=251.5, bsz=100, num_updates=15370, lr=2.70034e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=28961
2023-03-15 03:35:04 - progress_bar.py[line:274] - INFO: epoch 003:   2779 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.354, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.4, ups=0.87, wpb=251.4, bsz=100, num_updates=15380, lr=2.69867e-05, gnorm=0.775, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=28972
2023-03-15 03:35:16 - progress_bar.py[line:274] - INFO: epoch 003:   2789 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.2, ups=0.86, wpb=253.4, bsz=100, num_updates=15390, lr=2.697e-05, gnorm=0.773, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=28984
2023-03-15 03:35:28 - progress_bar.py[line:274] - INFO: epoch 003:   2799 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.6, ups=0.86, wpb=252.3, bsz=100, num_updates=15400, lr=2.69533e-05, gnorm=0.786, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=28996
2023-03-15 03:35:39 - progress_bar.py[line:274] - INFO: epoch 003:   2809 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.5, ups=0.87, wpb=254.4, bsz=100, num_updates=15410, lr=2.69367e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=29007
2023-03-15 03:35:51 - progress_bar.py[line:274] - INFO: epoch 003:   2819 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.6, ups=0.87, wpb=254.1, bsz=100, num_updates=15420, lr=2.692e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29019
2023-03-15 03:36:02 - progress_bar.py[line:274] - INFO: epoch 003:   2829 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.4, ups=0.86, wpb=252.6, bsz=100, num_updates=15430, lr=2.69033e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29030
2023-03-15 03:36:14 - progress_bar.py[line:274] - INFO: epoch 003:   2839 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=213.6, ups=0.85, wpb=251.9, bsz=100, num_updates=15440, lr=2.68867e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29042
2023-03-15 03:36:26 - progress_bar.py[line:274] - INFO: epoch 003:   2849 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=218.7, ups=0.87, wpb=251.6, bsz=100, num_updates=15450, lr=2.687e-05, gnorm=0.79, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29053
2023-03-15 03:36:37 - progress_bar.py[line:274] - INFO: epoch 003:   2859 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.8, ups=0.86, wpb=252.3, bsz=100, num_updates=15460, lr=2.68533e-05, gnorm=0.788, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=29065
2023-03-15 03:36:49 - progress_bar.py[line:274] - INFO: epoch 003:   2869 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.9, ups=0.86, wpb=255, bsz=100, num_updates=15470, lr=2.68366e-05, gnorm=0.683, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=29077
2023-03-15 03:37:01 - progress_bar.py[line:274] - INFO: epoch 003:   2879 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=257, nsentences=100, sample_size=257, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.1, ups=0.86, wpb=257, bsz=100, num_updates=15480, lr=2.682e-05, gnorm=0.664, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29088
2023-03-15 03:37:12 - progress_bar.py[line:274] - INFO: epoch 003:   2889 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215, ups=0.85, wpb=251.6, bsz=100, num_updates=15490, lr=2.68033e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=29100
2023-03-15 03:37:24 - progress_bar.py[line:274] - INFO: epoch 003:   2899 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.3, ups=0.87, wpb=251.8, bsz=100, num_updates=15500, lr=2.67866e-05, gnorm=0.747, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29112
2023-03-15 03:37:35 - progress_bar.py[line:274] - INFO: epoch 003:   2909 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.6, ups=0.86, wpb=254.4, bsz=100, num_updates=15510, lr=2.67699e-05, gnorm=0.753, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=29123
2023-03-15 03:37:47 - progress_bar.py[line:274] - INFO: epoch 003:   2919 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.4, ups=0.86, wpb=251.6, bsz=100, num_updates=15520, lr=2.67533e-05, gnorm=0.765, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29135
2023-03-15 03:37:59 - progress_bar.py[line:274] - INFO: epoch 003:   2929 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214.8, ups=0.86, wpb=250.5, bsz=100, num_updates=15530, lr=2.67366e-05, gnorm=0.739, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=29147
2023-03-15 03:38:10 - progress_bar.py[line:274] - INFO: epoch 003:   2939 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.8, ups=0.86, wpb=255.5, bsz=100, num_updates=15540, lr=2.67199e-05, gnorm=0.698, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=29158
2023-03-15 03:38:22 - progress_bar.py[line:274] - INFO: epoch 003:   2949 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=256.8, nsentences=100, sample_size=256.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.8, ups=0.86, wpb=256.8, bsz=100, num_updates=15550, lr=2.67032e-05, gnorm=0.683, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=29170
2023-03-15 03:38:34 - progress_bar.py[line:274] - INFO: epoch 003:   2959 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218, ups=0.85, wpb=255.8, bsz=100, num_updates=15560, lr=2.66866e-05, gnorm=0.644, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=29182
2023-03-15 03:38:46 - progress_bar.py[line:274] - INFO: epoch 003:   2969 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.2, ups=0.86, wpb=254, bsz=100, num_updates=15570, lr=2.66699e-05, gnorm=0.733, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29193
2023-03-15 03:38:57 - progress_bar.py[line:274] - INFO: epoch 003:   2979 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.9, ups=0.85, wpb=256.3, bsz=100, num_updates=15580, lr=2.66532e-05, gnorm=0.826, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=29205
2023-03-15 03:39:09 - progress_bar.py[line:274] - INFO: epoch 003:   2989 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.1, ups=0.86, wpb=253, bsz=100, num_updates=15590, lr=2.66365e-05, gnorm=0.761, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29217
2023-03-15 03:39:21 - progress_bar.py[line:274] - INFO: epoch 003:   2999 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.6, ups=0.86, wpb=252.8, bsz=100, num_updates=15600, lr=2.66199e-05, gnorm=0.752, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29228
2023-03-15 03:39:32 - progress_bar.py[line:274] - INFO: epoch 003:   3009 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=256.9, nsentences=100, sample_size=256.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.1, ups=0.86, wpb=256.9, bsz=100, num_updates=15610, lr=2.66032e-05, gnorm=0.692, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29240
2023-03-15 03:39:44 - progress_bar.py[line:274] - INFO: epoch 003:   3019 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=226.8, ups=0.88, wpb=256.4, bsz=100, num_updates=15620, lr=2.65865e-05, gnorm=0.74, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=29251
2023-03-15 03:39:55 - progress_bar.py[line:274] - INFO: epoch 003:   3029 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.3, ups=0.85, wpb=255.4, bsz=100, num_updates=15630, lr=2.65698e-05, gnorm=0.792, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29263
2023-03-15 03:40:07 - progress_bar.py[line:274] - INFO: epoch 003:   3039 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218, ups=0.86, wpb=253, bsz=100, num_updates=15640, lr=2.65532e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29275
2023-03-15 03:40:18 - progress_bar.py[line:274] - INFO: epoch 003:   3049 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.9, ups=0.87, wpb=250.3, bsz=100, num_updates=15650, lr=2.65365e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29286
2023-03-15 03:40:30 - progress_bar.py[line:274] - INFO: epoch 003:   3059 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222.5, ups=0.88, wpb=253.9, bsz=100, num_updates=15660, lr=2.65198e-05, gnorm=0.76, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29298
2023-03-15 03:40:42 - progress_bar.py[line:274] - INFO: epoch 003:   3069 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=256.8, nsentences=100, sample_size=256.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.7, ups=0.85, wpb=256.8, bsz=100, num_updates=15670, lr=2.65032e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29309
2023-03-15 03:40:53 - progress_bar.py[line:274] - INFO: epoch 003:   3079 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.3, ups=0.87, wpb=251.9, bsz=100, num_updates=15680, lr=2.64865e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29321
2023-03-15 03:41:05 - progress_bar.py[line:274] - INFO: epoch 003:   3089 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.9, ups=0.87, wpb=250.8, bsz=100, num_updates=15690, lr=2.64698e-05, gnorm=0.762, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29332
2023-03-15 03:41:16 - progress_bar.py[line:274] - INFO: epoch 003:   3099 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214.1, ups=0.85, wpb=253, bsz=100, num_updates=15700, lr=2.64531e-05, gnorm=0.785, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=29344
2023-03-15 03:41:28 - progress_bar.py[line:274] - INFO: epoch 003:   3109 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.4, ups=0.86, wpb=252, bsz=100, num_updates=15710, lr=2.64365e-05, gnorm=0.662, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=29356
2023-03-15 03:41:40 - progress_bar.py[line:274] - INFO: epoch 003:   3119 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.5, ups=0.86, wpb=250.4, bsz=100, num_updates=15720, lr=2.64198e-05, gnorm=0.734, clip=10, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=29367
2023-03-15 03:41:51 - progress_bar.py[line:274] - INFO: epoch 003:   3129 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.7, ups=0.86, wpb=253.5, bsz=100, num_updates=15730, lr=2.64031e-05, gnorm=0.723, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29379
2023-03-15 03:42:03 - progress_bar.py[line:274] - INFO: epoch 003:   3139 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.357, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.6, ups=0.86, wpb=253.8, bsz=100, num_updates=15740, lr=2.63864e-05, gnorm=0.793, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29391
2023-03-15 03:42:15 - progress_bar.py[line:274] - INFO: epoch 003:   3149 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.349, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.7, ups=0.85, wpb=251.3, bsz=100, num_updates=15750, lr=2.63698e-05, gnorm=0.713, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29403
2023-03-15 03:42:26 - progress_bar.py[line:274] - INFO: epoch 003:   3159 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.1, ups=0.86, wpb=253.5, bsz=100, num_updates=15760, lr=2.63531e-05, gnorm=0.714, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29414
2023-03-15 03:42:38 - progress_bar.py[line:274] - INFO: epoch 003:   3169 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219, ups=0.86, wpb=255.1, bsz=100, num_updates=15770, lr=2.63364e-05, gnorm=0.632, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29426
2023-03-15 03:42:47 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 03:42:51 - progress_bar.py[line:274] - INFO: epoch 003:   3180 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=202.8, ups=0.8, wpb=254.1, bsz=100, num_updates=15780, lr=2.63197e-05, gnorm=0.803, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29439
2023-03-15 03:43:02 - progress_bar.py[line:274] - INFO: epoch 003:   3190 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.6, ups=0.85, wpb=255.4, bsz=100, num_updates=15790, lr=2.63031e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29450
2023-03-15 03:43:14 - progress_bar.py[line:274] - INFO: epoch 003:   3200 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.3, ups=0.85, wpb=253.7, bsz=100, num_updates=15800, lr=2.62864e-05, gnorm=0.748, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29462
2023-03-15 03:43:26 - progress_bar.py[line:274] - INFO: epoch 003:   3210 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.3, ups=0.86, wpb=255.2, bsz=100, num_updates=15810, lr=2.62697e-05, gnorm=0.734, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=29474
2023-03-15 03:43:38 - progress_bar.py[line:274] - INFO: epoch 003:   3220 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217, ups=0.85, wpb=254.4, bsz=100, num_updates=15820, lr=2.6253e-05, gnorm=0.633, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29485
2023-03-15 03:43:49 - progress_bar.py[line:274] - INFO: epoch 003:   3230 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.5, ups=0.86, wpb=254.6, bsz=100, num_updates=15830, lr=2.62364e-05, gnorm=0.668, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=29497
2023-03-15 03:44:01 - progress_bar.py[line:274] - INFO: epoch 003:   3240 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219, ups=0.86, wpb=254.2, bsz=100, num_updates=15840, lr=2.62197e-05, gnorm=0.724, clip=0, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=29509
2023-03-15 03:44:13 - progress_bar.py[line:274] - INFO: epoch 003:   3250 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.6, ups=0.86, wpb=254.2, bsz=100, num_updates=15850, lr=2.6203e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29520
2023-03-15 03:44:24 - progress_bar.py[line:274] - INFO: epoch 003:   3260 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.7, ups=0.86, wpb=255.7, bsz=100, num_updates=15860, lr=2.61863e-05, gnorm=0.65, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29532
2023-03-15 03:44:36 - progress_bar.py[line:274] - INFO: epoch 003:   3270 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.9, ups=0.86, wpb=252.3, bsz=100, num_updates=15870, lr=2.61697e-05, gnorm=0.842, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=29544
2023-03-15 03:44:47 - progress_bar.py[line:274] - INFO: epoch 003:   3280 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.3, ups=0.86, wpb=255.5, bsz=100, num_updates=15880, lr=2.6153e-05, gnorm=0.785, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=29555
2023-03-15 03:44:59 - progress_bar.py[line:274] - INFO: epoch 003:   3290 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.3, ups=0.86, wpb=254.3, bsz=100, num_updates=15890, lr=2.61363e-05, gnorm=0.786, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29567
2023-03-15 03:45:11 - progress_bar.py[line:274] - INFO: epoch 003:   3300 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.3, ups=0.86, wpb=254, bsz=100, num_updates=15900, lr=2.61197e-05, gnorm=0.789, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=29579
2023-03-15 03:45:23 - progress_bar.py[line:274] - INFO: epoch 003:   3310 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.9, ups=0.85, wpb=254.7, bsz=100, num_updates=15910, lr=2.6103e-05, gnorm=0.804, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=29590
2023-03-15 03:45:34 - progress_bar.py[line:274] - INFO: epoch 003:   3320 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=213.7, ups=0.85, wpb=252.5, bsz=100, num_updates=15920, lr=2.60863e-05, gnorm=0.771, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=29602
2023-03-15 03:45:46 - progress_bar.py[line:274] - INFO: epoch 003:   3330 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.5, ups=0.86, wpb=255, bsz=100, num_updates=15930, lr=2.60696e-05, gnorm=0.768, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=29614
2023-03-15 03:45:57 - progress_bar.py[line:274] - INFO: epoch 003:   3340 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.6, ups=0.88, wpb=253.9, bsz=100, num_updates=15940, lr=2.6053e-05, gnorm=0.704, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29625
2023-03-15 03:46:09 - progress_bar.py[line:274] - INFO: epoch 003:   3350 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.4, ups=0.85, wpb=255.8, bsz=100, num_updates=15950, lr=2.60363e-05, gnorm=0.791, clip=20, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=29637
2023-03-15 03:46:21 - progress_bar.py[line:274] - INFO: epoch 003:   3360 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.2, ups=0.86, wpb=252.7, bsz=100, num_updates=15960, lr=2.60196e-05, gnorm=0.765, clip=0, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=29649
2023-03-15 03:46:32 - progress_bar.py[line:274] - INFO: epoch 003:   3370 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.8, ups=0.87, wpb=255.3, bsz=100, num_updates=15970, lr=2.60029e-05, gnorm=0.695, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=29660
2023-03-15 03:46:44 - progress_bar.py[line:274] - INFO: epoch 003:   3380 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=223.8, ups=0.88, wpb=253.6, bsz=100, num_updates=15980, lr=2.59863e-05, gnorm=0.764, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29672
2023-03-15 03:46:55 - progress_bar.py[line:274] - INFO: epoch 003:   3390 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.2, ups=0.86, wpb=253.2, bsz=100, num_updates=15990, lr=2.59696e-05, gnorm=0.773, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=29683
2023-03-15 03:47:07 - progress_bar.py[line:274] - INFO: epoch 003:   3400 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.9, ups=0.85, wpb=255.4, bsz=100, num_updates=16000, lr=2.59529e-05, gnorm=0.734, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29695
2023-03-15 03:47:19 - progress_bar.py[line:274] - INFO: epoch 003:   3410 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.7, ups=0.87, wpb=255.7, bsz=100, num_updates=16010, lr=2.59362e-05, gnorm=0.733, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29706
2023-03-15 03:47:30 - progress_bar.py[line:274] - INFO: epoch 003:   3420 / 6313 loss=0.569, loss_v1=0, loss_v2=0, nll_loss=0.353, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=216.5, ups=0.86, wpb=250.3, bsz=100, num_updates=16020, lr=2.59196e-05, gnorm=0.807, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29718
2023-03-15 03:47:42 - progress_bar.py[line:274] - INFO: epoch 003:   3430 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.3, ups=0.86, wpb=251.7, bsz=100, num_updates=16030, lr=2.59029e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29730
2023-03-15 03:47:54 - progress_bar.py[line:274] - INFO: epoch 003:   3440 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.6, ups=0.85, wpb=254.4, bsz=100, num_updates=16040, lr=2.58862e-05, gnorm=0.721, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=29742
2023-03-15 03:48:05 - progress_bar.py[line:274] - INFO: epoch 003:   3450 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.2, ups=0.87, wpb=253.3, bsz=100, num_updates=16050, lr=2.58695e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29753
2023-03-15 03:48:17 - progress_bar.py[line:274] - INFO: epoch 003:   3460 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.3, ups=0.86, wpb=253.3, bsz=100, num_updates=16060, lr=2.58529e-05, gnorm=0.68, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29765
2023-03-15 03:48:29 - progress_bar.py[line:274] - INFO: epoch 003:   3470 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215, ups=0.85, wpb=254.3, bsz=100, num_updates=16070, lr=2.58362e-05, gnorm=0.748, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29777
2023-03-15 03:48:40 - progress_bar.py[line:274] - INFO: epoch 003:   3480 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.8, ups=0.86, wpb=254.2, bsz=100, num_updates=16080, lr=2.58195e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29788
2023-03-15 03:48:52 - progress_bar.py[line:274] - INFO: epoch 003:   3490 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.7, ups=0.86, wpb=253.1, bsz=100, num_updates=16090, lr=2.58028e-05, gnorm=0.8, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29800
2023-03-15 03:49:04 - progress_bar.py[line:274] - INFO: epoch 003:   3500 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.1, ups=0.85, wpb=252.8, bsz=100, num_updates=16100, lr=2.57862e-05, gnorm=0.723, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=29812
2023-03-15 03:49:15 - progress_bar.py[line:274] - INFO: epoch 003:   3510 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.4, ups=0.86, wpb=254.1, bsz=100, num_updates=16110, lr=2.57695e-05, gnorm=0.759, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29823
2023-03-15 03:49:27 - progress_bar.py[line:274] - INFO: epoch 003:   3520 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=257.1, nsentences=100, sample_size=257.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.7, ups=0.86, wpb=257.1, bsz=100, num_updates=16120, lr=2.57528e-05, gnorm=0.712, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29835
2023-03-15 03:49:39 - progress_bar.py[line:274] - INFO: epoch 003:   3530 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.6, ups=0.86, wpb=253.3, bsz=100, num_updates=16130, lr=2.57362e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29846
2023-03-15 03:49:50 - progress_bar.py[line:274] - INFO: epoch 003:   3540 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.2, ups=0.86, wpb=255.2, bsz=100, num_updates=16140, lr=2.57195e-05, gnorm=0.788, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=29858
2023-03-15 03:50:02 - progress_bar.py[line:274] - INFO: epoch 003:   3550 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=213.1, ups=0.85, wpb=250.5, bsz=100, num_updates=16150, lr=2.57028e-05, gnorm=0.823, clip=20, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=29870
2023-03-15 03:50:14 - progress_bar.py[line:274] - INFO: epoch 003:   3560 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.1, ups=0.87, wpb=253, bsz=100, num_updates=16160, lr=2.56861e-05, gnorm=0.777, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=29881
2023-03-15 03:50:25 - progress_bar.py[line:274] - INFO: epoch 003:   3570 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.2, ups=0.86, wpb=255.5, bsz=100, num_updates=16170, lr=2.56695e-05, gnorm=0.698, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29893
2023-03-15 03:50:37 - progress_bar.py[line:274] - INFO: epoch 003:   3580 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219, ups=0.86, wpb=253.4, bsz=100, num_updates=16180, lr=2.56528e-05, gnorm=0.706, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=29905
2023-03-15 03:50:49 - progress_bar.py[line:274] - INFO: epoch 003:   3590 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.4, ups=0.85, wpb=255.6, bsz=100, num_updates=16190, lr=2.56361e-05, gnorm=0.752, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29916
2023-03-15 03:51:00 - progress_bar.py[line:274] - INFO: epoch 003:   3600 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.3, ups=0.86, wpb=251.7, bsz=100, num_updates=16200, lr=2.56194e-05, gnorm=0.82, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=29928
2023-03-15 03:51:12 - progress_bar.py[line:274] - INFO: epoch 003:   3610 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.1, ups=0.86, wpb=253.5, bsz=100, num_updates=16210, lr=2.56028e-05, gnorm=0.746, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29940
2023-03-15 03:51:23 - progress_bar.py[line:274] - INFO: epoch 003:   3620 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.6, ups=0.87, wpb=255.2, bsz=100, num_updates=16220, lr=2.55861e-05, gnorm=0.768, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=29951
2023-03-15 03:51:35 - progress_bar.py[line:274] - INFO: epoch 003:   3630 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.7, ups=0.87, wpb=254.8, bsz=100, num_updates=16230, lr=2.55694e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=29963
2023-03-15 03:51:47 - progress_bar.py[line:274] - INFO: epoch 003:   3640 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=249.1, nsentences=100, sample_size=249.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=211.9, ups=0.85, wpb=249.1, bsz=100, num_updates=16240, lr=2.55527e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=29975
2023-03-15 03:51:58 - progress_bar.py[line:274] - INFO: epoch 003:   3650 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.3, ups=0.86, wpb=254.1, bsz=100, num_updates=16250, lr=2.55361e-05, gnorm=0.762, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29986
2023-03-15 03:52:10 - progress_bar.py[line:274] - INFO: epoch 003:   3660 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.3, ups=0.86, wpb=253.1, bsz=100, num_updates=16260, lr=2.55194e-05, gnorm=0.772, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=29998
2023-03-15 03:52:22 - progress_bar.py[line:274] - INFO: epoch 003:   3670 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.1, ups=0.85, wpb=254.6, bsz=100, num_updates=16270, lr=2.55027e-05, gnorm=0.715, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30009
2023-03-15 03:52:33 - progress_bar.py[line:274] - INFO: epoch 003:   3680 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.8, ups=0.86, wpb=254.8, bsz=100, num_updates=16280, lr=2.5486e-05, gnorm=0.736, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30021
2023-03-15 03:52:45 - progress_bar.py[line:274] - INFO: epoch 003:   3690 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=257.9, nsentences=100, sample_size=257.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222, ups=0.86, wpb=257.9, bsz=100, num_updates=16290, lr=2.54694e-05, gnorm=0.78, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30033
2023-03-15 03:52:56 - progress_bar.py[line:274] - INFO: epoch 003:   3700 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.8, ups=0.86, wpb=252.5, bsz=100, num_updates=16300, lr=2.54527e-05, gnorm=0.759, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30044
2023-03-15 03:53:08 - progress_bar.py[line:274] - INFO: epoch 003:   3710 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=250, nsentences=100, sample_size=250, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=214.3, ups=0.86, wpb=250, bsz=100, num_updates=16310, lr=2.5436e-05, gnorm=0.822, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30056
2023-03-15 03:53:20 - progress_bar.py[line:274] - INFO: epoch 003:   3720 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217, ups=0.86, wpb=252.9, bsz=100, num_updates=16320, lr=2.54193e-05, gnorm=0.832, clip=10, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30068
2023-03-15 03:53:31 - progress_bar.py[line:274] - INFO: epoch 003:   3730 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.2, ups=0.86, wpb=253.4, bsz=100, num_updates=16330, lr=2.54027e-05, gnorm=0.869, clip=30, loss_scale=1024, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=30079
2023-03-15 03:53:43 - progress_bar.py[line:274] - INFO: epoch 003:   3740 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.9, ups=0.86, wpb=252.1, bsz=100, num_updates=16340, lr=2.5386e-05, gnorm=0.776, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30091
2023-03-15 03:53:55 - progress_bar.py[line:274] - INFO: epoch 003:   3750 / 6313 loss=0.573, loss_v1=0, loss_v2=0, nll_loss=0.361, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=215.7, ups=0.86, wpb=250.4, bsz=100, num_updates=16350, lr=2.53693e-05, gnorm=0.826, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30103
2023-03-15 03:54:07 - progress_bar.py[line:274] - INFO: epoch 003:   3760 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.5, ups=0.85, wpb=255.1, bsz=100, num_updates=16360, lr=2.53527e-05, gnorm=0.718, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30114
2023-03-15 03:54:18 - progress_bar.py[line:274] - INFO: epoch 003:   3770 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218, ups=0.86, wpb=254, bsz=100, num_updates=16370, lr=2.5336e-05, gnorm=0.772, clip=10, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=30126
2023-03-15 03:54:30 - progress_bar.py[line:274] - INFO: epoch 003:   3780 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.1, ups=0.86, wpb=254.9, bsz=100, num_updates=16380, lr=2.53193e-05, gnorm=0.736, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30138
2023-03-15 03:54:41 - progress_bar.py[line:274] - INFO: epoch 003:   3790 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=220.8, ups=0.87, wpb=254, bsz=100, num_updates=16390, lr=2.53026e-05, gnorm=0.689, clip=0, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=30149
2023-03-15 03:54:53 - progress_bar.py[line:274] - INFO: epoch 003:   3800 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215.4, ups=0.85, wpb=254.3, bsz=100, num_updates=16400, lr=2.5286e-05, gnorm=0.762, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30161
2023-03-15 03:55:04 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 03:55:06 - progress_bar.py[line:274] - INFO: epoch 003:   3811 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=199.2, ups=0.78, wpb=254.1, bsz=100, num_updates=16410, lr=2.52693e-05, gnorm=0.66, clip=0, loss_scale=512, train_wall=13, gb_free=10.4, ema_decay=0.9999, wall=30174
2023-03-15 03:55:18 - progress_bar.py[line:274] - INFO: epoch 003:   3821 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.6, ups=0.86, wpb=252.8, bsz=100, num_updates=16420, lr=2.52526e-05, gnorm=0.772, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=30185
2023-03-15 03:55:29 - progress_bar.py[line:274] - INFO: epoch 003:   3831 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.7, ups=0.86, wpb=254.4, bsz=100, num_updates=16430, lr=2.52359e-05, gnorm=0.792, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=30197
2023-03-15 03:55:41 - progress_bar.py[line:274] - INFO: epoch 003:   3841 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.2, ups=0.86, wpb=251.3, bsz=100, num_updates=16440, lr=2.52193e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=30209
2023-03-15 03:55:53 - progress_bar.py[line:274] - INFO: epoch 003:   3851 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.5, ups=0.86, wpb=251.7, bsz=100, num_updates=16450, lr=2.52026e-05, gnorm=0.697, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30220
2023-03-15 03:56:04 - progress_bar.py[line:274] - INFO: epoch 003:   3861 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.3, ups=0.86, wpb=250.4, bsz=100, num_updates=16460, lr=2.51859e-05, gnorm=0.81, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30232
2023-03-15 03:56:16 - progress_bar.py[line:274] - INFO: epoch 003:   3871 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.4, ups=0.86, wpb=255.1, bsz=100, num_updates=16470, lr=2.51692e-05, gnorm=0.653, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30244
2023-03-15 03:56:27 - progress_bar.py[line:274] - INFO: epoch 003:   3881 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.7, ups=0.87, wpb=251.1, bsz=100, num_updates=16480, lr=2.51526e-05, gnorm=0.845, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30255
2023-03-15 03:56:39 - progress_bar.py[line:274] - INFO: epoch 003:   3891 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.3, ups=0.87, wpb=255.4, bsz=100, num_updates=16490, lr=2.51359e-05, gnorm=0.69, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30267
2023-03-15 03:56:50 - progress_bar.py[line:274] - INFO: epoch 003:   3901 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.9, ups=0.87, wpb=251.5, bsz=100, num_updates=16500, lr=2.51192e-05, gnorm=0.8, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30278
2023-03-15 03:57:02 - progress_bar.py[line:274] - INFO: epoch 003:   3911 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.3, ups=0.86, wpb=253.3, bsz=100, num_updates=16510, lr=2.51025e-05, gnorm=0.783, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30290
2023-03-15 03:57:13 - progress_bar.py[line:274] - INFO: epoch 003:   3921 / 6313 loss=0.562, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.4, ups=0.86, wpb=250.4, bsz=100, num_updates=16520, lr=2.50859e-05, gnorm=0.755, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30301
2023-03-15 03:57:25 - progress_bar.py[line:274] - INFO: epoch 003:   3931 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.3, ups=0.85, wpb=254.1, bsz=100, num_updates=16530, lr=2.50692e-05, gnorm=0.785, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30313
2023-03-15 03:57:37 - progress_bar.py[line:274] - INFO: epoch 003:   3941 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.7, ups=0.86, wpb=254.7, bsz=100, num_updates=16540, lr=2.50525e-05, gnorm=0.753, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=30325
2023-03-15 03:57:49 - progress_bar.py[line:274] - INFO: epoch 003:   3951 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=215.7, ups=0.85, wpb=254.3, bsz=100, num_updates=16550, lr=2.50358e-05, gnorm=0.756, clip=0, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=30336
2023-03-15 03:58:00 - progress_bar.py[line:274] - INFO: epoch 003:   3961 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.7, ups=0.86, wpb=253.2, bsz=100, num_updates=16560, lr=2.50192e-05, gnorm=0.738, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30348
2023-03-15 03:58:12 - progress_bar.py[line:274] - INFO: epoch 003:   3971 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221, ups=0.87, wpb=253.3, bsz=100, num_updates=16570, lr=2.50025e-05, gnorm=0.784, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=30360
2023-03-15 03:58:24 - progress_bar.py[line:274] - INFO: epoch 003:   3981 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.8, ups=0.86, wpb=251.6, bsz=100, num_updates=16580, lr=2.49858e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30371
2023-03-15 03:58:35 - progress_bar.py[line:274] - INFO: epoch 003:   3991 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221, ups=0.87, wpb=253.4, bsz=100, num_updates=16590, lr=2.49692e-05, gnorm=0.708, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30383
2023-03-15 03:58:47 - progress_bar.py[line:274] - INFO: epoch 003:   4001 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.6, ups=0.85, wpb=254.7, bsz=100, num_updates=16600, lr=2.49525e-05, gnorm=0.778, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30395
2023-03-15 03:58:58 - progress_bar.py[line:274] - INFO: epoch 003:   4011 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.9, ups=0.86, wpb=251.2, bsz=100, num_updates=16610, lr=2.49358e-05, gnorm=0.741, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30406
2023-03-15 03:59:10 - progress_bar.py[line:274] - INFO: epoch 003:   4021 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.8, ups=0.86, wpb=252.1, bsz=100, num_updates=16620, lr=2.49191e-05, gnorm=0.737, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30418
2023-03-15 03:59:22 - progress_bar.py[line:274] - INFO: epoch 003:   4031 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.4, ups=0.86, wpb=250.3, bsz=100, num_updates=16630, lr=2.49025e-05, gnorm=0.763, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30429
2023-03-15 03:59:33 - progress_bar.py[line:274] - INFO: epoch 003:   4041 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.7, ups=0.86, wpb=252.3, bsz=100, num_updates=16640, lr=2.48858e-05, gnorm=0.8, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=30441
2023-03-15 03:59:45 - progress_bar.py[line:274] - INFO: epoch 003:   4051 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.4, ups=0.86, wpb=253.7, bsz=100, num_updates=16650, lr=2.48691e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30453
2023-03-15 03:59:57 - progress_bar.py[line:274] - INFO: epoch 003:   4061 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.5, ups=0.86, wpb=253.3, bsz=100, num_updates=16660, lr=2.48524e-05, gnorm=0.827, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30465
2023-03-15 04:00:08 - progress_bar.py[line:274] - INFO: epoch 003:   4071 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215.8, ups=0.86, wpb=250.6, bsz=100, num_updates=16670, lr=2.48358e-05, gnorm=0.847, clip=20, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=30476
2023-03-15 04:00:20 - progress_bar.py[line:274] - INFO: epoch 003:   4081 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.4, ups=0.86, wpb=251.3, bsz=100, num_updates=16680, lr=2.48191e-05, gnorm=0.742, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=30488
2023-03-15 04:00:32 - progress_bar.py[line:274] - INFO: epoch 003:   4091 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.7, ups=0.86, wpb=253.4, bsz=100, num_updates=16690, lr=2.48024e-05, gnorm=0.857, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30499
2023-03-15 04:00:43 - progress_bar.py[line:274] - INFO: epoch 003:   4101 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=223.1, ups=0.87, wpb=256.4, bsz=100, num_updates=16700, lr=2.47857e-05, gnorm=0.759, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30511
2023-03-15 04:00:55 - progress_bar.py[line:274] - INFO: epoch 003:   4111 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.9, ups=0.87, wpb=251.9, bsz=100, num_updates=16710, lr=2.47691e-05, gnorm=0.834, clip=10, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=30522
2023-03-15 04:01:06 - progress_bar.py[line:274] - INFO: epoch 003:   4121 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215.7, ups=0.85, wpb=252.6, bsz=100, num_updates=16720, lr=2.47524e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30534
2023-03-15 04:01:18 - progress_bar.py[line:274] - INFO: epoch 003:   4131 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.2, ups=0.86, wpb=254.9, bsz=100, num_updates=16730, lr=2.47357e-05, gnorm=0.782, clip=20, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=30546
2023-03-15 04:01:30 - progress_bar.py[line:274] - INFO: epoch 003:   4141 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.7, ups=0.86, wpb=253.7, bsz=100, num_updates=16740, lr=2.4719e-05, gnorm=0.745, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30557
2023-03-15 04:01:41 - progress_bar.py[line:274] - INFO: epoch 003:   4151 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.2, ups=0.86, wpb=254.6, bsz=100, num_updates=16750, lr=2.47024e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30569
2023-03-15 04:01:53 - progress_bar.py[line:274] - INFO: epoch 003:   4161 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=214.1, ups=0.85, wpb=251.6, bsz=100, num_updates=16760, lr=2.46857e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30581
2023-03-15 04:02:05 - progress_bar.py[line:274] - INFO: epoch 003:   4171 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215, ups=0.85, wpb=252.7, bsz=100, num_updates=16770, lr=2.4669e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=30593
2023-03-15 04:02:16 - progress_bar.py[line:274] - INFO: epoch 003:   4181 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.5, ups=0.87, wpb=252.2, bsz=100, num_updates=16780, lr=2.46523e-05, gnorm=0.765, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30604
2023-03-15 04:02:28 - progress_bar.py[line:274] - INFO: epoch 003:   4191 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=257.2, nsentences=100, sample_size=257.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.4, ups=0.85, wpb=257.2, bsz=100, num_updates=16790, lr=2.46357e-05, gnorm=0.697, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=30616
2023-03-15 04:02:40 - progress_bar.py[line:274] - INFO: epoch 003:   4201 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=219.4, ups=0.86, wpb=253.8, bsz=100, num_updates=16800, lr=2.4619e-05, gnorm=0.756, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30628
2023-03-15 04:02:51 - progress_bar.py[line:274] - INFO: epoch 003:   4211 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.8, ups=0.86, wpb=254.1, bsz=100, num_updates=16810, lr=2.46023e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30639
2023-03-15 04:03:03 - progress_bar.py[line:274] - INFO: epoch 003:   4221 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.8, ups=0.86, wpb=253.2, bsz=100, num_updates=16820, lr=2.45857e-05, gnorm=0.754, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30651
2023-03-15 04:03:15 - progress_bar.py[line:274] - INFO: epoch 003:   4231 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.2, ups=0.87, wpb=250.8, bsz=100, num_updates=16830, lr=2.4569e-05, gnorm=0.711, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30662
2023-03-15 04:03:26 - progress_bar.py[line:274] - INFO: epoch 003:   4241 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=257.3, nsentences=100, sample_size=257.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.3, ups=0.86, wpb=257.3, bsz=100, num_updates=16840, lr=2.45523e-05, gnorm=0.718, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=30674
2023-03-15 04:03:38 - progress_bar.py[line:274] - INFO: epoch 003:   4251 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218, ups=0.86, wpb=253.8, bsz=100, num_updates=16850, lr=2.45356e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30686
2023-03-15 04:03:49 - progress_bar.py[line:274] - INFO: epoch 003:   4261 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.2, ups=0.86, wpb=252.4, bsz=100, num_updates=16860, lr=2.4519e-05, gnorm=0.782, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30697
2023-03-15 04:04:01 - progress_bar.py[line:274] - INFO: epoch 003:   4271 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.7, ups=0.86, wpb=250.7, bsz=100, num_updates=16870, lr=2.45023e-05, gnorm=0.786, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30709
2023-03-15 04:04:13 - progress_bar.py[line:274] - INFO: epoch 003:   4281 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.7, ups=0.86, wpb=252.2, bsz=100, num_updates=16880, lr=2.44856e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=30720
2023-03-15 04:04:24 - progress_bar.py[line:274] - INFO: epoch 003:   4291 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221, ups=0.88, wpb=252.3, bsz=100, num_updates=16890, lr=2.44689e-05, gnorm=0.768, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30732
2023-03-15 04:04:36 - progress_bar.py[line:274] - INFO: epoch 003:   4301 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.7, ups=0.87, wpb=253.5, bsz=100, num_updates=16900, lr=2.44523e-05, gnorm=0.724, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=30743
2023-03-15 04:04:47 - progress_bar.py[line:274] - INFO: epoch 003:   4311 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.8, ups=0.86, wpb=255.2, bsz=100, num_updates=16910, lr=2.44356e-05, gnorm=0.698, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=30755
2023-03-15 04:04:59 - progress_bar.py[line:274] - INFO: epoch 003:   4321 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219, ups=0.86, wpb=254.7, bsz=100, num_updates=16920, lr=2.44189e-05, gnorm=0.787, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=30767
2023-03-15 04:05:10 - progress_bar.py[line:274] - INFO: epoch 003:   4331 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.2, ups=0.86, wpb=253.6, bsz=100, num_updates=16930, lr=2.44022e-05, gnorm=0.719, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=30778
2023-03-15 04:05:22 - progress_bar.py[line:274] - INFO: epoch 003:   4341 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.3, ups=0.86, wpb=256.4, bsz=100, num_updates=16940, lr=2.43856e-05, gnorm=0.752, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30790
2023-03-15 04:05:34 - progress_bar.py[line:274] - INFO: epoch 003:   4351 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.2, ups=0.86, wpb=251, bsz=100, num_updates=16950, lr=2.43689e-05, gnorm=0.843, clip=20, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30802
2023-03-15 04:05:45 - progress_bar.py[line:274] - INFO: epoch 003:   4361 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.9, ups=0.86, wpb=255.7, bsz=100, num_updates=16960, lr=2.43522e-05, gnorm=0.73, clip=0, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=30813
2023-03-15 04:05:57 - progress_bar.py[line:274] - INFO: epoch 003:   4371 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.6, ups=0.87, wpb=252.4, bsz=100, num_updates=16970, lr=2.43355e-05, gnorm=0.701, clip=0, loss_scale=1024, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=30825
2023-03-15 04:06:08 - progress_bar.py[line:274] - INFO: epoch 003:   4381 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.9, ups=0.86, wpb=254.2, bsz=100, num_updates=16980, lr=2.43189e-05, gnorm=0.738, clip=10, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30836
2023-03-15 04:06:20 - progress_bar.py[line:274] - INFO: epoch 003:   4391 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.8, ups=0.86, wpb=254.8, bsz=100, num_updates=16990, lr=2.43022e-05, gnorm=0.78, clip=20, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=30848
2023-03-15 04:06:31 - progress_bar.py[line:274] - INFO: epoch 003:   4401 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=224.3, ups=0.89, wpb=251.2, bsz=100, num_updates=17000, lr=2.42855e-05, gnorm=0.753, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=30859
2023-03-15 04:06:43 - progress_bar.py[line:274] - INFO: epoch 003:   4411 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.5, ups=0.85, wpb=252.5, bsz=100, num_updates=17010, lr=2.42688e-05, gnorm=0.809, clip=20, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30871
2023-03-15 04:06:55 - progress_bar.py[line:274] - INFO: epoch 003:   4421 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.6, ups=0.87, wpb=253.2, bsz=100, num_updates=17020, lr=2.42522e-05, gnorm=0.696, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=30882
2023-03-15 04:07:06 - progress_bar.py[line:274] - INFO: epoch 003:   4431 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.1, ups=0.86, wpb=253, bsz=100, num_updates=17030, lr=2.42355e-05, gnorm=0.705, clip=0, loss_scale=1024, train_wall=12, gb_free=11, ema_decay=0.9999, wall=30894
2023-03-15 04:07:18 - progress_bar.py[line:274] - INFO: epoch 003:   4441 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=256.7, nsentences=100, sample_size=256.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.3, ups=0.85, wpb=256.7, bsz=100, num_updates=17040, lr=2.42188e-05, gnorm=0.707, clip=0, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=30906
2023-03-15 04:07:29 - progress_bar.py[line:274] - INFO: epoch 003:   4451 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.6, ups=0.87, wpb=252.8, bsz=100, num_updates=17050, lr=2.42022e-05, gnorm=0.709, clip=0, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=30917
2023-03-15 04:07:41 - progress_bar.py[line:274] - INFO: epoch 003:   4461 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=249.5, nsentences=100, sample_size=249.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.7, ups=0.86, wpb=249.5, bsz=100, num_updates=17060, lr=2.41855e-05, gnorm=0.797, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30929
2023-03-15 04:07:53 - progress_bar.py[line:274] - INFO: epoch 003:   4471 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.3, ups=0.86, wpb=253.7, bsz=100, num_updates=17070, lr=2.41688e-05, gnorm=0.831, clip=20, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=30940
2023-03-15 04:08:04 - progress_bar.py[line:274] - INFO: epoch 003:   4481 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.9, ups=0.86, wpb=253.6, bsz=100, num_updates=17080, lr=2.41521e-05, gnorm=0.718, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30952
2023-03-15 04:08:16 - progress_bar.py[line:274] - INFO: epoch 003:   4491 / 6313 loss=0.564, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=249.8, nsentences=100, sample_size=249.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.8, ups=0.87, wpb=249.8, bsz=100, num_updates=17090, lr=2.41355e-05, gnorm=0.739, clip=0, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=30964
2023-03-15 04:08:23 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 04:08:28 - progress_bar.py[line:274] - INFO: epoch 003:   4502 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=201.8, ups=0.8, wpb=252.2, bsz=100, num_updates=17100, lr=2.41188e-05, gnorm=0.798, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=30976
2023-03-15 04:08:40 - progress_bar.py[line:274] - INFO: epoch 003:   4512 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.6, ups=0.86, wpb=254.6, bsz=100, num_updates=17110, lr=2.41021e-05, gnorm=1.078, clip=10, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=30988
2023-03-15 04:08:52 - progress_bar.py[line:274] - INFO: epoch 003:   4522 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.2, ups=0.86, wpb=254.6, bsz=100, num_updates=17120, lr=2.40854e-05, gnorm=0.81, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31000
2023-03-15 04:09:03 - progress_bar.py[line:274] - INFO: epoch 003:   4532 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.4, ups=0.86, wpb=253.6, bsz=100, num_updates=17130, lr=2.40688e-05, gnorm=0.781, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31011
2023-03-15 04:09:15 - progress_bar.py[line:274] - INFO: epoch 003:   4542 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.6, ups=0.86, wpb=251.9, bsz=100, num_updates=17140, lr=2.40521e-05, gnorm=0.79, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31023
2023-03-15 04:09:27 - progress_bar.py[line:274] - INFO: epoch 003:   4552 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.8, ups=0.87, wpb=252.8, bsz=100, num_updates=17150, lr=2.40354e-05, gnorm=0.768, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31034
2023-03-15 04:09:38 - progress_bar.py[line:274] - INFO: epoch 003:   4562 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.1, ups=0.86, wpb=251.8, bsz=100, num_updates=17160, lr=2.40187e-05, gnorm=0.689, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31046
2023-03-15 04:09:50 - progress_bar.py[line:274] - INFO: epoch 003:   4572 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.8, ups=0.86, wpb=253.7, bsz=100, num_updates=17170, lr=2.40021e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31058
2023-03-15 04:10:01 - progress_bar.py[line:274] - INFO: epoch 003:   4582 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.5, ups=0.86, wpb=253.6, bsz=100, num_updates=17180, lr=2.39854e-05, gnorm=0.752, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=31069
2023-03-15 04:10:13 - progress_bar.py[line:274] - INFO: epoch 003:   4592 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=223.9, ups=0.89, wpb=252.7, bsz=100, num_updates=17190, lr=2.39687e-05, gnorm=0.726, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31080
2023-03-15 04:10:24 - progress_bar.py[line:274] - INFO: epoch 003:   4602 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.9, ups=0.87, wpb=254.7, bsz=100, num_updates=17200, lr=2.3952e-05, gnorm=0.755, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31092
2023-03-15 04:10:36 - progress_bar.py[line:274] - INFO: epoch 003:   4612 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.3, ups=0.86, wpb=255.1, bsz=100, num_updates=17210, lr=2.39354e-05, gnorm=0.739, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31104
2023-03-15 04:10:47 - progress_bar.py[line:274] - INFO: epoch 003:   4622 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.6, ups=0.86, wpb=251.3, bsz=100, num_updates=17220, lr=2.39187e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31115
2023-03-15 04:10:59 - progress_bar.py[line:274] - INFO: epoch 003:   4632 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.6, ups=0.86, wpb=254.1, bsz=100, num_updates=17230, lr=2.3902e-05, gnorm=0.678, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31127
2023-03-15 04:11:11 - progress_bar.py[line:274] - INFO: epoch 003:   4642 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.6, ups=0.86, wpb=252.6, bsz=100, num_updates=17240, lr=2.38854e-05, gnorm=0.796, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31139
2023-03-15 04:11:22 - progress_bar.py[line:274] - INFO: epoch 003:   4652 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218, ups=0.85, wpb=256.5, bsz=100, num_updates=17250, lr=2.38687e-05, gnorm=0.789, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31150
2023-03-15 04:11:34 - progress_bar.py[line:274] - INFO: epoch 003:   4662 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=220.6, ups=0.87, wpb=252.2, bsz=100, num_updates=17260, lr=2.3852e-05, gnorm=0.854, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31162
2023-03-15 04:11:45 - progress_bar.py[line:274] - INFO: epoch 003:   4672 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.9, ups=0.87, wpb=253.2, bsz=100, num_updates=17270, lr=2.38353e-05, gnorm=0.801, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31173
2023-03-15 04:11:57 - progress_bar.py[line:274] - INFO: epoch 003:   4682 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.8, ups=0.85, wpb=252.4, bsz=100, num_updates=17280, lr=2.38187e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31185
2023-03-15 04:12:09 - progress_bar.py[line:274] - INFO: epoch 003:   4692 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=248.6, nsentences=100, sample_size=248.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.1, ups=0.87, wpb=248.6, bsz=100, num_updates=17290, lr=2.3802e-05, gnorm=0.767, clip=0, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=31196
2023-03-15 04:12:20 - progress_bar.py[line:274] - INFO: epoch 003:   4702 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.6, ups=0.86, wpb=255.2, bsz=100, num_updates=17300, lr=2.37853e-05, gnorm=0.757, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=31208
2023-03-15 04:12:32 - progress_bar.py[line:274] - INFO: epoch 003:   4712 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.2, ups=0.87, wpb=252.2, bsz=100, num_updates=17310, lr=2.37686e-05, gnorm=0.784, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31219
2023-03-15 04:12:43 - progress_bar.py[line:274] - INFO: epoch 003:   4722 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.6, ups=0.86, wpb=255.1, bsz=100, num_updates=17320, lr=2.3752e-05, gnorm=0.76, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=31231
2023-03-15 04:12:55 - progress_bar.py[line:274] - INFO: epoch 003:   4732 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.3, ups=0.86, wpb=253.8, bsz=100, num_updates=17330, lr=2.37353e-05, gnorm=0.755, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31243
2023-03-15 04:13:06 - progress_bar.py[line:274] - INFO: epoch 003:   4742 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.4, ups=0.86, wpb=251.5, bsz=100, num_updates=17340, lr=2.37186e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=31254
2023-03-15 04:13:18 - progress_bar.py[line:274] - INFO: epoch 003:   4752 / 6313 loss=0.579, loss_v1=0, loss_v2=0, nll_loss=0.364, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.29, wps=215.5, ups=0.86, wpb=250.1, bsz=100, num_updates=17350, lr=2.37019e-05, gnorm=0.848, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=31266
2023-03-15 04:13:30 - progress_bar.py[line:274] - INFO: epoch 003:   4762 / 6313 loss=0.572, loss_v1=0, loss_v2=0, nll_loss=0.355, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=217.2, ups=0.86, wpb=252.2, bsz=100, num_updates=17360, lr=2.36853e-05, gnorm=0.836, clip=30, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=31278
2023-03-15 04:13:41 - progress_bar.py[line:274] - INFO: epoch 003:   4772 / 6313 loss=0.566, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=213.4, ups=0.85, wpb=250.6, bsz=100, num_updates=17370, lr=2.36686e-05, gnorm=0.799, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=31289
2023-03-15 04:13:53 - progress_bar.py[line:274] - INFO: epoch 003:   4782 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.3, ups=0.86, wpb=253.6, bsz=100, num_updates=17380, lr=2.36519e-05, gnorm=0.787, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=31301
2023-03-15 04:14:05 - progress_bar.py[line:274] - INFO: epoch 003:   4792 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217, ups=0.86, wpb=252.1, bsz=100, num_updates=17390, lr=2.36352e-05, gnorm=0.75, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=31313
2023-03-15 04:14:16 - progress_bar.py[line:274] - INFO: epoch 003:   4802 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.6, ups=0.86, wpb=253.4, bsz=100, num_updates=17400, lr=2.36186e-05, gnorm=0.809, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31324
2023-03-15 04:14:28 - progress_bar.py[line:274] - INFO: epoch 003:   4812 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=223.2, ups=0.88, wpb=252.9, bsz=100, num_updates=17410, lr=2.36019e-05, gnorm=0.78, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31335
2023-03-15 04:14:39 - progress_bar.py[line:274] - INFO: epoch 003:   4822 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.9, ups=0.88, wpb=251.2, bsz=100, num_updates=17420, lr=2.35852e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31347
2023-03-15 04:14:51 - progress_bar.py[line:274] - INFO: epoch 003:   4832 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.6, ups=0.86, wpb=255.1, bsz=100, num_updates=17430, lr=2.35685e-05, gnorm=0.788, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31358
2023-03-15 04:15:02 - progress_bar.py[line:274] - INFO: epoch 003:   4842 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.9, ups=0.87, wpb=253.7, bsz=100, num_updates=17440, lr=2.35519e-05, gnorm=0.777, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31370
2023-03-15 04:15:14 - progress_bar.py[line:274] - INFO: epoch 003:   4852 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.4, ups=0.86, wpb=253.9, bsz=100, num_updates=17450, lr=2.35352e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=31382
2023-03-15 04:15:25 - progress_bar.py[line:274] - INFO: epoch 003:   4862 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.4, ups=0.86, wpb=256.1, bsz=100, num_updates=17460, lr=2.35185e-05, gnorm=0.756, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31393
2023-03-15 04:15:37 - progress_bar.py[line:274] - INFO: epoch 003:   4872 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.6, ups=0.86, wpb=255.5, bsz=100, num_updates=17470, lr=2.35019e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=31405
2023-03-15 04:15:48 - progress_bar.py[line:274] - INFO: epoch 003:   4882 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.3, ups=0.87, wpb=251.2, bsz=100, num_updates=17480, lr=2.34852e-05, gnorm=0.77, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=31416
2023-03-15 04:16:00 - progress_bar.py[line:274] - INFO: epoch 003:   4892 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.1, ups=0.86, wpb=252, bsz=100, num_updates=17490, lr=2.34685e-05, gnorm=0.784, clip=0, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=31428
2023-03-15 04:16:12 - progress_bar.py[line:274] - INFO: epoch 003:   4902 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221, ups=0.86, wpb=255.9, bsz=100, num_updates=17500, lr=2.34518e-05, gnorm=0.792, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31439
2023-03-15 04:16:23 - progress_bar.py[line:274] - INFO: epoch 003:   4912 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.9, ups=0.86, wpb=255.3, bsz=100, num_updates=17510, lr=2.34352e-05, gnorm=0.723, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31451
2023-03-15 04:16:35 - progress_bar.py[line:274] - INFO: epoch 003:   4922 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.9, ups=0.87, wpb=254, bsz=100, num_updates=17520, lr=2.34185e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31463
2023-03-15 04:16:46 - progress_bar.py[line:274] - INFO: epoch 003:   4932 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.3, ups=0.87, wpb=253.2, bsz=100, num_updates=17530, lr=2.34018e-05, gnorm=0.754, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31474
2023-03-15 04:16:58 - progress_bar.py[line:274] - INFO: epoch 003:   4942 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.7, ups=0.86, wpb=255.4, bsz=100, num_updates=17540, lr=2.33851e-05, gnorm=0.69, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31486
2023-03-15 04:17:09 - progress_bar.py[line:274] - INFO: epoch 003:   4952 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.8, ups=0.87, wpb=254.6, bsz=100, num_updates=17550, lr=2.33685e-05, gnorm=0.85, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31497
2023-03-15 04:17:21 - progress_bar.py[line:274] - INFO: epoch 003:   4962 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=223.5, ups=0.88, wpb=255, bsz=100, num_updates=17560, lr=2.33518e-05, gnorm=0.736, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31509
2023-03-15 04:17:33 - progress_bar.py[line:274] - INFO: epoch 003:   4972 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.9, ups=0.85, wpb=254.3, bsz=100, num_updates=17570, lr=2.33351e-05, gnorm=0.783, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31520
2023-03-15 04:17:44 - progress_bar.py[line:274] - INFO: epoch 003:   4982 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=225.2, ups=0.89, wpb=254.2, bsz=100, num_updates=17580, lr=2.33184e-05, gnorm=0.71, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31532
2023-03-15 04:17:55 - progress_bar.py[line:274] - INFO: epoch 003:   4992 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.3, ups=0.86, wpb=250.7, bsz=100, num_updates=17590, lr=2.33018e-05, gnorm=0.991, clip=30, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=31543
2023-03-15 04:18:07 - progress_bar.py[line:274] - INFO: epoch 003:   5002 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.3, ups=0.85, wpb=254.2, bsz=100, num_updates=17600, lr=2.32851e-05, gnorm=0.764, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31555
2023-03-15 04:18:19 - progress_bar.py[line:274] - INFO: epoch 003:   5012 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=213.9, ups=0.85, wpb=251.3, bsz=100, num_updates=17610, lr=2.32684e-05, gnorm=0.755, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31567
2023-03-15 04:18:31 - progress_bar.py[line:274] - INFO: epoch 003:   5022 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.9, ups=0.86, wpb=255.3, bsz=100, num_updates=17620, lr=2.32517e-05, gnorm=0.746, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31578
2023-03-15 04:18:42 - progress_bar.py[line:274] - INFO: epoch 003:   5032 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.5, ups=0.87, wpb=251.3, bsz=100, num_updates=17630, lr=2.32351e-05, gnorm=0.819, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=31590
2023-03-15 04:18:53 - progress_bar.py[line:274] - INFO: epoch 003:   5042 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.8, ups=0.87, wpb=252.6, bsz=100, num_updates=17640, lr=2.32184e-05, gnorm=0.754, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31601
2023-03-15 04:19:05 - progress_bar.py[line:274] - INFO: epoch 003:   5052 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.4, ups=0.86, wpb=255.3, bsz=100, num_updates=17650, lr=2.32017e-05, gnorm=0.733, clip=10, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=31613
2023-03-15 04:19:10 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 04:19:18 - progress_bar.py[line:274] - INFO: epoch 003:   5063 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=201.9, ups=0.8, wpb=252.3, bsz=100, num_updates=17660, lr=2.3185e-05, gnorm=0.847, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31625
2023-03-15 04:19:29 - progress_bar.py[line:274] - INFO: epoch 003:   5073 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.2, ups=0.86, wpb=255.2, bsz=100, num_updates=17670, lr=2.31684e-05, gnorm=0.743, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=31637
2023-03-15 04:19:41 - progress_bar.py[line:274] - INFO: epoch 003:   5083 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.2, ups=0.86, wpb=255.5, bsz=100, num_updates=17680, lr=2.31517e-05, gnorm=0.764, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=31649
2023-03-15 04:19:52 - progress_bar.py[line:274] - INFO: epoch 003:   5093 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.8, ups=0.85, wpb=252.7, bsz=100, num_updates=17690, lr=2.3135e-05, gnorm=0.81, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31660
2023-03-15 04:20:04 - progress_bar.py[line:274] - INFO: epoch 003:   5103 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=248.9, nsentences=100, sample_size=248.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.9, ups=0.86, wpb=248.9, bsz=100, num_updates=17700, lr=2.31184e-05, gnorm=0.797, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=31672
2023-03-15 04:20:16 - progress_bar.py[line:274] - INFO: epoch 003:   5113 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219, ups=0.86, wpb=253.9, bsz=100, num_updates=17710, lr=2.31017e-05, gnorm=0.791, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31683
2023-03-15 04:20:27 - progress_bar.py[line:274] - INFO: epoch 003:   5123 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.4, ups=0.86, wpb=253.2, bsz=100, num_updates=17720, lr=2.3085e-05, gnorm=0.848, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=31695
2023-03-15 04:20:39 - progress_bar.py[line:274] - INFO: epoch 003:   5133 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.7, ups=0.86, wpb=252.9, bsz=100, num_updates=17730, lr=2.30683e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31707
2023-03-15 04:20:50 - progress_bar.py[line:274] - INFO: epoch 003:   5143 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.9, ups=0.87, wpb=255.1, bsz=100, num_updates=17740, lr=2.30517e-05, gnorm=0.782, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=31718
2023-03-15 04:21:02 - progress_bar.py[line:274] - INFO: epoch 003:   5153 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.4, ups=0.87, wpb=251.1, bsz=100, num_updates=17750, lr=2.3035e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=31730
2023-03-15 04:21:14 - progress_bar.py[line:274] - INFO: epoch 003:   5163 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.8, ups=0.86, wpb=253.4, bsz=100, num_updates=17760, lr=2.30183e-05, gnorm=0.766, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=31741
2023-03-15 04:21:25 - progress_bar.py[line:274] - INFO: epoch 003:   5173 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.5, ups=0.86, wpb=254.4, bsz=100, num_updates=17770, lr=2.30016e-05, gnorm=0.873, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31753
2023-03-15 04:21:37 - progress_bar.py[line:274] - INFO: epoch 003:   5183 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.7, ups=0.86, wpb=254.2, bsz=100, num_updates=17780, lr=2.2985e-05, gnorm=0.732, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=31765
2023-03-15 04:21:48 - progress_bar.py[line:274] - INFO: epoch 003:   5193 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220, ups=0.86, wpb=255.3, bsz=100, num_updates=17790, lr=2.29683e-05, gnorm=0.736, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31776
2023-03-15 04:22:00 - progress_bar.py[line:274] - INFO: epoch 003:   5203 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.4, ups=0.87, wpb=252.4, bsz=100, num_updates=17800, lr=2.29516e-05, gnorm=0.804, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=31788
2023-03-15 04:22:12 - progress_bar.py[line:274] - INFO: epoch 003:   5213 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.6, ups=0.86, wpb=252, bsz=100, num_updates=17810, lr=2.29349e-05, gnorm=0.774, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31799
2023-03-15 04:22:23 - progress_bar.py[line:274] - INFO: epoch 003:   5223 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.5, ups=0.86, wpb=251.1, bsz=100, num_updates=17820, lr=2.29183e-05, gnorm=0.766, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31811
2023-03-15 04:22:35 - progress_bar.py[line:274] - INFO: epoch 003:   5233 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.7, ups=0.87, wpb=254.7, bsz=100, num_updates=17830, lr=2.29016e-05, gnorm=0.667, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=31822
2023-03-15 04:22:46 - progress_bar.py[line:274] - INFO: epoch 003:   5243 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.4, ups=0.86, wpb=255.3, bsz=100, num_updates=17840, lr=2.28849e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=31834
2023-03-15 04:22:58 - progress_bar.py[line:274] - INFO: epoch 003:   5253 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.5, ups=0.86, wpb=254.2, bsz=100, num_updates=17850, lr=2.28682e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=31846
2023-03-15 04:23:10 - progress_bar.py[line:274] - INFO: epoch 003:   5263 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215.2, ups=0.85, wpb=252.2, bsz=100, num_updates=17860, lr=2.28516e-05, gnorm=0.736, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31857
2023-03-15 04:23:21 - progress_bar.py[line:274] - INFO: epoch 003:   5273 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218, ups=0.86, wpb=253.4, bsz=100, num_updates=17870, lr=2.28349e-05, gnorm=0.67, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=31869
2023-03-15 04:23:33 - progress_bar.py[line:274] - INFO: epoch 003:   5283 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.6, ups=0.86, wpb=253.5, bsz=100, num_updates=17880, lr=2.28182e-05, gnorm=0.812, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=31881
2023-03-15 04:23:44 - progress_bar.py[line:274] - INFO: epoch 003:   5293 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.1, ups=0.86, wpb=253.7, bsz=100, num_updates=17890, lr=2.28015e-05, gnorm=0.727, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31892
2023-03-15 04:23:56 - progress_bar.py[line:274] - INFO: epoch 003:   5303 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.2, ups=0.86, wpb=253.8, bsz=100, num_updates=17900, lr=2.27849e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31904
2023-03-15 04:24:08 - progress_bar.py[line:274] - INFO: epoch 003:   5313 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.3, ups=0.86, wpb=251.3, bsz=100, num_updates=17910, lr=2.27682e-05, gnorm=0.842, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=31915
2023-03-15 04:24:19 - progress_bar.py[line:274] - INFO: epoch 003:   5323 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.6, ups=0.86, wpb=254.1, bsz=100, num_updates=17920, lr=2.27515e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31927
2023-03-15 04:24:31 - progress_bar.py[line:274] - INFO: epoch 003:   5333 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.9, ups=0.86, wpb=251.5, bsz=100, num_updates=17930, lr=2.27349e-05, gnorm=0.799, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=31939
2023-03-15 04:24:42 - progress_bar.py[line:274] - INFO: epoch 003:   5343 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=224.1, ups=0.87, wpb=256.5, bsz=100, num_updates=17940, lr=2.27182e-05, gnorm=0.725, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=31950
2023-03-15 04:24:54 - progress_bar.py[line:274] - INFO: epoch 003:   5353 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.2, ups=0.86, wpb=251.3, bsz=100, num_updates=17950, lr=2.27015e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=31962
2023-03-15 04:25:07 - progress_bar.py[line:274] - INFO: epoch 003:   5363 / 6313 loss=0.563, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=215, ups=0.86, wpb=250.3, bsz=100, num_updates=17960, lr=2.26848e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=31973
2023-03-15 04:25:18 - progress_bar.py[line:274] - INFO: epoch 003:   5373 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222, ups=0.87, wpb=256.2, bsz=100, num_updates=17970, lr=2.26682e-05, gnorm=0.752, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=31986
2023-03-15 04:25:30 - progress_bar.py[line:274] - INFO: epoch 003:   5383 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.5, ups=0.85, wpb=255.9, bsz=100, num_updates=17980, lr=2.26515e-05, gnorm=0.714, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=31998
2023-03-15 04:25:42 - progress_bar.py[line:274] - INFO: epoch 003:   5393 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.6, ups=0.86, wpb=252.1, bsz=100, num_updates=17990, lr=2.26348e-05, gnorm=0.714, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=32009
2023-03-15 04:25:53 - progress_bar.py[line:274] - INFO: epoch 003:   5403 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.8, ups=0.86, wpb=255.2, bsz=100, num_updates=18000, lr=2.26181e-05, gnorm=0.705, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32021
2023-03-15 04:26:05 - progress_bar.py[line:274] - INFO: epoch 003:   5413 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.8, ups=0.87, wpb=251.8, bsz=100, num_updates=18010, lr=2.26015e-05, gnorm=0.729, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32033
2023-03-15 04:26:16 - progress_bar.py[line:274] - INFO: epoch 003:   5423 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.2, ups=0.88, wpb=252.7, bsz=100, num_updates=18020, lr=2.25848e-05, gnorm=0.776, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=32044
2023-03-15 04:26:28 - progress_bar.py[line:274] - INFO: epoch 003:   5433 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218, ups=0.86, wpb=252.9, bsz=100, num_updates=18030, lr=2.25681e-05, gnorm=0.833, clip=20, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=32056
2023-03-15 04:26:39 - progress_bar.py[line:274] - INFO: epoch 003:   5443 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.4, ups=0.86, wpb=250.8, bsz=100, num_updates=18040, lr=2.25514e-05, gnorm=0.715, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32067
2023-03-15 04:26:51 - progress_bar.py[line:274] - INFO: epoch 003:   5453 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.5, ups=0.86, wpb=253.3, bsz=100, num_updates=18050, lr=2.25348e-05, gnorm=0.813, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32079
2023-03-15 04:27:02 - progress_bar.py[line:274] - INFO: epoch 003:   5463 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221, ups=0.87, wpb=253, bsz=100, num_updates=18060, lr=2.25181e-05, gnorm=0.719, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32090
2023-03-15 04:27:14 - progress_bar.py[line:274] - INFO: epoch 003:   5473 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=223.8, ups=0.87, wpb=255.8, bsz=100, num_updates=18070, lr=2.25014e-05, gnorm=0.727, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32102
2023-03-15 04:27:26 - progress_bar.py[line:274] - INFO: epoch 003:   5483 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.9, ups=0.86, wpb=256.4, bsz=100, num_updates=18080, lr=2.24847e-05, gnorm=0.686, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32113
2023-03-15 04:27:37 - progress_bar.py[line:274] - INFO: epoch 003:   5493 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.2, ups=0.86, wpb=255.1, bsz=100, num_updates=18090, lr=2.24681e-05, gnorm=0.713, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32125
2023-03-15 04:27:49 - progress_bar.py[line:274] - INFO: epoch 003:   5503 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215.6, ups=0.85, wpb=252.8, bsz=100, num_updates=18100, lr=2.24514e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=32137
2023-03-15 04:28:01 - progress_bar.py[line:274] - INFO: epoch 003:   5513 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.345, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.1, ups=0.86, wpb=252.5, bsz=100, num_updates=18110, lr=2.24347e-05, gnorm=0.726, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32148
2023-03-15 04:28:12 - progress_bar.py[line:274] - INFO: epoch 003:   5523 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.339, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=218.3, ups=0.86, wpb=252.8, bsz=100, num_updates=18120, lr=2.2418e-05, gnorm=0.721, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32160
2023-03-15 04:28:24 - progress_bar.py[line:274] - INFO: epoch 003:   5533 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.344, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.8, ups=0.86, wpb=251.3, bsz=100, num_updates=18130, lr=2.24014e-05, gnorm=0.788, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=32172
2023-03-15 04:28:35 - progress_bar.py[line:274] - INFO: epoch 003:   5543 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.2, ups=0.86, wpb=253.2, bsz=100, num_updates=18140, lr=2.23847e-05, gnorm=0.786, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32183
2023-03-15 04:28:47 - progress_bar.py[line:274] - INFO: epoch 003:   5553 / 6313 loss=0.568, loss_v1=0, loss_v2=0, nll_loss=0.35, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.6, ups=0.86, wpb=250.5, bsz=100, num_updates=18150, lr=2.2368e-05, gnorm=0.845, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=32195
2023-03-15 04:28:59 - progress_bar.py[line:274] - INFO: epoch 003:   5563 / 6313 loss=0.565, loss_v1=0, loss_v2=0, nll_loss=0.348, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.5, ups=0.87, wpb=251.2, bsz=100, num_updates=18160, lr=2.23514e-05, gnorm=0.841, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32206
2023-03-15 04:29:10 - progress_bar.py[line:274] - INFO: epoch 003:   5573 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.1, ups=0.87, wpb=250.3, bsz=100, num_updates=18170, lr=2.23347e-05, gnorm=0.713, clip=0, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32218
2023-03-15 04:29:14 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 04:29:23 - progress_bar.py[line:274] - INFO: epoch 003:   5584 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=201, ups=0.79, wpb=253.6, bsz=100, num_updates=18180, lr=2.2318e-05, gnorm=0.709, clip=0, loss_scale=512, train_wall=13, gb_free=10.8, ema_decay=0.9999, wall=32230
2023-03-15 04:29:34 - progress_bar.py[line:274] - INFO: epoch 003:   5594 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.6, ups=0.86, wpb=250.4, bsz=100, num_updates=18190, lr=2.23013e-05, gnorm=0.779, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32242
2023-03-15 04:29:46 - progress_bar.py[line:274] - INFO: epoch 003:   5604 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.1, ups=0.86, wpb=251.2, bsz=100, num_updates=18200, lr=2.22847e-05, gnorm=0.749, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32254
2023-03-15 04:29:57 - progress_bar.py[line:274] - INFO: epoch 003:   5614 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=223.6, ups=0.88, wpb=254.9, bsz=100, num_updates=18210, lr=2.2268e-05, gnorm=0.742, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32265
2023-03-15 04:30:09 - progress_bar.py[line:274] - INFO: epoch 003:   5624 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.9, ups=0.85, wpb=254.3, bsz=100, num_updates=18220, lr=2.22513e-05, gnorm=0.822, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32277
2023-03-15 04:30:20 - progress_bar.py[line:274] - INFO: epoch 003:   5634 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.3, ups=0.86, wpb=255.7, bsz=100, num_updates=18230, lr=2.22346e-05, gnorm=0.666, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=32288
2023-03-15 04:30:32 - progress_bar.py[line:274] - INFO: epoch 003:   5644 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.5, ups=0.87, wpb=251.8, bsz=100, num_updates=18240, lr=2.2218e-05, gnorm=0.729, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32300
2023-03-15 04:30:44 - progress_bar.py[line:274] - INFO: epoch 003:   5654 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.3, ups=0.86, wpb=251.9, bsz=100, num_updates=18250, lr=2.22013e-05, gnorm=0.782, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32311
2023-03-15 04:30:55 - progress_bar.py[line:274] - INFO: epoch 003:   5664 / 6313 loss=0.554, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.2, ups=0.87, wpb=252.2, bsz=100, num_updates=18260, lr=2.21846e-05, gnorm=0.733, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=32323
2023-03-15 04:31:07 - progress_bar.py[line:274] - INFO: epoch 003:   5674 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.1, ups=0.87, wpb=254.4, bsz=100, num_updates=18270, lr=2.21679e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32334
2023-03-15 04:31:18 - progress_bar.py[line:274] - INFO: epoch 003:   5684 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.2, ups=0.86, wpb=254.3, bsz=100, num_updates=18280, lr=2.21513e-05, gnorm=0.829, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32346
2023-03-15 04:31:30 - progress_bar.py[line:274] - INFO: epoch 003:   5694 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=249.8, nsentences=100, sample_size=249.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.3, ups=0.86, wpb=249.8, bsz=100, num_updates=18290, lr=2.21346e-05, gnorm=0.838, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32358
2023-03-15 04:31:41 - progress_bar.py[line:274] - INFO: epoch 003:   5704 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.6, ups=0.87, wpb=252.5, bsz=100, num_updates=18300, lr=2.21179e-05, gnorm=0.688, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32369
2023-03-15 04:31:53 - progress_bar.py[line:274] - INFO: epoch 003:   5714 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.6, ups=0.86, wpb=254.7, bsz=100, num_updates=18310, lr=2.21012e-05, gnorm=0.768, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32381
2023-03-15 04:32:04 - progress_bar.py[line:274] - INFO: epoch 003:   5724 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.6, ups=0.87, wpb=253.8, bsz=100, num_updates=18320, lr=2.20846e-05, gnorm=0.76, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32392
2023-03-15 04:32:16 - progress_bar.py[line:274] - INFO: epoch 003:   5734 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.9, ups=0.87, wpb=255.4, bsz=100, num_updates=18330, lr=2.20679e-05, gnorm=0.785, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=32404
2023-03-15 04:32:28 - progress_bar.py[line:274] - INFO: epoch 003:   5744 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=214.9, ups=0.85, wpb=252.1, bsz=100, num_updates=18340, lr=2.20512e-05, gnorm=0.779, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=32415
2023-03-15 04:32:39 - progress_bar.py[line:274] - INFO: epoch 003:   5754 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.9, ups=0.86, wpb=251.8, bsz=100, num_updates=18350, lr=2.20345e-05, gnorm=0.736, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=32427
2023-03-15 04:32:51 - progress_bar.py[line:274] - INFO: epoch 003:   5764 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.8, ups=0.86, wpb=252.6, bsz=100, num_updates=18360, lr=2.20179e-05, gnorm=0.789, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=32439
2023-03-15 04:33:03 - progress_bar.py[line:274] - INFO: epoch 003:   5774 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.6, ups=0.86, wpb=254, bsz=100, num_updates=18370, lr=2.20012e-05, gnorm=0.784, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32450
2023-03-15 04:33:14 - progress_bar.py[line:274] - INFO: epoch 003:   5784 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.5, ups=0.86, wpb=254.2, bsz=100, num_updates=18380, lr=2.19845e-05, gnorm=0.752, clip=20, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=32462
2023-03-15 04:33:26 - progress_bar.py[line:274] - INFO: epoch 003:   5794 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221, ups=0.87, wpb=253.3, bsz=100, num_updates=18390, lr=2.19679e-05, gnorm=0.724, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=32473
2023-03-15 04:33:37 - progress_bar.py[line:274] - INFO: epoch 003:   5804 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222, ups=0.87, wpb=253.9, bsz=100, num_updates=18400, lr=2.19512e-05, gnorm=0.799, clip=10, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=32485
2023-03-15 04:33:49 - progress_bar.py[line:274] - INFO: epoch 003:   5814 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.7, ups=0.87, wpb=255.1, bsz=100, num_updates=18410, lr=2.19345e-05, gnorm=0.786, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=32496
2023-03-15 04:34:00 - progress_bar.py[line:274] - INFO: epoch 003:   5824 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.5, ups=0.86, wpb=255, bsz=100, num_updates=18420, lr=2.19178e-05, gnorm=0.722, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=32508
2023-03-15 04:34:12 - progress_bar.py[line:274] - INFO: epoch 003:   5834 / 6313 loss=0.56, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=249.5, nsentences=100, sample_size=249.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=217.9, ups=0.87, wpb=249.5, bsz=100, num_updates=18430, lr=2.19012e-05, gnorm=0.751, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32519
2023-03-15 04:34:23 - progress_bar.py[line:274] - INFO: epoch 003:   5844 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=223.7, ups=0.88, wpb=255.5, bsz=100, num_updates=18440, lr=2.18845e-05, gnorm=0.804, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32531
2023-03-15 04:34:35 - progress_bar.py[line:274] - INFO: epoch 003:   5854 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=249.5, nsentences=100, sample_size=249.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.4, ups=0.87, wpb=249.5, bsz=100, num_updates=18450, lr=2.18678e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32542
2023-03-15 04:34:46 - progress_bar.py[line:274] - INFO: epoch 003:   5864 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.4, ups=0.86, wpb=252.6, bsz=100, num_updates=18460, lr=2.18511e-05, gnorm=0.795, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32554
2023-03-15 04:34:58 - progress_bar.py[line:274] - INFO: epoch 003:   5874 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=256.9, nsentences=100, sample_size=256.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.2, ups=0.86, wpb=256.9, bsz=100, num_updates=18470, lr=2.18345e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=32566
2023-03-15 04:35:09 - progress_bar.py[line:274] - INFO: epoch 003:   5884 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.337, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.2, ups=0.86, wpb=251.5, bsz=100, num_updates=18480, lr=2.18178e-05, gnorm=0.852, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32577
2023-03-15 04:35:21 - progress_bar.py[line:274] - INFO: epoch 003:   5894 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.5, ups=0.86, wpb=251.3, bsz=100, num_updates=18490, lr=2.18011e-05, gnorm=0.7, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32589
2023-03-15 04:35:33 - progress_bar.py[line:274] - INFO: epoch 003:   5904 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.3, ups=0.86, wpb=253.2, bsz=100, num_updates=18500, lr=2.17844e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=32601
2023-03-15 04:35:44 - progress_bar.py[line:274] - INFO: epoch 003:   5914 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.3, ups=0.86, wpb=252.9, bsz=100, num_updates=18510, lr=2.17678e-05, gnorm=0.674, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32612
2023-03-15 04:35:56 - progress_bar.py[line:274] - INFO: epoch 003:   5924 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.2, ups=0.86, wpb=253.1, bsz=100, num_updates=18520, lr=2.17511e-05, gnorm=0.815, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32624
2023-03-15 04:36:07 - progress_bar.py[line:274] - INFO: epoch 003:   5934 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.3, ups=0.87, wpb=254, bsz=100, num_updates=18530, lr=2.17344e-05, gnorm=0.673, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32635
2023-03-15 04:36:19 - progress_bar.py[line:274] - INFO: epoch 003:   5944 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.2, ups=0.86, wpb=252.6, bsz=100, num_updates=18540, lr=2.17177e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=32647
2023-03-15 04:36:30 - progress_bar.py[line:274] - INFO: epoch 003:   5954 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.3, ups=0.87, wpb=252.5, bsz=100, num_updates=18550, lr=2.17011e-05, gnorm=0.738, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32658
2023-03-15 04:36:42 - progress_bar.py[line:274] - INFO: epoch 003:   5964 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.9, ups=0.87, wpb=252.6, bsz=100, num_updates=18560, lr=2.16844e-05, gnorm=0.73, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=32670
2023-03-15 04:36:53 - progress_bar.py[line:274] - INFO: epoch 003:   5974 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.9, ups=0.88, wpb=254.2, bsz=100, num_updates=18570, lr=2.16677e-05, gnorm=0.742, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=32681
2023-03-15 04:37:05 - progress_bar.py[line:274] - INFO: epoch 003:   5984 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=215.1, ups=0.85, wpb=252.2, bsz=100, num_updates=18580, lr=2.1651e-05, gnorm=0.735, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=32693
2023-03-15 04:37:17 - progress_bar.py[line:274] - INFO: epoch 003:   5994 / 6313 loss=0.557, loss_v1=0, loss_v2=0, nll_loss=0.342, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=219.5, ups=0.86, wpb=254, bsz=100, num_updates=18590, lr=2.16344e-05, gnorm=0.776, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=32704
2023-03-15 04:37:28 - progress_bar.py[line:274] - INFO: epoch 003:   6004 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.6, ups=0.86, wpb=252.6, bsz=100, num_updates=18600, lr=2.16177e-05, gnorm=0.831, clip=20, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=32716
2023-03-15 04:37:40 - progress_bar.py[line:274] - INFO: epoch 003:   6014 / 6313 loss=0.558, loss_v1=0, loss_v2=0, nll_loss=0.338, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.5, ups=0.86, wpb=250.7, bsz=100, num_updates=18610, lr=2.1601e-05, gnorm=0.787, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32728
2023-03-15 04:37:51 - progress_bar.py[line:274] - INFO: epoch 003:   6024 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=224.6, ups=0.89, wpb=253.2, bsz=100, num_updates=18620, lr=2.15844e-05, gnorm=0.746, clip=0, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=32739
2023-03-15 04:38:03 - progress_bar.py[line:274] - INFO: epoch 003:   6034 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.9, ups=0.87, wpb=253.9, bsz=100, num_updates=18630, lr=2.15677e-05, gnorm=0.696, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=32750
2023-03-15 04:38:14 - progress_bar.py[line:274] - INFO: epoch 003:   6044 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.6, ups=0.86, wpb=253.2, bsz=100, num_updates=18640, lr=2.1551e-05, gnorm=0.799, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32762
2023-03-15 04:38:26 - progress_bar.py[line:274] - INFO: epoch 003:   6054 / 6313 loss=0.567, loss_v1=0, loss_v2=0, nll_loss=0.351, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.28, wps=222, ups=0.88, wpb=251.3, bsz=100, num_updates=18650, lr=2.15343e-05, gnorm=0.779, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=32773
2023-03-15 04:38:37 - progress_bar.py[line:274] - INFO: epoch 003:   6064 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=256.9, nsentences=100, sample_size=256.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.4, ups=0.86, wpb=256.9, bsz=100, num_updates=18660, lr=2.15177e-05, gnorm=0.72, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=32785
2023-03-15 04:38:49 - progress_bar.py[line:274] - INFO: epoch 003:   6074 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.8, ups=0.86, wpb=253.1, bsz=100, num_updates=18670, lr=2.1501e-05, gnorm=0.769, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32797
2023-03-15 04:39:00 - progress_bar.py[line:274] - INFO: epoch 003:   6084 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.2, ups=0.86, wpb=254.6, bsz=100, num_updates=18680, lr=2.14843e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=32808
2023-03-15 04:39:12 - progress_bar.py[line:274] - INFO: epoch 003:   6094 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.332, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217, ups=0.86, wpb=251.8, bsz=100, num_updates=18690, lr=2.14676e-05, gnorm=0.793, clip=0, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=32820
2023-03-15 04:39:24 - progress_bar.py[line:274] - INFO: epoch 003:   6104 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.4, ups=0.85, wpb=254.4, bsz=100, num_updates=18700, lr=2.1451e-05, gnorm=0.762, clip=0, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=32832
2023-03-15 04:39:35 - progress_bar.py[line:274] - INFO: epoch 003:   6114 / 6313 loss=0.555, loss_v1=0, loss_v2=0, nll_loss=0.335, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.4, ups=0.86, wpb=250.6, bsz=100, num_updates=18710, lr=2.14343e-05, gnorm=0.773, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32843
2023-03-15 04:39:47 - progress_bar.py[line:274] - INFO: epoch 003:   6124 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.5, ups=0.86, wpb=254.4, bsz=100, num_updates=18720, lr=2.14176e-05, gnorm=0.683, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32855
2023-03-15 04:39:59 - progress_bar.py[line:274] - INFO: epoch 003:   6134 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=214.1, ups=0.85, wpb=250.5, bsz=100, num_updates=18730, lr=2.14009e-05, gnorm=0.798, clip=10, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32866
2023-03-15 04:40:03 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 04:40:11 - progress_bar.py[line:274] - INFO: epoch 003:   6145 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=200.3, ups=0.79, wpb=253.1, bsz=100, num_updates=18740, lr=2.13843e-05, gnorm=0.693, clip=0, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=32879
2023-03-15 04:40:23 - progress_bar.py[line:274] - INFO: epoch 003:   6155 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=222.6, ups=0.87, wpb=254.6, bsz=100, num_updates=18750, lr=2.13676e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32891
2023-03-15 04:40:34 - progress_bar.py[line:274] - INFO: epoch 003:   6165 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.6, ups=0.86, wpb=253.9, bsz=100, num_updates=18760, lr=2.13509e-05, gnorm=0.743, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=32902
2023-03-15 04:40:46 - progress_bar.py[line:274] - INFO: epoch 003:   6175 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.336, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.3, ups=0.86, wpb=251.5, bsz=100, num_updates=18770, lr=2.13342e-05, gnorm=0.714, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=32914
2023-03-15 04:40:58 - progress_bar.py[line:274] - INFO: epoch 003:   6185 / 6313 loss=0.559, loss_v1=0, loss_v2=0, nll_loss=0.346, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.6, ups=0.85, wpb=253.5, bsz=100, num_updates=18780, lr=2.13176e-05, gnorm=0.702, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32925
2023-03-15 04:41:09 - progress_bar.py[line:274] - INFO: epoch 003:   6195 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221, ups=0.87, wpb=255.2, bsz=100, num_updates=18790, lr=2.13009e-05, gnorm=0.703, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=32937
2023-03-15 04:41:21 - progress_bar.py[line:274] - INFO: epoch 003:   6205 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.341, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=222.1, ups=0.88, wpb=253.7, bsz=100, num_updates=18800, lr=2.12842e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=32948
2023-03-15 04:41:32 - progress_bar.py[line:274] - INFO: epoch 003:   6215 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.5, ups=0.86, wpb=253.6, bsz=100, num_updates=18810, lr=2.12675e-05, gnorm=0.699, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32960
2023-03-15 04:41:44 - progress_bar.py[line:274] - INFO: epoch 003:   6225 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.2, ups=0.85, wpb=254.3, bsz=100, num_updates=18820, lr=2.12509e-05, gnorm=0.716, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=32972
2023-03-15 04:41:56 - progress_bar.py[line:274] - INFO: epoch 003:   6235 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.4, ups=0.86, wpb=252.5, bsz=100, num_updates=18830, lr=2.12342e-05, gnorm=0.733, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=32983
2023-03-15 04:42:07 - progress_bar.py[line:274] - INFO: epoch 003:   6245 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=256.7, nsentences=100, sample_size=256.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.9, ups=0.86, wpb=256.7, bsz=100, num_updates=18840, lr=2.12175e-05, gnorm=0.707, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=32995
2023-03-15 04:42:19 - progress_bar.py[line:274] - INFO: epoch 003:   6255 / 6313 loss=0.547, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.9, ups=0.86, wpb=253.6, bsz=100, num_updates=18850, lr=2.12009e-05, gnorm=0.782, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=33007
2023-03-15 04:42:30 - progress_bar.py[line:274] - INFO: epoch 003:   6265 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.331, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.8, ups=0.86, wpb=251.5, bsz=100, num_updates=18860, lr=2.11842e-05, gnorm=0.734, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=33018
2023-03-15 04:42:42 - progress_bar.py[line:274] - INFO: epoch 003:   6275 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.6, ups=0.86, wpb=254.5, bsz=100, num_updates=18870, lr=2.11675e-05, gnorm=0.723, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=33030
2023-03-15 04:42:53 - progress_bar.py[line:274] - INFO: epoch 003:   6285 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=257.5, nsentences=100, sample_size=257.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=227.8, ups=0.88, wpb=257.5, bsz=100, num_updates=18880, lr=2.11508e-05, gnorm=0.77, clip=0, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=33041
2023-03-15 04:43:05 - progress_bar.py[line:274] - INFO: epoch 003:   6295 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.1, ups=0.86, wpb=254.2, bsz=100, num_updates=18890, lr=2.11342e-05, gnorm=0.747, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=33053
2023-03-15 04:43:17 - progress_bar.py[line:274] - INFO: epoch 003:   6305 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.7, ups=0.86, wpb=253.7, bsz=100, num_updates=18900, lr=2.11175e-05, gnorm=0.788, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=33064
2023-03-15 04:43:26 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-03-15 04:43:27 - train.py[line:549] - INFO: 0 / 7052
2023-03-15 04:43:27 - train.py[line:551] - INFO: load:1.34 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-03-15 04:46:05 - train.py[line:549] - INFO: 200 / 7052
2023-03-15 04:46:05 - train.py[line:551] - INFO: load:1.36 valid_run:158.01 task_valid:148.18 collect_output:8.75
2023-03-15 04:48:39 - train.py[line:549] - INFO: 400 / 7052
2023-03-15 04:48:39 - train.py[line:551] - INFO: load:1.38 valid_run:312.03 task_valid:295.34 collect_output:14.57
2023-03-15 04:51:13 - train.py[line:549] - INFO: 600 / 7052
2023-03-15 04:51:13 - train.py[line:551] - INFO: load:1.40 valid_run:466.04 task_valid:440.64 collect_output:22.20
2023-03-15 04:53:51 - train.py[line:549] - INFO: 800 / 7052
2023-03-15 04:53:51 - train.py[line:551] - INFO: load:1.42 valid_run:623.27 task_valid:586.01 collect_output:33.04
2023-03-15 04:56:28 - train.py[line:549] - INFO: 1000 / 7052
2023-03-15 04:56:28 - train.py[line:551] - INFO: load:1.45 valid_run:780.62 task_valid:733.22 collect_output:42.16
2023-03-15 04:59:04 - train.py[line:549] - INFO: 1200 / 7052
2023-03-15 04:59:04 - train.py[line:551] - INFO: load:1.47 valid_run:936.14 task_valid:881.35 collect_output:48.51
2023-03-15 05:01:41 - train.py[line:549] - INFO: 1400 / 7052
2023-03-15 05:01:41 - train.py[line:551] - INFO: load:1.49 valid_run:1093.23 task_valid:1031.88 collect_output:53.98
2023-03-15 05:04:16 - train.py[line:549] - INFO: 1600 / 7052
2023-03-15 05:04:16 - train.py[line:551] - INFO: load:1.52 valid_run:1248.48 task_valid:1180.74 collect_output:59.34
2023-03-15 05:06:54 - train.py[line:549] - INFO: 1800 / 7052
2023-03-15 05:06:54 - train.py[line:551] - INFO: load:1.54 valid_run:1406.82 task_valid:1331.70 collect_output:65.66
2023-03-15 05:09:31 - train.py[line:549] - INFO: 2000 / 7052
2023-03-15 05:09:31 - train.py[line:551] - INFO: load:1.56 valid_run:1562.85 task_valid:1480.92 collect_output:71.42
2023-03-15 05:12:06 - train.py[line:549] - INFO: 2200 / 7052
2023-03-15 05:12:06 - train.py[line:551] - INFO: load:1.58 valid_run:1718.12 task_valid:1631.01 collect_output:75.52
2023-03-15 05:14:41 - train.py[line:549] - INFO: 2400 / 7052
2023-03-15 05:14:41 - train.py[line:551] - INFO: load:1.61 valid_run:1873.19 task_valid:1777.10 collect_output:83.47
2023-03-15 05:17:17 - train.py[line:549] - INFO: 2600 / 7052
2023-03-15 05:17:17 - train.py[line:551] - INFO: load:1.63 valid_run:2028.86 task_valid:1924.24 collect_output:90.96
2023-03-15 05:19:54 - train.py[line:549] - INFO: 2800 / 7052
2023-03-15 05:19:54 - train.py[line:551] - INFO: load:1.65 valid_run:2185.73 task_valid:2071.73 collect_output:99.27
2023-03-15 05:22:28 - train.py[line:549] - INFO: 3000 / 7052
2023-03-15 05:22:28 - train.py[line:551] - INFO: load:1.67 valid_run:2339.92 task_valid:2218.16 collect_output:105.99
2023-03-15 05:25:05 - train.py[line:549] - INFO: 3200 / 7052
2023-03-15 05:25:05 - train.py[line:551] - INFO: load:1.70 valid_run:2496.52 task_valid:2367.96 collect_output:111.74
2023-03-15 05:27:40 - train.py[line:549] - INFO: 3400 / 7052
2023-03-15 05:27:40 - train.py[line:551] - INFO: load:1.72 valid_run:2651.53 task_valid:2514.38 collect_output:119.29
2023-03-15 05:30:14 - train.py[line:549] - INFO: 3600 / 7052
2023-03-15 05:30:14 - train.py[line:551] - INFO: load:1.74 valid_run:2806.24 task_valid:2661.48 collect_output:125.83
2023-03-15 05:32:51 - train.py[line:549] - INFO: 3800 / 7052
2023-03-15 05:32:51 - train.py[line:551] - INFO: load:1.76 valid_run:2962.96 task_valid:2811.88 collect_output:131.10
2023-03-15 05:35:25 - train.py[line:549] - INFO: 4000 / 7052
2023-03-15 05:35:25 - train.py[line:551] - INFO: load:1.79 valid_run:3116.44 task_valid:2957.10 collect_output:138.27
2023-03-15 05:38:00 - train.py[line:549] - INFO: 4200 / 7052
2023-03-15 05:38:00 - train.py[line:551] - INFO: load:1.81 valid_run:3271.67 task_valid:3102.22 collect_output:147.31
2023-03-15 05:40:35 - train.py[line:549] - INFO: 4400 / 7052
2023-03-15 05:40:35 - train.py[line:551] - INFO: load:1.83 valid_run:3426.30 task_valid:3252.27 collect_output:150.84
2023-03-15 05:43:11 - train.py[line:549] - INFO: 4600 / 7052
2023-03-15 05:43:11 - train.py[line:551] - INFO: load:1.85 valid_run:3582.06 task_valid:3398.29 collect_output:159.50
2023-03-15 05:45:46 - train.py[line:549] - INFO: 4800 / 7052
2023-03-15 05:45:46 - train.py[line:551] - INFO: load:1.88 valid_run:3737.82 task_valid:3543.34 collect_output:169.13
2023-03-15 05:48:22 - train.py[line:549] - INFO: 5000 / 7052
2023-03-15 05:48:22 - train.py[line:551] - INFO: load:1.90 valid_run:3893.77 task_valid:3690.64 collect_output:176.72
2023-03-15 05:50:59 - train.py[line:549] - INFO: 5200 / 7052
2023-03-15 05:50:59 - train.py[line:551] - INFO: load:1.92 valid_run:4050.58 task_valid:3842.15 collect_output:180.95
2023-03-15 05:53:35 - train.py[line:549] - INFO: 5400 / 7052
2023-03-15 05:53:35 - train.py[line:551] - INFO: load:1.94 valid_run:4206.08 task_valid:3986.42 collect_output:191.13
2023-03-15 05:56:10 - train.py[line:549] - INFO: 5600 / 7052
2023-03-15 05:56:10 - train.py[line:551] - INFO: load:1.97 valid_run:4361.32 task_valid:4131.54 collect_output:200.17
2023-03-15 05:58:46 - train.py[line:549] - INFO: 5800 / 7052
2023-03-15 05:58:46 - train.py[line:551] - INFO: load:1.99 valid_run:4517.06 task_valid:4277.63 collect_output:208.76
2023-03-15 06:01:21 - train.py[line:549] - INFO: 6000 / 7052
2023-03-15 06:01:21 - train.py[line:551] - INFO: load:2.01 valid_run:4672.27 task_valid:4424.88 collect_output:215.65
2023-03-15 06:03:58 - train.py[line:549] - INFO: 6200 / 7052
2023-03-15 06:03:58 - train.py[line:551] - INFO: load:2.03 valid_run:4828.84 task_valid:4572.13 collect_output:223.92
2023-03-15 06:06:34 - train.py[line:549] - INFO: 6400 / 7052
2023-03-15 06:06:34 - train.py[line:551] - INFO: load:2.06 valid_run:4985.27 task_valid:4716.41 collect_output:235.04
2023-03-15 06:09:11 - train.py[line:549] - INFO: 6600 / 7052
2023-03-15 06:09:11 - train.py[line:551] - INFO: load:2.08 valid_run:5141.49 task_valid:4866.93 collect_output:239.68
2023-03-15 06:11:47 - train.py[line:549] - INFO: 6800 / 7052
2023-03-15 06:11:47 - train.py[line:551] - INFO: load:2.10 valid_run:5297.57 task_valid:5015.78 collect_output:245.83
2023-03-15 06:14:23 - train.py[line:549] - INFO: 7000 / 7052
2023-03-15 06:14:23 - train.py[line:551] - INFO: load:2.13 valid_run:5453.35 task_valid:5166.10 collect_output:250.24

====================================================================================================
SGG eval:     R @ 50: 0.6579;     R @ 100: 0.6714;     R @ 500: 0.6783;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2314;    mR @ 100: 0.2419;    mR @ 500: 0.2496;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.1007) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3782) (attached to:0.0000) (behind:0.4142) (belonging to:0.0000) (between:0.0000) (carrying:0.7500) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0769) (has:0.7540) (holding:0.3605) (in:0.3527) (in front of:0.3186) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5593) (of:0.3988) (on:0.8939) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.1690) (part of:0.0000) (playing:0.0000) (riding:0.5833) (says:0.0000) (sitting on:0.3675) (standing on:0.0769) (to:0.0000) (under:0.4167) (using:1.0000) (walking in:0.0000) (walking on:0.4958) (watching:0.3333) (wearing:0.9859) (wears:0.0000) (with:0.2247) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6579;     R @ 100: 0.6714;     R @ 500: 0.6783;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2314;    mR @ 100: 0.2419;    mR @ 500: 0.2496;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.1007) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3782) (attached to:0.0000) (behind:0.4142) (belonging to:0.0000) (between:0.0000) (carrying:0.7500) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0769) (has:0.7540) (holding:0.3605) (in:0.3527) (in front of:0.3186) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5593) (of:0.3988) (on:0.8939) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.1690) (part of:0.0000) (playing:0.0000) (riding:0.5833) (says:0.0000) (sitting on:0.3675) (standing on:0.0769) (to:0.0000) (under:0.4167) (using:1.0000) (walking in:0.0000) (walking on:0.4958) (watching:0.3333) (wearing:0.9859) (wears:0.0000) (with:0.2247) 
--------------------------------------------------------
====================================================================================================

2023-03-15 06:15:25 - train.py[line:487] - INFO: 0.6713760087975585

====================================================================================================
SGG eval:     R @ 50: 0.6579;     R @ 100: 0.6714;     R @ 500: 0.6783;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2314;    mR @ 100: 0.2419;    mR @ 500: 0.2496;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.1007) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3782) (attached to:0.0000) (behind:0.4142) (belonging to:0.0000) (between:0.0000) (carrying:0.7500) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0769) (has:0.7540) (holding:0.3605) (in:0.3527) (in front of:0.3186) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5593) (of:0.3988) (on:0.8939) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.1690) (part of:0.0000) (playing:0.0000) (riding:0.5833) (says:0.0000) (sitting on:0.3675) (standing on:0.0769) (to:0.0000) (under:0.4167) (using:1.0000) (walking in:0.0000) (walking on:0.4958) (watching:0.3333) (wearing:0.9859) (wears:0.0000) (with:0.2247) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6579;     R @ 100: 0.6714;     R @ 500: 0.6783;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2314;    mR @ 100: 0.2419;    mR @ 500: 0.2496;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.1007) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3782) (attached to:0.0000) (behind:0.4142) (belonging to:0.0000) (between:0.0000) (carrying:0.7500) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0769) (has:0.7540) (holding:0.3605) (in:0.3527) (in front of:0.3186) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5593) (of:0.3988) (on:0.8939) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.1690) (part of:0.0000) (playing:0.0000) (riding:0.5833) (says:0.0000) (sitting on:0.3675) (standing on:0.0769) (to:0.0000) (under:0.4167) (using:1.0000) (walking in:0.0000) (walking on:0.4958) (watching:0.3333) (wearing:0.9859) (wears:0.0000) (with:0.2247) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6579;     R @ 100: 0.6714;     R @ 500: 0.6783;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2314;    mR @ 100: 0.2419;    mR @ 500: 0.2496;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.1007) (across:0.0000) (against:0.0000) (along:0.0000) (and:0.0000) (at:0.3782) (attached to:0.0000) (behind:0.4142) (belonging to:0.0000) (between:0.0000) (carrying:0.7500) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.0769) (has:0.7540) (holding:0.3605) (in:0.3527) (in front of:0.3186) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5593) (of:0.3988) (on:0.8939) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.1690) (part of:0.0000) (playing:0.0000) (riding:0.5833) (says:0.0000) (sitting on:0.3675) (standing on:0.0769) (to:0.0000) (under:0.4167) (using:1.0000) (walking in:0.0000) (walking on:0.4958) (watching:0.3333) (wearing:0.9859) (wears:0.0000) (with:0.2247) 
--------------------------------------------------------
====================================================================================================

2023-03-15 06:15:25 - train.py[line:578] - INFO: logits:torch.Size([282060, 51]) sample_ids:torch.Size([282060])
2023-03-15 06:15:25 - progress_bar.py[line:282] - INFO: epoch 003 | valid on 'valid' subset | loss 0.312 | loss_v1 0 | loss_v2 0 | nll_loss 0.12 | ntokens 119.044 | nsentences 39.997 | sample_size 119.044 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.671376 | ppl 1.09 | vqa_score 0.6176 | wps 152.2 | wpb 119 | bsz 40 | num_updates 18908 | best_R@100 0.671376
2023-03-15 06:15:25 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 3 @ 18908 updates
2023-03-15 06:15:25 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint3.pt
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv slice_id 4 row count 126256 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv slice_id 2 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv slice_id 3 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv slice_id 1 row count 126257 total row count 631284
2023-03-15 06:15:35 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint3.pt
2023-03-15 06:15:41 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint3.pt (epoch 3 @ 18908 updates, score 0.6713760087975585) (writing took 15.462473833933473 seconds)
2023-03-15 06:15:41 - train.py[line:339] - INFO: end of epoch 3 (average epoch stats below)
2023-03-15 06:15:41 - progress_bar.py[line:282] - INFO: epoch 003 | loss 0.542 | loss_v1 0 | loss_v2 0 | nll_loss 0.325 | ntokens 253.351 | nsentences 99.997 | sample_size 253.351 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.25 | wps 124.1 | ups 0.49 | wpb 253.4 | bsz 100 | num_updates 18908 | lr 2.11041e-05 | gnorm 0.747 | clip 4.5 | loss_scale 512 | train_wall 7293 | gb_free 14.3 | ema_decay 0.9999 | wall 38608
2023-03-15 06:15:41 - trainer.py[line:694] - INFO: loading train data for epoch 4
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv slice_id 0 row count 126257 total row count 631284
2023-03-15 06:15:47 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-03-15 06:15:48 - trainer.py[line:758] - INFO: begin training epoch 4
2023-03-15 06:15:48 - train.py[line:312] - INFO: Start iterating over samples
2023-03-15 06:15:53 - progress_bar.py[line:274] - INFO: epoch 004:      2 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=247.9, nsentences=98.4, sample_size=247.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=0.4, ups=0, wpb=247.9, bsz=98.4, num_updates=18910, lr=2.11008e-05, gnorm=0.73, clip=0, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=38620
2023-03-15 06:16:04 - progress_bar.py[line:274] - INFO: epoch 004:     12 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.9, ups=0.86, wpb=252.8, bsz=100, num_updates=18920, lr=2.10841e-05, gnorm=0.733, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=38632
2023-03-15 06:16:16 - progress_bar.py[line:274] - INFO: epoch 004:     22 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.5, ups=0.86, wpb=253.6, bsz=100, num_updates=18930, lr=2.10675e-05, gnorm=0.758, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=38644
2023-03-15 06:16:27 - progress_bar.py[line:274] - INFO: epoch 004:     32 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.3, ups=0.86, wpb=252.4, bsz=100, num_updates=18940, lr=2.10508e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=38655
2023-03-15 06:16:39 - progress_bar.py[line:274] - INFO: epoch 004:     42 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.9, ups=0.87, wpb=253.8, bsz=100, num_updates=18950, lr=2.10341e-05, gnorm=0.697, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38667
2023-03-15 06:16:51 - progress_bar.py[line:274] - INFO: epoch 004:     52 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.6, ups=0.86, wpb=255.3, bsz=100, num_updates=18960, lr=2.10174e-05, gnorm=0.77, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=38678
2023-03-15 06:17:02 - progress_bar.py[line:274] - INFO: epoch 004:     62 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.4, ups=0.86, wpb=255.1, bsz=100, num_updates=18970, lr=2.10008e-05, gnorm=0.731, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=38690
2023-03-15 06:17:14 - progress_bar.py[line:274] - INFO: epoch 004:     72 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.7, ups=0.88, wpb=253, bsz=100, num_updates=18980, lr=2.09841e-05, gnorm=0.762, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38702
2023-03-15 06:17:25 - progress_bar.py[line:274] - INFO: epoch 004:     82 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.3, ups=0.86, wpb=252.9, bsz=100, num_updates=18990, lr=2.09674e-05, gnorm=0.824, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=38713
2023-03-15 06:17:37 - progress_bar.py[line:274] - INFO: epoch 004:     92 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.8, ups=0.88, wpb=251.8, bsz=100, num_updates=19000, lr=2.09507e-05, gnorm=0.794, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=38725
2023-03-15 06:17:48 - progress_bar.py[line:274] - INFO: epoch 004:    102 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.7, ups=0.87, wpb=254.3, bsz=100, num_updates=19010, lr=2.09341e-05, gnorm=0.796, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38736
2023-03-15 06:18:00 - progress_bar.py[line:274] - INFO: epoch 004:    112 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=214, ups=0.85, wpb=251.1, bsz=100, num_updates=19020, lr=2.09174e-05, gnorm=0.879, clip=20, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=38748
2023-03-15 06:18:12 - progress_bar.py[line:274] - INFO: epoch 004:    122 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.9, ups=0.86, wpb=252.8, bsz=100, num_updates=19030, lr=2.09007e-05, gnorm=0.823, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=38760
2023-03-15 06:18:23 - progress_bar.py[line:274] - INFO: epoch 004:    132 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.9, ups=0.86, wpb=252.3, bsz=100, num_updates=19040, lr=2.0884e-05, gnorm=0.865, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=38771
2023-03-15 06:18:35 - progress_bar.py[line:274] - INFO: epoch 004:    142 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216, ups=0.86, wpb=252.1, bsz=100, num_updates=19050, lr=2.08674e-05, gnorm=0.79, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=38783
2023-03-15 06:18:47 - progress_bar.py[line:274] - INFO: epoch 004:    152 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.6, ups=0.86, wpb=252.5, bsz=100, num_updates=19060, lr=2.08507e-05, gnorm=0.847, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=38795
2023-03-15 06:18:58 - progress_bar.py[line:274] - INFO: epoch 004:    162 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.9, ups=0.86, wpb=255, bsz=100, num_updates=19070, lr=2.0834e-05, gnorm=0.825, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=38806
2023-03-15 06:19:10 - progress_bar.py[line:274] - INFO: epoch 004:    172 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.4, ups=0.86, wpb=254, bsz=100, num_updates=19080, lr=2.08174e-05, gnorm=0.926, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=38818
2023-03-15 06:19:22 - progress_bar.py[line:274] - INFO: epoch 004:    182 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=257.7, nsentences=100, sample_size=257.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.9, ups=0.86, wpb=257.7, bsz=100, num_updates=19090, lr=2.08007e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=38829
2023-03-15 06:19:33 - progress_bar.py[line:274] - INFO: epoch 004:    192 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.5, ups=0.87, wpb=250.3, bsz=100, num_updates=19100, lr=2.0784e-05, gnorm=0.888, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=38841
2023-03-15 06:19:45 - progress_bar.py[line:274] - INFO: epoch 004:    202 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.4, ups=0.87, wpb=250.7, bsz=100, num_updates=19110, lr=2.07673e-05, gnorm=0.821, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38852
2023-03-15 06:19:56 - progress_bar.py[line:274] - INFO: epoch 004:    212 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.9, ups=0.86, wpb=254.6, bsz=100, num_updates=19120, lr=2.07507e-05, gnorm=0.826, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=38864
2023-03-15 06:20:08 - progress_bar.py[line:274] - INFO: epoch 004:    222 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.2, ups=0.86, wpb=252.4, bsz=100, num_updates=19130, lr=2.0734e-05, gnorm=0.74, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=38876
2023-03-15 06:20:19 - progress_bar.py[line:274] - INFO: epoch 004:    232 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.7, ups=0.86, wpb=254.3, bsz=100, num_updates=19140, lr=2.07173e-05, gnorm=0.88, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=38887
2023-03-15 06:20:31 - progress_bar.py[line:274] - INFO: epoch 004:    242 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=215.2, ups=0.86, wpb=250.8, bsz=100, num_updates=19150, lr=2.07006e-05, gnorm=0.886, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=38899
2023-03-15 06:20:43 - progress_bar.py[line:274] - INFO: epoch 004:    252 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.3, ups=0.86, wpb=252.3, bsz=100, num_updates=19160, lr=2.0684e-05, gnorm=0.845, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=38911
2023-03-15 06:20:54 - progress_bar.py[line:274] - INFO: epoch 004:    262 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.1, ups=0.86, wpb=254.9, bsz=100, num_updates=19170, lr=2.06673e-05, gnorm=0.829, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=38922
2023-03-15 06:21:06 - progress_bar.py[line:274] - INFO: epoch 004:    272 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=215.1, ups=0.86, wpb=251.2, bsz=100, num_updates=19180, lr=2.06506e-05, gnorm=0.927, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=38934
2023-03-15 06:21:18 - progress_bar.py[line:274] - INFO: epoch 004:    282 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.5, ups=0.86, wpb=251.8, bsz=100, num_updates=19190, lr=2.06339e-05, gnorm=0.784, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=38946
2023-03-15 06:21:29 - progress_bar.py[line:274] - INFO: epoch 004:    292 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.8, ups=0.86, wpb=252.3, bsz=100, num_updates=19200, lr=2.06173e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=38957
2023-03-15 06:21:41 - progress_bar.py[line:274] - INFO: epoch 004:    302 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.5, ups=0.86, wpb=252.8, bsz=100, num_updates=19210, lr=2.06006e-05, gnorm=0.825, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=38969
2023-03-15 06:21:52 - progress_bar.py[line:274] - INFO: epoch 004:    312 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.7, ups=0.88, wpb=254.5, bsz=100, num_updates=19220, lr=2.05839e-05, gnorm=0.866, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=38980
2023-03-15 06:22:04 - progress_bar.py[line:274] - INFO: epoch 004:    322 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.3, ups=0.87, wpb=251.9, bsz=100, num_updates=19230, lr=2.05672e-05, gnorm=0.887, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=38992
2023-03-15 06:22:15 - progress_bar.py[line:274] - INFO: epoch 004:    332 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.6, ups=0.87, wpb=254.2, bsz=100, num_updates=19240, lr=2.05506e-05, gnorm=0.828, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39003
2023-03-15 06:22:27 - progress_bar.py[line:274] - INFO: epoch 004:    342 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.6, ups=0.86, wpb=251.7, bsz=100, num_updates=19250, lr=2.05339e-05, gnorm=0.85, clip=20, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39015
2023-03-15 06:22:38 - progress_bar.py[line:274] - INFO: epoch 004:    352 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.5, ups=0.86, wpb=251.8, bsz=100, num_updates=19260, lr=2.05172e-05, gnorm=0.883, clip=20, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39026
2023-03-15 06:22:50 - progress_bar.py[line:274] - INFO: epoch 004:    362 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.3, ups=0.86, wpb=255, bsz=100, num_updates=19270, lr=2.05006e-05, gnorm=0.848, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=39038
2023-03-15 06:23:02 - progress_bar.py[line:274] - INFO: epoch 004:    372 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.5, ups=0.86, wpb=255, bsz=100, num_updates=19280, lr=2.04839e-05, gnorm=0.823, clip=10, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=39050
2023-03-15 06:23:13 - progress_bar.py[line:274] - INFO: epoch 004:    382 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.3, ups=0.86, wpb=253.6, bsz=100, num_updates=19290, lr=2.04672e-05, gnorm=0.884, clip=40, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=39061
2023-03-15 06:23:25 - progress_bar.py[line:274] - INFO: epoch 004:    392 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.4, ups=0.87, wpb=253.2, bsz=100, num_updates=19300, lr=2.04505e-05, gnorm=0.808, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39073
2023-03-15 06:23:37 - progress_bar.py[line:274] - INFO: epoch 004:    402 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.5, ups=0.86, wpb=253.1, bsz=100, num_updates=19310, lr=2.04339e-05, gnorm=0.798, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39084
2023-03-15 06:23:48 - progress_bar.py[line:274] - INFO: epoch 004:    412 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221, ups=0.86, wpb=255.6, bsz=100, num_updates=19320, lr=2.04172e-05, gnorm=0.837, clip=30, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=39096
2023-03-15 06:24:00 - progress_bar.py[line:274] - INFO: epoch 004:    422 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.9, ups=0.87, wpb=253, bsz=100, num_updates=19330, lr=2.04005e-05, gnorm=0.775, clip=10, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=39108
2023-03-15 06:24:11 - progress_bar.py[line:274] - INFO: epoch 004:    432 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218, ups=0.86, wpb=253, bsz=100, num_updates=19340, lr=2.03838e-05, gnorm=0.832, clip=10, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=39119
2023-03-15 06:24:23 - progress_bar.py[line:274] - INFO: epoch 004:    442 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.7, ups=0.86, wpb=253.5, bsz=100, num_updates=19350, lr=2.03672e-05, gnorm=0.828, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39131
2023-03-15 06:24:34 - progress_bar.py[line:274] - INFO: epoch 004:    452 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.3, ups=0.86, wpb=253.2, bsz=100, num_updates=19360, lr=2.03505e-05, gnorm=0.995, clip=30, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39142
2023-03-15 06:24:46 - progress_bar.py[line:274] - INFO: epoch 004:    462 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.9, ups=0.87, wpb=253, bsz=100, num_updates=19370, lr=2.03338e-05, gnorm=0.915, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39154
2023-03-15 06:24:58 - progress_bar.py[line:274] - INFO: epoch 004:    472 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.7, ups=0.85, wpb=253.7, bsz=100, num_updates=19380, lr=2.03171e-05, gnorm=0.904, clip=30, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39166
2023-03-15 06:25:02 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 06:25:10 - progress_bar.py[line:274] - INFO: epoch 004:    483 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=204.2, ups=0.81, wpb=252.8, bsz=100, num_updates=19390, lr=2.03005e-05, gnorm=0.845, clip=30, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=39178
2023-03-15 06:25:22 - progress_bar.py[line:274] - INFO: epoch 004:    493 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.4, ups=0.86, wpb=251.2, bsz=100, num_updates=19400, lr=2.02838e-05, gnorm=0.811, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39190
2023-03-15 06:25:33 - progress_bar.py[line:274] - INFO: epoch 004:    503 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.9, ups=0.86, wpb=252.7, bsz=100, num_updates=19410, lr=2.02671e-05, gnorm=0.849, clip=20, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=39201
2023-03-15 06:25:45 - progress_bar.py[line:274] - INFO: epoch 004:    513 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.9, ups=0.86, wpb=254.6, bsz=100, num_updates=19420, lr=2.02504e-05, gnorm=0.853, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39213
2023-03-15 06:25:57 - progress_bar.py[line:274] - INFO: epoch 004:    523 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.7, ups=0.86, wpb=255.9, bsz=100, num_updates=19430, lr=2.02338e-05, gnorm=0.787, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=39224
2023-03-15 06:26:08 - progress_bar.py[line:274] - INFO: epoch 004:    533 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.9, ups=0.87, wpb=252.9, bsz=100, num_updates=19440, lr=2.02171e-05, gnorm=0.843, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39236
2023-03-15 06:26:20 - progress_bar.py[line:274] - INFO: epoch 004:    543 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.1, ups=0.86, wpb=252.6, bsz=100, num_updates=19450, lr=2.02004e-05, gnorm=0.879, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39248
2023-03-15 06:26:32 - progress_bar.py[line:274] - INFO: epoch 004:    553 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218, ups=0.85, wpb=255.5, bsz=100, num_updates=19460, lr=2.01837e-05, gnorm=0.83, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39259
2023-03-15 06:26:43 - progress_bar.py[line:274] - INFO: epoch 004:    563 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=249.8, nsentences=100, sample_size=249.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.4, ups=0.87, wpb=249.8, bsz=100, num_updates=19470, lr=2.01671e-05, gnorm=0.961, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39271
2023-03-15 06:26:55 - progress_bar.py[line:274] - INFO: epoch 004:    573 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.5, ups=0.86, wpb=255.1, bsz=100, num_updates=19480, lr=2.01504e-05, gnorm=0.849, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39283
2023-03-15 06:27:06 - progress_bar.py[line:274] - INFO: epoch 004:    583 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.8, ups=0.86, wpb=252.7, bsz=100, num_updates=19490, lr=2.01337e-05, gnorm=0.798, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=39294
2023-03-15 06:27:18 - progress_bar.py[line:274] - INFO: epoch 004:    593 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.7, ups=0.86, wpb=255.6, bsz=100, num_updates=19500, lr=2.01171e-05, gnorm=0.914, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39306
2023-03-15 06:27:30 - progress_bar.py[line:274] - INFO: epoch 004:    603 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.9, ups=0.87, wpb=251.4, bsz=100, num_updates=19510, lr=2.01004e-05, gnorm=0.92, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=39317
2023-03-15 06:27:41 - progress_bar.py[line:274] - INFO: epoch 004:    613 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.2, ups=0.86, wpb=251.4, bsz=100, num_updates=19520, lr=2.00837e-05, gnorm=0.986, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39329
2023-03-15 06:27:53 - progress_bar.py[line:274] - INFO: epoch 004:    623 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219.3, ups=0.87, wpb=253.5, bsz=100, num_updates=19530, lr=2.0067e-05, gnorm=1.044, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=39341
2023-03-15 06:28:04 - progress_bar.py[line:274] - INFO: epoch 004:    633 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=215.8, ups=0.85, wpb=252.8, bsz=100, num_updates=19540, lr=2.00504e-05, gnorm=0.804, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=39352
2023-03-15 06:28:16 - progress_bar.py[line:274] - INFO: epoch 004:    643 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.4, ups=0.86, wpb=251.6, bsz=100, num_updates=19550, lr=2.00337e-05, gnorm=0.87, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=39364
2023-03-15 06:28:28 - progress_bar.py[line:274] - INFO: epoch 004:    653 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.3, ups=0.86, wpb=252.7, bsz=100, num_updates=19560, lr=2.0017e-05, gnorm=0.856, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=39375
2023-03-15 06:28:39 - progress_bar.py[line:274] - INFO: epoch 004:    663 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.2, ups=0.86, wpb=256, bsz=100, num_updates=19570, lr=2.00003e-05, gnorm=0.819, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39387
2023-03-15 06:28:51 - progress_bar.py[line:274] - INFO: epoch 004:    673 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.1, ups=0.86, wpb=255.9, bsz=100, num_updates=19580, lr=1.99837e-05, gnorm=0.842, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39399
2023-03-15 06:29:03 - progress_bar.py[line:274] - INFO: epoch 004:    683 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.8, ups=0.85, wpb=254.9, bsz=100, num_updates=19590, lr=1.9967e-05, gnorm=0.863, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39410
2023-03-15 06:29:15 - progress_bar.py[line:274] - INFO: epoch 004:    693 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.8, ups=0.86, wpb=252.4, bsz=100, num_updates=19600, lr=1.99503e-05, gnorm=0.868, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39422
2023-03-15 06:29:27 - progress_bar.py[line:274] - INFO: epoch 004:    703 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.7, ups=0.88, wpb=254.6, bsz=100, num_updates=19610, lr=1.99336e-05, gnorm=0.858, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39434
2023-03-15 06:29:39 - progress_bar.py[line:274] - INFO: epoch 004:    713 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.5, ups=0.87, wpb=255.8, bsz=100, num_updates=19620, lr=1.9917e-05, gnorm=0.822, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39446
2023-03-15 06:29:51 - progress_bar.py[line:274] - INFO: epoch 004:    723 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=215.8, ups=0.85, wpb=252.9, bsz=100, num_updates=19630, lr=1.99003e-05, gnorm=0.852, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39458
2023-03-15 06:30:03 - progress_bar.py[line:274] - INFO: epoch 004:    733 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.8, ups=0.86, wpb=253.7, bsz=100, num_updates=19640, lr=1.98836e-05, gnorm=0.794, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39470
2023-03-15 06:30:16 - progress_bar.py[line:274] - INFO: epoch 004:    743 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.2, ups=0.87, wpb=252.1, bsz=100, num_updates=19650, lr=1.98669e-05, gnorm=0.874, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39483
2023-03-15 06:30:29 - progress_bar.py[line:274] - INFO: epoch 004:    753 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.3, ups=0.87, wpb=252.7, bsz=100, num_updates=19660, lr=1.98503e-05, gnorm=0.981, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39495
2023-03-15 06:30:41 - progress_bar.py[line:274] - INFO: epoch 004:    763 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.7, ups=0.86, wpb=254.5, bsz=100, num_updates=19670, lr=1.98336e-05, gnorm=0.811, clip=0, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=39508
2023-03-15 06:30:54 - progress_bar.py[line:274] - INFO: epoch 004:    773 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.9, ups=0.87, wpb=250.7, bsz=100, num_updates=19680, lr=1.98169e-05, gnorm=0.873, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=39521
2023-03-15 06:31:07 - progress_bar.py[line:274] - INFO: epoch 004:    783 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.8, ups=0.88, wpb=251.2, bsz=100, num_updates=19690, lr=1.98002e-05, gnorm=0.871, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39534
2023-03-15 06:31:19 - progress_bar.py[line:274] - INFO: epoch 004:    793 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.3, ups=0.89, wpb=252, bsz=100, num_updates=19700, lr=1.97836e-05, gnorm=0.729, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39546
2023-03-15 06:31:32 - progress_bar.py[line:274] - INFO: epoch 004:    803 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.1, ups=0.87, wpb=253.4, bsz=100, num_updates=19710, lr=1.97669e-05, gnorm=0.899, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39559
2023-03-15 06:31:45 - progress_bar.py[line:274] - INFO: epoch 004:    813 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.3, ups=0.88, wpb=251.7, bsz=100, num_updates=19720, lr=1.97502e-05, gnorm=0.88, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39571
2023-03-15 06:31:58 - progress_bar.py[line:274] - INFO: epoch 004:    823 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.5, ups=0.87, wpb=253.4, bsz=100, num_updates=19730, lr=1.97336e-05, gnorm=0.878, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39584
2023-03-15 06:32:10 - progress_bar.py[line:274] - INFO: epoch 004:    833 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.4, ups=0.87, wpb=255.8, bsz=100, num_updates=19740, lr=1.97169e-05, gnorm=0.835, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39597
2023-03-15 06:32:23 - progress_bar.py[line:274] - INFO: epoch 004:    843 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.4, ups=0.88, wpb=254.2, bsz=100, num_updates=19750, lr=1.97002e-05, gnorm=0.876, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39610
2023-03-15 06:32:36 - progress_bar.py[line:274] - INFO: epoch 004:    853 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.7, ups=0.88, wpb=253.8, bsz=100, num_updates=19760, lr=1.96835e-05, gnorm=0.93, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39622
2023-03-15 06:32:49 - progress_bar.py[line:274] - INFO: epoch 004:    863 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219, ups=0.86, wpb=253.7, bsz=100, num_updates=19770, lr=1.96669e-05, gnorm=0.864, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=39635
2023-03-15 06:33:01 - progress_bar.py[line:274] - INFO: epoch 004:    873 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.2, ups=0.87, wpb=255.3, bsz=100, num_updates=19780, lr=1.96502e-05, gnorm=0.832, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39648
2023-03-15 06:33:14 - progress_bar.py[line:274] - INFO: epoch 004:    883 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.6, ups=0.88, wpb=252, bsz=100, num_updates=19790, lr=1.96335e-05, gnorm=0.891, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39660
2023-03-15 06:33:26 - progress_bar.py[line:274] - INFO: epoch 004:    893 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.8, ups=0.87, wpb=253.7, bsz=100, num_updates=19800, lr=1.96168e-05, gnorm=0.932, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39673
2023-03-15 06:33:38 - progress_bar.py[line:274] - INFO: epoch 004:    903 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217, ups=0.86, wpb=251.2, bsz=100, num_updates=19810, lr=1.96002e-05, gnorm=0.935, clip=30, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=39685
2023-03-15 06:33:50 - progress_bar.py[line:274] - INFO: epoch 004:    913 / 6313 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.4, ups=0.88, wpb=256.2, bsz=100, num_updates=19820, lr=1.95835e-05, gnorm=0.793, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=39697
2023-03-15 06:34:02 - progress_bar.py[line:274] - INFO: epoch 004:    923 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=249.2, nsentences=100, sample_size=249.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=214.5, ups=0.86, wpb=249.2, bsz=100, num_updates=19830, lr=1.95668e-05, gnorm=0.881, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=39710
2023-03-15 06:34:14 - progress_bar.py[line:274] - INFO: epoch 004:    933 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=226.5, ups=0.89, wpb=254.8, bsz=100, num_updates=19840, lr=1.95501e-05, gnorm=0.78, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39722
2023-03-15 06:34:27 - progress_bar.py[line:274] - INFO: epoch 004:    943 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.6, ups=0.86, wpb=256.1, bsz=100, num_updates=19850, lr=1.95335e-05, gnorm=0.829, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=39734
2023-03-15 06:34:39 - progress_bar.py[line:274] - INFO: epoch 004:    953 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.329, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=222.4, ups=0.89, wpb=251.1, bsz=100, num_updates=19860, lr=1.95168e-05, gnorm=0.916, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=39746
2023-03-15 06:34:51 - progress_bar.py[line:274] - INFO: epoch 004:    963 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.2, ups=0.85, wpb=253, bsz=100, num_updates=19870, lr=1.95001e-05, gnorm=0.781, clip=0, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=39758
2023-03-15 06:35:03 - progress_bar.py[line:274] - INFO: epoch 004:    973 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=221.9, ups=0.87, wpb=254.1, bsz=100, num_updates=19880, lr=1.94834e-05, gnorm=0.911, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39770
2023-03-15 06:35:16 - progress_bar.py[line:274] - INFO: epoch 004:    983 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.2, ups=0.86, wpb=251.9, bsz=100, num_updates=19890, lr=1.94668e-05, gnorm=0.806, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=39783
2023-03-15 06:35:28 - progress_bar.py[line:274] - INFO: epoch 004:    993 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.7, ups=0.87, wpb=254.3, bsz=100, num_updates=19900, lr=1.94501e-05, gnorm=0.839, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=39795
2023-03-15 06:35:40 - progress_bar.py[line:274] - INFO: epoch 004:   1003 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.6, ups=0.86, wpb=251.7, bsz=100, num_updates=19910, lr=1.94334e-05, gnorm=0.843, clip=0, loss_scale=1024, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=39807
2023-03-15 06:35:52 - progress_bar.py[line:274] - INFO: epoch 004:   1013 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222, ups=0.88, wpb=252.1, bsz=100, num_updates=19920, lr=1.94167e-05, gnorm=0.788, clip=10, loss_scale=1024, train_wall=11, gb_free=11, ema_decay=0.9999, wall=39819
2023-03-15 06:36:04 - progress_bar.py[line:274] - INFO: epoch 004:   1023 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=226.1, ups=0.89, wpb=255.2, bsz=100, num_updates=19930, lr=1.94001e-05, gnorm=0.909, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39831
2023-03-15 06:36:16 - progress_bar.py[line:274] - INFO: epoch 004:   1033 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.7, ups=0.87, wpb=252.8, bsz=100, num_updates=19940, lr=1.93834e-05, gnorm=0.869, clip=0, loss_scale=1024, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=39843
2023-03-15 06:36:28 - progress_bar.py[line:274] - INFO: epoch 004:   1043 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=223.1, ups=0.88, wpb=252.5, bsz=100, num_updates=19950, lr=1.93667e-05, gnorm=0.869, clip=0, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=39855
2023-03-15 06:36:35 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 06:36:41 - progress_bar.py[line:274] - INFO: epoch 004:   1054 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=205.1, ups=0.81, wpb=252.9, bsz=100, num_updates=19960, lr=1.93501e-05, gnorm=0.787, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39868
2023-03-15 06:36:53 - progress_bar.py[line:274] - INFO: epoch 004:   1064 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.2, ups=0.87, wpb=254.5, bsz=100, num_updates=19970, lr=1.93334e-05, gnorm=0.957, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=39880
2023-03-15 06:37:05 - progress_bar.py[line:274] - INFO: epoch 004:   1074 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=225.4, ups=0.89, wpb=252.9, bsz=100, num_updates=19980, lr=1.93167e-05, gnorm=0.866, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=39892
2023-03-15 06:37:17 - progress_bar.py[line:274] - INFO: epoch 004:   1084 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.5, ups=0.87, wpb=254.1, bsz=100, num_updates=19990, lr=1.93e-05, gnorm=0.875, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=39904
2023-03-15 06:37:29 - progress_bar.py[line:274] - INFO: epoch 004:   1094 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.9, ups=0.86, wpb=253, bsz=100, num_updates=20000, lr=1.92834e-05, gnorm=0.858, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=39916
2023-03-15 06:37:41 - progress_bar.py[line:274] - INFO: epoch 004:   1104 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.4, ups=0.86, wpb=251.8, bsz=100, num_updates=20010, lr=1.92667e-05, gnorm=0.844, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=39929
2023-03-15 06:37:54 - progress_bar.py[line:274] - INFO: epoch 004:   1114 / 6313 loss=0.561, loss_v1=0, loss_v2=0, nll_loss=0.343, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=214.1, ups=0.85, wpb=250.8, bsz=100, num_updates=20020, lr=1.925e-05, gnorm=0.903, clip=30, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=39941
2023-03-15 06:38:06 - progress_bar.py[line:274] - INFO: epoch 004:   1124 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223, ups=0.87, wpb=255.1, bsz=100, num_updates=20030, lr=1.92333e-05, gnorm=0.836, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=39953
2023-03-15 06:38:18 - progress_bar.py[line:274] - INFO: epoch 004:   1134 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.7, ups=0.87, wpb=253.8, bsz=100, num_updates=20040, lr=1.92167e-05, gnorm=0.932, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=39965
2023-03-15 06:38:30 - progress_bar.py[line:274] - INFO: epoch 004:   1144 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.1, ups=0.87, wpb=252.4, bsz=100, num_updates=20050, lr=1.92e-05, gnorm=0.845, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=39977
2023-03-15 06:38:42 - progress_bar.py[line:274] - INFO: epoch 004:   1154 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221, ups=0.87, wpb=253.7, bsz=100, num_updates=20060, lr=1.91833e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=39990
2023-03-15 06:38:55 - progress_bar.py[line:274] - INFO: epoch 004:   1164 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.2, ups=0.86, wpb=253, bsz=100, num_updates=20070, lr=1.91666e-05, gnorm=0.952, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=40002
2023-03-15 06:39:07 - progress_bar.py[line:274] - INFO: epoch 004:   1174 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.9, ups=0.88, wpb=251.9, bsz=100, num_updates=20080, lr=1.915e-05, gnorm=0.857, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40014
2023-03-15 06:39:19 - progress_bar.py[line:274] - INFO: epoch 004:   1184 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.7, ups=0.88, wpb=251.4, bsz=100, num_updates=20090, lr=1.91333e-05, gnorm=0.914, clip=20, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=40026
2023-03-15 06:39:30 - progress_bar.py[line:274] - INFO: epoch 004:   1194 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.4, ups=0.86, wpb=252, bsz=100, num_updates=20100, lr=1.91166e-05, gnorm=0.919, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=40038
2023-03-15 06:39:42 - progress_bar.py[line:274] - INFO: epoch 004:   1204 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.1, ups=0.86, wpb=253.5, bsz=100, num_updates=20110, lr=1.90999e-05, gnorm=0.858, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=40050
2023-03-15 06:39:54 - progress_bar.py[line:274] - INFO: epoch 004:   1214 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.2, ups=0.87, wpb=252.9, bsz=100, num_updates=20120, lr=1.90833e-05, gnorm=0.911, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40062
2023-03-15 06:40:06 - progress_bar.py[line:274] - INFO: epoch 004:   1224 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222, ups=0.88, wpb=253.7, bsz=100, num_updates=20130, lr=1.90666e-05, gnorm=0.987, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40073
2023-03-15 06:40:18 - progress_bar.py[line:274] - INFO: epoch 004:   1234 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.9, ups=0.85, wpb=253.7, bsz=100, num_updates=20140, lr=1.90499e-05, gnorm=0.847, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=40085
2023-03-15 06:40:29 - progress_bar.py[line:274] - INFO: epoch 004:   1244 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=223.5, ups=0.88, wpb=254.6, bsz=100, num_updates=20150, lr=1.90332e-05, gnorm=0.924, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40097
2023-03-15 06:40:41 - progress_bar.py[line:274] - INFO: epoch 004:   1254 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.2, ups=0.88, wpb=254.6, bsz=100, num_updates=20160, lr=1.90166e-05, gnorm=0.931, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40109
2023-03-15 06:40:53 - progress_bar.py[line:274] - INFO: epoch 004:   1264 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.1, ups=0.87, wpb=255.1, bsz=100, num_updates=20170, lr=1.89999e-05, gnorm=0.894, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40120
2023-03-15 06:41:04 - progress_bar.py[line:274] - INFO: epoch 004:   1274 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.5, ups=0.86, wpb=254.9, bsz=100, num_updates=20180, lr=1.89832e-05, gnorm=0.91, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40132
2023-03-15 06:41:16 - progress_bar.py[line:274] - INFO: epoch 004:   1284 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.8, ups=0.87, wpb=251.5, bsz=100, num_updates=20190, lr=1.89666e-05, gnorm=0.885, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40144
2023-03-15 06:41:28 - progress_bar.py[line:274] - INFO: epoch 004:   1294 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217, ups=0.86, wpb=252.2, bsz=100, num_updates=20200, lr=1.89499e-05, gnorm=0.935, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40156
2023-03-15 06:41:40 - progress_bar.py[line:274] - INFO: epoch 004:   1304 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.8, ups=0.87, wpb=254.7, bsz=100, num_updates=20210, lr=1.89332e-05, gnorm=0.858, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=40167
2023-03-15 06:41:51 - progress_bar.py[line:274] - INFO: epoch 004:   1314 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.7, ups=0.87, wpb=254.9, bsz=100, num_updates=20220, lr=1.89165e-05, gnorm=0.914, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40179
2023-03-15 06:42:03 - progress_bar.py[line:274] - INFO: epoch 004:   1324 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.1, ups=0.87, wpb=252.7, bsz=100, num_updates=20230, lr=1.88999e-05, gnorm=0.791, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=40191
2023-03-15 06:42:15 - progress_bar.py[line:274] - INFO: epoch 004:   1334 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.4, ups=0.86, wpb=253.1, bsz=100, num_updates=20240, lr=1.88832e-05, gnorm=0.835, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40203
2023-03-15 06:42:27 - progress_bar.py[line:274] - INFO: epoch 004:   1344 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=226, ups=0.89, wpb=253.1, bsz=100, num_updates=20250, lr=1.88665e-05, gnorm=0.775, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40214
2023-03-15 06:42:38 - progress_bar.py[line:274] - INFO: epoch 004:   1354 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.1, ups=0.87, wpb=255.4, bsz=100, num_updates=20260, lr=1.88498e-05, gnorm=0.846, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=40226
2023-03-15 06:42:50 - progress_bar.py[line:274] - INFO: epoch 004:   1364 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.1, ups=0.86, wpb=253, bsz=100, num_updates=20270, lr=1.88332e-05, gnorm=0.848, clip=0, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=40238
2023-03-15 06:43:02 - progress_bar.py[line:274] - INFO: epoch 004:   1374 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.9, ups=0.86, wpb=254.1, bsz=100, num_updates=20280, lr=1.88165e-05, gnorm=0.855, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=40250
2023-03-15 06:43:14 - progress_bar.py[line:274] - INFO: epoch 004:   1384 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.1, ups=0.88, wpb=252.4, bsz=100, num_updates=20290, lr=1.87998e-05, gnorm=0.878, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40261
2023-03-15 06:43:26 - progress_bar.py[line:274] - INFO: epoch 004:   1394 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.6, ups=0.86, wpb=250.2, bsz=100, num_updates=20300, lr=1.87831e-05, gnorm=0.965, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40273
2023-03-15 06:43:37 - progress_bar.py[line:274] - INFO: epoch 004:   1404 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.8, ups=0.86, wpb=255.4, bsz=100, num_updates=20310, lr=1.87665e-05, gnorm=0.817, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=40285
2023-03-15 06:43:49 - progress_bar.py[line:274] - INFO: epoch 004:   1414 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=249.9, nsentences=100, sample_size=249.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=215.6, ups=0.86, wpb=249.9, bsz=100, num_updates=20320, lr=1.87498e-05, gnorm=0.945, clip=30, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=40297
2023-03-15 06:44:01 - progress_bar.py[line:274] - INFO: epoch 004:   1424 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.9, ups=0.87, wpb=255.2, bsz=100, num_updates=20330, lr=1.87331e-05, gnorm=0.817, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40309
2023-03-15 06:44:13 - progress_bar.py[line:274] - INFO: epoch 004:   1434 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.4, ups=0.86, wpb=253.7, bsz=100, num_updates=20340, lr=1.87164e-05, gnorm=0.825, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=40320
2023-03-15 06:44:25 - progress_bar.py[line:274] - INFO: epoch 004:   1444 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.2, ups=0.86, wpb=252.8, bsz=100, num_updates=20350, lr=1.86998e-05, gnorm=0.9, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40332
2023-03-15 06:44:36 - progress_bar.py[line:274] - INFO: epoch 004:   1454 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.2, ups=0.87, wpb=252.3, bsz=100, num_updates=20360, lr=1.86831e-05, gnorm=0.899, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40344
2023-03-15 06:44:48 - progress_bar.py[line:274] - INFO: epoch 004:   1464 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.4, ups=0.86, wpb=254.8, bsz=100, num_updates=20370, lr=1.86664e-05, gnorm=0.92, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40356
2023-03-15 06:45:00 - progress_bar.py[line:274] - INFO: epoch 004:   1474 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221, ups=0.87, wpb=255.3, bsz=100, num_updates=20380, lr=1.86497e-05, gnorm=0.924, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40368
2023-03-15 06:45:12 - progress_bar.py[line:274] - INFO: epoch 004:   1484 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.8, ups=0.87, wpb=253.4, bsz=100, num_updates=20390, lr=1.86331e-05, gnorm=0.912, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40380
2023-03-15 06:45:24 - progress_bar.py[line:274] - INFO: epoch 004:   1494 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.4, ups=0.87, wpb=252.4, bsz=100, num_updates=20400, lr=1.86164e-05, gnorm=0.876, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40392
2023-03-15 06:45:36 - progress_bar.py[line:274] - INFO: epoch 004:   1504 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.5, ups=0.87, wpb=254.4, bsz=100, num_updates=20410, lr=1.85997e-05, gnorm=0.839, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=40403
2023-03-15 06:45:48 - progress_bar.py[line:274] - INFO: epoch 004:   1514 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=226.2, ups=0.89, wpb=254.2, bsz=100, num_updates=20420, lr=1.85831e-05, gnorm=0.903, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40415
2023-03-15 06:46:00 - progress_bar.py[line:274] - INFO: epoch 004:   1524 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219, ups=0.86, wpb=253.5, bsz=100, num_updates=20430, lr=1.85664e-05, gnorm=0.847, clip=10, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=40427
2023-03-15 06:46:11 - progress_bar.py[line:274] - INFO: epoch 004:   1534 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.2, ups=0.88, wpb=254.3, bsz=100, num_updates=20440, lr=1.85497e-05, gnorm=0.812, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=40439
2023-03-15 06:46:23 - progress_bar.py[line:274] - INFO: epoch 004:   1544 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.4, ups=0.86, wpb=255.1, bsz=100, num_updates=20450, lr=1.8533e-05, gnorm=0.854, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=40451
2023-03-15 06:46:35 - progress_bar.py[line:274] - INFO: epoch 004:   1554 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220, ups=0.86, wpb=255.9, bsz=100, num_updates=20460, lr=1.85164e-05, gnorm=0.941, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40463
2023-03-15 06:46:47 - progress_bar.py[line:274] - INFO: epoch 004:   1564 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.4, ups=0.87, wpb=255.4, bsz=100, num_updates=20470, lr=1.84997e-05, gnorm=0.855, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40475
2023-03-15 06:46:59 - progress_bar.py[line:274] - INFO: epoch 004:   1574 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=257.5, nsentences=100, sample_size=257.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.8, ups=0.87, wpb=257.5, bsz=100, num_updates=20480, lr=1.8483e-05, gnorm=0.9, clip=20, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=40486
2023-03-15 06:47:10 - progress_bar.py[line:274] - INFO: epoch 004:   1584 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.4, ups=0.88, wpb=252.7, bsz=100, num_updates=20490, lr=1.84663e-05, gnorm=0.895, clip=40, loss_scale=1024, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=40498
2023-03-15 06:47:22 - progress_bar.py[line:274] - INFO: epoch 004:   1594 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.7, ups=0.87, wpb=251, bsz=100, num_updates=20500, lr=1.84497e-05, gnorm=0.885, clip=10, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=40510
2023-03-15 06:47:34 - progress_bar.py[line:274] - INFO: epoch 004:   1604 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.5, ups=0.87, wpb=253.7, bsz=100, num_updates=20510, lr=1.8433e-05, gnorm=0.904, clip=20, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=40521
2023-03-15 06:47:45 - progress_bar.py[line:274] - INFO: epoch 004:   1614 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.6, ups=0.86, wpb=254.5, bsz=100, num_updates=20520, lr=1.84163e-05, gnorm=0.931, clip=20, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40533
2023-03-15 06:47:57 - progress_bar.py[line:274] - INFO: epoch 004:   1624 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=225.2, ups=0.89, wpb=254.5, bsz=100, num_updates=20530, lr=1.83996e-05, gnorm=0.838, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40545
2023-03-15 06:48:09 - progress_bar.py[line:274] - INFO: epoch 004:   1634 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=257.5, nsentences=100, sample_size=257.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224.6, ups=0.87, wpb=257.5, bsz=100, num_updates=20540, lr=1.8383e-05, gnorm=0.814, clip=0, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40556
2023-03-15 06:48:21 - progress_bar.py[line:274] - INFO: epoch 004:   1644 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.6, ups=0.87, wpb=254.2, bsz=100, num_updates=20550, lr=1.83663e-05, gnorm=0.844, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40568
2023-03-15 06:48:32 - progress_bar.py[line:274] - INFO: epoch 004:   1654 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.3, ups=0.87, wpb=252.2, bsz=100, num_updates=20560, lr=1.83496e-05, gnorm=0.818, clip=20, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=40580
2023-03-15 06:48:44 - progress_bar.py[line:274] - INFO: epoch 004:   1664 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.1, ups=0.86, wpb=251.3, bsz=100, num_updates=20570, lr=1.83329e-05, gnorm=0.807, clip=0, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=40592
2023-03-15 06:48:56 - progress_bar.py[line:274] - INFO: epoch 004:   1674 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.5, ups=0.87, wpb=253.3, bsz=100, num_updates=20580, lr=1.83163e-05, gnorm=0.843, clip=20, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=40603
2023-03-15 06:49:08 - progress_bar.py[line:274] - INFO: epoch 004:   1684 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.9, ups=0.86, wpb=254.4, bsz=100, num_updates=20590, lr=1.82996e-05, gnorm=0.908, clip=30, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=40615
2023-03-15 06:49:19 - progress_bar.py[line:274] - INFO: epoch 004:   1694 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.9, ups=0.88, wpb=252.3, bsz=100, num_updates=20600, lr=1.82829e-05, gnorm=0.865, clip=20, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40627
2023-03-15 06:49:31 - progress_bar.py[line:274] - INFO: epoch 004:   1704 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.7, ups=0.87, wpb=251.7, bsz=100, num_updates=20610, lr=1.82662e-05, gnorm=0.893, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40638
2023-03-15 06:49:43 - progress_bar.py[line:274] - INFO: epoch 004:   1714 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.6, ups=0.87, wpb=256.5, bsz=100, num_updates=20620, lr=1.82496e-05, gnorm=0.881, clip=20, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=40650
2023-03-15 06:49:47 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 06:49:56 - progress_bar.py[line:274] - INFO: epoch 004:   1725 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=200.4, ups=0.8, wpb=251, bsz=100, num_updates=20630, lr=1.82329e-05, gnorm=0.877, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=40663
2023-03-15 06:50:08 - progress_bar.py[line:274] - INFO: epoch 004:   1735 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.3, ups=0.86, wpb=254.1, bsz=100, num_updates=20640, lr=1.82162e-05, gnorm=0.985, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=40675
2023-03-15 06:50:19 - progress_bar.py[line:274] - INFO: epoch 004:   1745 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.5, ups=0.86, wpb=250.8, bsz=100, num_updates=20650, lr=1.81996e-05, gnorm=0.913, clip=20, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=40687
2023-03-15 06:50:31 - progress_bar.py[line:274] - INFO: epoch 004:   1755 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.5, ups=0.87, wpb=253.4, bsz=100, num_updates=20660, lr=1.81829e-05, gnorm=0.854, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40699
2023-03-15 06:50:43 - progress_bar.py[line:274] - INFO: epoch 004:   1765 / 6313 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=257, nsentences=100, sample_size=257, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=228.3, ups=0.89, wpb=257, bsz=100, num_updates=20670, lr=1.81662e-05, gnorm=0.741, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40710
2023-03-15 06:50:55 - progress_bar.py[line:274] - INFO: epoch 004:   1775 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.8, ups=0.87, wpb=254.8, bsz=100, num_updates=20680, lr=1.81495e-05, gnorm=0.847, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40722
2023-03-15 06:51:07 - progress_bar.py[line:274] - INFO: epoch 004:   1785 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.6, ups=0.87, wpb=254, bsz=100, num_updates=20690, lr=1.81329e-05, gnorm=0.761, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=40734
2023-03-15 06:51:19 - progress_bar.py[line:274] - INFO: epoch 004:   1795 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.8, ups=0.88, wpb=253.4, bsz=100, num_updates=20700, lr=1.81162e-05, gnorm=0.765, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40746
2023-03-15 06:51:31 - progress_bar.py[line:274] - INFO: epoch 004:   1805 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219, ups=0.86, wpb=254.1, bsz=100, num_updates=20710, lr=1.80995e-05, gnorm=0.733, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40758
2023-03-15 06:51:42 - progress_bar.py[line:274] - INFO: epoch 004:   1815 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.5, ups=0.87, wpb=252.4, bsz=100, num_updates=20720, lr=1.80828e-05, gnorm=0.87, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40770
2023-03-15 06:51:54 - progress_bar.py[line:274] - INFO: epoch 004:   1825 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.7, ups=0.86, wpb=252.4, bsz=100, num_updates=20730, lr=1.80662e-05, gnorm=0.92, clip=20, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=40782
2023-03-15 06:52:05 - progress_bar.py[line:274] - INFO: epoch 004:   1835 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=225.5, ups=0.89, wpb=253.1, bsz=100, num_updates=20740, lr=1.80495e-05, gnorm=0.905, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=40793
2023-03-15 06:52:17 - progress_bar.py[line:274] - INFO: epoch 004:   1845 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=250.2, nsentences=100, sample_size=250.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.9, ups=0.87, wpb=250.2, bsz=100, num_updates=20750, lr=1.80328e-05, gnorm=0.89, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=40805
2023-03-15 06:52:29 - progress_bar.py[line:274] - INFO: epoch 004:   1855 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.6, ups=0.87, wpb=254.7, bsz=100, num_updates=20760, lr=1.80161e-05, gnorm=0.893, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40817
2023-03-15 06:52:41 - progress_bar.py[line:274] - INFO: epoch 004:   1865 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.2, ups=0.88, wpb=253.5, bsz=100, num_updates=20770, lr=1.79995e-05, gnorm=0.883, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40828
2023-03-15 06:52:52 - progress_bar.py[line:274] - INFO: epoch 004:   1875 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=226, ups=0.89, wpb=254.9, bsz=100, num_updates=20780, lr=1.79828e-05, gnorm=0.864, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40840
2023-03-15 06:53:04 - progress_bar.py[line:274] - INFO: epoch 004:   1885 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.3, ups=0.88, wpb=253.5, bsz=100, num_updates=20790, lr=1.79661e-05, gnorm=0.854, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40852
2023-03-15 06:53:16 - progress_bar.py[line:274] - INFO: epoch 004:   1895 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=229, ups=0.9, wpb=255.2, bsz=100, num_updates=20800, lr=1.79494e-05, gnorm=0.842, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40863
2023-03-15 06:53:27 - progress_bar.py[line:274] - INFO: epoch 004:   1905 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.7, ups=0.89, wpb=252.3, bsz=100, num_updates=20810, lr=1.79328e-05, gnorm=0.869, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=40875
2023-03-15 06:53:39 - progress_bar.py[line:274] - INFO: epoch 004:   1915 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.1, ups=0.86, wpb=252.8, bsz=100, num_updates=20820, lr=1.79161e-05, gnorm=0.979, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=40887
2023-03-15 06:53:51 - progress_bar.py[line:274] - INFO: epoch 004:   1925 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.1, ups=0.87, wpb=255.1, bsz=100, num_updates=20830, lr=1.78994e-05, gnorm=0.901, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40898
2023-03-15 06:54:02 - progress_bar.py[line:274] - INFO: epoch 004:   1935 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.9, ups=0.86, wpb=253.4, bsz=100, num_updates=20840, lr=1.78827e-05, gnorm=0.809, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=40910
2023-03-15 06:54:14 - progress_bar.py[line:274] - INFO: epoch 004:   1945 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.5, ups=0.87, wpb=250.7, bsz=100, num_updates=20850, lr=1.78661e-05, gnorm=0.946, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=40922
2023-03-15 06:54:26 - progress_bar.py[line:274] - INFO: epoch 004:   1955 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221, ups=0.87, wpb=253.2, bsz=100, num_updates=20860, lr=1.78494e-05, gnorm=0.879, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40934
2023-03-15 06:54:38 - progress_bar.py[line:274] - INFO: epoch 004:   1965 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.2, ups=0.87, wpb=253.4, bsz=100, num_updates=20870, lr=1.78327e-05, gnorm=0.821, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40945
2023-03-15 06:54:49 - progress_bar.py[line:274] - INFO: epoch 004:   1975 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222, ups=0.87, wpb=253.8, bsz=100, num_updates=20880, lr=1.78161e-05, gnorm=0.878, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=40957
2023-03-15 06:55:01 - progress_bar.py[line:274] - INFO: epoch 004:   1985 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.8, ups=0.87, wpb=252.4, bsz=100, num_updates=20890, lr=1.77994e-05, gnorm=0.913, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=40969
2023-03-15 06:55:13 - progress_bar.py[line:274] - INFO: epoch 004:   1995 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.4, ups=0.87, wpb=252.3, bsz=100, num_updates=20900, lr=1.77827e-05, gnorm=0.944, clip=20, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=40980
2023-03-15 06:55:25 - progress_bar.py[line:274] - INFO: epoch 004:   2005 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.9, ups=0.88, wpb=252.4, bsz=100, num_updates=20910, lr=1.7766e-05, gnorm=0.897, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=40992
2023-03-15 06:55:37 - progress_bar.py[line:274] - INFO: epoch 004:   2015 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222, ups=0.88, wpb=253.1, bsz=100, num_updates=20920, lr=1.77494e-05, gnorm=0.877, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41004
2023-03-15 06:55:48 - progress_bar.py[line:274] - INFO: epoch 004:   2025 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.8, ups=0.87, wpb=255, bsz=100, num_updates=20930, lr=1.77327e-05, gnorm=0.977, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41016
2023-03-15 06:56:00 - progress_bar.py[line:274] - INFO: epoch 004:   2035 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.6, ups=0.87, wpb=254.8, bsz=100, num_updates=20940, lr=1.7716e-05, gnorm=0.946, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=41028
2023-03-15 06:56:12 - progress_bar.py[line:274] - INFO: epoch 004:   2045 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.3, ups=0.85, wpb=254.8, bsz=100, num_updates=20950, lr=1.76993e-05, gnorm=0.93, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41040
2023-03-15 06:56:24 - progress_bar.py[line:274] - INFO: epoch 004:   2055 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224, ups=0.89, wpb=253.1, bsz=100, num_updates=20960, lr=1.76827e-05, gnorm=0.95, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=41051
2023-03-15 06:56:36 - progress_bar.py[line:274] - INFO: epoch 004:   2065 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.3, ups=0.86, wpb=251.4, bsz=100, num_updates=20970, lr=1.7666e-05, gnorm=0.918, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41063
2023-03-15 06:56:48 - progress_bar.py[line:274] - INFO: epoch 004:   2075 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.3, ups=0.86, wpb=254.1, bsz=100, num_updates=20980, lr=1.76493e-05, gnorm=0.853, clip=10, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=41075
2023-03-15 06:56:59 - progress_bar.py[line:274] - INFO: epoch 004:   2085 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.5, ups=0.88, wpb=254.3, bsz=100, num_updates=20990, lr=1.76326e-05, gnorm=0.847, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41087
2023-03-15 06:57:11 - progress_bar.py[line:274] - INFO: epoch 004:   2095 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.8, ups=0.88, wpb=254.4, bsz=100, num_updates=21000, lr=1.7616e-05, gnorm=0.848, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41098
2023-03-15 06:57:22 - progress_bar.py[line:274] - INFO: epoch 004:   2105 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.2, ups=0.87, wpb=255.4, bsz=100, num_updates=21010, lr=1.75993e-05, gnorm=0.881, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=41110
2023-03-15 06:57:34 - progress_bar.py[line:274] - INFO: epoch 004:   2115 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223, ups=0.87, wpb=255.2, bsz=100, num_updates=21020, lr=1.75826e-05, gnorm=0.779, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=41122
2023-03-15 06:57:46 - progress_bar.py[line:274] - INFO: epoch 004:   2125 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.2, ups=0.86, wpb=251.5, bsz=100, num_updates=21030, lr=1.75659e-05, gnorm=0.956, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41134
2023-03-15 06:57:58 - progress_bar.py[line:274] - INFO: epoch 004:   2135 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=248.7, nsentences=100, sample_size=248.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=212.5, ups=0.85, wpb=248.7, bsz=100, num_updates=21040, lr=1.75493e-05, gnorm=0.906, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=41146
2023-03-15 06:58:10 - progress_bar.py[line:274] - INFO: epoch 004:   2145 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.2, ups=0.87, wpb=253.8, bsz=100, num_updates=21050, lr=1.75326e-05, gnorm=0.904, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=41158
2023-03-15 06:58:22 - progress_bar.py[line:274] - INFO: epoch 004:   2155 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=226.2, ups=0.89, wpb=254.9, bsz=100, num_updates=21060, lr=1.75159e-05, gnorm=0.848, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41169
2023-03-15 06:58:34 - progress_bar.py[line:274] - INFO: epoch 004:   2165 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=226.2, ups=0.89, wpb=253.9, bsz=100, num_updates=21070, lr=1.74992e-05, gnorm=0.952, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41181
2023-03-15 06:58:46 - progress_bar.py[line:274] - INFO: epoch 004:   2175 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.8, ups=0.87, wpb=255.3, bsz=100, num_updates=21080, lr=1.74826e-05, gnorm=0.869, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41193
2023-03-15 06:58:59 - progress_bar.py[line:274] - INFO: epoch 004:   2185 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.2, ups=0.86, wpb=252.6, bsz=100, num_updates=21090, lr=1.74659e-05, gnorm=0.799, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=41206
2023-03-15 06:59:11 - progress_bar.py[line:274] - INFO: epoch 004:   2195 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.4, ups=0.88, wpb=251.3, bsz=100, num_updates=21100, lr=1.74492e-05, gnorm=0.891, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=41218
2023-03-15 06:59:23 - progress_bar.py[line:274] - INFO: epoch 004:   2205 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.9, ups=0.88, wpb=251.1, bsz=100, num_updates=21110, lr=1.74326e-05, gnorm=0.847, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41230
2023-03-15 06:59:35 - progress_bar.py[line:274] - INFO: epoch 004:   2215 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.2, ups=0.87, wpb=252.2, bsz=100, num_updates=21120, lr=1.74159e-05, gnorm=0.845, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=41242
2023-03-15 06:59:47 - progress_bar.py[line:274] - INFO: epoch 004:   2225 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224.4, ups=0.88, wpb=254.9, bsz=100, num_updates=21130, lr=1.73992e-05, gnorm=0.802, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41254
2023-03-15 06:59:59 - progress_bar.py[line:274] - INFO: epoch 004:   2235 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=249.9, nsentences=100, sample_size=249.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.2, ups=0.87, wpb=249.9, bsz=100, num_updates=21140, lr=1.73825e-05, gnorm=0.833, clip=20, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=41266
2023-03-15 07:00:11 - progress_bar.py[line:274] - INFO: epoch 004:   2245 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=228.5, ups=0.9, wpb=254.3, bsz=100, num_updates=21150, lr=1.73659e-05, gnorm=0.875, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41278
2023-03-15 07:00:23 - progress_bar.py[line:274] - INFO: epoch 004:   2255 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.4, ups=0.88, wpb=251.8, bsz=100, num_updates=21160, lr=1.73492e-05, gnorm=0.856, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41290
2023-03-15 07:00:28 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 07:00:35 - progress_bar.py[line:274] - INFO: epoch 004:   2266 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=202.9, ups=0.8, wpb=252.8, bsz=100, num_updates=21170, lr=1.73325e-05, gnorm=0.804, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41303
2023-03-15 07:00:48 - progress_bar.py[line:274] - INFO: epoch 004:   2276 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.1, ups=0.87, wpb=253.5, bsz=100, num_updates=21180, lr=1.73158e-05, gnorm=0.967, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41315
2023-03-15 07:01:00 - progress_bar.py[line:274] - INFO: epoch 004:   2286 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.2, ups=0.86, wpb=250.5, bsz=100, num_updates=21190, lr=1.72992e-05, gnorm=0.869, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41327
2023-03-15 07:01:12 - progress_bar.py[line:274] - INFO: epoch 004:   2296 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.3, ups=0.86, wpb=255.3, bsz=100, num_updates=21200, lr=1.72825e-05, gnorm=0.841, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41339
2023-03-15 07:01:24 - progress_bar.py[line:274] - INFO: epoch 004:   2306 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.4, ups=0.88, wpb=250.8, bsz=100, num_updates=21210, lr=1.72658e-05, gnorm=0.961, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=41351
2023-03-15 07:01:37 - progress_bar.py[line:274] - INFO: epoch 004:   2316 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.2, ups=0.88, wpb=252.4, bsz=100, num_updates=21220, lr=1.72491e-05, gnorm=0.797, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=41363
2023-03-15 07:01:49 - progress_bar.py[line:274] - INFO: epoch 004:   2326 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218, ups=0.87, wpb=251.8, bsz=100, num_updates=21230, lr=1.72325e-05, gnorm=0.904, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=41376
2023-03-15 07:02:02 - progress_bar.py[line:274] - INFO: epoch 004:   2336 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.9, ups=0.86, wpb=254.6, bsz=100, num_updates=21240, lr=1.72158e-05, gnorm=0.919, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41389
2023-03-15 07:02:14 - progress_bar.py[line:274] - INFO: epoch 004:   2346 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225.3, ups=0.88, wpb=256.1, bsz=100, num_updates=21250, lr=1.71991e-05, gnorm=0.833, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41401
2023-03-15 07:02:26 - progress_bar.py[line:274] - INFO: epoch 004:   2356 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.9, ups=0.88, wpb=252.2, bsz=100, num_updates=21260, lr=1.71824e-05, gnorm=0.948, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41413
2023-03-15 07:02:38 - progress_bar.py[line:274] - INFO: epoch 004:   2366 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=225.8, ups=0.89, wpb=254.7, bsz=100, num_updates=21270, lr=1.71658e-05, gnorm=0.916, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=41425
2023-03-15 07:02:50 - progress_bar.py[line:274] - INFO: epoch 004:   2376 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.1, ups=0.87, wpb=252, bsz=100, num_updates=21280, lr=1.71491e-05, gnorm=0.901, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=41438
2023-03-15 07:03:02 - progress_bar.py[line:274] - INFO: epoch 004:   2386 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.7, ups=0.87, wpb=253.4, bsz=100, num_updates=21290, lr=1.71324e-05, gnorm=0.93, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41450
2023-03-15 07:03:14 - progress_bar.py[line:274] - INFO: epoch 004:   2396 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=225.4, ups=0.89, wpb=252.9, bsz=100, num_updates=21300, lr=1.71158e-05, gnorm=0.893, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41461
2023-03-15 07:03:26 - progress_bar.py[line:274] - INFO: epoch 004:   2406 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221, ups=0.86, wpb=255.6, bsz=100, num_updates=21310, lr=1.70991e-05, gnorm=0.925, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=41473
2023-03-15 07:03:38 - progress_bar.py[line:274] - INFO: epoch 004:   2416 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.4, ups=0.88, wpb=251.2, bsz=100, num_updates=21320, lr=1.70824e-05, gnorm=0.873, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=41485
2023-03-15 07:03:50 - progress_bar.py[line:274] - INFO: epoch 004:   2426 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=258.5, nsentences=100, sample_size=258.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=224.2, ups=0.87, wpb=258.5, bsz=100, num_updates=21330, lr=1.70657e-05, gnorm=0.865, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=41497
2023-03-15 07:04:02 - progress_bar.py[line:274] - INFO: epoch 004:   2436 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.9, ups=0.87, wpb=254.1, bsz=100, num_updates=21340, lr=1.70491e-05, gnorm=0.975, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41509
2023-03-15 07:04:14 - progress_bar.py[line:274] - INFO: epoch 004:   2446 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215.4, ups=0.85, wpb=252, bsz=100, num_updates=21350, lr=1.70324e-05, gnorm=1.012, clip=80, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=41522
2023-03-15 07:04:26 - progress_bar.py[line:274] - INFO: epoch 004:   2456 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.8, ups=0.88, wpb=251, bsz=100, num_updates=21360, lr=1.70157e-05, gnorm=0.917, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41534
2023-03-15 07:04:38 - progress_bar.py[line:274] - INFO: epoch 004:   2466 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=225, ups=0.89, wpb=252.8, bsz=100, num_updates=21370, lr=1.6999e-05, gnorm=0.922, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=41545
2023-03-15 07:04:50 - progress_bar.py[line:274] - INFO: epoch 004:   2476 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.7, ups=0.87, wpb=253, bsz=100, num_updates=21380, lr=1.69824e-05, gnorm=0.92, clip=40, loss_scale=512, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=41557
2023-03-15 07:05:02 - progress_bar.py[line:274] - INFO: epoch 004:   2486 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.7, ups=0.86, wpb=253.8, bsz=100, num_updates=21390, lr=1.69657e-05, gnorm=0.913, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41569
2023-03-15 07:05:14 - progress_bar.py[line:274] - INFO: epoch 004:   2496 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.7, ups=0.86, wpb=253.7, bsz=100, num_updates=21400, lr=1.6949e-05, gnorm=0.895, clip=20, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=41581
2023-03-15 07:05:26 - progress_bar.py[line:274] - INFO: epoch 004:   2506 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.5, ups=0.88, wpb=253, bsz=100, num_updates=21410, lr=1.69323e-05, gnorm=0.828, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=41593
2023-03-15 07:05:38 - progress_bar.py[line:274] - INFO: epoch 004:   2516 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219, ups=0.87, wpb=253.2, bsz=100, num_updates=21420, lr=1.69157e-05, gnorm=0.853, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41605
2023-03-15 07:05:50 - progress_bar.py[line:274] - INFO: epoch 004:   2526 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.7, ups=0.88, wpb=254.4, bsz=100, num_updates=21430, lr=1.6899e-05, gnorm=0.862, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=41617
2023-03-15 07:06:02 - progress_bar.py[line:274] - INFO: epoch 004:   2536 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.7, ups=0.86, wpb=251.3, bsz=100, num_updates=21440, lr=1.68823e-05, gnorm=1.017, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41629
2023-03-15 07:06:14 - progress_bar.py[line:274] - INFO: epoch 004:   2546 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.6, ups=0.88, wpb=253.5, bsz=100, num_updates=21450, lr=1.68656e-05, gnorm=0.894, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=41641
2023-03-15 07:06:26 - progress_bar.py[line:274] - INFO: epoch 004:   2556 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224.5, ups=0.89, wpb=253.6, bsz=100, num_updates=21460, lr=1.6849e-05, gnorm=0.858, clip=10, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=41653
2023-03-15 07:06:39 - progress_bar.py[line:274] - INFO: epoch 004:   2566 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=218.7, ups=0.87, wpb=251.5, bsz=100, num_updates=21470, lr=1.68323e-05, gnorm=0.902, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41666
2023-03-15 07:06:51 - progress_bar.py[line:274] - INFO: epoch 004:   2576 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.9, ups=0.86, wpb=253.1, bsz=100, num_updates=21480, lr=1.68156e-05, gnorm=0.807, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=41678
2023-03-15 07:07:04 - progress_bar.py[line:274] - INFO: epoch 004:   2586 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.2, ups=0.87, wpb=253.2, bsz=100, num_updates=21490, lr=1.67989e-05, gnorm=0.819, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41691
2023-03-15 07:07:16 - progress_bar.py[line:274] - INFO: epoch 004:   2596 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.9, ups=0.88, wpb=252.2, bsz=100, num_updates=21500, lr=1.67823e-05, gnorm=0.994, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=41703
2023-03-15 07:07:29 - progress_bar.py[line:274] - INFO: epoch 004:   2606 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.9, ups=0.86, wpb=253.6, bsz=100, num_updates=21510, lr=1.67656e-05, gnorm=0.812, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=41716
2023-03-15 07:07:41 - progress_bar.py[line:274] - INFO: epoch 004:   2616 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.5, ups=0.87, wpb=254, bsz=100, num_updates=21520, lr=1.67489e-05, gnorm=0.883, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=41728
2023-03-15 07:07:53 - progress_bar.py[line:274] - INFO: epoch 004:   2626 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220, ups=0.86, wpb=255.1, bsz=100, num_updates=21530, lr=1.67323e-05, gnorm=0.879, clip=20, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=41740
2023-03-15 07:08:05 - progress_bar.py[line:274] - INFO: epoch 004:   2636 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.9, ups=0.88, wpb=253.9, bsz=100, num_updates=21540, lr=1.67156e-05, gnorm=0.826, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=41752
2023-03-15 07:08:17 - progress_bar.py[line:274] - INFO: epoch 004:   2646 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.9, ups=0.86, wpb=255.2, bsz=100, num_updates=21550, lr=1.66989e-05, gnorm=0.861, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=41764
2023-03-15 07:08:29 - progress_bar.py[line:274] - INFO: epoch 004:   2656 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=223.3, ups=0.89, wpb=251.9, bsz=100, num_updates=21560, lr=1.66822e-05, gnorm=0.952, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41776
2023-03-15 07:08:41 - progress_bar.py[line:274] - INFO: epoch 004:   2666 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.2, ups=0.86, wpb=253.7, bsz=100, num_updates=21570, lr=1.66656e-05, gnorm=0.827, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=41788
2023-03-15 07:08:53 - progress_bar.py[line:274] - INFO: epoch 004:   2676 / 6313 loss=0.556, loss_v1=0, loss_v2=0, nll_loss=0.34, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=221.1, ups=0.87, wpb=252.9, bsz=100, num_updates=21580, lr=1.66489e-05, gnorm=0.935, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=41800
2023-03-15 07:09:05 - progress_bar.py[line:274] - INFO: epoch 004:   2686 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=216.5, ups=0.86, wpb=252.9, bsz=100, num_updates=21590, lr=1.66322e-05, gnorm=0.869, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=41812
2023-03-15 07:09:17 - progress_bar.py[line:274] - INFO: epoch 004:   2696 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.2, ups=0.87, wpb=253.9, bsz=100, num_updates=21600, lr=1.66155e-05, gnorm=0.865, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=41824
2023-03-15 07:09:30 - progress_bar.py[line:274] - INFO: epoch 004:   2706 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=256.3, nsentences=100, sample_size=256.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.3, ups=0.86, wpb=256.3, bsz=100, num_updates=21610, lr=1.65989e-05, gnorm=0.963, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=41837
2023-03-15 07:09:42 - progress_bar.py[line:274] - INFO: epoch 004:   2716 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=256.6, nsentences=100, sample_size=256.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=224.6, ups=0.88, wpb=256.6, bsz=100, num_updates=21620, lr=1.65822e-05, gnorm=0.794, clip=0, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=41849
2023-03-15 07:09:54 - progress_bar.py[line:274] - INFO: epoch 004:   2726 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.7, ups=0.86, wpb=254.5, bsz=100, num_updates=21630, lr=1.65655e-05, gnorm=0.844, clip=10, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=41861
2023-03-15 07:10:06 - progress_bar.py[line:274] - INFO: epoch 004:   2736 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.7, ups=0.86, wpb=255.6, bsz=100, num_updates=21640, lr=1.65488e-05, gnorm=0.959, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=41874
2023-03-15 07:10:18 - progress_bar.py[line:274] - INFO: epoch 004:   2746 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.8, ups=0.87, wpb=255, bsz=100, num_updates=21650, lr=1.65322e-05, gnorm=0.817, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=41886
2023-03-15 07:10:30 - progress_bar.py[line:274] - INFO: epoch 004:   2756 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=224.9, ups=0.89, wpb=253.7, bsz=100, num_updates=21660, lr=1.65155e-05, gnorm=0.866, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41897
2023-03-15 07:10:42 - progress_bar.py[line:274] - INFO: epoch 004:   2766 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.3, ups=0.86, wpb=255.6, bsz=100, num_updates=21670, lr=1.64988e-05, gnorm=0.888, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41910
2023-03-15 07:10:55 - progress_bar.py[line:274] - INFO: epoch 004:   2776 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.7, ups=0.86, wpb=253.2, bsz=100, num_updates=21680, lr=1.64821e-05, gnorm=0.922, clip=40, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=41922
2023-03-15 07:11:07 - progress_bar.py[line:274] - INFO: epoch 004:   2786 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.6, ups=0.88, wpb=254.5, bsz=100, num_updates=21690, lr=1.64655e-05, gnorm=0.914, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=41934
2023-03-15 07:11:18 - progress_bar.py[line:274] - INFO: epoch 004:   2796 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=225, ups=0.89, wpb=253.8, bsz=100, num_updates=21700, lr=1.64488e-05, gnorm=0.86, clip=20, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=41946
2023-03-15 07:11:31 - progress_bar.py[line:274] - INFO: epoch 004:   2806 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.7, ups=0.86, wpb=254.7, bsz=100, num_updates=21710, lr=1.64321e-05, gnorm=0.84, clip=0, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=41958
2023-03-15 07:11:43 - progress_bar.py[line:274] - INFO: epoch 004:   2816 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.2, ups=0.87, wpb=255.3, bsz=100, num_updates=21720, lr=1.64154e-05, gnorm=0.912, clip=30, loss_scale=1024, train_wall=11, gb_free=11.3, ema_decay=0.9999, wall=41970
2023-03-15 07:11:55 - progress_bar.py[line:274] - INFO: epoch 004:   2826 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.6, ups=0.87, wpb=256.2, bsz=100, num_updates=21730, lr=1.63988e-05, gnorm=0.825, clip=10, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=41982
2023-03-15 07:12:07 - progress_bar.py[line:274] - INFO: epoch 004:   2836 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.1, ups=0.87, wpb=254.8, bsz=100, num_updates=21740, lr=1.63821e-05, gnorm=0.906, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=41994
2023-03-15 07:12:19 - progress_bar.py[line:274] - INFO: epoch 004:   2846 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.7, ups=0.87, wpb=253.8, bsz=100, num_updates=21750, lr=1.63654e-05, gnorm=0.858, clip=20, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=42006
2023-03-15 07:12:31 - progress_bar.py[line:274] - INFO: epoch 004:   2856 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=258.3, nsentences=100, sample_size=258.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220, ups=0.85, wpb=258.3, bsz=100, num_updates=21760, lr=1.63488e-05, gnorm=0.818, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=42019
2023-03-15 07:12:34 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 07:12:45 - progress_bar.py[line:274] - INFO: epoch 004:   2867 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=202.3, ups=0.79, wpb=255, bsz=100, num_updates=21770, lr=1.63321e-05, gnorm=0.968, clip=30, loss_scale=512, train_wall=13, gb_free=9.4, ema_decay=0.9999, wall=42032
2023-03-15 07:12:57 - progress_bar.py[line:274] - INFO: epoch 004:   2877 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.9, ups=0.87, wpb=250.6, bsz=100, num_updates=21780, lr=1.63154e-05, gnorm=0.914, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42044
2023-03-15 07:13:08 - progress_bar.py[line:274] - INFO: epoch 004:   2887 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=227.4, ups=0.89, wpb=255.9, bsz=100, num_updates=21790, lr=1.62987e-05, gnorm=0.762, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42056
2023-03-15 07:13:21 - progress_bar.py[line:274] - INFO: epoch 004:   2897 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.5, ups=0.87, wpb=253.6, bsz=100, num_updates=21800, lr=1.62821e-05, gnorm=0.891, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42068
2023-03-15 07:13:32 - progress_bar.py[line:274] - INFO: epoch 004:   2907 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.9, ups=0.88, wpb=254.5, bsz=100, num_updates=21810, lr=1.62654e-05, gnorm=0.841, clip=0, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=42080
2023-03-15 07:13:45 - progress_bar.py[line:274] - INFO: epoch 004:   2917 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.9, ups=0.88, wpb=254.6, bsz=100, num_updates=21820, lr=1.62487e-05, gnorm=0.91, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=42092
2023-03-15 07:13:57 - progress_bar.py[line:274] - INFO: epoch 004:   2927 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.5, ups=0.87, wpb=253.7, bsz=100, num_updates=21830, lr=1.6232e-05, gnorm=0.87, clip=20, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=42104
2023-03-15 07:14:09 - progress_bar.py[line:274] - INFO: epoch 004:   2937 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.2, ups=0.85, wpb=254, bsz=100, num_updates=21840, lr=1.62154e-05, gnorm=0.858, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=42116
2023-03-15 07:14:21 - progress_bar.py[line:274] - INFO: epoch 004:   2947 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.7, ups=0.86, wpb=253.1, bsz=100, num_updates=21850, lr=1.61987e-05, gnorm=0.976, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=42128
2023-03-15 07:14:33 - progress_bar.py[line:274] - INFO: epoch 004:   2957 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.327, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.7, ups=0.88, wpb=251.7, bsz=100, num_updates=21860, lr=1.6182e-05, gnorm=0.951, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42140
2023-03-15 07:14:45 - progress_bar.py[line:274] - INFO: epoch 004:   2967 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=249.8, nsentences=100, sample_size=249.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=215.8, ups=0.86, wpb=249.8, bsz=100, num_updates=21870, lr=1.61653e-05, gnorm=0.938, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=42152
2023-03-15 07:14:58 - progress_bar.py[line:274] - INFO: epoch 004:   2977 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=214.8, ups=0.86, wpb=250.6, bsz=100, num_updates=21880, lr=1.61487e-05, gnorm=0.843, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=42164
2023-03-15 07:15:10 - progress_bar.py[line:274] - INFO: epoch 004:   2987 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.1, ups=0.86, wpb=253, bsz=100, num_updates=21890, lr=1.6132e-05, gnorm=0.925, clip=40, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=42177
2023-03-15 07:15:23 - progress_bar.py[line:274] - INFO: epoch 004:   2997 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.6, ups=0.88, wpb=254.2, bsz=100, num_updates=21900, lr=1.61153e-05, gnorm=0.841, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42190
2023-03-15 07:15:35 - progress_bar.py[line:274] - INFO: epoch 004:   3007 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=249.2, nsentences=100, sample_size=249.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.6, ups=0.87, wpb=249.2, bsz=100, num_updates=21910, lr=1.60986e-05, gnorm=0.878, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42202
2023-03-15 07:15:48 - progress_bar.py[line:274] - INFO: epoch 004:   3017 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.9, ups=0.86, wpb=252.7, bsz=100, num_updates=21920, lr=1.6082e-05, gnorm=0.842, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=42215
2023-03-15 07:16:00 - progress_bar.py[line:274] - INFO: epoch 004:   3027 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.3, ups=0.86, wpb=253.9, bsz=100, num_updates=21930, lr=1.60653e-05, gnorm=0.879, clip=30, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=42227
2023-03-15 07:16:12 - progress_bar.py[line:274] - INFO: epoch 004:   3037 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.3, ups=0.88, wpb=252.8, bsz=100, num_updates=21940, lr=1.60486e-05, gnorm=0.849, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42239
2023-03-15 07:16:24 - progress_bar.py[line:274] - INFO: epoch 004:   3047 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.3, ups=0.87, wpb=253.4, bsz=100, num_updates=21950, lr=1.60319e-05, gnorm=0.903, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=42252
2023-03-15 07:16:36 - progress_bar.py[line:274] - INFO: epoch 004:   3057 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221, ups=0.87, wpb=253.7, bsz=100, num_updates=21960, lr=1.60153e-05, gnorm=0.908, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=42263
2023-03-15 07:16:48 - progress_bar.py[line:274] - INFO: epoch 004:   3067 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=225.6, ups=0.89, wpb=254.2, bsz=100, num_updates=21970, lr=1.59986e-05, gnorm=0.876, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42275
2023-03-15 07:17:00 - progress_bar.py[line:274] - INFO: epoch 004:   3077 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219, ups=0.86, wpb=253.6, bsz=100, num_updates=21980, lr=1.59819e-05, gnorm=0.991, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=42288
2023-03-15 07:17:12 - progress_bar.py[line:274] - INFO: epoch 004:   3087 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.8, ups=0.86, wpb=253.2, bsz=100, num_updates=21990, lr=1.59653e-05, gnorm=0.962, clip=30, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=42300
2023-03-15 07:17:24 - progress_bar.py[line:274] - INFO: epoch 004:   3097 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.1, ups=0.87, wpb=255.4, bsz=100, num_updates=22000, lr=1.59486e-05, gnorm=0.953, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42312
2023-03-15 07:17:37 - progress_bar.py[line:274] - INFO: epoch 004:   3107 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.9, ups=0.87, wpb=255, bsz=100, num_updates=22010, lr=1.59319e-05, gnorm=0.821, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42324
2023-03-15 07:17:49 - progress_bar.py[line:274] - INFO: epoch 004:   3117 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218, ups=0.86, wpb=252.7, bsz=100, num_updates=22020, lr=1.59152e-05, gnorm=0.971, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=42336
2023-03-15 07:18:01 - progress_bar.py[line:274] - INFO: epoch 004:   3127 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.8, ups=0.86, wpb=255.5, bsz=100, num_updates=22030, lr=1.58986e-05, gnorm=0.824, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=42348
2023-03-15 07:18:13 - progress_bar.py[line:274] - INFO: epoch 004:   3137 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.6, ups=0.86, wpb=253, bsz=100, num_updates=22040, lr=1.58819e-05, gnorm=0.846, clip=20, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=42360
2023-03-15 07:18:25 - progress_bar.py[line:274] - INFO: epoch 004:   3147 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.4, ups=0.86, wpb=254.7, bsz=100, num_updates=22050, lr=1.58652e-05, gnorm=0.871, clip=20, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=42373
2023-03-15 07:18:37 - progress_bar.py[line:274] - INFO: epoch 004:   3157 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.1, ups=0.86, wpb=254.7, bsz=100, num_updates=22060, lr=1.58485e-05, gnorm=0.897, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=42385
2023-03-15 07:18:49 - progress_bar.py[line:274] - INFO: epoch 004:   3167 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.3, ups=0.87, wpb=254.2, bsz=100, num_updates=22070, lr=1.58319e-05, gnorm=0.779, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42397
2023-03-15 07:19:01 - progress_bar.py[line:274] - INFO: epoch 004:   3177 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.1, ups=0.86, wpb=254.5, bsz=100, num_updates=22080, lr=1.58152e-05, gnorm=0.847, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=42409
2023-03-15 07:19:13 - progress_bar.py[line:274] - INFO: epoch 004:   3187 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.9, ups=0.88, wpb=252.9, bsz=100, num_updates=22090, lr=1.57985e-05, gnorm=0.924, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42421
2023-03-15 07:19:25 - progress_bar.py[line:274] - INFO: epoch 004:   3197 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219.6, ups=0.86, wpb=254.3, bsz=100, num_updates=22100, lr=1.57818e-05, gnorm=0.878, clip=20, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=42433
2023-03-15 07:19:37 - progress_bar.py[line:274] - INFO: epoch 004:   3207 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.8, ups=0.87, wpb=254.3, bsz=100, num_updates=22110, lr=1.57652e-05, gnorm=0.908, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42445
2023-03-15 07:19:49 - progress_bar.py[line:274] - INFO: epoch 004:   3217 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=226, ups=0.89, wpb=255.3, bsz=100, num_updates=22120, lr=1.57485e-05, gnorm=0.86, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42457
2023-03-15 07:20:01 - progress_bar.py[line:274] - INFO: epoch 004:   3227 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.8, ups=0.86, wpb=254.6, bsz=100, num_updates=22130, lr=1.57318e-05, gnorm=0.826, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=42469
2023-03-15 07:20:13 - progress_bar.py[line:274] - INFO: epoch 004:   3237 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.5, ups=0.86, wpb=254.1, bsz=100, num_updates=22140, lr=1.57151e-05, gnorm=0.889, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=42481
2023-03-15 07:20:25 - progress_bar.py[line:274] - INFO: epoch 004:   3247 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.4, ups=0.86, wpb=254.8, bsz=100, num_updates=22150, lr=1.56985e-05, gnorm=0.926, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=42493
2023-03-15 07:20:38 - progress_bar.py[line:274] - INFO: epoch 004:   3257 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216, ups=0.85, wpb=252.9, bsz=100, num_updates=22160, lr=1.56818e-05, gnorm=0.936, clip=30, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=42505
2023-03-15 07:20:49 - progress_bar.py[line:274] - INFO: epoch 004:   3267 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.6, ups=0.88, wpb=252.6, bsz=100, num_updates=22170, lr=1.56651e-05, gnorm=0.917, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42517
2023-03-15 07:21:02 - progress_bar.py[line:274] - INFO: epoch 004:   3277 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.5, ups=0.86, wpb=254, bsz=100, num_updates=22180, lr=1.56484e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=42529
2023-03-15 07:21:14 - progress_bar.py[line:274] - INFO: epoch 004:   3287 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.9, ups=0.87, wpb=252.5, bsz=100, num_updates=22190, lr=1.56318e-05, gnorm=0.885, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=42541
2023-03-15 07:21:26 - progress_bar.py[line:274] - INFO: epoch 004:   3297 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.3, ups=0.87, wpb=251.8, bsz=100, num_updates=22200, lr=1.56151e-05, gnorm=0.912, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42553
2023-03-15 07:21:37 - progress_bar.py[line:274] - INFO: epoch 004:   3307 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.8, ups=0.89, wpb=251.6, bsz=100, num_updates=22210, lr=1.55984e-05, gnorm=0.823, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=42565
2023-03-15 07:21:49 - progress_bar.py[line:274] - INFO: epoch 004:   3317 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.7, ups=0.87, wpb=253.9, bsz=100, num_updates=22220, lr=1.55818e-05, gnorm=0.813, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42577
2023-03-15 07:22:02 - progress_bar.py[line:274] - INFO: epoch 004:   3327 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.9, ups=0.87, wpb=253.1, bsz=100, num_updates=22230, lr=1.55651e-05, gnorm=0.812, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=42589
2023-03-15 07:22:14 - progress_bar.py[line:274] - INFO: epoch 004:   3337 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.6, ups=0.88, wpb=254.1, bsz=100, num_updates=22240, lr=1.55484e-05, gnorm=0.893, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42601
2023-03-15 07:22:26 - progress_bar.py[line:274] - INFO: epoch 004:   3347 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.3, ups=0.87, wpb=255.1, bsz=100, num_updates=22250, lr=1.55317e-05, gnorm=0.879, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42613
2023-03-15 07:22:38 - progress_bar.py[line:274] - INFO: epoch 004:   3357 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.8, ups=0.88, wpb=254.3, bsz=100, num_updates=22260, lr=1.55151e-05, gnorm=0.842, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42625
2023-03-15 07:22:50 - progress_bar.py[line:274] - INFO: epoch 004:   3367 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.3, ups=0.87, wpb=253.4, bsz=100, num_updates=22270, lr=1.54984e-05, gnorm=0.89, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=42637
2023-03-15 07:23:02 - progress_bar.py[line:274] - INFO: epoch 004:   3377 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.7, ups=0.87, wpb=254.8, bsz=100, num_updates=22280, lr=1.54817e-05, gnorm=0.88, clip=20, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=42649
2023-03-15 07:23:14 - progress_bar.py[line:274] - INFO: epoch 004:   3387 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.7, ups=0.86, wpb=252.8, bsz=100, num_updates=22290, lr=1.5465e-05, gnorm=0.927, clip=20, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=42661
2023-03-15 07:23:16 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 07:23:27 - progress_bar.py[line:274] - INFO: epoch 004:   3398 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=203.6, ups=0.8, wpb=254.4, bsz=100, num_updates=22300, lr=1.54484e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=42675
2023-03-15 07:23:39 - progress_bar.py[line:274] - INFO: epoch 004:   3408 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.5, ups=0.87, wpb=252.4, bsz=100, num_updates=22310, lr=1.54317e-05, gnorm=0.896, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=42686
2023-03-15 07:23:51 - progress_bar.py[line:274] - INFO: epoch 004:   3418 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=227.8, ups=0.89, wpb=255.1, bsz=100, num_updates=22320, lr=1.5415e-05, gnorm=0.883, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=42698
2023-03-15 07:24:03 - progress_bar.py[line:274] - INFO: epoch 004:   3428 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.1, ups=0.87, wpb=250.6, bsz=100, num_updates=22330, lr=1.53983e-05, gnorm=0.932, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42710
2023-03-15 07:24:15 - progress_bar.py[line:274] - INFO: epoch 004:   3438 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.8, ups=0.87, wpb=254, bsz=100, num_updates=22340, lr=1.53817e-05, gnorm=0.86, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=42722
2023-03-15 07:24:27 - progress_bar.py[line:274] - INFO: epoch 004:   3448 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=227.2, ups=0.9, wpb=252.7, bsz=100, num_updates=22350, lr=1.5365e-05, gnorm=0.958, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42734
2023-03-15 07:24:39 - progress_bar.py[line:274] - INFO: epoch 004:   3458 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.1, ups=0.87, wpb=255.6, bsz=100, num_updates=22360, lr=1.53483e-05, gnorm=0.916, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42746
2023-03-15 07:24:52 - progress_bar.py[line:274] - INFO: epoch 004:   3468 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.1, ups=0.86, wpb=251.7, bsz=100, num_updates=22370, lr=1.53316e-05, gnorm=0.96, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=42759
2023-03-15 07:25:04 - progress_bar.py[line:274] - INFO: epoch 004:   3478 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.3, ups=0.86, wpb=256.2, bsz=100, num_updates=22380, lr=1.5315e-05, gnorm=0.842, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=42771
2023-03-15 07:25:17 - progress_bar.py[line:274] - INFO: epoch 004:   3488 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.4, ups=0.88, wpb=252.5, bsz=100, num_updates=22390, lr=1.52983e-05, gnorm=0.871, clip=0, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=42784
2023-03-15 07:25:29 - progress_bar.py[line:274] - INFO: epoch 004:   3498 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.6, ups=0.88, wpb=252.6, bsz=100, num_updates=22400, lr=1.52816e-05, gnorm=0.832, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42796
2023-03-15 07:25:41 - progress_bar.py[line:274] - INFO: epoch 004:   3508 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.8, ups=0.88, wpb=252.8, bsz=100, num_updates=22410, lr=1.52649e-05, gnorm=0.806, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=42808
2023-03-15 07:25:53 - progress_bar.py[line:274] - INFO: epoch 004:   3518 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=225.9, ups=0.88, wpb=255.8, bsz=100, num_updates=22420, lr=1.52483e-05, gnorm=0.802, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=42820
2023-03-15 07:26:05 - progress_bar.py[line:274] - INFO: epoch 004:   3528 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.2, ups=0.86, wpb=254.7, bsz=100, num_updates=22430, lr=1.52316e-05, gnorm=0.777, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=42832
2023-03-15 07:26:18 - progress_bar.py[line:274] - INFO: epoch 004:   3538 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=256.8, nsentences=100, sample_size=256.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217, ups=0.84, wpb=256.8, bsz=100, num_updates=22440, lr=1.52149e-05, gnorm=0.796, clip=0, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=42845
2023-03-15 07:26:31 - progress_bar.py[line:274] - INFO: epoch 004:   3548 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.328, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=219, ups=0.87, wpb=250.9, bsz=100, num_updates=22450, lr=1.51983e-05, gnorm=0.922, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42858
2023-03-15 07:26:44 - progress_bar.py[line:274] - INFO: epoch 004:   3558 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.5, ups=0.88, wpb=253.3, bsz=100, num_updates=22460, lr=1.51816e-05, gnorm=0.855, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=42870
2023-03-15 07:26:57 - progress_bar.py[line:274] - INFO: epoch 004:   3568 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=227.2, ups=0.89, wpb=255.7, bsz=100, num_updates=22470, lr=1.51649e-05, gnorm=0.988, clip=40, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=42883
2023-03-15 07:27:09 - progress_bar.py[line:274] - INFO: epoch 004:   3578 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.2, ups=0.86, wpb=253.4, bsz=100, num_updates=22480, lr=1.51482e-05, gnorm=0.823, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=42896
2023-03-15 07:27:21 - progress_bar.py[line:274] - INFO: epoch 004:   3588 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.4, ups=0.87, wpb=251.3, bsz=100, num_updates=22490, lr=1.51316e-05, gnorm=0.794, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42909
2023-03-15 07:27:33 - progress_bar.py[line:274] - INFO: epoch 004:   3598 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221, ups=0.87, wpb=252.9, bsz=100, num_updates=22500, lr=1.51149e-05, gnorm=0.897, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=42921
2023-03-15 07:27:45 - progress_bar.py[line:274] - INFO: epoch 004:   3608 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=224.8, ups=0.89, wpb=252.8, bsz=100, num_updates=22510, lr=1.50982e-05, gnorm=0.883, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=42932
2023-03-15 07:27:58 - progress_bar.py[line:274] - INFO: epoch 004:   3618 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.4, ups=0.86, wpb=254.3, bsz=100, num_updates=22520, lr=1.50815e-05, gnorm=0.832, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=42945
2023-03-15 07:28:11 - progress_bar.py[line:274] - INFO: epoch 004:   3628 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.4, ups=0.85, wpb=253.4, bsz=100, num_updates=22530, lr=1.50649e-05, gnorm=0.903, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=42958
2023-03-15 07:28:23 - progress_bar.py[line:274] - INFO: epoch 004:   3638 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=225.1, ups=0.88, wpb=254.4, bsz=100, num_updates=22540, lr=1.50482e-05, gnorm=0.862, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=42970
2023-03-15 07:28:35 - progress_bar.py[line:274] - INFO: epoch 004:   3648 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.7, ups=0.87, wpb=255.2, bsz=100, num_updates=22550, lr=1.50315e-05, gnorm=0.854, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=42982
2023-03-15 07:28:48 - progress_bar.py[line:274] - INFO: epoch 004:   3658 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.8, ups=0.87, wpb=253.7, bsz=100, num_updates=22560, lr=1.50148e-05, gnorm=0.813, clip=0, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=42995
2023-03-15 07:29:00 - progress_bar.py[line:274] - INFO: epoch 004:   3668 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.6, ups=0.87, wpb=251.5, bsz=100, num_updates=22570, lr=1.49982e-05, gnorm=0.904, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=43007
2023-03-15 07:29:13 - progress_bar.py[line:274] - INFO: epoch 004:   3678 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.1, ups=0.87, wpb=253, bsz=100, num_updates=22580, lr=1.49815e-05, gnorm=0.861, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43020
2023-03-15 07:29:25 - progress_bar.py[line:274] - INFO: epoch 004:   3688 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.5, ups=0.88, wpb=251.7, bsz=100, num_updates=22590, lr=1.49648e-05, gnorm=0.941, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43032
2023-03-15 07:29:37 - progress_bar.py[line:274] - INFO: epoch 004:   3698 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.4, ups=0.86, wpb=251.8, bsz=100, num_updates=22600, lr=1.49481e-05, gnorm=0.875, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=43044
2023-03-15 07:29:50 - progress_bar.py[line:274] - INFO: epoch 004:   3708 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.8, ups=0.87, wpb=253.4, bsz=100, num_updates=22610, lr=1.49315e-05, gnorm=0.966, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43057
2023-03-15 07:30:02 - progress_bar.py[line:274] - INFO: epoch 004:   3718 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.6, ups=0.86, wpb=254, bsz=100, num_updates=22620, lr=1.49148e-05, gnorm=0.95, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=43070
2023-03-15 07:30:14 - progress_bar.py[line:274] - INFO: epoch 004:   3728 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.6, ups=0.87, wpb=252.8, bsz=100, num_updates=22630, lr=1.48981e-05, gnorm=0.854, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43082
2023-03-15 07:30:26 - progress_bar.py[line:274] - INFO: epoch 004:   3738 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.8, ups=0.87, wpb=251.5, bsz=100, num_updates=22640, lr=1.48814e-05, gnorm=0.871, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43094
2023-03-15 07:30:38 - progress_bar.py[line:274] - INFO: epoch 004:   3748 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=258.3, nsentences=100, sample_size=258.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225.9, ups=0.87, wpb=258.3, bsz=100, num_updates=22650, lr=1.48648e-05, gnorm=0.814, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43105
2023-03-15 07:30:50 - progress_bar.py[line:274] - INFO: epoch 004:   3758 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.8, ups=0.85, wpb=255.9, bsz=100, num_updates=22660, lr=1.48481e-05, gnorm=0.814, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=43118
2023-03-15 07:31:02 - progress_bar.py[line:274] - INFO: epoch 004:   3768 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.1, ups=0.87, wpb=250.6, bsz=100, num_updates=22670, lr=1.48314e-05, gnorm=0.949, clip=50, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=43129
2023-03-15 07:31:14 - progress_bar.py[line:274] - INFO: epoch 004:   3778 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=219, ups=0.87, wpb=252.7, bsz=100, num_updates=22680, lr=1.48148e-05, gnorm=0.899, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43141
2023-03-15 07:31:25 - progress_bar.py[line:274] - INFO: epoch 004:   3788 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.1, ups=0.86, wpb=252, bsz=100, num_updates=22690, lr=1.47981e-05, gnorm=0.896, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=43153
2023-03-15 07:31:37 - progress_bar.py[line:274] - INFO: epoch 004:   3798 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.6, ups=0.87, wpb=252.8, bsz=100, num_updates=22700, lr=1.47814e-05, gnorm=0.889, clip=30, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=43165
2023-03-15 07:31:49 - progress_bar.py[line:274] - INFO: epoch 004:   3808 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.7, ups=0.89, wpb=251.5, bsz=100, num_updates=22710, lr=1.47647e-05, gnorm=1.163, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43176
2023-03-15 07:32:00 - progress_bar.py[line:274] - INFO: epoch 004:   3818 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=223, ups=0.88, wpb=252.3, bsz=100, num_updates=22720, lr=1.47481e-05, gnorm=0.842, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43188
2023-03-15 07:32:12 - progress_bar.py[line:274] - INFO: epoch 004:   3828 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.3, ups=0.87, wpb=255.5, bsz=100, num_updates=22730, lr=1.47314e-05, gnorm=0.953, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43199
2023-03-15 07:32:23 - progress_bar.py[line:274] - INFO: epoch 004:   3838 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.7, ups=0.87, wpb=253.2, bsz=100, num_updates=22740, lr=1.47147e-05, gnorm=0.902, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43211
2023-03-15 07:32:35 - progress_bar.py[line:274] - INFO: epoch 004:   3848 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.3, ups=0.88, wpb=252.4, bsz=100, num_updates=22750, lr=1.4698e-05, gnorm=0.847, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=43223
2023-03-15 07:32:47 - progress_bar.py[line:274] - INFO: epoch 004:   3858 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=227.2, ups=0.9, wpb=253.4, bsz=100, num_updates=22760, lr=1.46814e-05, gnorm=0.89, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43234
2023-03-15 07:32:58 - progress_bar.py[line:274] - INFO: epoch 004:   3868 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.4, ups=0.86, wpb=254.7, bsz=100, num_updates=22770, lr=1.46647e-05, gnorm=0.945, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=43246
2023-03-15 07:33:10 - progress_bar.py[line:274] - INFO: epoch 004:   3878 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.7, ups=0.86, wpb=251.5, bsz=100, num_updates=22780, lr=1.4648e-05, gnorm=0.93, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=43258
2023-03-15 07:33:22 - progress_bar.py[line:274] - INFO: epoch 004:   3888 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.1, ups=0.87, wpb=254.5, bsz=100, num_updates=22790, lr=1.46313e-05, gnorm=0.918, clip=30, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=43270
2023-03-15 07:33:34 - progress_bar.py[line:274] - INFO: epoch 004:   3898 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.8, ups=0.87, wpb=251.7, bsz=100, num_updates=22800, lr=1.46147e-05, gnorm=0.977, clip=20, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=43281
2023-03-15 07:33:46 - progress_bar.py[line:274] - INFO: epoch 004:   3908 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.5, ups=0.86, wpb=254, bsz=100, num_updates=22810, lr=1.4598e-05, gnorm=0.829, clip=20, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=43293
2023-03-15 07:33:58 - progress_bar.py[line:274] - INFO: epoch 004:   3918 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221, ups=0.87, wpb=253.5, bsz=100, num_updates=22820, lr=1.45813e-05, gnorm=0.895, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=43305
2023-03-15 07:34:09 - progress_bar.py[line:274] - INFO: epoch 004:   3928 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.2, ups=0.86, wpb=254.8, bsz=100, num_updates=22830, lr=1.45646e-05, gnorm=0.897, clip=30, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=43317
2023-03-15 07:34:13 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 07:34:22 - progress_bar.py[line:274] - INFO: epoch 004:   3939 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=202.8, ups=0.8, wpb=253.6, bsz=100, num_updates=22840, lr=1.4548e-05, gnorm=0.866, clip=10, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=43330
2023-03-15 07:34:34 - progress_bar.py[line:274] - INFO: epoch 004:   3949 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.2, ups=0.85, wpb=254.1, bsz=100, num_updates=22850, lr=1.45313e-05, gnorm=0.946, clip=30, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=43342
2023-03-15 07:34:46 - progress_bar.py[line:274] - INFO: epoch 004:   3959 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.8, ups=0.87, wpb=251.4, bsz=100, num_updates=22860, lr=1.45146e-05, gnorm=0.898, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43353
2023-03-15 07:34:57 - progress_bar.py[line:274] - INFO: epoch 004:   3969 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220, ups=0.87, wpb=252, bsz=100, num_updates=22870, lr=1.44979e-05, gnorm=0.862, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=43365
2023-03-15 07:35:09 - progress_bar.py[line:274] - INFO: epoch 004:   3979 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.1, ups=0.87, wpb=253.6, bsz=100, num_updates=22880, lr=1.44813e-05, gnorm=0.936, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43377
2023-03-15 07:35:21 - progress_bar.py[line:274] - INFO: epoch 004:   3989 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.3, ups=0.87, wpb=253.7, bsz=100, num_updates=22890, lr=1.44646e-05, gnorm=0.891, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43389
2023-03-15 07:35:33 - progress_bar.py[line:274] - INFO: epoch 004:   3999 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=215.7, ups=0.86, wpb=250.9, bsz=100, num_updates=22900, lr=1.44479e-05, gnorm=0.847, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=43400
2023-03-15 07:35:44 - progress_bar.py[line:274] - INFO: epoch 004:   4009 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=224, ups=0.87, wpb=256.1, bsz=100, num_updates=22910, lr=1.44313e-05, gnorm=0.905, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43412
2023-03-15 07:35:56 - progress_bar.py[line:274] - INFO: epoch 004:   4019 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.3, ups=0.87, wpb=253.1, bsz=100, num_updates=22920, lr=1.44146e-05, gnorm=0.972, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43424
2023-03-15 07:36:08 - progress_bar.py[line:274] - INFO: epoch 004:   4029 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.6, ups=0.86, wpb=253.3, bsz=100, num_updates=22930, lr=1.43979e-05, gnorm=0.898, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=43436
2023-03-15 07:36:20 - progress_bar.py[line:274] - INFO: epoch 004:   4039 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=225.7, ups=0.89, wpb=253, bsz=100, num_updates=22940, lr=1.43812e-05, gnorm=1.008, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43447
2023-03-15 07:36:31 - progress_bar.py[line:274] - INFO: epoch 004:   4049 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.1, ups=0.87, wpb=253.4, bsz=100, num_updates=22950, lr=1.43646e-05, gnorm=0.932, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43459
2023-03-15 07:36:43 - progress_bar.py[line:274] - INFO: epoch 004:   4059 / 6313 loss=0.541, loss_v1=0, loss_v2=0, nll_loss=0.323, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.1, ups=0.88, wpb=252.4, bsz=100, num_updates=22960, lr=1.43479e-05, gnorm=0.914, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=43470
2023-03-15 07:36:55 - progress_bar.py[line:274] - INFO: epoch 004:   4069 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221, ups=0.87, wpb=254.5, bsz=100, num_updates=22970, lr=1.43312e-05, gnorm=0.828, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43482
2023-03-15 07:37:06 - progress_bar.py[line:274] - INFO: epoch 004:   4079 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.6, ups=0.86, wpb=255.6, bsz=100, num_updates=22980, lr=1.43145e-05, gnorm=0.836, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=43494
2023-03-15 07:37:18 - progress_bar.py[line:274] - INFO: epoch 004:   4089 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.1, ups=0.87, wpb=253, bsz=100, num_updates=22990, lr=1.42979e-05, gnorm=0.811, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43506
2023-03-15 07:37:30 - progress_bar.py[line:274] - INFO: epoch 004:   4099 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.6, ups=0.87, wpb=252.7, bsz=100, num_updates=23000, lr=1.42812e-05, gnorm=0.961, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=43518
2023-03-15 07:37:42 - progress_bar.py[line:274] - INFO: epoch 004:   4109 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=215.7, ups=0.86, wpb=251, bsz=100, num_updates=23010, lr=1.42645e-05, gnorm=0.957, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=43530
2023-03-15 07:37:54 - progress_bar.py[line:274] - INFO: epoch 004:   4119 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=214.5, ups=0.85, wpb=251.5, bsz=100, num_updates=23020, lr=1.42478e-05, gnorm=0.906, clip=20, loss_scale=512, train_wall=12, gb_free=9.7, ema_decay=0.9999, wall=43542
2023-03-15 07:38:06 - progress_bar.py[line:274] - INFO: epoch 004:   4129 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.3, ups=0.87, wpb=253.3, bsz=100, num_updates=23030, lr=1.42312e-05, gnorm=0.851, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43553
2023-03-15 07:38:17 - progress_bar.py[line:274] - INFO: epoch 004:   4139 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224.2, ups=0.88, wpb=253.3, bsz=100, num_updates=23040, lr=1.42145e-05, gnorm=0.926, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43565
2023-03-15 07:38:29 - progress_bar.py[line:274] - INFO: epoch 004:   4149 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.8, ups=0.86, wpb=256.2, bsz=100, num_updates=23050, lr=1.41978e-05, gnorm=0.913, clip=20, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=43577
2023-03-15 07:38:41 - progress_bar.py[line:274] - INFO: epoch 004:   4159 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.1, ups=0.87, wpb=253.3, bsz=100, num_updates=23060, lr=1.41811e-05, gnorm=0.968, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43588
2023-03-15 07:38:53 - progress_bar.py[line:274] - INFO: epoch 004:   4169 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.3, ups=0.88, wpb=254, bsz=100, num_updates=23070, lr=1.41645e-05, gnorm=0.855, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=43600
2023-03-15 07:39:05 - progress_bar.py[line:274] - INFO: epoch 004:   4179 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.7, ups=0.86, wpb=253.6, bsz=100, num_updates=23080, lr=1.41478e-05, gnorm=0.875, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=43612
2023-03-15 07:39:17 - progress_bar.py[line:274] - INFO: epoch 004:   4189 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.333, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=216.4, ups=0.85, wpb=254.2, bsz=100, num_updates=23090, lr=1.41311e-05, gnorm=1.03, clip=50, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=43624
2023-03-15 07:39:28 - progress_bar.py[line:274] - INFO: epoch 004:   4199 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222, ups=0.88, wpb=253.5, bsz=100, num_updates=23100, lr=1.41144e-05, gnorm=0.863, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43636
2023-03-15 07:39:40 - progress_bar.py[line:274] - INFO: epoch 004:   4209 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.3, ups=0.88, wpb=252.4, bsz=100, num_updates=23110, lr=1.40978e-05, gnorm=0.849, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43647
2023-03-15 07:39:51 - progress_bar.py[line:274] - INFO: epoch 004:   4219 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.8, ups=0.87, wpb=250.8, bsz=100, num_updates=23120, lr=1.40811e-05, gnorm=0.881, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=43659
2023-03-15 07:40:03 - progress_bar.py[line:274] - INFO: epoch 004:   4229 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.9, ups=0.86, wpb=251, bsz=100, num_updates=23130, lr=1.40644e-05, gnorm=0.967, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=43671
2023-03-15 07:40:15 - progress_bar.py[line:274] - INFO: epoch 004:   4239 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.5, ups=0.86, wpb=251.2, bsz=100, num_updates=23140, lr=1.40478e-05, gnorm=0.915, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=43683
2023-03-15 07:40:26 - progress_bar.py[line:274] - INFO: epoch 004:   4249 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=225.1, ups=0.9, wpb=251.4, bsz=100, num_updates=23150, lr=1.40311e-05, gnorm=0.998, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43694
2023-03-15 07:40:38 - progress_bar.py[line:274] - INFO: epoch 004:   4259 / 6313 loss=0.571, loss_v1=0, loss_v2=0, nll_loss=0.347, ntokens=247.3, nsentences=100, sample_size=247.3, sample_size_v1=0, sample_size_v2=0, ppl=1.27, wps=216.3, ups=0.87, wpb=247.3, bsz=100, num_updates=23160, lr=1.40144e-05, gnorm=0.928, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43706
2023-03-15 07:40:50 - progress_bar.py[line:274] - INFO: epoch 004:   4269 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220, ups=0.87, wpb=254.2, bsz=100, num_updates=23170, lr=1.39977e-05, gnorm=0.843, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=43717
2023-03-15 07:41:02 - progress_bar.py[line:274] - INFO: epoch 004:   4279 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.3, ups=0.86, wpb=251.7, bsz=100, num_updates=23180, lr=1.39811e-05, gnorm=0.792, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=43729
2023-03-15 07:41:13 - progress_bar.py[line:274] - INFO: epoch 004:   4289 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=215.9, ups=0.86, wpb=250.5, bsz=100, num_updates=23190, lr=1.39644e-05, gnorm=0.96, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=43741
2023-03-15 07:41:25 - progress_bar.py[line:274] - INFO: epoch 004:   4299 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.6, ups=0.87, wpb=254.8, bsz=100, num_updates=23200, lr=1.39477e-05, gnorm=0.87, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=43753
2023-03-15 07:41:37 - progress_bar.py[line:274] - INFO: epoch 004:   4309 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.4, ups=0.87, wpb=255.6, bsz=100, num_updates=23210, lr=1.3931e-05, gnorm=0.882, clip=30, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=43764
2023-03-15 07:41:49 - progress_bar.py[line:274] - INFO: epoch 004:   4319 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218, ups=0.86, wpb=252.4, bsz=100, num_updates=23220, lr=1.39144e-05, gnorm=0.891, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=43776
2023-03-15 07:42:00 - progress_bar.py[line:274] - INFO: epoch 004:   4329 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.6, ups=0.88, wpb=253.1, bsz=100, num_updates=23230, lr=1.38977e-05, gnorm=0.904, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43788
2023-03-15 07:42:12 - progress_bar.py[line:274] - INFO: epoch 004:   4339 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.9, ups=0.87, wpb=251.9, bsz=100, num_updates=23240, lr=1.3881e-05, gnorm=0.863, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=43800
2023-03-15 07:42:24 - progress_bar.py[line:274] - INFO: epoch 004:   4349 / 6313 loss=0.548, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=249, nsentences=100, sample_size=249, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.5, ups=0.89, wpb=249, bsz=100, num_updates=23250, lr=1.38643e-05, gnorm=1.023, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43811
2023-03-15 07:42:35 - progress_bar.py[line:274] - INFO: epoch 004:   4359 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.8, ups=0.86, wpb=253.3, bsz=100, num_updates=23260, lr=1.38477e-05, gnorm=0.947, clip=30, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=43823
2023-03-15 07:42:47 - progress_bar.py[line:274] - INFO: epoch 004:   4369 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.5, ups=0.87, wpb=251.2, bsz=100, num_updates=23270, lr=1.3831e-05, gnorm=0.847, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43835
2023-03-15 07:42:59 - progress_bar.py[line:274] - INFO: epoch 004:   4379 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.1, ups=0.86, wpb=252.3, bsz=100, num_updates=23280, lr=1.38143e-05, gnorm=0.805, clip=0, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=43847
2023-03-15 07:43:11 - progress_bar.py[line:274] - INFO: epoch 004:   4389 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.1, ups=0.87, wpb=254.8, bsz=100, num_updates=23290, lr=1.37976e-05, gnorm=0.865, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43858
2023-03-15 07:43:23 - progress_bar.py[line:274] - INFO: epoch 004:   4399 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.7, ups=0.87, wpb=253.5, bsz=100, num_updates=23300, lr=1.3781e-05, gnorm=0.864, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=43870
2023-03-15 07:43:34 - progress_bar.py[line:274] - INFO: epoch 004:   4409 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.6, ups=0.87, wpb=251.7, bsz=100, num_updates=23310, lr=1.37643e-05, gnorm=0.934, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=43882
2023-03-15 07:43:46 - progress_bar.py[line:274] - INFO: epoch 004:   4419 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224, ups=0.88, wpb=255, bsz=100, num_updates=23320, lr=1.37476e-05, gnorm=0.85, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=43894
2023-03-15 07:43:58 - progress_bar.py[line:274] - INFO: epoch 004:   4429 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.5, ups=0.88, wpb=252.9, bsz=100, num_updates=23330, lr=1.3731e-05, gnorm=0.937, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=43905
2023-03-15 07:44:10 - progress_bar.py[line:274] - INFO: epoch 004:   4439 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.7, ups=0.86, wpb=252.8, bsz=100, num_updates=23340, lr=1.37143e-05, gnorm=0.875, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=43917
2023-03-15 07:44:20 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 07:44:22 - progress_bar.py[line:274] - INFO: epoch 004:   4450 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=202.5, ups=0.8, wpb=253.4, bsz=100, num_updates=23350, lr=1.36976e-05, gnorm=0.965, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=43930
2023-03-15 07:44:34 - progress_bar.py[line:274] - INFO: epoch 004:   4460 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.4, ups=0.86, wpb=254.9, bsz=100, num_updates=23360, lr=1.36809e-05, gnorm=0.859, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=43942
2023-03-15 07:44:46 - progress_bar.py[line:274] - INFO: epoch 004:   4470 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=226.4, ups=0.88, wpb=256.5, bsz=100, num_updates=23370, lr=1.36643e-05, gnorm=0.868, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=43953
2023-03-15 07:44:58 - progress_bar.py[line:274] - INFO: epoch 004:   4480 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.9, ups=0.87, wpb=254.2, bsz=100, num_updates=23380, lr=1.36476e-05, gnorm=0.968, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=43965
2023-03-15 07:45:09 - progress_bar.py[line:274] - INFO: epoch 004:   4490 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.4, ups=0.87, wpb=255.1, bsz=100, num_updates=23390, lr=1.36309e-05, gnorm=0.902, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=43977
2023-03-15 07:45:21 - progress_bar.py[line:274] - INFO: epoch 004:   4500 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.8, ups=0.86, wpb=254.1, bsz=100, num_updates=23400, lr=1.36142e-05, gnorm=0.839, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=43989
2023-03-15 07:45:33 - progress_bar.py[line:274] - INFO: epoch 004:   4510 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=214.2, ups=0.85, wpb=250.9, bsz=100, num_updates=23410, lr=1.35976e-05, gnorm=0.926, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=44001
2023-03-15 07:45:45 - progress_bar.py[line:274] - INFO: epoch 004:   4520 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.4, ups=0.87, wpb=254.2, bsz=100, num_updates=23420, lr=1.35809e-05, gnorm=0.906, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44013
2023-03-15 07:45:57 - progress_bar.py[line:274] - INFO: epoch 004:   4530 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.7, ups=0.87, wpb=250.1, bsz=100, num_updates=23430, lr=1.35642e-05, gnorm=0.984, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44024
2023-03-15 07:46:08 - progress_bar.py[line:274] - INFO: epoch 004:   4540 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.4, ups=0.87, wpb=254.8, bsz=100, num_updates=23440, lr=1.35475e-05, gnorm=0.941, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44036
2023-03-15 07:46:20 - progress_bar.py[line:274] - INFO: epoch 004:   4550 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.7, ups=0.87, wpb=254.2, bsz=100, num_updates=23450, lr=1.35309e-05, gnorm=0.967, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44048
2023-03-15 07:46:32 - progress_bar.py[line:274] - INFO: epoch 004:   4560 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218, ups=0.85, wpb=255.5, bsz=100, num_updates=23460, lr=1.35142e-05, gnorm=0.9, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=44060
2023-03-15 07:46:44 - progress_bar.py[line:274] - INFO: epoch 004:   4570 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222, ups=0.88, wpb=250.9, bsz=100, num_updates=23470, lr=1.34975e-05, gnorm=0.911, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=44071
2023-03-15 07:46:55 - progress_bar.py[line:274] - INFO: epoch 004:   4580 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.4, ups=0.88, wpb=252.6, bsz=100, num_updates=23480, lr=1.34808e-05, gnorm=0.878, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44083
2023-03-15 07:47:07 - progress_bar.py[line:274] - INFO: epoch 004:   4590 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.7, ups=0.86, wpb=252.1, bsz=100, num_updates=23490, lr=1.34642e-05, gnorm=0.857, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=44095
2023-03-15 07:47:21 - progress_bar.py[line:274] - INFO: epoch 004:   4600 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.8, ups=0.87, wpb=251.7, bsz=100, num_updates=23500, lr=1.34475e-05, gnorm=0.907, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44106
2023-03-15 07:47:33 - progress_bar.py[line:274] - INFO: epoch 004:   4610 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221, ups=0.87, wpb=254.4, bsz=100, num_updates=23510, lr=1.34308e-05, gnorm=0.908, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44120
2023-03-15 07:47:47 - progress_bar.py[line:274] - INFO: epoch 004:   4620 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.5, ups=0.86, wpb=253.8, bsz=100, num_updates=23520, lr=1.34141e-05, gnorm=0.814, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=44133
2023-03-15 07:48:00 - progress_bar.py[line:274] - INFO: epoch 004:   4630 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.9, ups=0.88, wpb=252, bsz=100, num_updates=23530, lr=1.33975e-05, gnorm=1.004, clip=70, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=44146
2023-03-15 07:48:16 - progress_bar.py[line:274] - INFO: epoch 004:   4640 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.7, ups=0.88, wpb=251.8, bsz=100, num_updates=23540, lr=1.33808e-05, gnorm=0.953, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=44160
2023-03-15 07:48:32 - progress_bar.py[line:274] - INFO: epoch 004:   4650 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=216.6, ups=0.86, wpb=253, bsz=100, num_updates=23550, lr=1.33641e-05, gnorm=0.905, clip=20, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=44175
2023-03-15 07:48:47 - progress_bar.py[line:274] - INFO: epoch 004:   4660 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.6, ups=0.86, wpb=254.1, bsz=100, num_updates=23560, lr=1.33475e-05, gnorm=0.921, clip=40, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=44191
2023-03-15 07:49:03 - progress_bar.py[line:274] - INFO: epoch 004:   4670 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.3, ups=0.87, wpb=254.2, bsz=100, num_updates=23570, lr=1.33308e-05, gnorm=0.808, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44207
2023-03-15 07:49:18 - progress_bar.py[line:274] - INFO: epoch 004:   4680 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=215, ups=0.85, wpb=251.7, bsz=100, num_updates=23580, lr=1.33141e-05, gnorm=0.924, clip=30, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=44223
2023-03-15 07:49:31 - progress_bar.py[line:274] - INFO: epoch 004:   4690 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.5, ups=0.88, wpb=253, bsz=100, num_updates=23590, lr=1.32974e-05, gnorm=0.933, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44237
2023-03-15 07:49:44 - progress_bar.py[line:274] - INFO: epoch 004:   4700 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.5, ups=0.88, wpb=253, bsz=100, num_updates=23600, lr=1.32808e-05, gnorm=0.84, clip=10, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=44250
2023-03-15 07:49:59 - progress_bar.py[line:274] - INFO: epoch 004:   4710 / 6313 loss=0.552, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220.1, ups=0.88, wpb=250.7, bsz=100, num_updates=23610, lr=1.32641e-05, gnorm=1.039, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44263
2023-03-15 07:50:13 - progress_bar.py[line:274] - INFO: epoch 004:   4720 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.1, ups=0.87, wpb=254.9, bsz=100, num_updates=23620, lr=1.32474e-05, gnorm=0.87, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44278
2023-03-15 07:50:29 - progress_bar.py[line:274] - INFO: epoch 004:   4730 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=215.6, ups=0.86, wpb=250.3, bsz=100, num_updates=23630, lr=1.32307e-05, gnorm=0.901, clip=30, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=44293
2023-03-15 07:50:44 - progress_bar.py[line:274] - INFO: epoch 004:   4740 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.5, ups=0.87, wpb=252.9, bsz=100, num_updates=23640, lr=1.32141e-05, gnorm=0.809, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44308
2023-03-15 07:51:00 - progress_bar.py[line:274] - INFO: epoch 004:   4750 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.3, ups=0.86, wpb=253.2, bsz=100, num_updates=23650, lr=1.31974e-05, gnorm=0.838, clip=0, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=44324
2023-03-15 07:51:13 - progress_bar.py[line:274] - INFO: epoch 004:   4760 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=215.1, ups=0.86, wpb=250.6, bsz=100, num_updates=23660, lr=1.31807e-05, gnorm=0.819, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=44339
2023-03-15 07:51:26 - progress_bar.py[line:274] - INFO: epoch 004:   4770 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=225.3, ups=0.89, wpb=252, bsz=100, num_updates=23670, lr=1.3164e-05, gnorm=0.895, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44352
2023-03-15 07:51:39 - progress_bar.py[line:274] - INFO: epoch 004:   4780 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=226.2, ups=0.89, wpb=255.4, bsz=100, num_updates=23680, lr=1.31474e-05, gnorm=0.818, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44365
2023-03-15 07:51:53 - progress_bar.py[line:274] - INFO: epoch 004:   4790 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.3, ups=0.86, wpb=254, bsz=100, num_updates=23690, lr=1.31307e-05, gnorm=1.075, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=44378
2023-03-15 07:52:06 - progress_bar.py[line:274] - INFO: epoch 004:   4800 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.8, ups=0.86, wpb=253, bsz=100, num_updates=23700, lr=1.3114e-05, gnorm=0.844, clip=0, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=44392
2023-03-15 07:52:19 - progress_bar.py[line:274] - INFO: epoch 004:   4810 / 6313 loss=0.544, loss_v1=0, loss_v2=0, nll_loss=0.321, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=222.5, ups=0.88, wpb=251.6, bsz=100, num_updates=23710, lr=1.30973e-05, gnorm=0.887, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44405
2023-03-15 07:52:32 - progress_bar.py[line:274] - INFO: epoch 004:   4820 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.1, ups=0.86, wpb=252.8, bsz=100, num_updates=23720, lr=1.30807e-05, gnorm=0.885, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=44418
2023-03-15 07:52:45 - progress_bar.py[line:274] - INFO: epoch 004:   4830 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.6, ups=0.87, wpb=254, bsz=100, num_updates=23730, lr=1.3064e-05, gnorm=0.978, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44432
2023-03-15 07:52:59 - progress_bar.py[line:274] - INFO: epoch 004:   4840 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.5, ups=0.86, wpb=253.8, bsz=100, num_updates=23740, lr=1.30473e-05, gnorm=0.879, clip=20, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=44445
2023-03-15 07:53:13 - progress_bar.py[line:274] - INFO: epoch 004:   4850 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.3, ups=0.85, wpb=255.5, bsz=100, num_updates=23750, lr=1.30306e-05, gnorm=0.853, clip=10, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=44459
2023-03-15 07:53:26 - progress_bar.py[line:274] - INFO: epoch 004:   4860 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.5, ups=0.87, wpb=253.3, bsz=100, num_updates=23760, lr=1.3014e-05, gnorm=0.925, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=44472
2023-03-15 07:53:39 - progress_bar.py[line:274] - INFO: epoch 004:   4870 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.9, ups=0.86, wpb=253.6, bsz=100, num_updates=23770, lr=1.29973e-05, gnorm=0.886, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=44485
2023-03-15 07:53:52 - progress_bar.py[line:274] - INFO: epoch 004:   4880 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.7, ups=0.87, wpb=253.2, bsz=100, num_updates=23780, lr=1.29806e-05, gnorm=0.838, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44498
2023-03-15 07:54:06 - progress_bar.py[line:274] - INFO: epoch 004:   4890 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.2, ups=0.87, wpb=253.4, bsz=100, num_updates=23790, lr=1.2964e-05, gnorm=0.866, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44512
2023-03-15 07:54:19 - progress_bar.py[line:274] - INFO: epoch 004:   4900 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.3, ups=0.87, wpb=252.9, bsz=100, num_updates=23800, lr=1.29473e-05, gnorm=0.911, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44525
2023-03-15 07:54:33 - progress_bar.py[line:274] - INFO: epoch 004:   4910 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.8, ups=0.86, wpb=254.5, bsz=100, num_updates=23810, lr=1.29306e-05, gnorm=0.929, clip=40, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=44539
2023-03-15 07:54:46 - progress_bar.py[line:274] - INFO: epoch 004:   4920 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.2, ups=0.87, wpb=252.8, bsz=100, num_updates=23820, lr=1.29139e-05, gnorm=0.849, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44552
2023-03-15 07:54:59 - progress_bar.py[line:274] - INFO: epoch 004:   4930 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.1, ups=0.87, wpb=255.6, bsz=100, num_updates=23830, lr=1.28973e-05, gnorm=0.854, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44565
2023-03-15 07:55:13 - progress_bar.py[line:274] - INFO: epoch 004:   4940 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219, ups=0.87, wpb=251, bsz=100, num_updates=23840, lr=1.28806e-05, gnorm=0.944, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44579
2023-03-15 07:55:26 - progress_bar.py[line:274] - INFO: epoch 004:   4950 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.5, ups=0.86, wpb=252.7, bsz=100, num_updates=23850, lr=1.28639e-05, gnorm=0.934, clip=50, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=44592
2023-03-15 07:55:40 - progress_bar.py[line:274] - INFO: epoch 004:   4960 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.7, ups=0.87, wpb=253.5, bsz=100, num_updates=23860, lr=1.28472e-05, gnorm=0.864, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44606
2023-03-15 07:55:54 - progress_bar.py[line:274] - INFO: epoch 004:   4970 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.2, ups=0.85, wpb=255.8, bsz=100, num_updates=23870, lr=1.28306e-05, gnorm=0.848, clip=0, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=44619
2023-03-15 07:56:07 - progress_bar.py[line:274] - INFO: epoch 004:   4980 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.9, ups=0.88, wpb=252, bsz=100, num_updates=23880, lr=1.28139e-05, gnorm=0.887, clip=10, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=44633
2023-03-15 07:56:21 - progress_bar.py[line:274] - INFO: epoch 004:   4990 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.2, ups=0.87, wpb=254.5, bsz=100, num_updates=23890, lr=1.27972e-05, gnorm=0.881, clip=10, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=44647
2023-03-15 07:56:34 - progress_bar.py[line:274] - INFO: epoch 004:   5000 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.1, ups=0.88, wpb=253.3, bsz=100, num_updates=23900, lr=1.27805e-05, gnorm=0.903, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44660
2023-03-15 07:56:44 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 07:56:49 - progress_bar.py[line:274] - INFO: epoch 004:   5011 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=200.5, ups=0.79, wpb=253.4, bsz=100, num_updates=23910, lr=1.27639e-05, gnorm=1.009, clip=30, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=44675
2023-03-15 07:57:02 - progress_bar.py[line:274] - INFO: epoch 004:   5021 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.5, ups=0.86, wpb=253.3, bsz=100, num_updates=23920, lr=1.27472e-05, gnorm=0.928, clip=20, loss_scale=512, train_wall=12, gb_free=9.9, ema_decay=0.9999, wall=44688
2023-03-15 07:57:15 - progress_bar.py[line:274] - INFO: epoch 004:   5031 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.6, ups=0.87, wpb=254.7, bsz=100, num_updates=23930, lr=1.27305e-05, gnorm=0.844, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44701
2023-03-15 07:57:28 - progress_bar.py[line:274] - INFO: epoch 004:   5041 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=226.3, ups=0.89, wpb=253.3, bsz=100, num_updates=23940, lr=1.27138e-05, gnorm=1.023, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44714
2023-03-15 07:57:42 - progress_bar.py[line:274] - INFO: epoch 004:   5051 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.4, ups=0.87, wpb=254.2, bsz=100, num_updates=23950, lr=1.26972e-05, gnorm=0.851, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=44728
2023-03-15 07:57:55 - progress_bar.py[line:274] - INFO: epoch 004:   5061 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220, ups=0.87, wpb=252.6, bsz=100, num_updates=23960, lr=1.26805e-05, gnorm=0.904, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=44741
2023-03-15 07:58:09 - progress_bar.py[line:274] - INFO: epoch 004:   5071 / 6313 loss=0.55, loss_v1=0, loss_v2=0, nll_loss=0.334, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=220, ups=0.87, wpb=252.8, bsz=100, num_updates=23970, lr=1.26638e-05, gnorm=1.029, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44755
2023-03-15 07:58:23 - progress_bar.py[line:274] - INFO: epoch 004:   5081 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=216.6, ups=0.85, wpb=254, bsz=100, num_updates=23980, lr=1.26471e-05, gnorm=0.908, clip=10, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=44769
2023-03-15 07:58:36 - progress_bar.py[line:274] - INFO: epoch 004:   5091 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=248.8, nsentences=100, sample_size=248.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.4, ups=0.89, wpb=248.8, bsz=100, num_updates=23990, lr=1.26305e-05, gnorm=0.919, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44782
2023-03-15 07:58:49 - progress_bar.py[line:274] - INFO: epoch 004:   5101 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.4, ups=0.88, wpb=254, bsz=100, num_updates=24000, lr=1.26138e-05, gnorm=0.885, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44795
2023-03-15 07:59:02 - progress_bar.py[line:274] - INFO: epoch 004:   5111 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.32, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.1, ups=0.86, wpb=252.4, bsz=100, num_updates=24010, lr=1.25971e-05, gnorm=0.932, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=44808
2023-03-15 07:59:16 - progress_bar.py[line:274] - INFO: epoch 004:   5121 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.2, ups=0.87, wpb=255.1, bsz=100, num_updates=24020, lr=1.25805e-05, gnorm=0.862, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=44822
2023-03-15 07:59:30 - progress_bar.py[line:274] - INFO: epoch 004:   5131 / 6313 loss=0.542, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.2, ups=0.88, wpb=250.3, bsz=100, num_updates=24030, lr=1.25638e-05, gnorm=0.906, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44835
2023-03-15 07:59:47 - progress_bar.py[line:274] - INFO: epoch 004:   5141 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.5, ups=0.87, wpb=251.6, bsz=100, num_updates=24040, lr=1.25471e-05, gnorm=0.981, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=44850
2023-03-15 08:00:02 - progress_bar.py[line:274] - INFO: epoch 004:   5151 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.2, ups=0.87, wpb=255.9, bsz=100, num_updates=24050, lr=1.25304e-05, gnorm=0.848, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44866
2023-03-15 08:00:18 - progress_bar.py[line:274] - INFO: epoch 004:   5161 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.7, ups=0.85, wpb=253.9, bsz=100, num_updates=24060, lr=1.25138e-05, gnorm=0.977, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=44882
2023-03-15 08:00:32 - progress_bar.py[line:274] - INFO: epoch 004:   5171 / 6313 loss=0.551, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=248.5, nsentences=100, sample_size=248.5, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=217.5, ups=0.88, wpb=248.5, bsz=100, num_updates=24070, lr=1.24971e-05, gnorm=0.977, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=44897
2023-03-15 08:00:45 - progress_bar.py[line:274] - INFO: epoch 004:   5181 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.8, ups=0.87, wpb=254.1, bsz=100, num_updates=24080, lr=1.24804e-05, gnorm=0.913, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=44911
2023-03-15 08:00:59 - progress_bar.py[line:274] - INFO: epoch 004:   5191 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.4, ups=0.88, wpb=251.7, bsz=100, num_updates=24090, lr=1.24637e-05, gnorm=0.857, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44924
2023-03-15 08:01:12 - progress_bar.py[line:274] - INFO: epoch 004:   5201 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.2, ups=0.86, wpb=252.5, bsz=100, num_updates=24100, lr=1.24471e-05, gnorm=0.887, clip=30, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=44938
2023-03-15 08:01:25 - progress_bar.py[line:274] - INFO: epoch 004:   5211 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.8, ups=0.87, wpb=252.8, bsz=100, num_updates=24110, lr=1.24304e-05, gnorm=0.919, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=44951
2023-03-15 08:01:38 - progress_bar.py[line:274] - INFO: epoch 004:   5221 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=256.9, nsentences=100, sample_size=256.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.2, ups=0.86, wpb=256.9, bsz=100, num_updates=24120, lr=1.24137e-05, gnorm=0.947, clip=50, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=44965
2023-03-15 08:01:51 - progress_bar.py[line:274] - INFO: epoch 004:   5231 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=220.6, ups=0.88, wpb=251, bsz=100, num_updates=24130, lr=1.2397e-05, gnorm=0.97, clip=20, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=44977
2023-03-15 08:02:04 - progress_bar.py[line:274] - INFO: epoch 004:   5241 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.314, ntokens=249.7, nsentences=100, sample_size=249.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=213.3, ups=0.85, wpb=249.7, bsz=100, num_updates=24140, lr=1.23804e-05, gnorm=0.998, clip=50, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=44991
2023-03-15 08:02:17 - progress_bar.py[line:274] - INFO: epoch 004:   5251 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.313, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.7, ups=0.88, wpb=253.9, bsz=100, num_updates=24150, lr=1.23637e-05, gnorm=0.892, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45004
2023-03-15 08:02:31 - progress_bar.py[line:274] - INFO: epoch 004:   5261 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.3, ups=0.86, wpb=253, bsz=100, num_updates=24160, lr=1.2347e-05, gnorm=0.887, clip=20, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=45017
2023-03-15 08:02:46 - progress_bar.py[line:274] - INFO: epoch 004:   5271 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.7, ups=0.86, wpb=252.5, bsz=100, num_updates=24170, lr=1.23303e-05, gnorm=0.928, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=45030
2023-03-15 08:03:01 - progress_bar.py[line:274] - INFO: epoch 004:   5281 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=215.5, ups=0.85, wpb=252.4, bsz=100, num_updates=24180, lr=1.23137e-05, gnorm=0.912, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=45046
2023-03-15 08:03:17 - progress_bar.py[line:274] - INFO: epoch 004:   5291 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=225.7, ups=0.89, wpb=253.9, bsz=100, num_updates=24190, lr=1.2297e-05, gnorm=0.798, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45060
2023-03-15 08:03:33 - progress_bar.py[line:274] - INFO: epoch 004:   5301 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.9, ups=0.87, wpb=252.5, bsz=100, num_updates=24200, lr=1.22803e-05, gnorm=0.888, clip=10, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45076
2023-03-15 08:03:46 - progress_bar.py[line:274] - INFO: epoch 004:   5311 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.5, ups=0.88, wpb=254.2, bsz=100, num_updates=24210, lr=1.22636e-05, gnorm=0.916, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45092
2023-03-15 08:04:00 - progress_bar.py[line:274] - INFO: epoch 004:   5321 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.7, ups=0.86, wpb=253.7, bsz=100, num_updates=24220, lr=1.2247e-05, gnorm=0.937, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=45106
2023-03-15 08:04:14 - progress_bar.py[line:274] - INFO: epoch 004:   5331 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.3, ups=0.86, wpb=252.9, bsz=100, num_updates=24230, lr=1.22303e-05, gnorm=0.947, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=45120
2023-03-15 08:04:27 - progress_bar.py[line:274] - INFO: epoch 004:   5341 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.1, ups=0.86, wpb=254.4, bsz=100, num_updates=24240, lr=1.22136e-05, gnorm=0.88, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=45133
2023-03-15 08:04:40 - progress_bar.py[line:274] - INFO: epoch 004:   5351 / 6313 loss=0.533, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.2, ups=0.87, wpb=250.5, bsz=100, num_updates=24250, lr=1.2197e-05, gnorm=1.026, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45147
2023-03-15 08:04:55 - progress_bar.py[line:274] - INFO: epoch 004:   5361 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=215.4, ups=0.85, wpb=253, bsz=100, num_updates=24260, lr=1.21803e-05, gnorm=0.906, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=45160
2023-03-15 08:05:07 - progress_bar.py[line:274] - INFO: epoch 004:   5371 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224.6, ups=0.89, wpb=253.4, bsz=100, num_updates=24270, lr=1.21636e-05, gnorm=0.974, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45174
2023-03-15 08:05:20 - progress_bar.py[line:274] - INFO: epoch 004:   5381 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.3, ups=0.87, wpb=255.8, bsz=100, num_updates=24280, lr=1.21469e-05, gnorm=0.98, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45187
2023-03-15 08:05:34 - progress_bar.py[line:274] - INFO: epoch 004:   5391 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=225.5, ups=0.88, wpb=256.1, bsz=100, num_updates=24290, lr=1.21303e-05, gnorm=0.925, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45200
2023-03-15 08:05:47 - progress_bar.py[line:274] - INFO: epoch 004:   5401 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.3, ups=0.87, wpb=254.3, bsz=100, num_updates=24300, lr=1.21136e-05, gnorm=0.845, clip=10, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=45213
2023-03-15 08:06:00 - progress_bar.py[line:274] - INFO: epoch 004:   5411 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224.9, ups=0.88, wpb=255.1, bsz=100, num_updates=24310, lr=1.20969e-05, gnorm=0.981, clip=50, loss_scale=512, train_wall=11, gb_free=9.6, ema_decay=0.9999, wall=45226
2023-03-15 08:06:13 - progress_bar.py[line:274] - INFO: epoch 004:   5421 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=226.6, ups=0.89, wpb=254.7, bsz=100, num_updates=24320, lr=1.20802e-05, gnorm=0.885, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=45239
2023-03-15 08:06:26 - progress_bar.py[line:274] - INFO: epoch 004:   5431 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=227.9, ups=0.9, wpb=253.5, bsz=100, num_updates=24330, lr=1.20636e-05, gnorm=0.922, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45252
2023-03-15 08:06:39 - progress_bar.py[line:274] - INFO: epoch 004:   5441 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.9, ups=0.86, wpb=251.7, bsz=100, num_updates=24340, lr=1.20469e-05, gnorm=0.908, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=45265
2023-03-15 08:06:53 - progress_bar.py[line:274] - INFO: epoch 004:   5451 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.5, ups=0.87, wpb=252.7, bsz=100, num_updates=24350, lr=1.20302e-05, gnorm=0.961, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45278
2023-03-15 08:07:06 - progress_bar.py[line:274] - INFO: epoch 004:   5461 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.7, ups=0.87, wpb=251.2, bsz=100, num_updates=24360, lr=1.20135e-05, gnorm=0.95, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45292
2023-03-15 08:07:19 - progress_bar.py[line:274] - INFO: epoch 004:   5471 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.6, ups=0.87, wpb=252.4, bsz=100, num_updates=24370, lr=1.19969e-05, gnorm=0.888, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45305
2023-03-15 08:07:32 - progress_bar.py[line:274] - INFO: epoch 004:   5481 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=256.8, nsentences=100, sample_size=256.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=226.1, ups=0.88, wpb=256.8, bsz=100, num_updates=24380, lr=1.19802e-05, gnorm=0.935, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45318
2023-03-15 08:07:46 - progress_bar.py[line:274] - INFO: epoch 004:   5491 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219, ups=0.86, wpb=253.8, bsz=100, num_updates=24390, lr=1.19635e-05, gnorm=0.947, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=45331
2023-03-15 08:07:59 - progress_bar.py[line:274] - INFO: epoch 004:   5501 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=225.8, ups=0.88, wpb=255.2, bsz=100, num_updates=24400, lr=1.19468e-05, gnorm=0.929, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45345
2023-03-15 08:08:12 - progress_bar.py[line:274] - INFO: epoch 004:   5511 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.3, ups=0.87, wpb=254.4, bsz=100, num_updates=24410, lr=1.19302e-05, gnorm=0.964, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=45358
2023-03-15 08:08:25 - progress_bar.py[line:274] - INFO: epoch 004:   5521 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.9, ups=0.86, wpb=254.3, bsz=100, num_updates=24420, lr=1.19135e-05, gnorm=0.825, clip=10, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=45372
2023-03-15 08:08:39 - progress_bar.py[line:274] - INFO: epoch 004:   5531 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.1, ups=0.88, wpb=254.3, bsz=100, num_updates=24430, lr=1.18968e-05, gnorm=0.973, clip=30, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=45385
2023-03-15 08:08:47 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 08:08:54 - progress_bar.py[line:274] - INFO: epoch 004:   5542 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=203.2, ups=0.8, wpb=253.4, bsz=100, num_updates=24440, lr=1.18801e-05, gnorm=0.899, clip=20, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=45399
2023-03-15 08:09:07 - progress_bar.py[line:274] - INFO: epoch 004:   5552 / 6313 loss=0.553, loss_v1=0, loss_v2=0, nll_loss=0.33, ntokens=249.7, nsentences=100, sample_size=249.7, sample_size_v1=0, sample_size_v2=0, ppl=1.26, wps=215.6, ups=0.86, wpb=249.7, bsz=100, num_updates=24450, lr=1.18635e-05, gnorm=0.935, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=45413
2023-03-15 08:09:20 - progress_bar.py[line:274] - INFO: epoch 004:   5562 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=214.4, ups=0.86, wpb=250.4, bsz=100, num_updates=24460, lr=1.18468e-05, gnorm=0.99, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=45426
2023-03-15 08:09:33 - progress_bar.py[line:274] - INFO: epoch 004:   5572 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.8, ups=0.85, wpb=253.6, bsz=100, num_updates=24470, lr=1.18301e-05, gnorm=0.907, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=45440
2023-03-15 08:09:47 - progress_bar.py[line:274] - INFO: epoch 004:   5582 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.2, ups=0.86, wpb=253.7, bsz=100, num_updates=24480, lr=1.18135e-05, gnorm=0.839, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=45453
2023-03-15 08:10:00 - progress_bar.py[line:274] - INFO: epoch 004:   5592 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.7, ups=0.86, wpb=255.7, bsz=100, num_updates=24490, lr=1.17968e-05, gnorm=0.854, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=45466
2023-03-15 08:10:14 - progress_bar.py[line:274] - INFO: epoch 004:   5602 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.2, ups=0.87, wpb=254, bsz=100, num_updates=24500, lr=1.17801e-05, gnorm=0.896, clip=20, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=45480
2023-03-15 08:10:27 - progress_bar.py[line:274] - INFO: epoch 004:   5612 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.6, ups=0.88, wpb=255.2, bsz=100, num_updates=24510, lr=1.17634e-05, gnorm=0.853, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45493
2023-03-15 08:10:40 - progress_bar.py[line:274] - INFO: epoch 004:   5622 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.309, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222, ups=0.87, wpb=254.8, bsz=100, num_updates=24520, lr=1.17468e-05, gnorm=0.944, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45507
2023-03-15 08:10:53 - progress_bar.py[line:274] - INFO: epoch 004:   5632 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=225, ups=0.88, wpb=256.2, bsz=100, num_updates=24530, lr=1.17301e-05, gnorm=0.851, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45520
2023-03-15 08:11:06 - progress_bar.py[line:274] - INFO: epoch 004:   5642 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.8, ups=0.88, wpb=254.2, bsz=100, num_updates=24540, lr=1.17134e-05, gnorm=0.848, clip=0, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=45533
2023-03-15 08:11:19 - progress_bar.py[line:274] - INFO: epoch 004:   5652 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.8, ups=0.87, wpb=255.8, bsz=100, num_updates=24550, lr=1.16967e-05, gnorm=0.802, clip=10, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45546
2023-03-15 08:11:33 - progress_bar.py[line:274] - INFO: epoch 004:   5662 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.2, ups=0.88, wpb=250.5, bsz=100, num_updates=24560, lr=1.16801e-05, gnorm=0.881, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45559
2023-03-15 08:11:46 - progress_bar.py[line:274] - INFO: epoch 004:   5672 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=248.8, nsentences=100, sample_size=248.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.2, ups=0.87, wpb=248.8, bsz=100, num_updates=24570, lr=1.16634e-05, gnorm=0.905, clip=30, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=45572
2023-03-15 08:12:00 - progress_bar.py[line:274] - INFO: epoch 004:   5682 / 6313 loss=0.531, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=213.9, ups=0.85, wpb=251.4, bsz=100, num_updates=24580, lr=1.16467e-05, gnorm=0.894, clip=10, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=45586
2023-03-15 08:12:13 - progress_bar.py[line:274] - INFO: epoch 004:   5692 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=225.7, ups=0.89, wpb=254.1, bsz=100, num_updates=24590, lr=1.163e-05, gnorm=0.894, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=45599
2023-03-15 08:12:26 - progress_bar.py[line:274] - INFO: epoch 004:   5702 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.6, ups=0.87, wpb=253.1, bsz=100, num_updates=24600, lr=1.16134e-05, gnorm=0.845, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45612
2023-03-15 08:12:39 - progress_bar.py[line:274] - INFO: epoch 004:   5712 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.2, ups=0.87, wpb=254, bsz=100, num_updates=24610, lr=1.15967e-05, gnorm=0.905, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45625
2023-03-15 08:12:53 - progress_bar.py[line:274] - INFO: epoch 004:   5722 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.9, ups=0.87, wpb=256.1, bsz=100, num_updates=24620, lr=1.158e-05, gnorm=0.964, clip=30, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=45639
2023-03-15 08:13:06 - progress_bar.py[line:274] - INFO: epoch 004:   5732 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.8, ups=0.86, wpb=254.2, bsz=100, num_updates=24630, lr=1.15633e-05, gnorm=0.913, clip=30, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=45652
2023-03-15 08:13:21 - progress_bar.py[line:274] - INFO: epoch 004:   5742 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=256.7, nsentences=100, sample_size=256.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.3, ups=0.87, wpb=256.7, bsz=100, num_updates=24640, lr=1.15467e-05, gnorm=0.92, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=45666
2023-03-15 08:13:36 - progress_bar.py[line:274] - INFO: epoch 004:   5752 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.4, ups=0.86, wpb=255.4, bsz=100, num_updates=24650, lr=1.153e-05, gnorm=0.899, clip=20, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=45680
2023-03-15 08:13:51 - progress_bar.py[line:274] - INFO: epoch 004:   5762 / 6313 loss=0.538, loss_v1=0, loss_v2=0, nll_loss=0.316, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.6, ups=0.86, wpb=252.3, bsz=100, num_updates=24660, lr=1.15133e-05, gnorm=0.938, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=45695
2023-03-15 08:14:07 - progress_bar.py[line:274] - INFO: epoch 004:   5772 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.4, ups=0.87, wpb=252.6, bsz=100, num_updates=24670, lr=1.14966e-05, gnorm=0.89, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45710
2023-03-15 08:14:23 - progress_bar.py[line:274] - INFO: epoch 004:   5782 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.5, ups=0.89, wpb=252.1, bsz=100, num_updates=24680, lr=1.148e-05, gnorm=0.901, clip=20, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=45726
2023-03-15 08:14:37 - progress_bar.py[line:274] - INFO: epoch 004:   5792 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=223, ups=0.88, wpb=254.1, bsz=100, num_updates=24690, lr=1.14633e-05, gnorm=0.895, clip=20, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=45743
2023-03-15 08:14:51 - progress_bar.py[line:274] - INFO: epoch 004:   5802 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.4, ups=0.86, wpb=255.8, bsz=100, num_updates=24700, lr=1.14466e-05, gnorm=0.883, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=45757
2023-03-15 08:15:04 - progress_bar.py[line:274] - INFO: epoch 004:   5812 / 6313 loss=0.532, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=214.7, ups=0.85, wpb=252, bsz=100, num_updates=24710, lr=1.143e-05, gnorm=0.986, clip=40, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=45770
2023-03-15 08:15:17 - progress_bar.py[line:274] - INFO: epoch 004:   5822 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.6, ups=0.87, wpb=256.1, bsz=100, num_updates=24720, lr=1.14133e-05, gnorm=0.902, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=45783
2023-03-15 08:15:32 - progress_bar.py[line:274] - INFO: epoch 004:   5832 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.2, ups=0.88, wpb=254.5, bsz=100, num_updates=24730, lr=1.13966e-05, gnorm=0.87, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45796
2023-03-15 08:15:47 - progress_bar.py[line:274] - INFO: epoch 004:   5842 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.9, ups=0.87, wpb=252.5, bsz=100, num_updates=24740, lr=1.13799e-05, gnorm=0.915, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45811
2023-03-15 08:16:02 - progress_bar.py[line:274] - INFO: epoch 004:   5852 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.7, ups=0.86, wpb=255.6, bsz=100, num_updates=24750, lr=1.13633e-05, gnorm=0.935, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=45826
2023-03-15 08:16:17 - progress_bar.py[line:274] - INFO: epoch 004:   5862 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223, ups=0.87, wpb=255, bsz=100, num_updates=24760, lr=1.13466e-05, gnorm=0.968, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45841
2023-03-15 08:16:33 - progress_bar.py[line:274] - INFO: epoch 004:   5872 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.8, ups=0.86, wpb=255.8, bsz=100, num_updates=24770, lr=1.13299e-05, gnorm=0.846, clip=10, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=45857
2023-03-15 08:16:49 - progress_bar.py[line:274] - INFO: epoch 004:   5882 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.7, ups=0.86, wpb=250.9, bsz=100, num_updates=24780, lr=1.13132e-05, gnorm=0.902, clip=20, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=45872
2023-03-15 08:17:04 - progress_bar.py[line:274] - INFO: epoch 004:   5892 / 6313 loss=0.536, loss_v1=0, loss_v2=0, nll_loss=0.318, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=220.9, ups=0.87, wpb=254.5, bsz=100, num_updates=24790, lr=1.12966e-05, gnorm=0.915, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=45888
2023-03-15 08:17:19 - progress_bar.py[line:274] - INFO: epoch 004:   5902 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.4, ups=0.86, wpb=251.6, bsz=100, num_updates=24800, lr=1.12799e-05, gnorm=0.911, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=45903
2023-03-15 08:17:35 - progress_bar.py[line:274] - INFO: epoch 004:   5912 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.8, ups=0.87, wpb=250.9, bsz=100, num_updates=24810, lr=1.12632e-05, gnorm=0.97, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=45918
2023-03-15 08:17:48 - progress_bar.py[line:274] - INFO: epoch 004:   5922 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.7, ups=0.87, wpb=255.4, bsz=100, num_updates=24820, lr=1.12465e-05, gnorm=0.833, clip=0, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45934
2023-03-15 08:18:01 - progress_bar.py[line:274] - INFO: epoch 004:   5932 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.6, ups=0.87, wpb=251, bsz=100, num_updates=24830, lr=1.12299e-05, gnorm=0.926, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45948
2023-03-15 08:18:13 - progress_bar.py[line:274] - INFO: epoch 004:   5942 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.5, ups=0.88, wpb=252.9, bsz=100, num_updates=24840, lr=1.12132e-05, gnorm=0.948, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45961
2023-03-15 08:18:25 - progress_bar.py[line:274] - INFO: epoch 004:   5952 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223, ups=0.88, wpb=253.8, bsz=100, num_updates=24850, lr=1.11965e-05, gnorm=0.98, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=45972
2023-03-15 08:18:37 - progress_bar.py[line:274] - INFO: epoch 004:   5962 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.2, ups=0.86, wpb=250.5, bsz=100, num_updates=24860, lr=1.11798e-05, gnorm=0.966, clip=50, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=45984
2023-03-15 08:18:48 - progress_bar.py[line:274] - INFO: epoch 004:   5972 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.7, ups=0.88, wpb=252, bsz=100, num_updates=24870, lr=1.11632e-05, gnorm=0.951, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=45996
2023-03-15 08:19:00 - progress_bar.py[line:274] - INFO: epoch 004:   5982 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.4, ups=0.88, wpb=252.5, bsz=100, num_updates=24880, lr=1.11465e-05, gnorm=0.828, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=46008
2023-03-15 08:19:12 - progress_bar.py[line:274] - INFO: epoch 004:   5992 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.3, ups=0.88, wpb=253.7, bsz=100, num_updates=24890, lr=1.11298e-05, gnorm=0.95, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=46019
2023-03-15 08:19:24 - progress_bar.py[line:274] - INFO: epoch 004:   6002 / 6313 loss=0.545, loss_v1=0, loss_v2=0, nll_loss=0.324, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.3, ups=0.86, wpb=251.5, bsz=100, num_updates=24900, lr=1.11131e-05, gnorm=0.953, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=46031
2023-03-15 08:19:36 - progress_bar.py[line:274] - INFO: epoch 004:   6012 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219, ups=0.87, wpb=252.7, bsz=100, num_updates=24910, lr=1.10965e-05, gnorm=0.88, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=46043
2023-03-15 08:19:47 - progress_bar.py[line:274] - INFO: epoch 004:   6022 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=224.1, ups=0.88, wpb=254.3, bsz=100, num_updates=24920, lr=1.10798e-05, gnorm=0.891, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=46055
2023-03-15 08:19:59 - progress_bar.py[line:274] - INFO: epoch 004:   6032 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.9, ups=0.86, wpb=253.5, bsz=100, num_updates=24930, lr=1.10631e-05, gnorm=0.917, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=46067
2023-03-15 08:20:11 - progress_bar.py[line:274] - INFO: epoch 004:   6042 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.5, ups=0.86, wpb=253.1, bsz=100, num_updates=24940, lr=1.10465e-05, gnorm=0.875, clip=0, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=46079
2023-03-15 08:20:23 - progress_bar.py[line:274] - INFO: epoch 004:   6052 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.5, ups=0.86, wpb=252, bsz=100, num_updates=24950, lr=1.10298e-05, gnorm=0.871, clip=10, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=46091
2023-03-15 08:20:34 - progress_bar.py[line:274] - INFO: epoch 004:   6062 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=224.6, ups=0.9, wpb=250.6, bsz=100, num_updates=24960, lr=1.10131e-05, gnorm=0.907, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=46102
2023-03-15 08:20:46 - progress_bar.py[line:274] - INFO: epoch 004:   6072 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.6, ups=0.88, wpb=255.1, bsz=100, num_updates=24970, lr=1.09964e-05, gnorm=0.943, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=46114
2023-03-15 08:20:58 - progress_bar.py[line:274] - INFO: epoch 004:   6082 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=225.8, ups=0.89, wpb=254.8, bsz=100, num_updates=24980, lr=1.09798e-05, gnorm=0.922, clip=30, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=46125
2023-03-15 08:21:10 - progress_bar.py[line:274] - INFO: epoch 004:   6092 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=222.9, ups=0.88, wpb=254, bsz=100, num_updates=24990, lr=1.09631e-05, gnorm=0.895, clip=20, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=46137
2023-03-15 08:21:22 - progress_bar.py[line:274] - INFO: epoch 004:   6102 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217, ups=0.86, wpb=251.7, bsz=100, num_updates=25000, lr=1.09464e-05, gnorm=0.842, clip=10, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=46149
2023-03-15 08:21:34 - progress_bar.py[line:274] - INFO: epoch 004:   6112 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223, ups=0.88, wpb=254.3, bsz=100, num_updates=25010, lr=1.09297e-05, gnorm=0.909, clip=10, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=46161
2023-03-15 08:21:45 - progress_bar.py[line:274] - INFO: epoch 004:   6122 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.6, ups=0.86, wpb=251.8, bsz=100, num_updates=25020, lr=1.09131e-05, gnorm=1.028, clip=40, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=46173
2023-03-15 08:21:57 - progress_bar.py[line:274] - INFO: epoch 004:   6132 / 6313 loss=0.549, loss_v1=0, loss_v2=0, nll_loss=0.325, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218, ups=0.87, wpb=251.2, bsz=100, num_updates=25030, lr=1.08964e-05, gnorm=0.951, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=46185
2023-03-15 08:22:09 - progress_bar.py[line:274] - INFO: epoch 004:   6142 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=217.5, ups=0.86, wpb=252.8, bsz=100, num_updates=25040, lr=1.08797e-05, gnorm=0.911, clip=30, loss_scale=1024, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=46196
2023-03-15 08:22:21 - progress_bar.py[line:274] - INFO: epoch 004:   6152 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.7, ups=0.87, wpb=252, bsz=100, num_updates=25050, lr=1.0863e-05, gnorm=0.965, clip=30, loss_scale=1024, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=46208
2023-03-15 08:22:32 - progress_bar.py[line:274] - INFO: epoch 004:   6162 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.8, ups=0.88, wpb=253.2, bsz=100, num_updates=25060, lr=1.08464e-05, gnorm=0.917, clip=30, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=46220
2023-03-15 08:22:36 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 08:22:45 - progress_bar.py[line:274] - INFO: epoch 004:   6173 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=203.7, ups=0.81, wpb=251.4, bsz=100, num_updates=25070, lr=1.08297e-05, gnorm=0.933, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=46232
2023-03-15 08:22:56 - progress_bar.py[line:274] - INFO: epoch 004:   6183 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.1, ups=0.87, wpb=256, bsz=100, num_updates=25080, lr=1.0813e-05, gnorm=0.928, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=46244
2023-03-15 08:23:08 - progress_bar.py[line:274] - INFO: epoch 004:   6193 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.299, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.6, ups=0.87, wpb=253.8, bsz=100, num_updates=25090, lr=1.07963e-05, gnorm=0.89, clip=10, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=46256
2023-03-15 08:23:20 - progress_bar.py[line:274] - INFO: epoch 004:   6203 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219, ups=0.87, wpb=253.1, bsz=100, num_updates=25100, lr=1.07797e-05, gnorm=0.973, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=46268
2023-03-15 08:23:32 - progress_bar.py[line:274] - INFO: epoch 004:   6213 / 6313 loss=0.54, loss_v1=0, loss_v2=0, nll_loss=0.322, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=221.6, ups=0.87, wpb=253.8, bsz=100, num_updates=25110, lr=1.0763e-05, gnorm=1.013, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=46279
2023-03-15 08:23:44 - progress_bar.py[line:274] - INFO: epoch 004:   6223 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.6, ups=0.88, wpb=253.1, bsz=100, num_updates=25120, lr=1.07463e-05, gnorm=0.941, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=46291
2023-03-15 08:23:56 - progress_bar.py[line:274] - INFO: epoch 004:   6233 / 6313 loss=0.543, loss_v1=0, loss_v2=0, nll_loss=0.326, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.3, ups=0.86, wpb=253.4, bsz=100, num_updates=25130, lr=1.07296e-05, gnorm=0.928, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=46303
2023-03-15 08:24:08 - progress_bar.py[line:274] - INFO: epoch 004:   6243 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.7, ups=0.86, wpb=252.9, bsz=100, num_updates=25140, lr=1.0713e-05, gnorm=0.905, clip=20, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=46315
2023-03-15 08:24:20 - progress_bar.py[line:274] - INFO: epoch 004:   6253 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.1, ups=0.86, wpb=253.7, bsz=100, num_updates=25150, lr=1.06963e-05, gnorm=0.947, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=46327
2023-03-15 08:24:31 - progress_bar.py[line:274] - INFO: epoch 004:   6263 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.5, ups=0.88, wpb=253.5, bsz=100, num_updates=25160, lr=1.06796e-05, gnorm=0.911, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=46339
2023-03-15 08:24:43 - progress_bar.py[line:274] - INFO: epoch 004:   6273 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.6, ups=0.87, wpb=255.8, bsz=100, num_updates=25170, lr=1.0663e-05, gnorm=0.887, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=46351
2023-03-15 08:24:54 - progress_bar.py[line:274] - INFO: epoch 004:   6283 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225, ups=0.89, wpb=254.2, bsz=100, num_updates=25180, lr=1.06463e-05, gnorm=0.881, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=46362
2023-03-15 08:25:06 - progress_bar.py[line:274] - INFO: epoch 004:   6293 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=222.4, ups=0.87, wpb=255.9, bsz=100, num_updates=25190, lr=1.06296e-05, gnorm=0.809, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=46374
2023-03-15 08:25:18 - progress_bar.py[line:274] - INFO: epoch 004:   6303 / 6313 loss=0.537, loss_v1=0, loss_v2=0, nll_loss=0.317, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=215.5, ups=0.85, wpb=252.3, bsz=100, num_updates=25200, lr=1.06129e-05, gnorm=0.98, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=46386
2023-03-15 08:25:29 - progress_bar.py[line:274] - INFO: epoch 004:   6313 / 6313 loss=0.546, loss_v1=0, loss_v2=0, nll_loss=0.319, ntokens=245.2, nsentences=98.4, sample_size=245.2, sample_size_v1=0, sample_size_v2=0, ppl=1.25, wps=218.1, ups=0.89, wpb=245.2, bsz=98.4, num_updates=25210, lr=1.05963e-05, gnorm=0.98, clip=30, loss_scale=512, train_wall=11, gb_free=14.2, ema_decay=0.9999, wall=46397
2023-03-15 08:25:29 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-03-15 08:25:31 - train.py[line:549] - INFO: 0 / 7052
2023-03-15 08:25:31 - train.py[line:551] - INFO: load:1.73 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-03-15 08:25:32 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 8.04 GiB (GPU 4; 39.59 GiB total capacity; 10.10 GiB already allocated; 1.42 GiB free; 35.40 GiB reserved in total by PyTorch)
2023-03-15 08:25:32 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 08:25:32 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 08:25:32 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 08:25:32 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 08:25:32 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 27        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   10342 MB |   11935 MB |    6343 TB |    6343 TB |
|       from large pool |   10197 MB |   11789 MB |    6338 TB |    6338 TB |
|       from small pool |     145 MB |     145 MB |       5 TB |       5 TB |
|---------------------------------------------------------------------------|
| Active memory         |   10342 MB |   11935 MB |    6343 TB |    6343 TB |
|       from large pool |   10197 MB |   11789 MB |    6338 TB |    6338 TB |
|       from small pool |     145 MB |     145 MB |       5 TB |       5 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   36252 MB |   36360 MB |  323200 MB |  286948 MB |
|       from large pool |   36106 MB |   36198 MB |  322676 MB |  286570 MB |
|       from small pool |     146 MB |     162 MB |     524 MB |     378 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   25909 MB |   25909 MB |    6525 TB |    6525 TB |
|       from large pool |   25908 MB |   25908 MB |    6519 TB |    6519 TB |
|       from small pool |       0 MB |       2 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3658    |    3672    |  273912 K  |  273908 K  |
|       from large pool |     563    |     575    |   99179 K  |   99178 K  |
|       from small pool |    3095    |    3114    |  174733 K  |  174730 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3658    |    3672    |  273912 K  |  273908 K  |
|       from large pool |     563    |     575    |   99179 K  |   99178 K  |
|       from small pool |    3095    |    3114    |  174733 K  |  174730 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     175    |     185    |     753    |     578    |
|       from large pool |     102    |     104    |     491    |     389    |
|       from small pool |      73    |      81    |     262    |     189    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     145    |     148    |  180685 K  |  180685 K  |
|       from large pool |     105    |     106    |   47600 K  |   47600 K  |
|       from small pool |      40    |      49    |  133084 K  |  133084 K  |
|===========================================================================|

2023-03-15 08:25:32 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-03-15 08:28:08 - train.py[line:549] - INFO: 200 / 7052
2023-03-15 08:28:08 - train.py[line:551] - INFO: load:1.75 valid_run:156.31 task_valid:146.54 collect_output:8.70
2023-03-15 08:30:42 - train.py[line:549] - INFO: 400 / 7052
2023-03-15 08:30:42 - train.py[line:551] - INFO: load:1.77 valid_run:310.29 task_valid:293.76 collect_output:14.44
2023-03-15 08:33:17 - train.py[line:549] - INFO: 600 / 7052
2023-03-15 08:33:17 - train.py[line:551] - INFO: load:1.80 valid_run:465.41 task_valid:439.16 collect_output:23.11
2023-03-15 08:35:54 - train.py[line:549] - INFO: 800 / 7052
2023-03-15 08:35:54 - train.py[line:551] - INFO: load:1.82 valid_run:622.68 task_valid:584.19 collect_output:34.34
2023-03-15 08:38:32 - train.py[line:549] - INFO: 1000 / 7052
2023-03-15 08:38:32 - train.py[line:551] - INFO: load:1.84 valid_run:779.70 task_valid:731.47 collect_output:43.05
2023-03-15 08:41:07 - train.py[line:549] - INFO: 1200 / 7052
2023-03-15 08:41:07 - train.py[line:551] - INFO: load:1.86 valid_run:935.15 task_valid:879.11 collect_output:49.84
2023-03-15 08:43:43 - train.py[line:549] - INFO: 1400 / 7052
2023-03-15 08:43:43 - train.py[line:551] - INFO: load:1.89 valid_run:1090.77 task_valid:1028.16 collect_output:55.40
2023-03-15 08:46:18 - train.py[line:549] - INFO: 1600 / 7052
2023-03-15 08:46:18 - train.py[line:551] - INFO: load:1.91 valid_run:1245.83 task_valid:1176.83 collect_output:60.75
2023-03-15 08:48:56 - train.py[line:549] - INFO: 1800 / 7052
2023-03-15 08:48:56 - train.py[line:551] - INFO: load:1.94 valid_run:1403.40 task_valid:1327.03 collect_output:67.10
2023-03-15 08:51:31 - train.py[line:549] - INFO: 2000 / 7052
2023-03-15 08:51:31 - train.py[line:551] - INFO: load:1.96 valid_run:1558.91 task_valid:1475.96 collect_output:72.67
2023-03-15 08:54:06 - train.py[line:549] - INFO: 2200 / 7052
2023-03-15 08:54:06 - train.py[line:551] - INFO: load:1.98 valid_run:1713.67 task_valid:1625.08 collect_output:77.28
2023-03-15 08:56:41 - train.py[line:549] - INFO: 2400 / 7052
2023-03-15 08:56:41 - train.py[line:551] - INFO: load:2.00 valid_run:1868.22 task_valid:1770.64 collect_output:85.26
2023-03-15 08:59:16 - train.py[line:549] - INFO: 2600 / 7052
2023-03-15 08:59:16 - train.py[line:551] - INFO: load:2.03 valid_run:2023.52 task_valid:1917.53 collect_output:92.65
2023-03-15 09:01:53 - train.py[line:549] - INFO: 2800 / 7052
2023-03-15 09:01:54 - train.py[line:551] - INFO: load:2.05 valid_run:2180.58 task_valid:2064.40 collect_output:101.78
2023-03-15 09:04:28 - train.py[line:549] - INFO: 3000 / 7052
2023-03-15 09:04:28 - train.py[line:551] - INFO: load:2.07 valid_run:2335.04 task_valid:2210.74 collect_output:108.87
2023-03-15 09:07:04 - train.py[line:549] - INFO: 3200 / 7052
2023-03-15 09:07:04 - train.py[line:551] - INFO: load:2.09 valid_run:2490.75 task_valid:2359.49 collect_output:114.81
2023-03-15 09:09:38 - train.py[line:549] - INFO: 3400 / 7052
2023-03-15 09:09:38 - train.py[line:551] - INFO: load:2.12 valid_run:2645.16 task_valid:2505.31 collect_output:122.37
2023-03-15 09:12:13 - train.py[line:549] - INFO: 3600 / 7052
2023-03-15 09:12:13 - train.py[line:551] - INFO: load:2.14 valid_run:2799.82 task_valid:2652.09 collect_output:129.25
2023-03-15 09:14:50 - train.py[line:549] - INFO: 3800 / 7052
2023-03-15 09:14:50 - train.py[line:551] - INFO: load:2.16 valid_run:2956.35 task_valid:2801.83 collect_output:135.03
2023-03-15 09:17:23 - train.py[line:549] - INFO: 4000 / 7052
2023-03-15 09:17:23 - train.py[line:551] - INFO: load:2.18 valid_run:3109.21 task_valid:2946.49 collect_output:142.20
2023-03-15 09:19:58 - train.py[line:549] - INFO: 4200 / 7052
2023-03-15 09:19:58 - train.py[line:551] - INFO: load:2.21 valid_run:3263.93 task_valid:3091.13 collect_output:151.26
2023-03-15 09:22:32 - train.py[line:549] - INFO: 4400 / 7052
2023-03-15 09:22:32 - train.py[line:551] - INFO: load:2.23 valid_run:3417.98 task_valid:3240.64 collect_output:154.77
2023-03-15 09:25:07 - train.py[line:549] - INFO: 4600 / 7052
2023-03-15 09:25:07 - train.py[line:551] - INFO: load:2.25 valid_run:3573.21 task_valid:3386.09 collect_output:163.51
2023-03-15 09:27:42 - train.py[line:549] - INFO: 4800 / 7052
2023-03-15 09:27:42 - train.py[line:551] - INFO: load:2.28 valid_run:3728.40 task_valid:3530.25 collect_output:173.53
2023-03-15 09:30:18 - train.py[line:549] - INFO: 5000 / 7052
2023-03-15 09:30:18 - train.py[line:551] - INFO: load:2.30 valid_run:3883.76 task_valid:3676.80 collect_output:181.33
2023-03-15 09:32:54 - train.py[line:549] - INFO: 5200 / 7052
2023-03-15 09:32:54 - train.py[line:551] - INFO: load:2.32 valid_run:4040.07 task_valid:3827.65 collect_output:185.77
2023-03-15 09:35:29 - train.py[line:549] - INFO: 5400 / 7052
2023-03-15 09:35:29 - train.py[line:551] - INFO: load:2.35 valid_run:4194.90 task_valid:3971.57 collect_output:195.67
2023-03-15 09:38:04 - train.py[line:549] - INFO: 5600 / 7052
2023-03-15 09:38:04 - train.py[line:551] - INFO: load:2.37 valid_run:4349.45 task_valid:4115.99 collect_output:204.77
2023-03-15 09:40:39 - train.py[line:549] - INFO: 5800 / 7052
2023-03-15 09:40:39 - train.py[line:551] - INFO: load:2.39 valid_run:4504.77 task_valid:4261.75 collect_output:213.29
2023-03-15 09:43:14 - train.py[line:549] - INFO: 6000 / 7052
2023-03-15 09:43:14 - train.py[line:551] - INFO: load:2.42 valid_run:4659.65 task_valid:4408.27 collect_output:220.64
2023-03-15 09:45:50 - train.py[line:549] - INFO: 6200 / 7052
2023-03-15 09:45:50 - train.py[line:551] - INFO: load:2.44 valid_run:4815.62 task_valid:4554.79 collect_output:229.07
2023-03-15 09:48:27 - train.py[line:549] - INFO: 6400 / 7052
2023-03-15 09:48:27 - train.py[line:551] - INFO: load:2.46 valid_run:4971.90 task_valid:4699.06 collect_output:240.03
2023-03-15 09:51:03 - train.py[line:549] - INFO: 6600 / 7052
2023-03-15 09:51:03 - train.py[line:551] - INFO: load:2.48 valid_run:5128.00 task_valid:4849.28 collect_output:244.89
2023-03-15 09:53:38 - train.py[line:549] - INFO: 6800 / 7052
2023-03-15 09:53:38 - train.py[line:551] - INFO: load:2.51 valid_run:5283.65 task_valid:4997.57 collect_output:251.23
2023-03-15 09:56:14 - train.py[line:549] - INFO: 7000 / 7052
2023-03-15 09:56:14 - train.py[line:551] - INFO: load:2.53 valid_run:5438.98 task_valid:5147.12 collect_output:255.99

====================================================================================================
SGG eval:     R @ 50: 0.6580;     R @ 100: 0.6720;     R @ 500: 0.6776;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2377;    mR @ 100: 0.2497;    mR @ 500: 0.2548;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.1655) (across:0.0000) (against:0.1429) (along:0.0000) (and:0.0909) (at:0.3654) (attached to:0.0357) (behind:0.4025) (belonging to:0.0000) (between:0.0000) (carrying:0.6667) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.1538) (has:0.7531) (holding:0.3605) (in:0.3596) (in front of:0.3473) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5715) (of:0.3806) (on:0.8879) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.1976) (part of:0.0000) (playing:0.0000) (riding:0.6667) (says:0.0000) (sitting on:0.3265) (standing on:0.0769) (to:0.0000) (under:0.4167) (using:1.0000) (walking in:0.0000) (walking on:0.4648) (watching:0.3333) (wearing:0.9859) (wears:0.0000) (with:0.2480) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6580;     R @ 100: 0.6720;     R @ 500: 0.6776;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2377;    mR @ 100: 0.2497;    mR @ 500: 0.2548;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.1655) (across:0.0000) (against:0.1429) (along:0.0000) (and:0.0909) (at:0.3654) (attached to:0.0357) (behind:0.4025) (belonging to:0.0000) (between:0.0000) (carrying:0.6667) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.1538) (has:0.7531) (holding:0.3605) (in:0.3596) (in front of:0.3473) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5715) (of:0.3806) (on:0.8879) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.1976) (part of:0.0000) (playing:0.0000) (riding:0.6667) (says:0.0000) (sitting on:0.3265) (standing on:0.0769) (to:0.0000) (under:0.4167) (using:1.0000) (walking in:0.0000) (walking on:0.4648) (watching:0.3333) (wearing:0.9859) (wears:0.0000) (with:0.2480) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6580;     R @ 100: 0.6720;     R @ 500: 0.6776;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2377;    mR @ 100: 0.2497;    mR @ 500: 0.2548;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.1655) (across:0.0000) (against:0.1429) (along:0.0000) (and:0.0909) (at:0.3654) (attached to:0.0357) (behind:0.4025) (belonging to:0.0000) (between:0.0000) (carrying:0.6667) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.1538) (has:0.7531) (holding:0.3605) (in:0.3596) (in front of:0.3473) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5715) (of:0.3806) (on:0.8879) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.1976) (part of:0.0000) (playing:0.0000) (riding:0.6667) (says:0.0000) (sitting on:0.3265) (standing on:0.0769) (to:0.0000) (under:0.4167) (using:1.0000) (walking in:0.0000) (walking on:0.4648) (watching:0.3333) (wearing:0.9859) (wears:0.0000) (with:0.2480) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6580;     R @ 100: 0.6720;     R @ 500: 0.6776;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2377;    mR @ 100: 0.2497;    mR @ 500: 0.2548;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.1655) (across:0.0000) (against:0.1429) (along:0.0000) (and:0.0909) (at:0.3654) (attached to:0.0357) (behind:0.4025) (belonging to:0.0000) (between:0.0000) (carrying:0.6667) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.1538) (has:0.7531) (holding:0.3605) (in:0.3596) (in front of:0.3473) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5715) (of:0.3806) (on:0.8879) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.1976) (part of:0.0000) (playing:0.0000) (riding:0.6667) (says:0.0000) (sitting on:0.3265) (standing on:0.0769) (to:0.0000) (under:0.4167) (using:1.0000) (walking in:0.0000) (walking on:0.4648) (watching:0.3333) (wearing:0.9859) (wears:0.0000) (with:0.2480) 
--------------------------------------------------------
====================================================================================================


====================================================================================================
SGG eval:     R @ 50: 0.6580;     R @ 100: 0.6720;     R @ 500: 0.6776;  for mode=predcls, type=Recall(Main).
SGG eval:    mR @ 50: 0.2377;    mR @ 100: 0.2497;    mR @ 500: 0.2548;  for mode=predcls, type=Mean Recall.
----------------------- Details ------------------------
(above:0.1655) (across:0.0000) (against:0.1429) (along:0.0000) (and:0.0909) (at:0.3654) (attached to:0.0357) (behind:0.4025) (belonging to:0.0000) (between:0.0000) (carrying:0.6667) (covered in:0.6667) (covering:0.0000) (eating:1.0000) (flying in:0.0000) (for:0.0000) (from:0.0000) (growing on:0.0000) (hanging from:0.1538) (has:0.7531) (holding:0.3605) (in:0.3596) (in front of:0.3473) (laying on:0.0000) (looking at:0.2500) (lying on:0.0000) (made of:0.0000) (mounted on:0.0000) (near:0.5715) (of:0.3806) (on:0.8879) (on back of:0.0000) (over:0.1667) (painted on:0.0000) (parked on:0.1976) (part of:0.0000) (playing:0.0000) (riding:0.6667) (says:0.0000) (sitting on:0.3265) (standing on:0.0769) (to:0.0000) (under:0.4167) (using:1.0000) (walking in:0.0000) (walking on:0.4648) (watching:0.3333) (wearing:0.9859) (wears:0.0000) (with:0.2480) 
--------------------------------------------------------
====================================================================================================

2023-03-15 09:57:17 - train.py[line:487] - INFO: 0.6719616501639893
2023-03-15 09:57:17 - train.py[line:578] - INFO: logits:torch.Size([282060, 51]) sample_ids:torch.Size([282060])
2023-03-15 09:57:18 - progress_bar.py[line:282] - INFO: epoch 004 | valid on 'valid' subset | loss 0.307 | loss_v1 0 | loss_v2 0 | nll_loss 0.114 | ntokens 119.044 | nsentences 39.997 | sample_size 119.044 | sample_size_v1 0 | sample_size_v2 0 | R@100 0.671962 | ppl 1.08 | vqa_score 0.6145 | wps 152.5 | wpb 119 | bsz 40 | num_updates 25210 | best_R@100 0.671962
2023-03-15 09:57:18 - checkpoint_utils.py[line:64] - INFO: Preparing to save checkpoint for epoch 4 @ 25210 updates
2023-03-15 09:57:18 - trainer.py[line:472] - INFO: Saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint4.pt
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv slice_id 3 row count 126257 total row count 631284
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv slice_id 2 row count 126257 total row count 631284file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv slice_id 4 row count 126256 total row count 631284file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv slice_id 1 row count 126257 total row count 631284


2023-03-15 09:58:13 - trainer.py[line:482] - INFO: Finished saving checkpoint to ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint4.pt
2023-03-15 10:01:36 - checkpoint_utils.py[line:135] - INFO: Saved checkpoint ./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480/checkpoint4.pt (epoch 4 @ 25210 updates, score 0.6719616501639893) (writing took 258.3463058061898 seconds)
2023-03-15 10:01:36 - train.py[line:339] - INFO: end of epoch 4 (average epoch stats below)
2023-03-15 10:01:37 - progress_bar.py[line:282] - INFO: epoch 004 | loss 0.522 | loss_v1 0 | loss_v2 0 | nll_loss 0.3 | ntokens 253.348 | nsentences 99.997 | sample_size 253.348 | sample_size_v1 0 | sample_size_v2 0 | ppl 1.23 | wps 117.8 | ups 0.46 | wpb 253.3 | bsz 100 | num_updates 25210 | lr 1.05963e-05 | gnorm 0.886 | clip 20.7 | loss_scale 512 | train_wall 7226 | gb_free 14.2 | ema_decay 0.9999 | wall 52164
2023-03-15 10:01:37 - trainer.py[line:694] - INFO: loading train data for epoch 5
file /data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv slice_id 0 row count 126257 total row count 631284
2023-03-15 10:01:37 - tsv_file.py[line:93] - INFO: loading lineidx: /data/private/yutianyu/OFA/data/mm_data/../../../datasets/VisualGenome/b64_feat.lineidx
2023-03-15 10:01:38 - trainer.py[line:758] - INFO: begin training epoch 5
2023-03-15 10:01:38 - train.py[line:312] - INFO: Start iterating over samples
2023-03-15 10:01:52 - progress_bar.py[line:274] - INFO: epoch 005:     10 / 6313 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=256.9, nsentences=100, sample_size=256.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=0.4, ups=0, wpb=256.9, bsz=100, num_updates=25220, lr=1.05796e-05, gnorm=0.816, clip=0, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=52179
2023-03-15 10:02:05 - progress_bar.py[line:274] - INFO: epoch 005:     20 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.2, ups=0.88, wpb=252.7, bsz=100, num_updates=25230, lr=1.05629e-05, gnorm=0.819, clip=0, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52192
2023-03-15 10:02:17 - progress_bar.py[line:274] - INFO: epoch 005:     30 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.9, ups=0.87, wpb=255.4, bsz=100, num_updates=25240, lr=1.05462e-05, gnorm=0.837, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52204
2023-03-15 10:02:29 - progress_bar.py[line:274] - INFO: epoch 005:     40 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.6, ups=0.88, wpb=251.9, bsz=100, num_updates=25250, lr=1.05296e-05, gnorm=0.883, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52216
2023-03-15 10:02:42 - progress_bar.py[line:274] - INFO: epoch 005:     50 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=224.6, ups=0.89, wpb=253.4, bsz=100, num_updates=25260, lr=1.05129e-05, gnorm=0.811, clip=10, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=52229
2023-03-15 10:02:54 - progress_bar.py[line:274] - INFO: epoch 005:     60 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.9, ups=0.87, wpb=253.2, bsz=100, num_updates=25270, lr=1.04962e-05, gnorm=0.855, clip=0, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52241
2023-03-15 10:03:06 - progress_bar.py[line:274] - INFO: epoch 005:     70 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.6, ups=0.88, wpb=254.2, bsz=100, num_updates=25280, lr=1.04795e-05, gnorm=0.849, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52253
2023-03-15 10:03:18 - progress_bar.py[line:274] - INFO: epoch 005:     80 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.4, ups=0.86, wpb=251, bsz=100, num_updates=25290, lr=1.04629e-05, gnorm=0.868, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=52265
2023-03-15 10:03:30 - progress_bar.py[line:274] - INFO: epoch 005:     90 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.1, ups=0.88, wpb=254.4, bsz=100, num_updates=25300, lr=1.04462e-05, gnorm=0.894, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52277
2023-03-15 10:03:42 - progress_bar.py[line:274] - INFO: epoch 005:    100 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.3, ups=0.86, wpb=252.7, bsz=100, num_updates=25310, lr=1.04295e-05, gnorm=0.956, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=52289
2023-03-15 10:03:54 - progress_bar.py[line:274] - INFO: epoch 005:    110 / 6313 loss=0.53, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=221.6, ups=0.88, wpb=253.2, bsz=100, num_updates=25320, lr=1.04128e-05, gnorm=1.067, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52301
2023-03-15 10:04:06 - progress_bar.py[line:274] - INFO: epoch 005:    120 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.8, ups=0.86, wpb=254.2, bsz=100, num_updates=25330, lr=1.03962e-05, gnorm=0.924, clip=10, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=52313
2023-03-15 10:04:18 - progress_bar.py[line:274] - INFO: epoch 005:    130 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.6, ups=0.87, wpb=252.2, bsz=100, num_updates=25340, lr=1.03795e-05, gnorm=0.973, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52325
2023-03-15 10:04:31 - progress_bar.py[line:274] - INFO: epoch 005:    140 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.3, ups=0.86, wpb=255.8, bsz=100, num_updates=25350, lr=1.03628e-05, gnorm=0.962, clip=20, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=52338
2023-03-15 10:04:43 - progress_bar.py[line:274] - INFO: epoch 005:    150 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220, ups=0.86, wpb=254.6, bsz=100, num_updates=25360, lr=1.03461e-05, gnorm=0.984, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=52350
2023-03-15 10:04:56 - progress_bar.py[line:274] - INFO: epoch 005:    160 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.4, ups=0.89, wpb=252.1, bsz=100, num_updates=25370, lr=1.03295e-05, gnorm=0.922, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52363
2023-03-15 10:05:08 - progress_bar.py[line:274] - INFO: epoch 005:    170 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.9, ups=0.87, wpb=254.1, bsz=100, num_updates=25380, lr=1.03128e-05, gnorm=1.011, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=52375
2023-03-15 10:05:20 - progress_bar.py[line:274] - INFO: epoch 005:    180 / 6313 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.6, ups=0.87, wpb=255.6, bsz=100, num_updates=25390, lr=1.02961e-05, gnorm=0.913, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52387
2023-03-15 10:05:32 - progress_bar.py[line:274] - INFO: epoch 005:    190 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.8, ups=0.88, wpb=252.4, bsz=100, num_updates=25400, lr=1.02795e-05, gnorm=1.063, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=52399
2023-03-15 10:05:44 - progress_bar.py[line:274] - INFO: epoch 005:    200 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.4, ups=0.85, wpb=254.5, bsz=100, num_updates=25410, lr=1.02628e-05, gnorm=0.976, clip=40, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=52412
2023-03-15 10:05:56 - progress_bar.py[line:274] - INFO: epoch 005:    210 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.3, ups=0.86, wpb=252.3, bsz=100, num_updates=25420, lr=1.02461e-05, gnorm=0.972, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=52424
2023-03-15 10:06:08 - progress_bar.py[line:274] - INFO: epoch 005:    220 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.8, ups=0.89, wpb=252.8, bsz=100, num_updates=25430, lr=1.02294e-05, gnorm=0.971, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52436
2023-03-15 10:06:20 - progress_bar.py[line:274] - INFO: epoch 005:    230 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.7, ups=0.87, wpb=254.8, bsz=100, num_updates=25440, lr=1.02128e-05, gnorm=0.931, clip=10, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52448
2023-03-15 10:06:32 - progress_bar.py[line:274] - INFO: epoch 005:    240 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.7, ups=0.87, wpb=252.4, bsz=100, num_updates=25450, lr=1.01961e-05, gnorm=0.946, clip=20, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52460
2023-03-15 10:06:44 - progress_bar.py[line:274] - INFO: epoch 005:    250 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=226.3, ups=0.89, wpb=253.2, bsz=100, num_updates=25460, lr=1.01794e-05, gnorm=0.996, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52471
2023-03-15 10:06:56 - progress_bar.py[line:274] - INFO: epoch 005:    260 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=224, ups=0.88, wpb=255.6, bsz=100, num_updates=25470, lr=1.01627e-05, gnorm=0.903, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52483
2023-03-15 10:07:08 - progress_bar.py[line:274] - INFO: epoch 005:    270 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.6, ups=0.88, wpb=253.3, bsz=100, num_updates=25480, lr=1.01461e-05, gnorm=0.995, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52495
2023-03-15 10:07:20 - progress_bar.py[line:274] - INFO: epoch 005:    280 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.9, ups=0.87, wpb=256.5, bsz=100, num_updates=25490, lr=1.01294e-05, gnorm=0.946, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=52507
2023-03-15 10:07:32 - progress_bar.py[line:274] - INFO: epoch 005:    290 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=222.1, ups=0.88, wpb=253.6, bsz=100, num_updates=25500, lr=1.01127e-05, gnorm=0.89, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=52519
2023-03-15 10:07:44 - progress_bar.py[line:274] - INFO: epoch 005:    300 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.7, ups=0.87, wpb=252.5, bsz=100, num_updates=25510, lr=1.0096e-05, gnorm=0.968, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52531
2023-03-15 10:07:56 - progress_bar.py[line:274] - INFO: epoch 005:    310 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.9, ups=0.88, wpb=254, bsz=100, num_updates=25520, lr=1.00794e-05, gnorm=0.913, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52543
2023-03-15 10:08:08 - progress_bar.py[line:274] - INFO: epoch 005:    320 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.8, ups=0.87, wpb=252, bsz=100, num_updates=25530, lr=1.00627e-05, gnorm=0.954, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52555
2023-03-15 10:08:20 - progress_bar.py[line:274] - INFO: epoch 005:    330 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.3, ups=0.88, wpb=254.7, bsz=100, num_updates=25540, lr=1.0046e-05, gnorm=0.978, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52567
2023-03-15 10:08:33 - progress_bar.py[line:274] - INFO: epoch 005:    340 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=226.5, ups=0.89, wpb=255.6, bsz=100, num_updates=25550, lr=1.00293e-05, gnorm=1.012, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52579
2023-03-15 10:08:45 - progress_bar.py[line:274] - INFO: epoch 005:    350 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=226.5, ups=0.89, wpb=255.8, bsz=100, num_updates=25560, lr=1.00127e-05, gnorm=1.031, clip=50, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=52592
2023-03-15 10:08:57 - progress_bar.py[line:274] - INFO: epoch 005:    360 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=218.6, ups=0.86, wpb=253.1, bsz=100, num_updates=25570, lr=9.996e-06, gnorm=0.974, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=52604
2023-03-15 10:09:10 - progress_bar.py[line:274] - INFO: epoch 005:    370 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.9, ups=0.88, wpb=254.5, bsz=100, num_updates=25580, lr=9.97932e-06, gnorm=0.988, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=52617
2023-03-15 10:09:22 - progress_bar.py[line:274] - INFO: epoch 005:    380 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.6, ups=0.86, wpb=252.4, bsz=100, num_updates=25590, lr=9.96265e-06, gnorm=0.926, clip=30, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=52629
2023-03-15 10:09:34 - progress_bar.py[line:274] - INFO: epoch 005:    390 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=215.5, ups=0.86, wpb=251.2, bsz=100, num_updates=25600, lr=9.94598e-06, gnorm=0.928, clip=30, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=52642
2023-03-15 10:09:46 - progress_bar.py[line:274] - INFO: epoch 005:    400 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221, ups=0.88, wpb=252.2, bsz=100, num_updates=25610, lr=9.9293e-06, gnorm=0.994, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52654
2023-03-15 10:09:59 - progress_bar.py[line:274] - INFO: epoch 005:    410 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.3, ups=0.86, wpb=253.8, bsz=100, num_updates=25620, lr=9.91263e-06, gnorm=1.081, clip=60, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=52666
2023-03-15 10:10:11 - progress_bar.py[line:274] - INFO: epoch 005:    420 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.7, ups=0.87, wpb=252.5, bsz=100, num_updates=25630, lr=9.89595e-06, gnorm=0.875, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52678
2023-03-15 10:10:23 - progress_bar.py[line:274] - INFO: epoch 005:    430 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.3, ups=0.87, wpb=255, bsz=100, num_updates=25640, lr=9.87928e-06, gnorm=0.903, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52690
2023-03-15 10:10:35 - progress_bar.py[line:274] - INFO: epoch 005:    440 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.7, ups=0.87, wpb=252.6, bsz=100, num_updates=25650, lr=9.86261e-06, gnorm=0.921, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52702
2023-03-15 10:10:47 - progress_bar.py[line:274] - INFO: epoch 005:    450 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.4, ups=0.88, wpb=254.1, bsz=100, num_updates=25660, lr=9.84593e-06, gnorm=1.055, clip=60, loss_scale=1024, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=52714
2023-03-15 10:10:59 - progress_bar.py[line:274] - INFO: epoch 005:    460 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.9, ups=0.89, wpb=253.8, bsz=100, num_updates=25670, lr=9.82926e-06, gnorm=1.02, clip=60, loss_scale=1024, train_wall=11, gb_free=9.5, ema_decay=0.9999, wall=52726
2023-03-15 10:11:11 - progress_bar.py[line:274] - INFO: epoch 005:    470 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.8, ups=0.87, wpb=252.8, bsz=100, num_updates=25680, lr=9.81259e-06, gnorm=1.06, clip=60, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52738
2023-03-15 10:11:23 - progress_bar.py[line:274] - INFO: epoch 005:    480 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.8, ups=0.88, wpb=252.7, bsz=100, num_updates=25690, lr=9.79591e-06, gnorm=0.941, clip=30, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52750
2023-03-15 10:11:35 - progress_bar.py[line:274] - INFO: epoch 005:    490 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.9, ups=0.88, wpb=251.6, bsz=100, num_updates=25700, lr=9.77924e-06, gnorm=0.937, clip=30, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52762
2023-03-15 10:11:47 - progress_bar.py[line:274] - INFO: epoch 005:    500 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.8, ups=0.87, wpb=252.7, bsz=100, num_updates=25710, lr=9.76256e-06, gnorm=1.015, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52774
2023-03-15 10:11:59 - progress_bar.py[line:274] - INFO: epoch 005:    510 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.2, ups=0.87, wpb=254.6, bsz=100, num_updates=25720, lr=9.74589e-06, gnorm=0.97, clip=50, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=52787
2023-03-15 10:12:11 - progress_bar.py[line:274] - INFO: epoch 005:    520 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.5, ups=0.86, wpb=255.1, bsz=100, num_updates=25730, lr=9.72922e-06, gnorm=1.025, clip=40, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=52799
2023-03-15 10:12:24 - progress_bar.py[line:274] - INFO: epoch 005:    530 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=222.7, ups=0.88, wpb=254.5, bsz=100, num_updates=25740, lr=9.71254e-06, gnorm=1.004, clip=30, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52811
2023-03-15 10:12:29 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 10:12:37 - progress_bar.py[line:274] - INFO: epoch 005:    541 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=201.4, ups=0.8, wpb=251.4, bsz=100, num_updates=25750, lr=9.69587e-06, gnorm=1.01, clip=50, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=52824
2023-03-15 10:12:49 - progress_bar.py[line:274] - INFO: epoch 005:    551 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=256.9, nsentences=100, sample_size=256.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=224.8, ups=0.88, wpb=256.9, bsz=100, num_updates=25760, lr=9.67919e-06, gnorm=1.017, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52836
2023-03-15 10:13:01 - progress_bar.py[line:274] - INFO: epoch 005:    561 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.1, ups=0.87, wpb=255.2, bsz=100, num_updates=25770, lr=9.66252e-06, gnorm=1.011, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52848
2023-03-15 10:13:13 - progress_bar.py[line:274] - INFO: epoch 005:    571 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.4, ups=0.86, wpb=254.2, bsz=100, num_updates=25780, lr=9.64585e-06, gnorm=1.062, clip=70, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=52860
2023-03-15 10:13:25 - progress_bar.py[line:274] - INFO: epoch 005:    581 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222, ups=0.87, wpb=254.5, bsz=100, num_updates=25790, lr=9.62917e-06, gnorm=0.981, clip=20, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52872
2023-03-15 10:13:37 - progress_bar.py[line:274] - INFO: epoch 005:    591 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.6, ups=0.88, wpb=252.8, bsz=100, num_updates=25800, lr=9.6125e-06, gnorm=0.962, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=52884
2023-03-15 10:13:49 - progress_bar.py[line:274] - INFO: epoch 005:    601 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.8, ups=0.87, wpb=252.6, bsz=100, num_updates=25810, lr=9.59582e-06, gnorm=0.927, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52896
2023-03-15 10:14:01 - progress_bar.py[line:274] - INFO: epoch 005:    611 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.9, ups=0.87, wpb=253, bsz=100, num_updates=25820, lr=9.57915e-06, gnorm=0.967, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=52909
2023-03-15 10:14:13 - progress_bar.py[line:274] - INFO: epoch 005:    621 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=222.2, ups=0.88, wpb=253.5, bsz=100, num_updates=25830, lr=9.56248e-06, gnorm=1.028, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52920
2023-03-15 10:14:26 - progress_bar.py[line:274] - INFO: epoch 005:    631 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.6, ups=0.87, wpb=255.6, bsz=100, num_updates=25840, lr=9.5458e-06, gnorm=1.245, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=52933
2023-03-15 10:14:39 - progress_bar.py[line:274] - INFO: epoch 005:    641 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.1, ups=0.88, wpb=253.2, bsz=100, num_updates=25850, lr=9.52913e-06, gnorm=0.977, clip=30, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=52945
2023-03-15 10:14:51 - progress_bar.py[line:274] - INFO: epoch 005:    651 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=216.8, ups=0.86, wpb=251.2, bsz=100, num_updates=25860, lr=9.51246e-06, gnorm=1, clip=60, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=52958
2023-03-15 10:15:04 - progress_bar.py[line:274] - INFO: epoch 005:    661 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.4, ups=0.85, wpb=253.1, bsz=100, num_updates=25870, lr=9.49578e-06, gnorm=0.976, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=52971
2023-03-15 10:15:17 - progress_bar.py[line:274] - INFO: epoch 005:    671 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.9, ups=0.88, wpb=254.3, bsz=100, num_updates=25880, lr=9.47911e-06, gnorm=0.914, clip=10, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=52983
2023-03-15 10:15:29 - progress_bar.py[line:274] - INFO: epoch 005:    681 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.3, ups=0.86, wpb=252.9, bsz=100, num_updates=25890, lr=9.46243e-06, gnorm=1.016, clip=60, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=52996
2023-03-15 10:15:42 - progress_bar.py[line:274] - INFO: epoch 005:    691 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.8, ups=0.87, wpb=252.4, bsz=100, num_updates=25900, lr=9.44576e-06, gnorm=0.99, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53009
2023-03-15 10:15:54 - progress_bar.py[line:274] - INFO: epoch 005:    701 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.7, ups=0.87, wpb=253.6, bsz=100, num_updates=25910, lr=9.42909e-06, gnorm=0.923, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=53021
2023-03-15 10:16:07 - progress_bar.py[line:274] - INFO: epoch 005:    711 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225.7, ups=0.89, wpb=254.8, bsz=100, num_updates=25920, lr=9.41241e-06, gnorm=0.924, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=53033
2023-03-15 10:16:19 - progress_bar.py[line:274] - INFO: epoch 005:    721 / 6313 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.3, ups=0.86, wpb=253.4, bsz=100, num_updates=25930, lr=9.39574e-06, gnorm=0.908, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=53046
2023-03-15 10:16:32 - progress_bar.py[line:274] - INFO: epoch 005:    731 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.305, ntokens=249.6, nsentences=100, sample_size=249.6, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=214.9, ups=0.86, wpb=249.6, bsz=100, num_updates=25940, lr=9.37906e-06, gnorm=1.074, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=53059
2023-03-15 10:16:45 - progress_bar.py[line:274] - INFO: epoch 005:    741 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.9, ups=0.88, wpb=253, bsz=100, num_updates=25950, lr=9.36239e-06, gnorm=0.954, clip=50, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=53072
2023-03-15 10:16:57 - progress_bar.py[line:274] - INFO: epoch 005:    751 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.6, ups=0.87, wpb=252.1, bsz=100, num_updates=25960, lr=9.34572e-06, gnorm=0.965, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=53084
2023-03-15 10:17:09 - progress_bar.py[line:274] - INFO: epoch 005:    761 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=257.5, nsentences=100, sample_size=257.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225.5, ups=0.88, wpb=257.5, bsz=100, num_updates=25970, lr=9.32904e-06, gnorm=0.951, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53096
2023-03-15 10:17:21 - progress_bar.py[line:274] - INFO: epoch 005:    771 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221, ups=0.87, wpb=253.4, bsz=100, num_updates=25980, lr=9.31237e-06, gnorm=0.942, clip=30, loss_scale=512, train_wall=11, gb_free=9.9, ema_decay=0.9999, wall=53108
2023-03-15 10:17:33 - progress_bar.py[line:274] - INFO: epoch 005:    781 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.2, ups=0.87, wpb=250.8, bsz=100, num_updates=25990, lr=9.29569e-06, gnorm=1.084, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=53120
2023-03-15 10:17:45 - progress_bar.py[line:274] - INFO: epoch 005:    791 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.4, ups=0.88, wpb=253, bsz=100, num_updates=26000, lr=9.27902e-06, gnorm=1.016, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=53133
2023-03-15 10:17:58 - progress_bar.py[line:274] - INFO: epoch 005:    801 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.3, ups=0.86, wpb=253.6, bsz=100, num_updates=26010, lr=9.26235e-06, gnorm=0.999, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=53145
2023-03-15 10:18:09 - progress_bar.py[line:274] - INFO: epoch 005:    811 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224.9, ups=0.88, wpb=254.3, bsz=100, num_updates=26020, lr=9.24567e-06, gnorm=1.041, clip=60, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=53157
2023-03-15 10:18:22 - progress_bar.py[line:274] - INFO: epoch 005:    821 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.4, ups=0.87, wpb=253.4, bsz=100, num_updates=26030, lr=9.229e-06, gnorm=1.006, clip=50, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=53169
2023-03-15 10:18:34 - progress_bar.py[line:274] - INFO: epoch 005:    831 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219, ups=0.86, wpb=254, bsz=100, num_updates=26040, lr=9.21233e-06, gnorm=0.952, clip=20, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=53181
2023-03-15 10:18:46 - progress_bar.py[line:274] - INFO: epoch 005:    841 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221, ups=0.88, wpb=252.6, bsz=100, num_updates=26050, lr=9.19565e-06, gnorm=1.045, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=53194
2023-03-15 10:18:59 - progress_bar.py[line:274] - INFO: epoch 005:    851 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.3, ups=0.87, wpb=251.1, bsz=100, num_updates=26060, lr=9.17898e-06, gnorm=1.172, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53206
2023-03-15 10:19:11 - progress_bar.py[line:274] - INFO: epoch 005:    861 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.2, ups=0.86, wpb=254.8, bsz=100, num_updates=26070, lr=9.1623e-06, gnorm=0.997, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53219
2023-03-15 10:19:24 - progress_bar.py[line:274] - INFO: epoch 005:    871 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=215.9, ups=0.85, wpb=253.3, bsz=100, num_updates=26080, lr=9.14563e-06, gnorm=1.02, clip=60, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=53231
2023-03-15 10:19:36 - progress_bar.py[line:274] - INFO: epoch 005:    881 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=249, nsentences=100, sample_size=249, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=215.4, ups=0.87, wpb=249, bsz=100, num_updates=26090, lr=9.12896e-06, gnorm=0.962, clip=30, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=53243
2023-03-15 10:19:48 - progress_bar.py[line:274] - INFO: epoch 005:    891 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.6, ups=0.88, wpb=252.8, bsz=100, num_updates=26100, lr=9.11228e-06, gnorm=1.009, clip=60, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=53255
2023-03-15 10:20:00 - progress_bar.py[line:274] - INFO: epoch 005:    901 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.2, ups=0.86, wpb=252.4, bsz=100, num_updates=26110, lr=9.09561e-06, gnorm=0.909, clip=10, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53267
2023-03-15 10:20:12 - progress_bar.py[line:274] - INFO: epoch 005:    911 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.7, ups=0.86, wpb=253.1, bsz=100, num_updates=26120, lr=9.07893e-06, gnorm=1.133, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=53280
2023-03-15 10:20:25 - progress_bar.py[line:274] - INFO: epoch 005:    921 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.6, ups=0.86, wpb=252, bsz=100, num_updates=26130, lr=9.06226e-06, gnorm=1.013, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=53292
2023-03-15 10:20:37 - progress_bar.py[line:274] - INFO: epoch 005:    931 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=226.5, ups=0.89, wpb=254.5, bsz=100, num_updates=26140, lr=9.04559e-06, gnorm=1.035, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53304
2023-03-15 10:20:49 - progress_bar.py[line:274] - INFO: epoch 005:    941 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.3, ups=0.86, wpb=254.9, bsz=100, num_updates=26150, lr=9.02891e-06, gnorm=0.969, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=53316
2023-03-15 10:21:01 - progress_bar.py[line:274] - INFO: epoch 005:    951 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.4, ups=0.87, wpb=254.2, bsz=100, num_updates=26160, lr=9.01224e-06, gnorm=1.03, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=53328
2023-03-15 10:21:13 - progress_bar.py[line:274] - INFO: epoch 005:    961 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.7, ups=0.87, wpb=252.8, bsz=100, num_updates=26170, lr=8.99556e-06, gnorm=0.988, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53340
2023-03-15 10:21:25 - progress_bar.py[line:274] - INFO: epoch 005:    971 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.1, ups=0.87, wpb=251.8, bsz=100, num_updates=26180, lr=8.97889e-06, gnorm=1.036, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=53352
2023-03-15 10:21:37 - progress_bar.py[line:274] - INFO: epoch 005:    981 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=215.5, ups=0.85, wpb=252.6, bsz=100, num_updates=26190, lr=8.96222e-06, gnorm=1.057, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53365
2023-03-15 10:21:50 - progress_bar.py[line:274] - INFO: epoch 005:    991 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.8, ups=0.86, wpb=252.6, bsz=100, num_updates=26200, lr=8.94554e-06, gnorm=1.055, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53377
2023-03-15 10:22:02 - progress_bar.py[line:274] - INFO: epoch 005:   1001 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.6, ups=0.86, wpb=254.6, bsz=100, num_updates=26210, lr=8.92887e-06, gnorm=1.004, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=53389
2023-03-15 10:22:14 - progress_bar.py[line:274] - INFO: epoch 005:   1011 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.1, ups=0.87, wpb=253.2, bsz=100, num_updates=26220, lr=8.9122e-06, gnorm=1.04, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=53401
2023-03-15 10:22:26 - progress_bar.py[line:274] - INFO: epoch 005:   1021 / 6313 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.8, ups=0.86, wpb=255.5, bsz=100, num_updates=26230, lr=8.89552e-06, gnorm=0.916, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53414
2023-03-15 10:22:38 - progress_bar.py[line:274] - INFO: epoch 005:   1031 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.8, ups=0.87, wpb=253.8, bsz=100, num_updates=26240, lr=8.87885e-06, gnorm=1.021, clip=60, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=53426
2023-03-15 10:22:51 - progress_bar.py[line:274] - INFO: epoch 005:   1041 / 6313 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.8, ups=0.87, wpb=255.2, bsz=100, num_updates=26250, lr=8.86217e-06, gnorm=1.03, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=53438
2023-03-15 10:23:03 - progress_bar.py[line:274] - INFO: epoch 005:   1051 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=222, ups=0.88, wpb=253.5, bsz=100, num_updates=26260, lr=8.8455e-06, gnorm=1.028, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=53450
2023-03-15 10:23:16 - progress_bar.py[line:274] - INFO: epoch 005:   1061 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.7, ups=0.88, wpb=252.9, bsz=100, num_updates=26270, lr=8.82883e-06, gnorm=1.075, clip=80, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=53463
2023-03-15 10:23:29 - progress_bar.py[line:274] - INFO: epoch 005:   1071 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.7, ups=0.87, wpb=253.5, bsz=100, num_updates=26280, lr=8.81215e-06, gnorm=1.034, clip=50, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53475
2023-03-15 10:23:36 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 10:23:42 - progress_bar.py[line:274] - INFO: epoch 005:   1082 / 6313 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=204.7, ups=0.81, wpb=252.8, bsz=100, num_updates=26290, lr=8.79548e-06, gnorm=0.918, clip=20, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=53489
2023-03-15 10:23:54 - progress_bar.py[line:274] - INFO: epoch 005:   1092 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.8, ups=0.88, wpb=251.6, bsz=100, num_updates=26300, lr=8.7788e-06, gnorm=0.935, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=53501
2023-03-15 10:24:07 - progress_bar.py[line:274] - INFO: epoch 005:   1102 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.7, ups=0.86, wpb=252.3, bsz=100, num_updates=26310, lr=8.76213e-06, gnorm=0.919, clip=30, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=53514
2023-03-15 10:24:19 - progress_bar.py[line:274] - INFO: epoch 005:   1112 / 6313 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=225.4, ups=0.89, wpb=254.7, bsz=100, num_updates=26320, lr=8.74546e-06, gnorm=0.971, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53526
2023-03-15 10:24:32 - progress_bar.py[line:274] - INFO: epoch 005:   1122 / 6313 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=221.6, ups=0.86, wpb=256.2, bsz=100, num_updates=26330, lr=8.72878e-06, gnorm=0.935, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53539
2023-03-15 10:24:44 - progress_bar.py[line:274] - INFO: epoch 005:   1132 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=225.4, ups=0.88, wpb=256.1, bsz=100, num_updates=26340, lr=8.71211e-06, gnorm=0.956, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=53551
2023-03-15 10:24:57 - progress_bar.py[line:274] - INFO: epoch 005:   1142 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.5, ups=0.87, wpb=251.9, bsz=100, num_updates=26350, lr=8.69543e-06, gnorm=1.223, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53563
2023-03-15 10:25:09 - progress_bar.py[line:274] - INFO: epoch 005:   1152 / 6313 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.9, ups=0.88, wpb=254.8, bsz=100, num_updates=26360, lr=8.67876e-06, gnorm=0.99, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=53576
2023-03-15 10:25:22 - progress_bar.py[line:274] - INFO: epoch 005:   1162 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.7, ups=0.87, wpb=252.3, bsz=100, num_updates=26370, lr=8.66209e-06, gnorm=0.941, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=53589
2023-03-15 10:25:34 - progress_bar.py[line:274] - INFO: epoch 005:   1172 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=249.9, nsentences=100, sample_size=249.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=215.9, ups=0.86, wpb=249.9, bsz=100, num_updates=26380, lr=8.64541e-06, gnorm=1.053, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=53601
2023-03-15 10:25:47 - progress_bar.py[line:274] - INFO: epoch 005:   1182 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.9, ups=0.87, wpb=251.4, bsz=100, num_updates=26390, lr=8.62874e-06, gnorm=1.029, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53614
2023-03-15 10:25:58 - progress_bar.py[line:274] - INFO: epoch 005:   1192 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=224.1, ups=0.88, wpb=254.6, bsz=100, num_updates=26400, lr=8.61207e-06, gnorm=1.086, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53626
2023-03-15 10:26:11 - progress_bar.py[line:274] - INFO: epoch 005:   1202 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221, ups=0.87, wpb=253.1, bsz=100, num_updates=26410, lr=8.59539e-06, gnorm=0.906, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53638
2023-03-15 10:26:23 - progress_bar.py[line:274] - INFO: epoch 005:   1212 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.2, ups=0.88, wpb=253.9, bsz=100, num_updates=26420, lr=8.57872e-06, gnorm=1.018, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=53650
2023-03-15 10:26:35 - progress_bar.py[line:274] - INFO: epoch 005:   1222 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=212, ups=0.85, wpb=250.1, bsz=100, num_updates=26430, lr=8.56204e-06, gnorm=1.002, clip=40, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=53662
2023-03-15 10:26:47 - progress_bar.py[line:274] - INFO: epoch 005:   1232 / 6313 loss=0.472, loss_v1=0, loss_v2=0, nll_loss=0.243, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=221.4, ups=0.87, wpb=253.9, bsz=100, num_updates=26440, lr=8.54537e-06, gnorm=0.913, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53674
2023-03-15 10:26:59 - progress_bar.py[line:274] - INFO: epoch 005:   1242 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.8, ups=0.88, wpb=250.8, bsz=100, num_updates=26450, lr=8.5287e-06, gnorm=1.027, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=53686
2023-03-15 10:27:11 - progress_bar.py[line:274] - INFO: epoch 005:   1252 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.6, ups=0.87, wpb=252.7, bsz=100, num_updates=26460, lr=8.51202e-06, gnorm=1.022, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=53698
2023-03-15 10:27:23 - progress_bar.py[line:274] - INFO: epoch 005:   1262 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.9, ups=0.87, wpb=253.6, bsz=100, num_updates=26470, lr=8.49535e-06, gnorm=0.945, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=53710
2023-03-15 10:27:35 - progress_bar.py[line:274] - INFO: epoch 005:   1272 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.4, ups=0.88, wpb=254.9, bsz=100, num_updates=26480, lr=8.47867e-06, gnorm=0.924, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=53722
2023-03-15 10:27:48 - progress_bar.py[line:274] - INFO: epoch 005:   1282 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.306, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=217.2, ups=0.85, wpb=255.4, bsz=100, num_updates=26490, lr=8.462e-06, gnorm=1.073, clip=70, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=53735
2023-03-15 10:27:59 - progress_bar.py[line:274] - INFO: epoch 005:   1292 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.1, ups=0.88, wpb=252.5, bsz=100, num_updates=26500, lr=8.44533e-06, gnorm=0.971, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53747
2023-03-15 10:28:12 - progress_bar.py[line:274] - INFO: epoch 005:   1302 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.6, ups=0.89, wpb=251.8, bsz=100, num_updates=26510, lr=8.42865e-06, gnorm=0.932, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53759
2023-03-15 10:28:24 - progress_bar.py[line:274] - INFO: epoch 005:   1312 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.4, ups=0.86, wpb=253.7, bsz=100, num_updates=26520, lr=8.41198e-06, gnorm=1.052, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53771
2023-03-15 10:28:37 - progress_bar.py[line:274] - INFO: epoch 005:   1322 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.9, ups=0.89, wpb=251.7, bsz=100, num_updates=26530, lr=8.3953e-06, gnorm=1.069, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53784
2023-03-15 10:28:49 - progress_bar.py[line:274] - INFO: epoch 005:   1332 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.7, ups=0.86, wpb=251.9, bsz=100, num_updates=26540, lr=8.37863e-06, gnorm=1.014, clip=50, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=53796
2023-03-15 10:29:02 - progress_bar.py[line:274] - INFO: epoch 005:   1342 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=226.3, ups=0.89, wpb=255.6, bsz=100, num_updates=26550, lr=8.36196e-06, gnorm=0.973, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=53809
2023-03-15 10:29:14 - progress_bar.py[line:274] - INFO: epoch 005:   1352 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=216.7, ups=0.86, wpb=251.7, bsz=100, num_updates=26560, lr=8.34528e-06, gnorm=0.949, clip=30, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=53821
2023-03-15 10:29:27 - progress_bar.py[line:274] - INFO: epoch 005:   1362 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=217.5, ups=0.86, wpb=252.1, bsz=100, num_updates=26570, lr=8.32861e-06, gnorm=0.945, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53834
2023-03-15 10:29:40 - progress_bar.py[line:274] - INFO: epoch 005:   1372 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221, ups=0.87, wpb=253, bsz=100, num_updates=26580, lr=8.31194e-06, gnorm=1.084, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=53847
2023-03-15 10:29:53 - progress_bar.py[line:274] - INFO: epoch 005:   1382 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=249.2, nsentences=100, sample_size=249.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=214.9, ups=0.86, wpb=249.2, bsz=100, num_updates=26590, lr=8.29526e-06, gnorm=0.988, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53859
2023-03-15 10:30:05 - progress_bar.py[line:274] - INFO: epoch 005:   1392 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.5, ups=0.88, wpb=254.1, bsz=100, num_updates=26600, lr=8.27859e-06, gnorm=1.071, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53872
2023-03-15 10:30:18 - progress_bar.py[line:274] - INFO: epoch 005:   1402 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=215.4, ups=0.85, wpb=252.4, bsz=100, num_updates=26610, lr=8.26191e-06, gnorm=0.983, clip=20, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=53885
2023-03-15 10:30:30 - progress_bar.py[line:274] - INFO: epoch 005:   1412 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=227.2, ups=0.89, wpb=256, bsz=100, num_updates=26620, lr=8.24524e-06, gnorm=1.061, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=53897
2023-03-15 10:30:42 - progress_bar.py[line:274] - INFO: epoch 005:   1422 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.9, ups=0.88, wpb=253.3, bsz=100, num_updates=26630, lr=8.22857e-06, gnorm=1.086, clip=60, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=53910
2023-03-15 10:30:55 - progress_bar.py[line:274] - INFO: epoch 005:   1432 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.5, ups=0.86, wpb=251.4, bsz=100, num_updates=26640, lr=8.21189e-06, gnorm=1.059, clip=70, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=53922
2023-03-15 10:31:07 - progress_bar.py[line:274] - INFO: epoch 005:   1442 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.7, ups=0.87, wpb=254.6, bsz=100, num_updates=26650, lr=8.19522e-06, gnorm=0.95, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=53934
2023-03-15 10:31:19 - progress_bar.py[line:274] - INFO: epoch 005:   1452 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.2, ups=0.86, wpb=252.1, bsz=100, num_updates=26660, lr=8.17854e-06, gnorm=1.01, clip=60, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=53946
2023-03-15 10:31:31 - progress_bar.py[line:274] - INFO: epoch 005:   1462 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.1, ups=0.87, wpb=250.7, bsz=100, num_updates=26670, lr=8.16187e-06, gnorm=1.071, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53958
2023-03-15 10:31:43 - progress_bar.py[line:274] - INFO: epoch 005:   1472 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.4, ups=0.87, wpb=253.4, bsz=100, num_updates=26680, lr=8.1452e-06, gnorm=1.036, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=53970
2023-03-15 10:31:55 - progress_bar.py[line:274] - INFO: epoch 005:   1482 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.8, ups=0.86, wpb=255.5, bsz=100, num_updates=26690, lr=8.12852e-06, gnorm=0.983, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=53982
2023-03-15 10:32:07 - progress_bar.py[line:274] - INFO: epoch 005:   1492 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.7, ups=0.87, wpb=255.4, bsz=100, num_updates=26700, lr=8.11185e-06, gnorm=1.096, clip=70, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=53995
2023-03-15 10:32:19 - progress_bar.py[line:274] - INFO: epoch 005:   1502 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=249.6, nsentences=100, sample_size=249.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218, ups=0.87, wpb=249.6, bsz=100, num_updates=26710, lr=8.09517e-06, gnorm=0.978, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54007
2023-03-15 10:32:31 - progress_bar.py[line:274] - INFO: epoch 005:   1512 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=215.8, ups=0.86, wpb=251, bsz=100, num_updates=26720, lr=8.0785e-06, gnorm=1.003, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=54019
2023-03-15 10:32:44 - progress_bar.py[line:274] - INFO: epoch 005:   1522 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.7, ups=0.86, wpb=252.6, bsz=100, num_updates=26730, lr=8.06183e-06, gnorm=1.089, clip=70, loss_scale=512, train_wall=12, gb_free=10, ema_decay=0.9999, wall=54031
2023-03-15 10:32:56 - progress_bar.py[line:274] - INFO: epoch 005:   1532 / 6313 loss=0.527, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.4, ups=0.86, wpb=252.8, bsz=100, num_updates=26740, lr=8.04515e-06, gnorm=1.159, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=54043
2023-03-15 10:33:08 - progress_bar.py[line:274] - INFO: epoch 005:   1542 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.315, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.3, ups=0.86, wpb=254.1, bsz=100, num_updates=26750, lr=8.02848e-06, gnorm=1.05, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=54055
2023-03-15 10:33:21 - progress_bar.py[line:274] - INFO: epoch 005:   1552 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.6, ups=0.86, wpb=254, bsz=100, num_updates=26760, lr=8.01181e-06, gnorm=1.058, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54068
2023-03-15 10:33:33 - progress_bar.py[line:274] - INFO: epoch 005:   1562 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.9, ups=0.86, wpb=254.3, bsz=100, num_updates=26770, lr=7.99513e-06, gnorm=0.984, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=54080
2023-03-15 10:33:46 - progress_bar.py[line:274] - INFO: epoch 005:   1572 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.9, ups=0.87, wpb=252.1, bsz=100, num_updates=26780, lr=7.97846e-06, gnorm=0.947, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54093
2023-03-15 10:33:59 - progress_bar.py[line:274] - INFO: epoch 005:   1582 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=250, nsentences=100, sample_size=250, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=213.3, ups=0.85, wpb=250, bsz=100, num_updates=26790, lr=7.96178e-06, gnorm=1.072, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54105
2023-03-15 10:34:11 - progress_bar.py[line:274] - INFO: epoch 005:   1592 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.5, ups=0.86, wpb=255.3, bsz=100, num_updates=26800, lr=7.94511e-06, gnorm=0.952, clip=40, loss_scale=1024, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=54118
2023-03-15 10:34:24 - progress_bar.py[line:274] - INFO: epoch 005:   1602 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=214.8, ups=0.85, wpb=251.2, bsz=100, num_updates=26810, lr=7.92844e-06, gnorm=1.049, clip=70, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54131
2023-03-15 10:34:36 - progress_bar.py[line:274] - INFO: epoch 005:   1612 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.6, ups=0.87, wpb=254.4, bsz=100, num_updates=26820, lr=7.91176e-06, gnorm=1.036, clip=50, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54143
2023-03-15 10:34:48 - progress_bar.py[line:274] - INFO: epoch 005:   1622 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.6, ups=0.86, wpb=252.6, bsz=100, num_updates=26830, lr=7.89509e-06, gnorm=1.042, clip=70, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=54155
2023-03-15 10:34:55 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 10:35:01 - progress_bar.py[line:274] - INFO: epoch 005:   1633 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=246.2, nsentences=100, sample_size=246.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=196.4, ups=0.8, wpb=246.2, bsz=100, num_updates=26840, lr=7.87841e-06, gnorm=0.995, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54168
2023-03-15 10:35:13 - progress_bar.py[line:274] - INFO: epoch 005:   1643 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.4, ups=0.88, wpb=251.3, bsz=100, num_updates=26850, lr=7.86174e-06, gnorm=0.99, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54180
2023-03-15 10:35:25 - progress_bar.py[line:274] - INFO: epoch 005:   1653 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.2, ups=0.86, wpb=252.4, bsz=100, num_updates=26860, lr=7.84507e-06, gnorm=1.045, clip=70, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=54192
2023-03-15 10:35:37 - progress_bar.py[line:274] - INFO: epoch 005:   1663 / 6313 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=221.1, ups=0.87, wpb=253.2, bsz=100, num_updates=26870, lr=7.82839e-06, gnorm=0.945, clip=10, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54204
2023-03-15 10:35:49 - progress_bar.py[line:274] - INFO: epoch 005:   1673 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.3, ups=0.87, wpb=253.6, bsz=100, num_updates=26880, lr=7.81172e-06, gnorm=0.99, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54216
2023-03-15 10:36:02 - progress_bar.py[line:274] - INFO: epoch 005:   1683 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.8, ups=0.86, wpb=255.6, bsz=100, num_updates=26890, lr=7.79504e-06, gnorm=0.962, clip=30, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54229
2023-03-15 10:36:14 - progress_bar.py[line:274] - INFO: epoch 005:   1693 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=217.7, ups=0.86, wpb=252, bsz=100, num_updates=26900, lr=7.77837e-06, gnorm=0.958, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=54241
2023-03-15 10:36:27 - progress_bar.py[line:274] - INFO: epoch 005:   1703 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.1, ups=0.86, wpb=254.9, bsz=100, num_updates=26910, lr=7.7617e-06, gnorm=1.123, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54254
2023-03-15 10:36:39 - progress_bar.py[line:274] - INFO: epoch 005:   1713 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.1, ups=0.88, wpb=252.2, bsz=100, num_updates=26920, lr=7.74502e-06, gnorm=0.975, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54266
2023-03-15 10:36:51 - progress_bar.py[line:274] - INFO: epoch 005:   1723 / 6313 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.6, ups=0.88, wpb=256, bsz=100, num_updates=26930, lr=7.72835e-06, gnorm=0.92, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54278
2023-03-15 10:37:04 - progress_bar.py[line:274] - INFO: epoch 005:   1733 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=215.6, ups=0.86, wpb=250.1, bsz=100, num_updates=26940, lr=7.71168e-06, gnorm=1.045, clip=50, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=54291
2023-03-15 10:37:16 - progress_bar.py[line:274] - INFO: epoch 005:   1743 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.5, ups=0.87, wpb=254.3, bsz=100, num_updates=26950, lr=7.695e-06, gnorm=1.08, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54303
2023-03-15 10:37:28 - progress_bar.py[line:274] - INFO: epoch 005:   1753 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.1, ups=0.86, wpb=254.5, bsz=100, num_updates=26960, lr=7.67833e-06, gnorm=1.108, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=54316
2023-03-15 10:37:40 - progress_bar.py[line:274] - INFO: epoch 005:   1763 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.6, ups=0.87, wpb=252.9, bsz=100, num_updates=26970, lr=7.66165e-06, gnorm=1.007, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54328
2023-03-15 10:37:52 - progress_bar.py[line:274] - INFO: epoch 005:   1773 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.2, ups=0.87, wpb=254.8, bsz=100, num_updates=26980, lr=7.64498e-06, gnorm=1.092, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54340
2023-03-15 10:38:04 - progress_bar.py[line:274] - INFO: epoch 005:   1783 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.1, ups=0.86, wpb=254.5, bsz=100, num_updates=26990, lr=7.62831e-06, gnorm=1.059, clip=60, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=54352
2023-03-15 10:38:16 - progress_bar.py[line:274] - INFO: epoch 005:   1793 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.1, ups=0.88, wpb=252.7, bsz=100, num_updates=27000, lr=7.61163e-06, gnorm=1.031, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54364
2023-03-15 10:38:29 - progress_bar.py[line:274] - INFO: epoch 005:   1803 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.9, ups=0.86, wpb=253.1, bsz=100, num_updates=27010, lr=7.59496e-06, gnorm=1.006, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54376
2023-03-15 10:38:41 - progress_bar.py[line:274] - INFO: epoch 005:   1813 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.4, ups=0.87, wpb=253.4, bsz=100, num_updates=27020, lr=7.57828e-06, gnorm=1.031, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54388
2023-03-15 10:38:53 - progress_bar.py[line:274] - INFO: epoch 005:   1823 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.6, ups=0.86, wpb=253.2, bsz=100, num_updates=27030, lr=7.56161e-06, gnorm=1.092, clip=90, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=54400
2023-03-15 10:39:05 - progress_bar.py[line:274] - INFO: epoch 005:   1833 / 6313 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=257.5, nsentences=100, sample_size=257.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=221.3, ups=0.86, wpb=257.5, bsz=100, num_updates=27040, lr=7.54494e-06, gnorm=0.901, clip=40, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=54412
2023-03-15 10:39:17 - progress_bar.py[line:274] - INFO: epoch 005:   1843 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.5, ups=0.87, wpb=253.7, bsz=100, num_updates=27050, lr=7.52826e-06, gnorm=1.049, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54425
2023-03-15 10:39:29 - progress_bar.py[line:274] - INFO: epoch 005:   1853 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225.2, ups=0.88, wpb=255.4, bsz=100, num_updates=27060, lr=7.51159e-06, gnorm=1.027, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=54436
2023-03-15 10:39:41 - progress_bar.py[line:274] - INFO: epoch 005:   1863 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.5, ups=0.88, wpb=253.1, bsz=100, num_updates=27070, lr=7.49491e-06, gnorm=1.081, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54448
2023-03-15 10:39:53 - progress_bar.py[line:274] - INFO: epoch 005:   1873 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=215.8, ups=0.85, wpb=253.2, bsz=100, num_updates=27080, lr=7.47824e-06, gnorm=1.064, clip=70, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54461
2023-03-15 10:40:06 - progress_bar.py[line:274] - INFO: epoch 005:   1883 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.4, ups=0.87, wpb=251.7, bsz=100, num_updates=27090, lr=7.46157e-06, gnorm=1.12, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54473
2023-03-15 10:40:18 - progress_bar.py[line:274] - INFO: epoch 005:   1893 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.1, ups=0.85, wpb=253.2, bsz=100, num_updates=27100, lr=7.44489e-06, gnorm=0.969, clip=30, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=54485
2023-03-15 10:40:30 - progress_bar.py[line:274] - INFO: epoch 005:   1903 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.4, ups=0.86, wpb=252.8, bsz=100, num_updates=27110, lr=7.42822e-06, gnorm=1.049, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54497
2023-03-15 10:40:42 - progress_bar.py[line:274] - INFO: epoch 005:   1913 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.4, ups=0.88, wpb=253.8, bsz=100, num_updates=27120, lr=7.41155e-06, gnorm=0.953, clip=30, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=54509
2023-03-15 10:40:54 - progress_bar.py[line:274] - INFO: epoch 005:   1923 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=224.7, ups=0.89, wpb=252.2, bsz=100, num_updates=27130, lr=7.39487e-06, gnorm=0.962, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54521
2023-03-15 10:41:06 - progress_bar.py[line:274] - INFO: epoch 005:   1933 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.8, ups=0.86, wpb=252.6, bsz=100, num_updates=27140, lr=7.3782e-06, gnorm=1.084, clip=80, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=54533
2023-03-15 10:41:19 - progress_bar.py[line:274] - INFO: epoch 005:   1943 / 6313 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=219.2, ups=0.86, wpb=254.8, bsz=100, num_updates=27150, lr=7.36152e-06, gnorm=1.004, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=54546
2023-03-15 10:41:31 - progress_bar.py[line:274] - INFO: epoch 005:   1953 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.2, ups=0.86, wpb=252.6, bsz=100, num_updates=27160, lr=7.34485e-06, gnorm=1.173, clip=50, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=54558
2023-03-15 10:41:44 - progress_bar.py[line:274] - INFO: epoch 005:   1963 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.3, ups=0.87, wpb=251.2, bsz=100, num_updates=27170, lr=7.32818e-06, gnorm=1.078, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=54571
2023-03-15 10:41:57 - progress_bar.py[line:274] - INFO: epoch 005:   1973 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.3, ups=0.86, wpb=254.3, bsz=100, num_updates=27180, lr=7.3115e-06, gnorm=0.99, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=54583
2023-03-15 10:42:10 - progress_bar.py[line:274] - INFO: epoch 005:   1983 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218, ups=0.86, wpb=252.9, bsz=100, num_updates=27190, lr=7.29483e-06, gnorm=1.047, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=54596
2023-03-15 10:42:24 - progress_bar.py[line:274] - INFO: epoch 005:   1993 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.5, ups=0.88, wpb=251.1, bsz=100, num_updates=27200, lr=7.27815e-06, gnorm=0.979, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54609
2023-03-15 10:42:36 - progress_bar.py[line:274] - INFO: epoch 005:   2003 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.6, ups=0.86, wpb=252.8, bsz=100, num_updates=27210, lr=7.26148e-06, gnorm=1.164, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=54623
2023-03-15 10:42:49 - progress_bar.py[line:274] - INFO: epoch 005:   2013 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=249.9, nsentences=100, sample_size=249.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=214.2, ups=0.86, wpb=249.9, bsz=100, num_updates=27220, lr=7.24481e-06, gnorm=1.118, clip=90, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54636
2023-03-15 10:43:02 - progress_bar.py[line:274] - INFO: epoch 005:   2023 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.4, ups=0.87, wpb=252, bsz=100, num_updates=27230, lr=7.22813e-06, gnorm=1.072, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54648
2023-03-15 10:43:14 - progress_bar.py[line:274] - INFO: epoch 005:   2033 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.2, ups=0.87, wpb=255.7, bsz=100, num_updates=27240, lr=7.21146e-06, gnorm=1.074, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54661
2023-03-15 10:43:27 - progress_bar.py[line:274] - INFO: epoch 005:   2043 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=216.9, ups=0.86, wpb=252, bsz=100, num_updates=27250, lr=7.19478e-06, gnorm=1.002, clip=50, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=54674
2023-03-15 10:43:40 - progress_bar.py[line:274] - INFO: epoch 005:   2053 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.1, ups=0.86, wpb=255.2, bsz=100, num_updates=27260, lr=7.17811e-06, gnorm=1.109, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=54686
2023-03-15 10:43:52 - progress_bar.py[line:274] - INFO: epoch 005:   2063 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219, ups=0.86, wpb=254, bsz=100, num_updates=27270, lr=7.16144e-06, gnorm=1.029, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54699
2023-03-15 10:44:05 - progress_bar.py[line:274] - INFO: epoch 005:   2073 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.6, ups=0.87, wpb=252.1, bsz=100, num_updates=27280, lr=7.14476e-06, gnorm=1.01, clip=50, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=54712
2023-03-15 10:44:18 - progress_bar.py[line:274] - INFO: epoch 005:   2083 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=216.4, ups=0.86, wpb=251.1, bsz=100, num_updates=27290, lr=7.12809e-06, gnorm=1.128, clip=80, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=54725
2023-03-15 10:44:31 - progress_bar.py[line:274] - INFO: epoch 005:   2093 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.1, ups=0.87, wpb=252.8, bsz=100, num_updates=27300, lr=7.11141e-06, gnorm=1.018, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54737
2023-03-15 10:44:43 - progress_bar.py[line:274] - INFO: epoch 005:   2103 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.4, ups=0.87, wpb=253.3, bsz=100, num_updates=27310, lr=7.09474e-06, gnorm=1.03, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54750
2023-03-15 10:44:55 - progress_bar.py[line:274] - INFO: epoch 005:   2113 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.308, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=225.2, ups=0.89, wpb=254.3, bsz=100, num_updates=27320, lr=7.07807e-06, gnorm=1.217, clip=80, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=54762
2023-03-15 10:45:08 - progress_bar.py[line:274] - INFO: epoch 005:   2123 / 6313 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.2, ups=0.86, wpb=254.2, bsz=100, num_updates=27330, lr=7.06139e-06, gnorm=0.933, clip=30, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=54775
2023-03-15 10:45:20 - progress_bar.py[line:274] - INFO: epoch 005:   2133 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.8, ups=0.87, wpb=252.9, bsz=100, num_updates=27340, lr=7.04472e-06, gnorm=0.989, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=54787
2023-03-15 10:45:33 - progress_bar.py[line:274] - INFO: epoch 005:   2143 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.9, ups=0.86, wpb=251.8, bsz=100, num_updates=27350, lr=7.02805e-06, gnorm=1.042, clip=50, loss_scale=1024, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=54800
2023-03-15 10:45:46 - progress_bar.py[line:274] - INFO: epoch 005:   2153 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.1, ups=0.86, wpb=254.2, bsz=100, num_updates=27360, lr=7.01137e-06, gnorm=0.988, clip=40, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=54813
2023-03-15 10:45:59 - progress_bar.py[line:274] - INFO: epoch 005:   2163 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.8, ups=0.87, wpb=252.4, bsz=100, num_updates=27370, lr=6.9947e-06, gnorm=0.97, clip=60, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=54825
2023-03-15 10:46:12 - progress_bar.py[line:274] - INFO: epoch 005:   2173 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.7, ups=0.87, wpb=254.6, bsz=100, num_updates=27380, lr=6.97802e-06, gnorm=1.006, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54838
2023-03-15 10:46:24 - progress_bar.py[line:274] - INFO: epoch 005:   2183 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.8, ups=0.88, wpb=251.6, bsz=100, num_updates=27390, lr=6.96135e-06, gnorm=1.028, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54851
2023-03-15 10:46:37 - progress_bar.py[line:274] - INFO: epoch 005:   2193 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=215.4, ups=0.85, wpb=252.8, bsz=100, num_updates=27400, lr=6.94468e-06, gnorm=0.996, clip=40, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=54864
2023-03-15 10:46:49 - progress_bar.py[line:274] - INFO: epoch 005:   2203 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=224.3, ups=0.89, wpb=253.4, bsz=100, num_updates=27410, lr=6.928e-06, gnorm=1.114, clip=70, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54876
2023-03-15 10:47:02 - progress_bar.py[line:274] - INFO: epoch 005:   2213 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.9, ups=0.86, wpb=252.3, bsz=100, num_updates=27420, lr=6.91133e-06, gnorm=1.074, clip=70, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54889
2023-03-15 10:47:14 - progress_bar.py[line:274] - INFO: epoch 005:   2223 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=257.4, nsentences=100, sample_size=257.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225.2, ups=0.87, wpb=257.4, bsz=100, num_updates=27430, lr=6.89465e-06, gnorm=1.1, clip=60, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=54901
2023-03-15 10:47:26 - progress_bar.py[line:274] - INFO: epoch 005:   2233 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=226.3, ups=0.89, wpb=253.3, bsz=100, num_updates=27440, lr=6.87798e-06, gnorm=0.991, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=54913
2023-03-15 10:47:38 - progress_bar.py[line:274] - INFO: epoch 005:   2243 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=217.2, ups=0.86, wpb=252.9, bsz=100, num_updates=27450, lr=6.86131e-06, gnorm=0.974, clip=50, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=54925
2023-03-15 10:47:49 - progress_bar.py[line:274] - INFO: epoch 005:   2253 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.4, ups=0.86, wpb=253.1, bsz=100, num_updates=27460, lr=6.84463e-06, gnorm=0.998, clip=40, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=54937
2023-03-15 10:48:01 - progress_bar.py[line:274] - INFO: epoch 005:   2263 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=250.1, nsentences=100, sample_size=250.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=214.4, ups=0.86, wpb=250.1, bsz=100, num_updates=27470, lr=6.82796e-06, gnorm=1.055, clip=60, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=54949
2023-03-15 10:48:13 - progress_bar.py[line:274] - INFO: epoch 005:   2273 / 6313 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.5, ups=0.87, wpb=253.8, bsz=100, num_updates=27480, lr=6.81128e-06, gnorm=0.988, clip=50, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=54961
2023-03-15 10:48:25 - progress_bar.py[line:274] - INFO: epoch 005:   2283 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=216.6, ups=0.86, wpb=251.7, bsz=100, num_updates=27490, lr=6.79461e-06, gnorm=1.012, clip=50, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=54972
2023-03-15 10:48:33 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 10:48:38 - progress_bar.py[line:274] - INFO: epoch 005:   2294 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=201.8, ups=0.8, wpb=252.7, bsz=100, num_updates=27500, lr=6.77794e-06, gnorm=0.996, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54985
2023-03-15 10:48:50 - progress_bar.py[line:274] - INFO: epoch 005:   2304 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.8, ups=0.86, wpb=255.9, bsz=100, num_updates=27510, lr=6.76126e-06, gnorm=1.063, clip=70, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=54997
2023-03-15 10:49:01 - progress_bar.py[line:274] - INFO: epoch 005:   2314 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.2, ups=0.88, wpb=254, bsz=100, num_updates=27520, lr=6.74459e-06, gnorm=1.105, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55009
2023-03-15 10:49:13 - progress_bar.py[line:274] - INFO: epoch 005:   2324 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.6, ups=0.87, wpb=251.4, bsz=100, num_updates=27530, lr=6.72792e-06, gnorm=1.024, clip=70, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=55021
2023-03-15 10:49:25 - progress_bar.py[line:274] - INFO: epoch 005:   2334 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.6, ups=0.88, wpb=252.4, bsz=100, num_updates=27540, lr=6.71124e-06, gnorm=1.083, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55032
2023-03-15 10:49:37 - progress_bar.py[line:274] - INFO: epoch 005:   2344 / 6313 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=226, ups=0.88, wpb=255.8, bsz=100, num_updates=27550, lr=6.69457e-06, gnorm=0.926, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55044
2023-03-15 10:49:49 - progress_bar.py[line:274] - INFO: epoch 005:   2354 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=214.4, ups=0.85, wpb=251.9, bsz=100, num_updates=27560, lr=6.67789e-06, gnorm=0.994, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=55057
2023-03-15 10:50:01 - progress_bar.py[line:274] - INFO: epoch 005:   2364 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.5, ups=0.87, wpb=252.5, bsz=100, num_updates=27570, lr=6.66122e-06, gnorm=0.992, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55069
2023-03-15 10:50:13 - progress_bar.py[line:274] - INFO: epoch 005:   2374 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.2, ups=0.86, wpb=254.3, bsz=100, num_updates=27580, lr=6.64455e-06, gnorm=0.897, clip=20, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=55081
2023-03-15 10:50:25 - progress_bar.py[line:274] - INFO: epoch 005:   2384 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.5, ups=0.87, wpb=252.3, bsz=100, num_updates=27590, lr=6.62787e-06, gnorm=1.125, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55092
2023-03-15 10:50:37 - progress_bar.py[line:274] - INFO: epoch 005:   2394 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=224, ups=0.89, wpb=251.4, bsz=100, num_updates=27600, lr=6.6112e-06, gnorm=1.031, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=55104
2023-03-15 10:50:48 - progress_bar.py[line:274] - INFO: epoch 005:   2404 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.8, ups=0.87, wpb=254.7, bsz=100, num_updates=27610, lr=6.59452e-06, gnorm=1.012, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55116
2023-03-15 10:51:00 - progress_bar.py[line:274] - INFO: epoch 005:   2414 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.5, ups=0.87, wpb=253.3, bsz=100, num_updates=27620, lr=6.57785e-06, gnorm=0.997, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55128
2023-03-15 10:51:12 - progress_bar.py[line:274] - INFO: epoch 005:   2424 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.8, ups=0.87, wpb=253.4, bsz=100, num_updates=27630, lr=6.56118e-06, gnorm=1.054, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55140
2023-03-15 10:51:25 - progress_bar.py[line:274] - INFO: epoch 005:   2434 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.8, ups=0.87, wpb=254.9, bsz=100, num_updates=27640, lr=6.5445e-06, gnorm=1.059, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55152
2023-03-15 10:51:37 - progress_bar.py[line:274] - INFO: epoch 005:   2444 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.8, ups=0.87, wpb=253.3, bsz=100, num_updates=27650, lr=6.52783e-06, gnorm=1.06, clip=70, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=55164
2023-03-15 10:51:48 - progress_bar.py[line:274] - INFO: epoch 005:   2454 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.1, ups=0.88, wpb=252.6, bsz=100, num_updates=27660, lr=6.51115e-06, gnorm=1.054, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55176
2023-03-15 10:52:00 - progress_bar.py[line:274] - INFO: epoch 005:   2464 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.1, ups=0.86, wpb=253.8, bsz=100, num_updates=27670, lr=6.49448e-06, gnorm=1.03, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=55188
2023-03-15 10:52:12 - progress_bar.py[line:274] - INFO: epoch 005:   2474 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=229.1, ups=0.89, wpb=256.5, bsz=100, num_updates=27680, lr=6.47781e-06, gnorm=0.99, clip=30, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=55199
2023-03-15 10:52:24 - progress_bar.py[line:274] - INFO: epoch 005:   2484 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=224.2, ups=0.88, wpb=256.1, bsz=100, num_updates=27690, lr=6.46113e-06, gnorm=0.988, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55211
2023-03-15 10:52:36 - progress_bar.py[line:274] - INFO: epoch 005:   2494 / 6313 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=220.2, ups=0.87, wpb=253.6, bsz=100, num_updates=27700, lr=6.44446e-06, gnorm=1.039, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55223
2023-03-15 10:52:48 - progress_bar.py[line:274] - INFO: epoch 005:   2504 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=222.6, ups=0.87, wpb=254.8, bsz=100, num_updates=27710, lr=6.42779e-06, gnorm=0.946, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55235
2023-03-15 10:52:59 - progress_bar.py[line:274] - INFO: epoch 005:   2514 / 6313 loss=0.534, loss_v1=0, loss_v2=0, nll_loss=0.31, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.5, ups=0.87, wpb=251.9, bsz=100, num_updates=27720, lr=6.41111e-06, gnorm=1.182, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55247
2023-03-15 10:53:11 - progress_bar.py[line:274] - INFO: epoch 005:   2524 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=215.9, ups=0.86, wpb=251.5, bsz=100, num_updates=27730, lr=6.39444e-06, gnorm=1.071, clip=60, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=55259
2023-03-15 10:53:23 - progress_bar.py[line:274] - INFO: epoch 005:   2534 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=257.6, nsentences=100, sample_size=257.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.7, ups=0.86, wpb=257.6, bsz=100, num_updates=27740, lr=6.37776e-06, gnorm=1.041, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=55271
2023-03-15 10:53:35 - progress_bar.py[line:274] - INFO: epoch 005:   2544 / 6313 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223, ups=0.87, wpb=256.2, bsz=100, num_updates=27750, lr=6.36109e-06, gnorm=0.988, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=55282
2023-03-15 10:53:46 - progress_bar.py[line:274] - INFO: epoch 005:   2554 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=225.1, ups=0.89, wpb=252.6, bsz=100, num_updates=27760, lr=6.34442e-06, gnorm=1.053, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55294
2023-03-15 10:53:58 - progress_bar.py[line:274] - INFO: epoch 005:   2564 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=217.5, ups=0.85, wpb=254.5, bsz=100, num_updates=27770, lr=6.32774e-06, gnorm=1.076, clip=70, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=55306
2023-03-15 10:54:10 - progress_bar.py[line:274] - INFO: epoch 005:   2574 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.4, ups=0.87, wpb=254.7, bsz=100, num_updates=27780, lr=6.31107e-06, gnorm=1.012, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55318
2023-03-15 10:54:22 - progress_bar.py[line:274] - INFO: epoch 005:   2584 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=217, ups=0.86, wpb=252.6, bsz=100, num_updates=27790, lr=6.29439e-06, gnorm=1.131, clip=70, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=55330
2023-03-15 10:54:34 - progress_bar.py[line:274] - INFO: epoch 005:   2594 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=256.7, nsentences=100, sample_size=256.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=224, ups=0.87, wpb=256.7, bsz=100, num_updates=27800, lr=6.27772e-06, gnorm=1.093, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55342
2023-03-15 10:54:46 - progress_bar.py[line:274] - INFO: epoch 005:   2604 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.3, ups=0.88, wpb=254.3, bsz=100, num_updates=27810, lr=6.26105e-06, gnorm=0.966, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55353
2023-03-15 10:54:58 - progress_bar.py[line:274] - INFO: epoch 005:   2614 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.4, ups=0.87, wpb=254.4, bsz=100, num_updates=27820, lr=6.24437e-06, gnorm=1.051, clip=70, loss_scale=512, train_wall=11, gb_free=11.1, ema_decay=0.9999, wall=55365
2023-03-15 10:55:09 - progress_bar.py[line:274] - INFO: epoch 005:   2624 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.3, ups=0.87, wpb=252.4, bsz=100, num_updates=27830, lr=6.2277e-06, gnorm=1.099, clip=70, loss_scale=512, train_wall=11, gb_free=9.8, ema_decay=0.9999, wall=55377
2023-03-15 10:55:21 - progress_bar.py[line:274] - INFO: epoch 005:   2634 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.2, ups=0.87, wpb=254.2, bsz=100, num_updates=27840, lr=6.21102e-06, gnorm=1.065, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55389
2023-03-15 10:55:33 - progress_bar.py[line:274] - INFO: epoch 005:   2644 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=250.8, nsentences=100, sample_size=250.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.6, ups=0.88, wpb=250.8, bsz=100, num_updates=27850, lr=6.19435e-06, gnorm=1.08, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55400
2023-03-15 10:55:45 - progress_bar.py[line:274] - INFO: epoch 005:   2654 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.3, ups=0.87, wpb=254.1, bsz=100, num_updates=27860, lr=6.17768e-06, gnorm=1.043, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=55412
2023-03-15 10:55:57 - progress_bar.py[line:274] - INFO: epoch 005:   2664 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.9, ups=0.87, wpb=252.9, bsz=100, num_updates=27870, lr=6.161e-06, gnorm=1.04, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=55424
2023-03-15 10:56:08 - progress_bar.py[line:274] - INFO: epoch 005:   2674 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.6, ups=0.87, wpb=251.5, bsz=100, num_updates=27880, lr=6.14433e-06, gnorm=1.257, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=55436
2023-03-15 10:56:20 - progress_bar.py[line:274] - INFO: epoch 005:   2684 / 6313 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=220.4, ups=0.87, wpb=254.7, bsz=100, num_updates=27890, lr=6.12766e-06, gnorm=1.007, clip=70, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=55448
2023-03-15 10:56:32 - progress_bar.py[line:274] - INFO: epoch 005:   2694 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=227.6, ups=0.89, wpb=254.9, bsz=100, num_updates=27900, lr=6.11098e-06, gnorm=1.001, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55459
2023-03-15 10:56:43 - progress_bar.py[line:274] - INFO: epoch 005:   2704 / 6313 loss=0.479, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=257, nsentences=100, sample_size=257, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=224.2, ups=0.87, wpb=257, bsz=100, num_updates=27910, lr=6.09431e-06, gnorm=0.99, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55471
2023-03-15 10:56:55 - progress_bar.py[line:274] - INFO: epoch 005:   2714 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.8, ups=0.87, wpb=255.2, bsz=100, num_updates=27920, lr=6.07763e-06, gnorm=0.988, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=55483
2023-03-15 10:57:07 - progress_bar.py[line:274] - INFO: epoch 005:   2724 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.4, ups=0.89, wpb=252.2, bsz=100, num_updates=27930, lr=6.06096e-06, gnorm=1.051, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55494
2023-03-15 10:57:18 - progress_bar.py[line:274] - INFO: epoch 005:   2734 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=225.3, ups=0.89, wpb=253.2, bsz=100, num_updates=27940, lr=6.04429e-06, gnorm=1.082, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55506
2023-03-15 10:57:30 - progress_bar.py[line:274] - INFO: epoch 005:   2744 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225.1, ups=0.88, wpb=254.6, bsz=100, num_updates=27950, lr=6.02761e-06, gnorm=1.04, clip=60, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=55517
2023-03-15 10:57:42 - progress_bar.py[line:274] - INFO: epoch 005:   2754 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.2, ups=0.86, wpb=253.8, bsz=100, num_updates=27960, lr=6.01094e-06, gnorm=1.041, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=55529
2023-03-15 10:57:54 - progress_bar.py[line:274] - INFO: epoch 005:   2764 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.7, ups=0.86, wpb=254.4, bsz=100, num_updates=27970, lr=5.99426e-06, gnorm=0.971, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=55541
2023-03-15 10:58:06 - progress_bar.py[line:274] - INFO: epoch 005:   2774 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.1, ups=0.87, wpb=251.9, bsz=100, num_updates=27980, lr=5.97759e-06, gnorm=0.993, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55553
2023-03-15 10:58:18 - progress_bar.py[line:274] - INFO: epoch 005:   2784 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.2, ups=0.86, wpb=255.5, bsz=100, num_updates=27990, lr=5.96092e-06, gnorm=0.998, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=55565
2023-03-15 10:58:30 - progress_bar.py[line:274] - INFO: epoch 005:   2794 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=215.3, ups=0.85, wpb=252.7, bsz=100, num_updates=28000, lr=5.94424e-06, gnorm=0.989, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=55577
2023-03-15 10:58:42 - progress_bar.py[line:274] - INFO: epoch 005:   2804 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.5, ups=0.86, wpb=253.5, bsz=100, num_updates=28010, lr=5.92757e-06, gnorm=1.102, clip=60, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=55589
2023-03-15 10:58:54 - progress_bar.py[line:274] - INFO: epoch 005:   2814 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.4, ups=0.87, wpb=251.2, bsz=100, num_updates=28020, lr=5.91089e-06, gnorm=1.118, clip=80, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=55601
2023-03-15 10:59:05 - progress_bar.py[line:274] - INFO: epoch 005:   2824 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=224.3, ups=0.88, wpb=253.6, bsz=100, num_updates=28030, lr=5.89422e-06, gnorm=1.027, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55613
2023-03-15 10:59:18 - progress_bar.py[line:274] - INFO: epoch 005:   2834 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.1, ups=0.87, wpb=251.8, bsz=100, num_updates=28040, lr=5.87755e-06, gnorm=1.033, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55625
2023-03-15 10:59:31 - progress_bar.py[line:274] - INFO: epoch 005:   2844 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.3, ups=0.87, wpb=251.3, bsz=100, num_updates=28050, lr=5.86087e-06, gnorm=1.018, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55637
2023-03-15 10:59:43 - progress_bar.py[line:274] - INFO: epoch 005:   2854 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222, ups=0.87, wpb=254.2, bsz=100, num_updates=28060, lr=5.8442e-06, gnorm=1.177, clip=80, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55650
2023-03-15 10:59:49 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 10:59:56 - progress_bar.py[line:274] - INFO: epoch 005:   2865 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=200.6, ups=0.79, wpb=253.7, bsz=100, num_updates=28070, lr=5.82753e-06, gnorm=1.104, clip=60, loss_scale=512, train_wall=13, gb_free=10.5, ema_decay=0.9999, wall=55663
2023-03-15 11:00:08 - progress_bar.py[line:274] - INFO: epoch 005:   2875 / 6313 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=220.6, ups=0.86, wpb=255.4, bsz=100, num_updates=28080, lr=5.81085e-06, gnorm=1.014, clip=40, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=55675
2023-03-15 11:00:20 - progress_bar.py[line:274] - INFO: epoch 005:   2885 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.2, ups=0.86, wpb=253.9, bsz=100, num_updates=28090, lr=5.79418e-06, gnorm=1.103, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=55688
2023-03-15 11:00:32 - progress_bar.py[line:274] - INFO: epoch 005:   2895 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.3, ups=0.88, wpb=256.2, bsz=100, num_updates=28100, lr=5.7775e-06, gnorm=1.086, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55699
2023-03-15 11:00:44 - progress_bar.py[line:274] - INFO: epoch 005:   2905 / 6313 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=218.7, ups=0.86, wpb=252.9, bsz=100, num_updates=28110, lr=5.76083e-06, gnorm=1.137, clip=80, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=55712
2023-03-15 11:00:57 - progress_bar.py[line:274] - INFO: epoch 005:   2915 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=215.7, ups=0.85, wpb=252.8, bsz=100, num_updates=28120, lr=5.74416e-06, gnorm=0.997, clip=50, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=55724
2023-03-15 11:01:08 - progress_bar.py[line:274] - INFO: epoch 005:   2925 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.8, ups=0.87, wpb=251, bsz=100, num_updates=28130, lr=5.72748e-06, gnorm=1.089, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55736
2023-03-15 11:01:21 - progress_bar.py[line:274] - INFO: epoch 005:   2935 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=216.5, ups=0.85, wpb=253.2, bsz=100, num_updates=28140, lr=5.71081e-06, gnorm=1.128, clip=90, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=55748
2023-03-15 11:01:33 - progress_bar.py[line:274] - INFO: epoch 005:   2945 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.3, ups=0.88, wpb=252.7, bsz=100, num_updates=28150, lr=5.69413e-06, gnorm=1.075, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55760
2023-03-15 11:01:46 - progress_bar.py[line:274] - INFO: epoch 005:   2955 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.7, ups=0.87, wpb=252.8, bsz=100, num_updates=28160, lr=5.67746e-06, gnorm=1.067, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=55773
2023-03-15 11:01:58 - progress_bar.py[line:274] - INFO: epoch 005:   2965 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.4, ups=0.87, wpb=252.7, bsz=100, num_updates=28170, lr=5.66079e-06, gnorm=1.049, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55785
2023-03-15 11:02:10 - progress_bar.py[line:274] - INFO: epoch 005:   2975 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.1, ups=0.87, wpb=253.1, bsz=100, num_updates=28180, lr=5.64411e-06, gnorm=1.139, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55797
2023-03-15 11:02:23 - progress_bar.py[line:274] - INFO: epoch 005:   2985 / 6313 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=256.7, nsentences=100, sample_size=256.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.5, ups=0.86, wpb=256.7, bsz=100, num_updates=28190, lr=5.62744e-06, gnorm=1.001, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=55810
2023-03-15 11:02:35 - progress_bar.py[line:274] - INFO: epoch 005:   2995 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=227.5, ups=0.89, wpb=254.5, bsz=100, num_updates=28200, lr=5.61076e-06, gnorm=1.03, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55822
2023-03-15 11:02:47 - progress_bar.py[line:274] - INFO: epoch 005:   3005 / 6313 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.2, ups=0.88, wpb=254.9, bsz=100, num_updates=28210, lr=5.59409e-06, gnorm=1, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55835
2023-03-15 11:03:00 - progress_bar.py[line:274] - INFO: epoch 005:   3015 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.4, ups=0.86, wpb=253.2, bsz=100, num_updates=28220, lr=5.57742e-06, gnorm=1.06, clip=70, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=55847
2023-03-15 11:03:12 - progress_bar.py[line:274] - INFO: epoch 005:   3025 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.1, ups=0.88, wpb=252, bsz=100, num_updates=28230, lr=5.56074e-06, gnorm=1.166, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=55859
2023-03-15 11:03:24 - progress_bar.py[line:274] - INFO: epoch 005:   3035 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.2, ups=0.87, wpb=255.8, bsz=100, num_updates=28240, lr=5.54407e-06, gnorm=1.025, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55871
2023-03-15 11:03:37 - progress_bar.py[line:274] - INFO: epoch 005:   3045 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=255.5, nsentences=100, sample_size=255.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.4, ups=0.87, wpb=255.5, bsz=100, num_updates=28250, lr=5.5274e-06, gnorm=1.024, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55884
2023-03-15 11:03:49 - progress_bar.py[line:274] - INFO: epoch 005:   3055 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.5, ups=0.88, wpb=252, bsz=100, num_updates=28260, lr=5.51072e-06, gnorm=1.091, clip=90, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55896
2023-03-15 11:04:02 - progress_bar.py[line:274] - INFO: epoch 005:   3065 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.1, ups=0.86, wpb=251.7, bsz=100, num_updates=28270, lr=5.49405e-06, gnorm=1.089, clip=70, loss_scale=512, train_wall=12, gb_free=9.6, ema_decay=0.9999, wall=55908
2023-03-15 11:04:14 - progress_bar.py[line:274] - INFO: epoch 005:   3075 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.8, ups=0.87, wpb=250.4, bsz=100, num_updates=28280, lr=5.47737e-06, gnorm=1.08, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55921
2023-03-15 11:04:27 - progress_bar.py[line:274] - INFO: epoch 005:   3085 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.4, ups=0.87, wpb=253.4, bsz=100, num_updates=28290, lr=5.4607e-06, gnorm=1.055, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=55933
2023-03-15 11:04:39 - progress_bar.py[line:274] - INFO: epoch 005:   3095 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.8, ups=0.88, wpb=250.3, bsz=100, num_updates=28300, lr=5.44403e-06, gnorm=1.103, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=55946
2023-03-15 11:04:52 - progress_bar.py[line:274] - INFO: epoch 005:   3105 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.1, ups=0.85, wpb=255.1, bsz=100, num_updates=28310, lr=5.42735e-06, gnorm=0.948, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=55958
2023-03-15 11:05:04 - progress_bar.py[line:274] - INFO: epoch 005:   3115 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.3, ups=0.88, wpb=254.9, bsz=100, num_updates=28320, lr=5.41068e-06, gnorm=0.937, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55971
2023-03-15 11:05:16 - progress_bar.py[line:274] - INFO: epoch 005:   3125 / 6313 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=222.5, ups=0.87, wpb=254.4, bsz=100, num_updates=28330, lr=5.394e-06, gnorm=1.013, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=55983
2023-03-15 11:05:28 - progress_bar.py[line:274] - INFO: epoch 005:   3135 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.8, ups=0.88, wpb=253.3, bsz=100, num_updates=28340, lr=5.37733e-06, gnorm=1.044, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=55995
2023-03-15 11:05:40 - progress_bar.py[line:274] - INFO: epoch 005:   3145 / 6313 loss=0.471, loss_v1=0, loss_v2=0, nll_loss=0.246, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=221.3, ups=0.86, wpb=256.4, bsz=100, num_updates=28350, lr=5.36066e-06, gnorm=0.984, clip=60, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=56007
2023-03-15 11:05:52 - progress_bar.py[line:274] - INFO: epoch 005:   3155 / 6313 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=221.7, ups=0.87, wpb=253.7, bsz=100, num_updates=28360, lr=5.34398e-06, gnorm=0.99, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=56019
2023-03-15 11:06:04 - progress_bar.py[line:274] - INFO: epoch 005:   3165 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=256.2, nsentences=100, sample_size=256.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=226.8, ups=0.89, wpb=256.2, bsz=100, num_updates=28370, lr=5.32731e-06, gnorm=0.982, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56031
2023-03-15 11:06:16 - progress_bar.py[line:274] - INFO: epoch 005:   3175 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=215.3, ups=0.86, wpb=251.4, bsz=100, num_updates=28380, lr=5.31063e-06, gnorm=1.063, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=56044
2023-03-15 11:06:29 - progress_bar.py[line:274] - INFO: epoch 005:   3185 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=257, nsentences=100, sample_size=257, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=224.5, ups=0.87, wpb=257, bsz=100, num_updates=28390, lr=5.29396e-06, gnorm=1.055, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56056
2023-03-15 11:06:40 - progress_bar.py[line:274] - INFO: epoch 005:   3195 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.6, ups=0.88, wpb=254.7, bsz=100, num_updates=28400, lr=5.27729e-06, gnorm=1.032, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56068
2023-03-15 11:06:52 - progress_bar.py[line:274] - INFO: epoch 005:   3205 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.7, ups=0.87, wpb=252.5, bsz=100, num_updates=28410, lr=5.26061e-06, gnorm=1.029, clip=40, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=56080
2023-03-15 11:07:05 - progress_bar.py[line:274] - INFO: epoch 005:   3215 / 6313 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=220.7, ups=0.87, wpb=255, bsz=100, num_updates=28420, lr=5.24394e-06, gnorm=0.938, clip=40, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=56092
2023-03-15 11:07:17 - progress_bar.py[line:274] - INFO: epoch 005:   3225 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.4, ups=0.87, wpb=253.3, bsz=100, num_updates=28430, lr=5.22727e-06, gnorm=0.964, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56104
2023-03-15 11:07:30 - progress_bar.py[line:274] - INFO: epoch 005:   3235 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.8, ups=0.86, wpb=254.3, bsz=100, num_updates=28440, lr=5.21059e-06, gnorm=1.045, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=56117
2023-03-15 11:07:42 - progress_bar.py[line:274] - INFO: epoch 005:   3245 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.6, ups=0.87, wpb=252.7, bsz=100, num_updates=28450, lr=5.19392e-06, gnorm=1.036, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=56130
2023-03-15 11:07:55 - progress_bar.py[line:274] - INFO: epoch 005:   3255 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.9, ups=0.86, wpb=254, bsz=100, num_updates=28460, lr=5.17724e-06, gnorm=1.05, clip=60, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=56142
2023-03-15 11:08:07 - progress_bar.py[line:274] - INFO: epoch 005:   3265 / 6313 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=255.9, nsentences=100, sample_size=255.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.1, ups=0.88, wpb=255.9, bsz=100, num_updates=28470, lr=5.16057e-06, gnorm=0.99, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56154
2023-03-15 11:08:19 - progress_bar.py[line:274] - INFO: epoch 005:   3275 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.7, ups=0.86, wpb=255.4, bsz=100, num_updates=28480, lr=5.1439e-06, gnorm=0.983, clip=50, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=56166
2023-03-15 11:08:31 - progress_bar.py[line:274] - INFO: epoch 005:   3285 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.7, ups=0.87, wpb=252.4, bsz=100, num_updates=28490, lr=5.12722e-06, gnorm=1.152, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56179
2023-03-15 11:08:44 - progress_bar.py[line:274] - INFO: epoch 005:   3295 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.7, ups=0.86, wpb=254.7, bsz=100, num_updates=28500, lr=5.11055e-06, gnorm=1.037, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=56191
2023-03-15 11:08:56 - progress_bar.py[line:274] - INFO: epoch 005:   3305 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.7, ups=0.86, wpb=254.5, bsz=100, num_updates=28510, lr=5.09387e-06, gnorm=1.072, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=56203
2023-03-15 11:09:08 - progress_bar.py[line:274] - INFO: epoch 005:   3315 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.7, ups=0.88, wpb=253.5, bsz=100, num_updates=28520, lr=5.0772e-06, gnorm=1.03, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56215
2023-03-15 11:09:20 - progress_bar.py[line:274] - INFO: epoch 005:   3325 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.9, ups=0.87, wpb=252.8, bsz=100, num_updates=28530, lr=5.06053e-06, gnorm=1.054, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56228
2023-03-15 11:09:32 - progress_bar.py[line:274] - INFO: epoch 005:   3335 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.304, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.8, ups=0.86, wpb=253.2, bsz=100, num_updates=28540, lr=5.04385e-06, gnorm=1.046, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=56240
2023-03-15 11:09:44 - progress_bar.py[line:274] - INFO: epoch 005:   3345 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225.2, ups=0.89, wpb=253.8, bsz=100, num_updates=28550, lr=5.02718e-06, gnorm=0.961, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56252
2023-03-15 11:09:56 - progress_bar.py[line:274] - INFO: epoch 005:   3355 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.6, ups=0.86, wpb=253.6, bsz=100, num_updates=28560, lr=5.0105e-06, gnorm=1.002, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=56264
2023-03-15 11:10:09 - progress_bar.py[line:274] - INFO: epoch 005:   3365 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.9, ups=0.87, wpb=251.5, bsz=100, num_updates=28570, lr=4.99383e-06, gnorm=1.113, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56276
2023-03-15 11:10:22 - progress_bar.py[line:274] - INFO: epoch 005:   3375 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.5, ups=0.88, wpb=255.4, bsz=100, num_updates=28580, lr=4.97716e-06, gnorm=0.974, clip=60, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=56288
2023-03-15 11:10:34 - progress_bar.py[line:274] - INFO: epoch 005:   3385 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.6, ups=0.87, wpb=254.9, bsz=100, num_updates=28590, lr=4.96048e-06, gnorm=0.973, clip=50, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=56301
2023-03-15 11:10:46 - progress_bar.py[line:274] - INFO: epoch 005:   3395 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.5, ups=0.88, wpb=250.7, bsz=100, num_updates=28600, lr=4.94381e-06, gnorm=1.121, clip=80, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56313
2023-03-15 11:10:58 - progress_bar.py[line:274] - INFO: epoch 005:   3405 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.3, ups=0.87, wpb=256, bsz=100, num_updates=28610, lr=4.92714e-06, gnorm=1.173, clip=80, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56325
2023-03-15 11:11:10 - progress_bar.py[line:274] - INFO: epoch 005:   3415 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.5, ups=0.85, wpb=253.5, bsz=100, num_updates=28620, lr=4.91046e-06, gnorm=1.021, clip=50, loss_scale=1024, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=56338
2023-03-15 11:11:22 - progress_bar.py[line:274] - INFO: epoch 005:   3425 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.3, ups=0.87, wpb=253.2, bsz=100, num_updates=28630, lr=4.89379e-06, gnorm=1.034, clip=50, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56350
2023-03-15 11:11:34 - progress_bar.py[line:274] - INFO: epoch 005:   3435 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.3, ups=0.86, wpb=252.4, bsz=100, num_updates=28640, lr=4.87711e-06, gnorm=1.117, clip=50, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=56362
2023-03-15 11:11:47 - progress_bar.py[line:274] - INFO: epoch 005:   3445 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.3, ups=0.86, wpb=255.1, bsz=100, num_updates=28650, lr=4.86044e-06, gnorm=0.958, clip=30, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=56374
2023-03-15 11:11:58 - progress_bar.py[line:274] - INFO: epoch 005:   3455 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.9, ups=0.88, wpb=254.5, bsz=100, num_updates=28660, lr=4.84377e-06, gnorm=1.11, clip=60, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56386
2023-03-15 11:12:04 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 11:12:12 - progress_bar.py[line:274] - INFO: epoch 005:   3466 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=201.2, ups=0.79, wpb=253.5, bsz=100, num_updates=28670, lr=4.82709e-06, gnorm=0.918, clip=30, loss_scale=512, train_wall=13, gb_free=10.3, ema_decay=0.9999, wall=56399
2023-03-15 11:12:25 - progress_bar.py[line:274] - INFO: epoch 005:   3476 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219, ups=0.86, wpb=253.9, bsz=100, num_updates=28680, lr=4.81042e-06, gnorm=1.127, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=56411
2023-03-15 11:12:37 - progress_bar.py[line:274] - INFO: epoch 005:   3486 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221, ups=0.88, wpb=252.6, bsz=100, num_updates=28690, lr=4.79374e-06, gnorm=1.046, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=56424
2023-03-15 11:12:49 - progress_bar.py[line:274] - INFO: epoch 005:   3496 / 6313 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=222.9, ups=0.87, wpb=254.8, bsz=100, num_updates=28700, lr=4.77707e-06, gnorm=0.986, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56436
2023-03-15 11:13:02 - progress_bar.py[line:274] - INFO: epoch 005:   3506 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=224.3, ups=0.89, wpb=253.3, bsz=100, num_updates=28710, lr=4.7604e-06, gnorm=1.063, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56449
2023-03-15 11:13:14 - progress_bar.py[line:274] - INFO: epoch 005:   3516 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.3, ups=0.89, wpb=252.2, bsz=100, num_updates=28720, lr=4.74372e-06, gnorm=1.02, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56461
2023-03-15 11:13:27 - progress_bar.py[line:274] - INFO: epoch 005:   3526 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.2, ups=0.86, wpb=252.8, bsz=100, num_updates=28730, lr=4.72705e-06, gnorm=1.055, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=56474
2023-03-15 11:13:39 - progress_bar.py[line:274] - INFO: epoch 005:   3536 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=227.5, ups=0.89, wpb=254.6, bsz=100, num_updates=28740, lr=4.71037e-06, gnorm=0.974, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56486
2023-03-15 11:13:52 - progress_bar.py[line:274] - INFO: epoch 005:   3546 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.4, ups=0.87, wpb=253.1, bsz=100, num_updates=28750, lr=4.6937e-06, gnorm=1.027, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56498
2023-03-15 11:14:04 - progress_bar.py[line:274] - INFO: epoch 005:   3556 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.2, ups=0.86, wpb=252.4, bsz=100, num_updates=28760, lr=4.67703e-06, gnorm=1.037, clip=60, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=56511
2023-03-15 11:14:17 - progress_bar.py[line:274] - INFO: epoch 005:   3566 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.5, ups=0.87, wpb=253.1, bsz=100, num_updates=28770, lr=4.66035e-06, gnorm=0.999, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=56523
2023-03-15 11:14:29 - progress_bar.py[line:274] - INFO: epoch 005:   3576 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=224.7, ups=0.88, wpb=255.7, bsz=100, num_updates=28780, lr=4.64368e-06, gnorm=1.076, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56536
2023-03-15 11:14:42 - progress_bar.py[line:274] - INFO: epoch 005:   3586 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.2, ups=0.88, wpb=252, bsz=100, num_updates=28790, lr=4.62701e-06, gnorm=1.06, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56548
2023-03-15 11:14:54 - progress_bar.py[line:274] - INFO: epoch 005:   3596 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.3, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.3, ups=0.87, wpb=250.9, bsz=100, num_updates=28800, lr=4.61033e-06, gnorm=1.033, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56561
2023-03-15 11:15:06 - progress_bar.py[line:274] - INFO: epoch 005:   3606 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223, ups=0.88, wpb=254.2, bsz=100, num_updates=28810, lr=4.59366e-06, gnorm=0.945, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56573
2023-03-15 11:15:19 - progress_bar.py[line:274] - INFO: epoch 005:   3616 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.2, ups=0.87, wpb=253.3, bsz=100, num_updates=28820, lr=4.57698e-06, gnorm=1.061, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56586
2023-03-15 11:15:31 - progress_bar.py[line:274] - INFO: epoch 005:   3626 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.6, ups=0.87, wpb=250.6, bsz=100, num_updates=28830, lr=4.56031e-06, gnorm=1.069, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=56598
2023-03-15 11:15:44 - progress_bar.py[line:274] - INFO: epoch 005:   3636 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221, ups=0.87, wpb=252.7, bsz=100, num_updates=28840, lr=4.54364e-06, gnorm=1.034, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56610
2023-03-15 11:15:56 - progress_bar.py[line:274] - INFO: epoch 005:   3646 / 6313 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=227.7, ups=0.89, wpb=256.5, bsz=100, num_updates=28850, lr=4.52696e-06, gnorm=1.006, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56623
2023-03-15 11:16:08 - progress_bar.py[line:274] - INFO: epoch 005:   3656 / 6313 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=219.3, ups=0.86, wpb=253.5, bsz=100, num_updates=28860, lr=4.51029e-06, gnorm=1.025, clip=50, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=56635
2023-03-15 11:16:20 - progress_bar.py[line:274] - INFO: epoch 005:   3666 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=251.8, nsentences=100, sample_size=251.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.9, ups=0.87, wpb=251.8, bsz=100, num_updates=28870, lr=4.49361e-06, gnorm=0.914, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56647
2023-03-15 11:16:31 - progress_bar.py[line:274] - INFO: epoch 005:   3676 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.3, ups=0.89, wpb=252.2, bsz=100, num_updates=28880, lr=4.47694e-06, gnorm=1.014, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=56659
2023-03-15 11:16:44 - progress_bar.py[line:274] - INFO: epoch 005:   3686 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220, ups=0.87, wpb=253.8, bsz=100, num_updates=28890, lr=4.46027e-06, gnorm=0.993, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56671
2023-03-15 11:16:55 - progress_bar.py[line:274] - INFO: epoch 005:   3696 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.1, ups=0.88, wpb=252.5, bsz=100, num_updates=28900, lr=4.44359e-06, gnorm=0.889, clip=20, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=56683
2023-03-15 11:17:08 - progress_bar.py[line:274] - INFO: epoch 005:   3706 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.9, ups=0.88, wpb=253.2, bsz=100, num_updates=28910, lr=4.42692e-06, gnorm=0.976, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56695
2023-03-15 11:17:20 - progress_bar.py[line:274] - INFO: epoch 005:   3716 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.289, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.7, ups=0.88, wpb=253.3, bsz=100, num_updates=28920, lr=4.41024e-06, gnorm=1.066, clip=80, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=56708
2023-03-15 11:17:32 - progress_bar.py[line:274] - INFO: epoch 005:   3726 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=257.4, nsentences=100, sample_size=257.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=227.4, ups=0.88, wpb=257.4, bsz=100, num_updates=28930, lr=4.39357e-06, gnorm=1.065, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=56719
2023-03-15 11:17:44 - progress_bar.py[line:274] - INFO: epoch 005:   3736 / 6313 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=222.5, ups=0.87, wpb=254.3, bsz=100, num_updates=28940, lr=4.3769e-06, gnorm=0.983, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56731
2023-03-15 11:17:56 - progress_bar.py[line:274] - INFO: epoch 005:   3746 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.5, ups=0.87, wpb=252.3, bsz=100, num_updates=28950, lr=4.36022e-06, gnorm=1.086, clip=90, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=56743
2023-03-15 11:18:08 - progress_bar.py[line:274] - INFO: epoch 005:   3756 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.4, ups=0.88, wpb=252.7, bsz=100, num_updates=28960, lr=4.34355e-06, gnorm=1.035, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56755
2023-03-15 11:18:21 - progress_bar.py[line:274] - INFO: epoch 005:   3766 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=221.8, ups=0.88, wpb=251.6, bsz=100, num_updates=28970, lr=4.32687e-06, gnorm=1.017, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56767
2023-03-15 11:18:33 - progress_bar.py[line:274] - INFO: epoch 005:   3776 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.9, ups=0.87, wpb=252.4, bsz=100, num_updates=28980, lr=4.3102e-06, gnorm=1, clip=40, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=56780
2023-03-15 11:18:46 - progress_bar.py[line:274] - INFO: epoch 005:   3786 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=222.7, ups=0.87, wpb=254.7, bsz=100, num_updates=28990, lr=4.29353e-06, gnorm=0.997, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=56793
2023-03-15 11:18:58 - progress_bar.py[line:274] - INFO: epoch 005:   3796 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.3, ups=0.87, wpb=250.9, bsz=100, num_updates=29000, lr=4.27685e-06, gnorm=1.009, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=56805
2023-03-15 11:19:11 - progress_bar.py[line:274] - INFO: epoch 005:   3806 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.5, ups=0.87, wpb=253.8, bsz=100, num_updates=29010, lr=4.26018e-06, gnorm=1.003, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=56817
2023-03-15 11:19:23 - progress_bar.py[line:274] - INFO: epoch 005:   3816 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=227.7, ups=0.89, wpb=255.1, bsz=100, num_updates=29020, lr=4.24351e-06, gnorm=0.948, clip=20, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=56830
2023-03-15 11:19:35 - progress_bar.py[line:274] - INFO: epoch 005:   3826 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.8, ups=0.87, wpb=252.9, bsz=100, num_updates=29030, lr=4.22683e-06, gnorm=1.127, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56842
2023-03-15 11:19:47 - progress_bar.py[line:274] - INFO: epoch 005:   3836 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=222.3, ups=0.88, wpb=253.5, bsz=100, num_updates=29040, lr=4.21016e-06, gnorm=1.064, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=56854
2023-03-15 11:19:59 - progress_bar.py[line:274] - INFO: epoch 005:   3846 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219, ups=0.86, wpb=254.3, bsz=100, num_updates=29050, lr=4.19348e-06, gnorm=0.986, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=56866
2023-03-15 11:20:11 - progress_bar.py[line:274] - INFO: epoch 005:   3856 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.1, ups=0.87, wpb=255.4, bsz=100, num_updates=29060, lr=4.17681e-06, gnorm=1.027, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56878
2023-03-15 11:20:23 - progress_bar.py[line:274] - INFO: epoch 005:   3866 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.5, ups=0.86, wpb=252.7, bsz=100, num_updates=29070, lr=4.16014e-06, gnorm=1.003, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=56891
2023-03-15 11:20:35 - progress_bar.py[line:274] - INFO: epoch 005:   3876 / 6313 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=225.9, ups=0.89, wpb=255.2, bsz=100, num_updates=29080, lr=4.14346e-06, gnorm=0.964, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=56902
2023-03-15 11:20:47 - progress_bar.py[line:274] - INFO: epoch 005:   3886 / 6313 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=217.5, ups=0.86, wpb=252.2, bsz=100, num_updates=29090, lr=4.12679e-06, gnorm=1.032, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=56915
2023-03-15 11:20:59 - progress_bar.py[line:274] - INFO: epoch 005:   3896 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222, ups=0.88, wpb=253.5, bsz=100, num_updates=29100, lr=4.11011e-06, gnorm=1.054, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=56927
2023-03-15 11:21:12 - progress_bar.py[line:274] - INFO: epoch 005:   3906 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=214.6, ups=0.85, wpb=251.9, bsz=100, num_updates=29110, lr=4.09344e-06, gnorm=1.05, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=56939
2023-03-15 11:21:24 - progress_bar.py[line:274] - INFO: epoch 005:   3916 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.3, ups=0.86, wpb=253.6, bsz=100, num_updates=29120, lr=4.07677e-06, gnorm=0.984, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=56951
2023-03-15 11:21:36 - progress_bar.py[line:274] - INFO: epoch 005:   3926 / 6313 loss=0.518, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=250.4, nsentences=100, sample_size=250.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=213.8, ups=0.85, wpb=250.4, bsz=100, num_updates=29130, lr=4.06009e-06, gnorm=1.061, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=56963
2023-03-15 11:21:48 - progress_bar.py[line:274] - INFO: epoch 005:   3936 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=249.6, nsentences=100, sample_size=249.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=215.7, ups=0.86, wpb=249.6, bsz=100, num_updates=29140, lr=4.04342e-06, gnorm=1.1, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=56976
2023-03-15 11:22:00 - progress_bar.py[line:274] - INFO: epoch 005:   3946 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=223.9, ups=0.88, wpb=255.1, bsz=100, num_updates=29150, lr=4.02674e-06, gnorm=1.09, clip=90, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=56988
2023-03-15 11:22:12 - progress_bar.py[line:274] - INFO: epoch 005:   3956 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.4, ups=0.86, wpb=255.1, bsz=100, num_updates=29160, lr=4.01007e-06, gnorm=1.034, clip=50, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=57000
2023-03-15 11:22:24 - progress_bar.py[line:274] - INFO: epoch 005:   3966 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.7, ups=0.88, wpb=253.3, bsz=100, num_updates=29170, lr=3.9934e-06, gnorm=0.897, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=57012
2023-03-15 11:22:36 - progress_bar.py[line:274] - INFO: epoch 005:   3976 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.3, ups=0.87, wpb=253.3, bsz=100, num_updates=29180, lr=3.97672e-06, gnorm=1.033, clip=80, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=57024
2023-03-15 11:22:48 - progress_bar.py[line:274] - INFO: epoch 005:   3986 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=249.7, nsentences=100, sample_size=249.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.8, ups=0.88, wpb=249.7, bsz=100, num_updates=29190, lr=3.96005e-06, gnorm=0.97, clip=30, loss_scale=1024, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=57036
2023-03-15 11:22:49 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 11:23:01 - progress_bar.py[line:274] - INFO: epoch 005:   3997 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=202.8, ups=0.81, wpb=251.4, bsz=100, num_updates=29200, lr=3.94338e-06, gnorm=0.989, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=57049
2023-03-15 11:23:14 - progress_bar.py[line:274] - INFO: epoch 005:   4007 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.5, ups=0.87, wpb=254.2, bsz=100, num_updates=29210, lr=3.9267e-06, gnorm=0.96, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=57061
2023-03-15 11:23:26 - progress_bar.py[line:274] - INFO: epoch 005:   4017 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.3, ups=0.85, wpb=253.9, bsz=100, num_updates=29220, lr=3.91003e-06, gnorm=1.108, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=57074
2023-03-15 11:23:39 - progress_bar.py[line:274] - INFO: epoch 005:   4027 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.1, ups=0.86, wpb=252.5, bsz=100, num_updates=29230, lr=3.89335e-06, gnorm=1.053, clip=90, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=57086
2023-03-15 11:23:51 - progress_bar.py[line:274] - INFO: epoch 005:   4037 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223, ups=0.88, wpb=252.9, bsz=100, num_updates=29240, lr=3.87668e-06, gnorm=1.075, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=57098
2023-03-15 11:24:03 - progress_bar.py[line:274] - INFO: epoch 005:   4047 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=224, ups=0.89, wpb=252.7, bsz=100, num_updates=29250, lr=3.86001e-06, gnorm=0.974, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=57110
2023-03-15 11:24:15 - progress_bar.py[line:274] - INFO: epoch 005:   4057 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.2, ups=0.86, wpb=254.7, bsz=100, num_updates=29260, lr=3.84333e-06, gnorm=1.077, clip=70, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=57123
2023-03-15 11:24:27 - progress_bar.py[line:274] - INFO: epoch 005:   4067 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.4, ups=0.86, wpb=254.2, bsz=100, num_updates=29270, lr=3.82666e-06, gnorm=0.961, clip=30, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=57135
2023-03-15 11:24:39 - progress_bar.py[line:274] - INFO: epoch 005:   4077 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=254.9, nsentences=100, sample_size=254.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.3, ups=0.88, wpb=254.9, bsz=100, num_updates=29280, lr=3.80998e-06, gnorm=1.012, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57147
2023-03-15 11:24:52 - progress_bar.py[line:274] - INFO: epoch 005:   4087 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.7, ups=0.86, wpb=256.4, bsz=100, num_updates=29290, lr=3.79331e-06, gnorm=1.105, clip=70, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=57159
2023-03-15 11:25:03 - progress_bar.py[line:274] - INFO: epoch 005:   4097 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.6, ups=0.88, wpb=252.8, bsz=100, num_updates=29300, lr=3.77664e-06, gnorm=0.937, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=57171
2023-03-15 11:25:16 - progress_bar.py[line:274] - INFO: epoch 005:   4107 / 6313 loss=0.485, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.9, ups=0.86, wpb=253.4, bsz=100, num_updates=29310, lr=3.75996e-06, gnorm=1.047, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=57183
2023-03-15 11:25:28 - progress_bar.py[line:274] - INFO: epoch 005:   4117 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.1, ups=0.88, wpb=252.3, bsz=100, num_updates=29320, lr=3.74329e-06, gnorm=1.07, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=57195
2023-03-15 11:25:40 - progress_bar.py[line:274] - INFO: epoch 005:   4127 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.7, ups=0.87, wpb=251.6, bsz=100, num_updates=29330, lr=3.72661e-06, gnorm=1.149, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57207
2023-03-15 11:25:52 - progress_bar.py[line:274] - INFO: epoch 005:   4137 / 6313 loss=0.529, loss_v1=0, loss_v2=0, nll_loss=0.307, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=219.3, ups=0.87, wpb=252.3, bsz=100, num_updates=29340, lr=3.70994e-06, gnorm=1.124, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57219
2023-03-15 11:26:04 - progress_bar.py[line:274] - INFO: epoch 005:   4147 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.5, ups=0.86, wpb=253.7, bsz=100, num_updates=29350, lr=3.69327e-06, gnorm=1.121, clip=60, loss_scale=512, train_wall=12, gb_free=9.3, ema_decay=0.9999, wall=57231
2023-03-15 11:26:16 - progress_bar.py[line:274] - INFO: epoch 005:   4157 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224, ups=0.89, wpb=252.1, bsz=100, num_updates=29360, lr=3.67659e-06, gnorm=1.055, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57243
2023-03-15 11:26:28 - progress_bar.py[line:274] - INFO: epoch 005:   4167 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.1, ups=0.87, wpb=251.6, bsz=100, num_updates=29370, lr=3.65992e-06, gnorm=1.078, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57255
2023-03-15 11:26:40 - progress_bar.py[line:274] - INFO: epoch 005:   4177 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223, ups=0.87, wpb=255.6, bsz=100, num_updates=29380, lr=3.64325e-06, gnorm=1.006, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=57267
2023-03-15 11:26:52 - progress_bar.py[line:274] - INFO: epoch 005:   4187 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.3, ups=0.88, wpb=254, bsz=100, num_updates=29390, lr=3.62657e-06, gnorm=1.07, clip=80, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=57279
2023-03-15 11:27:04 - progress_bar.py[line:274] - INFO: epoch 005:   4197 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=216.2, ups=0.85, wpb=252.9, bsz=100, num_updates=29400, lr=3.6099e-06, gnorm=1.054, clip=60, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=57291
2023-03-15 11:27:17 - progress_bar.py[line:274] - INFO: epoch 005:   4207 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.6, ups=0.86, wpb=252.8, bsz=100, num_updates=29410, lr=3.59322e-06, gnorm=1.067, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=57304
2023-03-15 11:27:29 - progress_bar.py[line:274] - INFO: epoch 005:   4217 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=222.7, ups=0.87, wpb=255.4, bsz=100, num_updates=29420, lr=3.57655e-06, gnorm=1.074, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=57316
2023-03-15 11:27:41 - progress_bar.py[line:274] - INFO: epoch 005:   4227 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.5, ups=0.86, wpb=251.6, bsz=100, num_updates=29430, lr=3.55988e-06, gnorm=1.047, clip=70, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=57328
2023-03-15 11:27:53 - progress_bar.py[line:274] - INFO: epoch 005:   4237 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.5, ups=0.86, wpb=253.2, bsz=100, num_updates=29440, lr=3.5432e-06, gnorm=1.016, clip=70, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=57340
2023-03-15 11:28:05 - progress_bar.py[line:274] - INFO: epoch 005:   4247 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.3, ups=0.87, wpb=253.9, bsz=100, num_updates=29450, lr=3.52653e-06, gnorm=1, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=57352
2023-03-15 11:28:17 - progress_bar.py[line:274] - INFO: epoch 005:   4257 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223, ups=0.88, wpb=252.5, bsz=100, num_updates=29460, lr=3.50985e-06, gnorm=1.095, clip=50, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=57365
2023-03-15 11:28:29 - progress_bar.py[line:274] - INFO: epoch 005:   4267 / 6313 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=255.2, nsentences=100, sample_size=255.2, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=220.2, ups=0.86, wpb=255.2, bsz=100, num_updates=29470, lr=3.49318e-06, gnorm=0.998, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=57377
2023-03-15 11:28:42 - progress_bar.py[line:274] - INFO: epoch 005:   4277 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.1, ups=0.86, wpb=254.6, bsz=100, num_updates=29480, lr=3.47651e-06, gnorm=1.027, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=57389
2023-03-15 11:28:53 - progress_bar.py[line:274] - INFO: epoch 005:   4287 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.2, ups=0.88, wpb=252.6, bsz=100, num_updates=29490, lr=3.45983e-06, gnorm=1.095, clip=90, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=57401
2023-03-15 11:29:06 - progress_bar.py[line:274] - INFO: epoch 005:   4297 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219, ups=0.86, wpb=253.9, bsz=100, num_updates=29500, lr=3.44316e-06, gnorm=1.151, clip=90, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=57413
2023-03-15 11:29:17 - progress_bar.py[line:274] - INFO: epoch 005:   4307 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=227.6, ups=0.9, wpb=253.8, bsz=100, num_updates=29510, lr=3.42648e-06, gnorm=1.015, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=57425
2023-03-15 11:29:30 - progress_bar.py[line:274] - INFO: epoch 005:   4317 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220, ups=0.86, wpb=255.1, bsz=100, num_updates=29520, lr=3.40981e-06, gnorm=1.021, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=57437
2023-03-15 11:29:41 - progress_bar.py[line:274] - INFO: epoch 005:   4327 / 6313 loss=0.467, loss_v1=0, loss_v2=0, nll_loss=0.242, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.18, wps=224.3, ups=0.87, wpb=256.5, bsz=100, num_updates=29530, lr=3.39314e-06, gnorm=0.934, clip=30, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=57449
2023-03-15 11:29:54 - progress_bar.py[line:274] - INFO: epoch 005:   4337 / 6313 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=256.1, nsentences=100, sample_size=256.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=227.5, ups=0.89, wpb=256.1, bsz=100, num_updates=29540, lr=3.37646e-06, gnorm=1.067, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=57461
2023-03-15 11:30:06 - progress_bar.py[line:274] - INFO: epoch 005:   4347 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.9, ups=0.87, wpb=253.1, bsz=100, num_updates=29550, lr=3.35979e-06, gnorm=1.042, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=57473
2023-03-15 11:30:18 - progress_bar.py[line:274] - INFO: epoch 005:   4357 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.4, ups=0.87, wpb=252.1, bsz=100, num_updates=29560, lr=3.34312e-06, gnorm=1.063, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57485
2023-03-15 11:30:31 - progress_bar.py[line:274] - INFO: epoch 005:   4367 / 6313 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=225.3, ups=0.89, wpb=254.1, bsz=100, num_updates=29570, lr=3.32644e-06, gnorm=0.991, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=57498
2023-03-15 11:30:44 - progress_bar.py[line:274] - INFO: epoch 005:   4377 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=214.9, ups=0.86, wpb=251.3, bsz=100, num_updates=29580, lr=3.30977e-06, gnorm=0.999, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=57510
2023-03-15 11:30:56 - progress_bar.py[line:274] - INFO: epoch 005:   4387 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.8, ups=0.86, wpb=252.1, bsz=100, num_updates=29590, lr=3.29309e-06, gnorm=1.067, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=57523
2023-03-15 11:31:09 - progress_bar.py[line:274] - INFO: epoch 005:   4397 / 6313 loss=0.52, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.7, ups=0.87, wpb=253.7, bsz=100, num_updates=29600, lr=3.27642e-06, gnorm=1.089, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=57535
2023-03-15 11:31:21 - progress_bar.py[line:274] - INFO: epoch 005:   4407 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.2, ups=0.87, wpb=253.1, bsz=100, num_updates=29610, lr=3.25975e-06, gnorm=0.998, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=57548
2023-03-15 11:31:33 - progress_bar.py[line:274] - INFO: epoch 005:   4417 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218, ups=0.87, wpb=252, bsz=100, num_updates=29620, lr=3.24307e-06, gnorm=1.151, clip=70, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=57561
2023-03-15 11:31:45 - progress_bar.py[line:274] - INFO: epoch 005:   4427 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=219.8, ups=0.87, wpb=252, bsz=100, num_updates=29630, lr=3.2264e-06, gnorm=1.146, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57572
2023-03-15 11:31:57 - progress_bar.py[line:274] - INFO: epoch 005:   4437 / 6313 loss=0.519, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=250.5, nsentences=100, sample_size=250.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=214.1, ups=0.85, wpb=250.5, bsz=100, num_updates=29640, lr=3.20972e-06, gnorm=1.192, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=57585
2023-03-15 11:32:09 - progress_bar.py[line:274] - INFO: epoch 005:   4447 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.6, ups=0.86, wpb=254.2, bsz=100, num_updates=29650, lr=3.19305e-06, gnorm=1.156, clip=80, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=57597
2023-03-15 11:32:21 - progress_bar.py[line:274] - INFO: epoch 005:   4457 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.2, ups=0.87, wpb=250.9, bsz=100, num_updates=29660, lr=3.17638e-06, gnorm=1.087, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=57609
2023-03-15 11:32:33 - progress_bar.py[line:274] - INFO: epoch 005:   4467 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.9, ups=0.87, wpb=252.7, bsz=100, num_updates=29670, lr=3.1597e-06, gnorm=1.024, clip=60, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=57620
2023-03-15 11:32:45 - progress_bar.py[line:274] - INFO: epoch 005:   4477 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.4, ups=0.88, wpb=253, bsz=100, num_updates=29680, lr=3.14303e-06, gnorm=1.06, clip=60, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=57632
2023-03-15 11:32:57 - progress_bar.py[line:274] - INFO: epoch 005:   4487 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=224.5, ups=0.88, wpb=256.4, bsz=100, num_updates=29690, lr=3.12635e-06, gnorm=1.02, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57644
2023-03-15 11:33:08 - progress_bar.py[line:274] - INFO: epoch 005:   4497 / 6313 loss=0.481, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=257, nsentences=100, sample_size=257, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.7, ups=0.87, wpb=257, bsz=100, num_updates=29700, lr=3.10968e-06, gnorm=1.055, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=57656
2023-03-15 11:33:20 - progress_bar.py[line:274] - INFO: epoch 005:   4507 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.9, ups=0.87, wpb=253.5, bsz=100, num_updates=29710, lr=3.09301e-06, gnorm=1.117, clip=70, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=57668
2023-03-15 11:33:32 - progress_bar.py[line:274] - INFO: epoch 005:   4517 / 6313 loss=0.526, loss_v1=0, loss_v2=0, nll_loss=0.303, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224.2, ups=0.89, wpb=252.8, bsz=100, num_updates=29720, lr=3.07633e-06, gnorm=1.127, clip=80, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=57679
2023-03-15 11:33:36 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 11:33:45 - progress_bar.py[line:274] - INFO: epoch 005:   4528 / 6313 loss=0.535, loss_v1=0, loss_v2=0, nll_loss=0.312, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=199.9, ups=0.79, wpb=252.3, bsz=100, num_updates=29730, lr=3.05966e-06, gnorm=1.183, clip=80, loss_scale=512, train_wall=13, gb_free=10.6, ema_decay=0.9999, wall=57692
2023-03-15 11:33:57 - progress_bar.py[line:274] - INFO: epoch 005:   4538 / 6313 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.257, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=218.7, ups=0.86, wpb=254, bsz=100, num_updates=29740, lr=3.04299e-06, gnorm=0.998, clip=50, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=57704
2023-03-15 11:34:08 - progress_bar.py[line:274] - INFO: epoch 005:   4548 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.8, ups=0.86, wpb=253.7, bsz=100, num_updates=29750, lr=3.02631e-06, gnorm=1.103, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=57716
2023-03-15 11:34:20 - progress_bar.py[line:274] - INFO: epoch 005:   4558 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.8, ups=0.87, wpb=252.9, bsz=100, num_updates=29760, lr=3.00964e-06, gnorm=1.038, clip=40, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=57728
2023-03-15 11:34:32 - progress_bar.py[line:274] - INFO: epoch 005:   4568 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225.4, ups=0.89, wpb=254.4, bsz=100, num_updates=29770, lr=2.99296e-06, gnorm=1.098, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57739
2023-03-15 11:34:43 - progress_bar.py[line:274] - INFO: epoch 005:   4578 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.2, ups=0.89, wpb=251.9, bsz=100, num_updates=29780, lr=2.97629e-06, gnorm=1.055, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=57751
2023-03-15 11:34:55 - progress_bar.py[line:274] - INFO: epoch 005:   4588 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.3, ups=0.86, wpb=253.7, bsz=100, num_updates=29790, lr=2.95962e-06, gnorm=1.106, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=57763
2023-03-15 11:35:07 - progress_bar.py[line:274] - INFO: epoch 005:   4598 / 6313 loss=0.51, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=216.6, ups=0.86, wpb=251.1, bsz=100, num_updates=29800, lr=2.94294e-06, gnorm=1.064, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=57775
2023-03-15 11:35:18 - progress_bar.py[line:274] - INFO: epoch 005:   4608 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.2, ups=0.88, wpb=253.5, bsz=100, num_updates=29810, lr=2.92627e-06, gnorm=0.955, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=57786
2023-03-15 11:35:30 - progress_bar.py[line:274] - INFO: epoch 005:   4618 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.2, ups=0.88, wpb=251.9, bsz=100, num_updates=29820, lr=2.90959e-06, gnorm=1.093, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57798
2023-03-15 11:35:42 - progress_bar.py[line:274] - INFO: epoch 005:   4628 / 6313 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.7, ups=0.87, wpb=253.8, bsz=100, num_updates=29830, lr=2.89292e-06, gnorm=0.985, clip=30, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=57809
2023-03-15 11:35:54 - progress_bar.py[line:274] - INFO: epoch 005:   4638 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.1, ups=0.87, wpb=254.5, bsz=100, num_updates=29840, lr=2.87625e-06, gnorm=1.043, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=57821
2023-03-15 11:36:06 - progress_bar.py[line:274] - INFO: epoch 005:   4648 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.1, ups=0.87, wpb=252.2, bsz=100, num_updates=29850, lr=2.85957e-06, gnorm=1.062, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57833
2023-03-15 11:36:17 - progress_bar.py[line:274] - INFO: epoch 005:   4658 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.3, ups=0.87, wpb=253.8, bsz=100, num_updates=29860, lr=2.8429e-06, gnorm=1.037, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=57845
2023-03-15 11:36:29 - progress_bar.py[line:274] - INFO: epoch 005:   4668 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.4, ups=0.87, wpb=255, bsz=100, num_updates=29870, lr=2.82622e-06, gnorm=1.065, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=57857
2023-03-15 11:36:41 - progress_bar.py[line:274] - INFO: epoch 005:   4678 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=216.6, ups=0.85, wpb=253.7, bsz=100, num_updates=29880, lr=2.80955e-06, gnorm=1.077, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=57869
2023-03-15 11:36:53 - progress_bar.py[line:274] - INFO: epoch 005:   4688 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.1, ups=0.88, wpb=254.7, bsz=100, num_updates=29890, lr=2.79288e-06, gnorm=0.974, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=57880
2023-03-15 11:37:04 - progress_bar.py[line:274] - INFO: epoch 005:   4698 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.6, ups=0.88, wpb=253.7, bsz=100, num_updates=29900, lr=2.7762e-06, gnorm=0.973, clip=40, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=57892
2023-03-15 11:37:16 - progress_bar.py[line:274] - INFO: epoch 005:   4708 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.6, ups=0.87, wpb=254.5, bsz=100, num_updates=29910, lr=2.75953e-06, gnorm=1.032, clip=40, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=57903
2023-03-15 11:37:28 - progress_bar.py[line:274] - INFO: epoch 005:   4718 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.7, ups=0.87, wpb=252.4, bsz=100, num_updates=29920, lr=2.74286e-06, gnorm=1.093, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=57915
2023-03-15 11:37:39 - progress_bar.py[line:274] - INFO: epoch 005:   4728 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.4, ups=0.86, wpb=253, bsz=100, num_updates=29930, lr=2.72618e-06, gnorm=1.065, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=57927
2023-03-15 11:37:51 - progress_bar.py[line:274] - INFO: epoch 005:   4738 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.2, ups=0.86, wpb=253.9, bsz=100, num_updates=29940, lr=2.70951e-06, gnorm=1.02, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=57939
2023-03-15 11:38:03 - progress_bar.py[line:274] - INFO: epoch 005:   4748 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.6, ups=0.86, wpb=254.5, bsz=100, num_updates=29950, lr=2.69283e-06, gnorm=0.979, clip=40, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=57951
2023-03-15 11:38:15 - progress_bar.py[line:274] - INFO: epoch 005:   4758 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=251.2, nsentences=100, sample_size=251.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.9, ups=0.86, wpb=251.2, bsz=100, num_updates=29960, lr=2.67616e-06, gnorm=1.099, clip=80, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=57963
2023-03-15 11:38:27 - progress_bar.py[line:274] - INFO: epoch 005:   4768 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.8, ups=0.87, wpb=254.7, bsz=100, num_updates=29970, lr=2.65949e-06, gnorm=1.12, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=57974
2023-03-15 11:38:39 - progress_bar.py[line:274] - INFO: epoch 005:   4778 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=215.4, ups=0.86, wpb=251.3, bsz=100, num_updates=29980, lr=2.64281e-06, gnorm=1.13, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=57986
2023-03-15 11:38:51 - progress_bar.py[line:274] - INFO: epoch 005:   4788 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.6, ups=0.87, wpb=253.8, bsz=100, num_updates=29990, lr=2.62614e-06, gnorm=1.019, clip=60, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=57998
2023-03-15 11:39:02 - progress_bar.py[line:274] - INFO: epoch 005:   4798 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225.9, ups=0.89, wpb=254.8, bsz=100, num_updates=30000, lr=2.60946e-06, gnorm=0.992, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58010
2023-03-15 11:39:14 - progress_bar.py[line:274] - INFO: epoch 005:   4808 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219, ups=0.87, wpb=250.3, bsz=100, num_updates=30010, lr=2.59279e-06, gnorm=0.963, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=58021
2023-03-15 11:39:26 - progress_bar.py[line:274] - INFO: epoch 005:   4818 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.7, ups=0.88, wpb=253.9, bsz=100, num_updates=30020, lr=2.57612e-06, gnorm=1.122, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=58033
2023-03-15 11:39:38 - progress_bar.py[line:274] - INFO: epoch 005:   4828 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.3, ups=0.87, wpb=256.4, bsz=100, num_updates=30030, lr=2.55944e-06, gnorm=1.024, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58045
2023-03-15 11:39:49 - progress_bar.py[line:274] - INFO: epoch 005:   4838 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=225.9, ups=0.89, wpb=254.1, bsz=100, num_updates=30040, lr=2.54277e-06, gnorm=1.008, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58057
2023-03-15 11:40:01 - progress_bar.py[line:274] - INFO: epoch 005:   4848 / 6313 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.1, ups=0.86, wpb=253.8, bsz=100, num_updates=30050, lr=2.52609e-06, gnorm=0.959, clip=40, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=58069
2023-03-15 11:40:13 - progress_bar.py[line:274] - INFO: epoch 005:   4858 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=251, nsentences=100, sample_size=251, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.2, ups=0.87, wpb=251, bsz=100, num_updates=30060, lr=2.50942e-06, gnorm=1.116, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58081
2023-03-15 11:40:25 - progress_bar.py[line:274] - INFO: epoch 005:   4868 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=224, ups=0.89, wpb=252.9, bsz=100, num_updates=30070, lr=2.49275e-06, gnorm=1.026, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58092
2023-03-15 11:40:36 - progress_bar.py[line:274] - INFO: epoch 005:   4878 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.7, ups=0.87, wpb=252.6, bsz=100, num_updates=30080, lr=2.47607e-06, gnorm=0.957, clip=50, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=58104
2023-03-15 11:40:48 - progress_bar.py[line:274] - INFO: epoch 005:   4888 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.5, ups=0.86, wpb=253, bsz=100, num_updates=30090, lr=2.4594e-06, gnorm=1.031, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=58116
2023-03-15 11:41:00 - progress_bar.py[line:274] - INFO: epoch 005:   4898 / 6313 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.4, ups=0.86, wpb=254.2, bsz=100, num_updates=30100, lr=2.44273e-06, gnorm=1.096, clip=70, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=58128
2023-03-15 11:41:12 - progress_bar.py[line:274] - INFO: epoch 005:   4908 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=256.5, nsentences=100, sample_size=256.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=224.7, ups=0.88, wpb=256.5, bsz=100, num_updates=30110, lr=2.42605e-06, gnorm=1.098, clip=90, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=58140
2023-03-15 11:41:24 - progress_bar.py[line:274] - INFO: epoch 005:   4918 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.7, ups=0.87, wpb=252.5, bsz=100, num_updates=30120, lr=2.40938e-06, gnorm=1.09, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58151
2023-03-15 11:41:35 - progress_bar.py[line:274] - INFO: epoch 005:   4928 / 6313 loss=0.525, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.3, ups=0.87, wpb=250.6, bsz=100, num_updates=30130, lr=2.3927e-06, gnorm=1.022, clip=50, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=58163
2023-03-15 11:41:47 - progress_bar.py[line:274] - INFO: epoch 005:   4938 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=217.8, ups=0.86, wpb=253.3, bsz=100, num_updates=30140, lr=2.37603e-06, gnorm=1.054, clip=70, loss_scale=512, train_wall=12, gb_free=11, ema_decay=0.9999, wall=58175
2023-03-15 11:41:59 - progress_bar.py[line:274] - INFO: epoch 005:   4948 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.8, ups=0.89, wpb=252.5, bsz=100, num_updates=30150, lr=2.35936e-06, gnorm=1.136, clip=90, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=58186
2023-03-15 11:42:11 - progress_bar.py[line:274] - INFO: epoch 005:   4958 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.6, ups=0.87, wpb=252.5, bsz=100, num_updates=30160, lr=2.34268e-06, gnorm=0.97, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58198
2023-03-15 11:42:23 - progress_bar.py[line:274] - INFO: epoch 005:   4968 / 6313 loss=0.528, loss_v1=0, loss_v2=0, nll_loss=0.302, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.7, ups=0.87, wpb=250.6, bsz=100, num_updates=30170, lr=2.32601e-06, gnorm=1.115, clip=70, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58210
2023-03-15 11:42:34 - progress_bar.py[line:274] - INFO: epoch 005:   4978 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.2, ups=0.88, wpb=254.4, bsz=100, num_updates=30180, lr=2.30933e-06, gnorm=1.132, clip=80, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=58222
2023-03-15 11:42:46 - progress_bar.py[line:274] - INFO: epoch 005:   4988 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=216, ups=0.86, wpb=250.3, bsz=100, num_updates=30190, lr=2.29266e-06, gnorm=1.022, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=58234
2023-03-15 11:42:58 - progress_bar.py[line:274] - INFO: epoch 005:   4998 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=216.2, ups=0.86, wpb=251.1, bsz=100, num_updates=30200, lr=2.27599e-06, gnorm=1.033, clip=70, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=58246
2023-03-15 11:43:10 - progress_bar.py[line:274] - INFO: epoch 005:   5008 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.6, ups=0.86, wpb=255.8, bsz=100, num_updates=30210, lr=2.25931e-06, gnorm=1.057, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=58257
2023-03-15 11:43:22 - progress_bar.py[line:274] - INFO: epoch 005:   5018 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.9, ups=0.86, wpb=253.8, bsz=100, num_updates=30220, lr=2.24264e-06, gnorm=1.06, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=58269
2023-03-15 11:43:34 - progress_bar.py[line:274] - INFO: epoch 005:   5028 / 6313 loss=0.524, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.5, ups=0.86, wpb=252.2, bsz=100, num_updates=30230, lr=2.22596e-06, gnorm=1.121, clip=80, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=58281
2023-03-15 11:43:45 - progress_bar.py[line:274] - INFO: epoch 005:   5038 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.3, ups=0.88, wpb=252.8, bsz=100, num_updates=30240, lr=2.20929e-06, gnorm=0.965, clip=40, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=58293
2023-03-15 11:43:57 - progress_bar.py[line:274] - INFO: epoch 005:   5048 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.5, ups=0.88, wpb=253, bsz=100, num_updates=30250, lr=2.19262e-06, gnorm=1.067, clip=60, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=58304
2023-03-15 11:44:09 - progress_bar.py[line:274] - INFO: epoch 005:   5058 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=249.9, nsentences=100, sample_size=249.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=215.3, ups=0.86, wpb=249.9, bsz=100, num_updates=30260, lr=2.17594e-06, gnorm=1.067, clip=60, loss_scale=1024, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=58317
2023-03-15 11:44:21 - progress_bar.py[line:274] - INFO: epoch 005:   5068 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.6, ups=0.86, wpb=255.4, bsz=100, num_updates=30270, lr=2.15927e-06, gnorm=0.957, clip=20, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=58328
2023-03-15 11:44:32 - progress_bar.py[line:274] - INFO: epoch 005:   5078 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=251.6, nsentences=100, sample_size=251.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.3, ups=0.88, wpb=251.6, bsz=100, num_updates=30280, lr=2.1426e-06, gnorm=1.038, clip=80, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58340
2023-03-15 11:44:44 - progress_bar.py[line:274] - INFO: epoch 005:   5088 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221, ups=0.87, wpb=252.8, bsz=100, num_updates=30290, lr=2.12592e-06, gnorm=1.144, clip=50, loss_scale=1024, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=58352
2023-03-15 11:44:56 - progress_bar.py[line:274] - INFO: epoch 005:   5098 / 6313 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.259, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.9, ups=0.89, wpb=252.1, bsz=100, num_updates=30300, lr=2.10925e-06, gnorm=0.958, clip=20, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58363
2023-03-15 11:45:07 - progress_bar.py[line:274] - INFO: epoch 005:   5108 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.1, ups=0.86, wpb=253.2, bsz=100, num_updates=30310, lr=2.09257e-06, gnorm=1.149, clip=80, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=58375
2023-03-15 11:45:10 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 11:45:20 - progress_bar.py[line:274] - INFO: epoch 005:   5119 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=204.6, ups=0.81, wpb=252.5, bsz=100, num_updates=30320, lr=2.0759e-06, gnorm=1.06, clip=60, loss_scale=512, train_wall=12, gb_free=10.8, ema_decay=0.9999, wall=58388
2023-03-15 11:45:32 - progress_bar.py[line:274] - INFO: epoch 005:   5129 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.9, ups=0.88, wpb=255.7, bsz=100, num_updates=30330, lr=2.05923e-06, gnorm=1.021, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58399
2023-03-15 11:45:43 - progress_bar.py[line:274] - INFO: epoch 005:   5139 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.8, ups=0.88, wpb=253.4, bsz=100, num_updates=30340, lr=2.04255e-06, gnorm=1.026, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58411
2023-03-15 11:45:55 - progress_bar.py[line:274] - INFO: epoch 005:   5149 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.3, ups=0.86, wpb=254.1, bsz=100, num_updates=30350, lr=2.02588e-06, gnorm=1.072, clip=70, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=58423
2023-03-15 11:46:07 - progress_bar.py[line:274] - INFO: epoch 005:   5159 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.1, ups=0.89, wpb=252, bsz=100, num_updates=30360, lr=2.0092e-06, gnorm=1.027, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58434
2023-03-15 11:46:18 - progress_bar.py[line:274] - INFO: epoch 005:   5169 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.1, ups=0.88, wpb=253.6, bsz=100, num_updates=30370, lr=1.99253e-06, gnorm=1.037, clip=70, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=58446
2023-03-15 11:46:30 - progress_bar.py[line:274] - INFO: epoch 005:   5179 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217, ups=0.86, wpb=251.9, bsz=100, num_updates=30380, lr=1.97586e-06, gnorm=1.07, clip=50, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=58458
2023-03-15 11:46:42 - progress_bar.py[line:274] - INFO: epoch 005:   5189 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.6, ups=0.87, wpb=254.5, bsz=100, num_updates=30390, lr=1.95918e-06, gnorm=1.146, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58469
2023-03-15 11:46:53 - progress_bar.py[line:274] - INFO: epoch 005:   5199 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=254.7, nsentences=100, sample_size=254.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=222.8, ups=0.87, wpb=254.7, bsz=100, num_updates=30400, lr=1.94251e-06, gnorm=0.941, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58481
2023-03-15 11:47:05 - progress_bar.py[line:274] - INFO: epoch 005:   5209 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.262, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=225.8, ups=0.89, wpb=254.8, bsz=100, num_updates=30410, lr=1.92583e-06, gnorm=1.026, clip=40, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58493
2023-03-15 11:47:17 - progress_bar.py[line:274] - INFO: epoch 005:   5219 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.9, ups=0.86, wpb=252.6, bsz=100, num_updates=30420, lr=1.90916e-06, gnorm=1.111, clip=60, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=58504
2023-03-15 11:47:29 - progress_bar.py[line:274] - INFO: epoch 005:   5229 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=255.3, nsentences=100, sample_size=255.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.2, ups=0.87, wpb=255.3, bsz=100, num_updates=30430, lr=1.89249e-06, gnorm=1.066, clip=70, loss_scale=512, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=58516
2023-03-15 11:47:40 - progress_bar.py[line:274] - INFO: epoch 005:   5239 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.296, ntokens=250.6, nsentences=100, sample_size=250.6, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.1, ups=0.87, wpb=250.6, bsz=100, num_updates=30440, lr=1.87581e-06, gnorm=0.987, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58528
2023-03-15 11:47:52 - progress_bar.py[line:274] - INFO: epoch 005:   5249 / 6313 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=221.6, ups=0.88, wpb=253.1, bsz=100, num_updates=30450, lr=1.85914e-06, gnorm=1.011, clip=30, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58540
2023-03-15 11:48:04 - progress_bar.py[line:274] - INFO: epoch 005:   5259 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.7, ups=0.88, wpb=254.3, bsz=100, num_updates=30460, lr=1.84247e-06, gnorm=1.074, clip=60, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=58551
2023-03-15 11:48:16 - progress_bar.py[line:274] - INFO: epoch 005:   5269 / 6313 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.256, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=218.2, ups=0.86, wpb=254.8, bsz=100, num_updates=30470, lr=1.82579e-06, gnorm=1.002, clip=50, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=58563
2023-03-15 11:48:27 - progress_bar.py[line:274] - INFO: epoch 005:   5279 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.4, ups=0.86, wpb=251.9, bsz=100, num_updates=30480, lr=1.80912e-06, gnorm=1.107, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=58575
2023-03-15 11:48:39 - progress_bar.py[line:274] - INFO: epoch 005:   5289 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.7, ups=0.88, wpb=253.1, bsz=100, num_updates=30490, lr=1.79244e-06, gnorm=1.034, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58587
2023-03-15 11:48:51 - progress_bar.py[line:274] - INFO: epoch 005:   5299 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220, ups=0.87, wpb=252, bsz=100, num_updates=30500, lr=1.77577e-06, gnorm=1.018, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58598
2023-03-15 11:49:02 - progress_bar.py[line:274] - INFO: epoch 005:   5309 / 6313 loss=0.483, loss_v1=0, loss_v2=0, nll_loss=0.255, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=221.3, ups=0.87, wpb=253.3, bsz=100, num_updates=30510, lr=1.7591e-06, gnorm=1.005, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58610
2023-03-15 11:49:14 - progress_bar.py[line:274] - INFO: epoch 005:   5319 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=257.1, nsentences=100, sample_size=257.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.1, ups=0.86, wpb=257.1, bsz=100, num_updates=30520, lr=1.74242e-06, gnorm=1.032, clip=70, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=58622
2023-03-15 11:49:26 - progress_bar.py[line:274] - INFO: epoch 005:   5329 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=255.4, nsentences=100, sample_size=255.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.3, ups=0.87, wpb=255.4, bsz=100, num_updates=30530, lr=1.72575e-06, gnorm=1.086, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58633
2023-03-15 11:49:43 - progress_bar.py[line:274] - INFO: epoch 005:   5339 / 6313 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=218.7, ups=0.86, wpb=253.3, bsz=100, num_updates=30540, lr=1.70907e-06, gnorm=0.961, clip=30, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=58645
2023-03-15 11:49:55 - progress_bar.py[line:274] - INFO: epoch 005:   5349 / 6313 loss=0.495, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.5, ups=0.86, wpb=253.6, bsz=100, num_updates=30550, lr=1.6924e-06, gnorm=0.958, clip=40, loss_scale=512, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=58663
2023-03-15 11:50:07 - progress_bar.py[line:274] - INFO: epoch 005:   5359 / 6313 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.247, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=218.9, ups=0.86, wpb=254.4, bsz=100, num_updates=30560, lr=1.67573e-06, gnorm=0.918, clip=20, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=58675
2023-03-15 11:50:19 - progress_bar.py[line:274] - INFO: epoch 005:   5369 / 6313 loss=0.516, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.3, ups=0.88, wpb=252.8, bsz=100, num_updates=30570, lr=1.65905e-06, gnorm=1.03, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58686
2023-03-15 11:50:30 - progress_bar.py[line:274] - INFO: epoch 005:   5379 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=215.9, ups=0.86, wpb=250.7, bsz=100, num_updates=30580, lr=1.64238e-06, gnorm=1.068, clip=50, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=58698
2023-03-15 11:50:42 - progress_bar.py[line:274] - INFO: epoch 005:   5389 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=250.3, nsentences=100, sample_size=250.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=214.7, ups=0.86, wpb=250.3, bsz=100, num_updates=30590, lr=1.6257e-06, gnorm=1.038, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=58710
2023-03-15 11:50:54 - progress_bar.py[line:274] - INFO: epoch 005:   5399 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.8, ups=0.86, wpb=253.1, bsz=100, num_updates=30600, lr=1.60903e-06, gnorm=1.098, clip=60, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=58722
2023-03-15 11:51:06 - progress_bar.py[line:274] - INFO: epoch 005:   5409 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.7, ups=0.88, wpb=255.6, bsz=100, num_updates=30610, lr=1.59236e-06, gnorm=1.013, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58733
2023-03-15 11:51:17 - progress_bar.py[line:274] - INFO: epoch 005:   5419 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.6, ups=0.88, wpb=254.3, bsz=100, num_updates=30620, lr=1.57568e-06, gnorm=1.126, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58745
2023-03-15 11:51:29 - progress_bar.py[line:274] - INFO: epoch 005:   5429 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=251.5, nsentences=100, sample_size=251.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.2, ups=0.88, wpb=251.5, bsz=100, num_updates=30630, lr=1.55901e-06, gnorm=1.079, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58757
2023-03-15 11:51:41 - progress_bar.py[line:274] - INFO: epoch 005:   5439 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.295, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.1, ups=0.86, wpb=252.7, bsz=100, num_updates=30640, lr=1.54234e-06, gnorm=1.118, clip=80, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=58768
2023-03-15 11:51:53 - progress_bar.py[line:274] - INFO: epoch 005:   5449 / 6313 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.258, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.7, ups=0.86, wpb=255.6, bsz=100, num_updates=30650, lr=1.52566e-06, gnorm=1.01, clip=40, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=58780
2023-03-15 11:52:05 - progress_bar.py[line:274] - INFO: epoch 005:   5459 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.1, ups=0.85, wpb=255, bsz=100, num_updates=30660, lr=1.50899e-06, gnorm=1.09, clip=70, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=58792
2023-03-15 11:52:17 - progress_bar.py[line:274] - INFO: epoch 005:   5469 / 6313 loss=0.512, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217, ups=0.85, wpb=253.9, bsz=100, num_updates=30670, lr=1.49231e-06, gnorm=1.032, clip=60, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=58804
2023-03-15 11:52:28 - progress_bar.py[line:274] - INFO: epoch 005:   5479 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=225.3, ups=0.89, wpb=254.3, bsz=100, num_updates=30680, lr=1.47564e-06, gnorm=1.146, clip=90, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58816
2023-03-15 11:52:40 - progress_bar.py[line:274] - INFO: epoch 005:   5489 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.2, ups=0.89, wpb=251.7, bsz=100, num_updates=30690, lr=1.45897e-06, gnorm=1.112, clip=90, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=58827
2023-03-15 11:52:51 - progress_bar.py[line:274] - INFO: epoch 005:   5499 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=226.4, ups=0.9, wpb=252.8, bsz=100, num_updates=30700, lr=1.44229e-06, gnorm=1.079, clip=70, loss_scale=512, train_wall=11, gb_free=11, ema_decay=0.9999, wall=58839
2023-03-15 11:53:03 - progress_bar.py[line:274] - INFO: epoch 005:   5509 / 6313 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=255.7, nsentences=100, sample_size=255.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=223.1, ups=0.87, wpb=255.7, bsz=100, num_updates=30710, lr=1.42562e-06, gnorm=1.017, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58850
2023-03-15 11:53:14 - progress_bar.py[line:274] - INFO: epoch 005:   5519 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=227.4, ups=0.9, wpb=253.2, bsz=100, num_updates=30720, lr=1.40894e-06, gnorm=1.05, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58862
2023-03-15 11:53:26 - progress_bar.py[line:274] - INFO: epoch 005:   5529 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223, ups=0.88, wpb=253.2, bsz=100, num_updates=30730, lr=1.39227e-06, gnorm=1.065, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58873
2023-03-15 11:53:37 - progress_bar.py[line:274] - INFO: epoch 005:   5539 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=253.6, nsentences=100, sample_size=253.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.1, ups=0.86, wpb=253.6, bsz=100, num_updates=30740, lr=1.3756e-06, gnorm=1.091, clip=60, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=58885
2023-03-15 11:53:49 - progress_bar.py[line:274] - INFO: epoch 005:   5549 / 6313 loss=0.488, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=219.5, ups=0.87, wpb=253.4, bsz=100, num_updates=30750, lr=1.35892e-06, gnorm=1.177, clip=80, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=58897
2023-03-15 11:54:01 - progress_bar.py[line:274] - INFO: epoch 005:   5559 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.9, ups=0.88, wpb=253.5, bsz=100, num_updates=30760, lr=1.34225e-06, gnorm=1.051, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58908
2023-03-15 11:54:13 - progress_bar.py[line:274] - INFO: epoch 005:   5569 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.8, ups=0.88, wpb=254.6, bsz=100, num_updates=30770, lr=1.32557e-06, gnorm=0.995, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58920
2023-03-15 11:54:25 - progress_bar.py[line:274] - INFO: epoch 005:   5579 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.7, ups=0.86, wpb=253.7, bsz=100, num_updates=30780, lr=1.3089e-06, gnorm=1.097, clip=70, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=58932
2023-03-15 11:54:36 - progress_bar.py[line:274] - INFO: epoch 005:   5589 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.281, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.2, ups=0.86, wpb=253.8, bsz=100, num_updates=30790, lr=1.29223e-06, gnorm=1.111, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=58944
2023-03-15 11:54:49 - progress_bar.py[line:274] - INFO: epoch 005:   5599 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.4, ups=0.86, wpb=254.1, bsz=100, num_updates=30800, lr=1.27555e-06, gnorm=1.17, clip=80, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=58956
2023-03-15 11:55:00 - progress_bar.py[line:274] - INFO: epoch 005:   5609 / 6313 loss=0.498, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.1, ups=0.87, wpb=252.9, bsz=100, num_updates=30810, lr=1.25888e-06, gnorm=0.998, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=58968
2023-03-15 11:55:12 - progress_bar.py[line:274] - INFO: epoch 005:   5619 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=251.3, nsentences=100, sample_size=251.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.5, ups=0.87, wpb=251.3, bsz=100, num_updates=30820, lr=1.2422e-06, gnorm=1.091, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=58980
2023-03-15 11:55:24 - progress_bar.py[line:274] - INFO: epoch 005:   5629 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.27, ntokens=253, nsentences=100, sample_size=253, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.1, ups=0.88, wpb=253, bsz=100, num_updates=30830, lr=1.22553e-06, gnorm=1.074, clip=70, loss_scale=1024, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=58991
2023-03-15 11:55:36 - progress_bar.py[line:274] - INFO: epoch 005:   5639 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=252.2, nsentences=100, sample_size=252.2, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.9, ups=0.86, wpb=252.2, bsz=100, num_updates=30840, lr=1.20886e-06, gnorm=1.06, clip=50, loss_scale=1024, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=59003
2023-03-15 11:55:47 - progress_bar.py[line:274] - INFO: epoch 005:   5649 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=252.4, nsentences=100, sample_size=252.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=217.2, ups=0.86, wpb=252.4, bsz=100, num_updates=30850, lr=1.19218e-06, gnorm=1.109, clip=70, loss_scale=1024, train_wall=12, gb_free=10.7, ema_decay=0.9999, wall=59015
2023-03-15 11:55:59 - progress_bar.py[line:274] - INFO: epoch 005:   5659 / 6313 loss=0.499, loss_v1=0, loss_v2=0, nll_loss=0.274, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.2, ups=0.88, wpb=252.7, bsz=100, num_updates=30860, lr=1.17551e-06, gnorm=0.987, clip=40, loss_scale=1024, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59027
2023-03-15 11:56:11 - progress_bar.py[line:274] - INFO: epoch 005:   5669 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=221.9, ups=0.88, wpb=252.8, bsz=100, num_updates=30870, lr=1.15884e-06, gnorm=1.067, clip=40, loss_scale=1024, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59038
2023-03-15 11:56:22 - progress_bar.py[line:274] - INFO: epoch 005:   5679 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=253.9, nsentences=100, sample_size=253.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.8, ups=0.88, wpb=253.9, bsz=100, num_updates=30880, lr=1.14216e-06, gnorm=1.003, clip=50, loss_scale=1024, train_wall=11, gb_free=10.1, ema_decay=0.9999, wall=59050
2023-03-15 11:56:34 - progress_bar.py[line:274] - INFO: epoch 005:   5689 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221, ups=0.86, wpb=255.8, bsz=100, num_updates=30890, lr=1.12549e-06, gnorm=1.175, clip=80, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=59062
2023-03-15 11:56:46 - progress_bar.py[line:274] - INFO: epoch 005:   5699 / 6313 loss=0.476, loss_v1=0, loss_v2=0, nll_loss=0.249, ntokens=256, nsentences=100, sample_size=256, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=224.3, ups=0.88, wpb=256, bsz=100, num_updates=30900, lr=1.10881e-06, gnorm=0.99, clip=40, loss_scale=1024, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59073
2023-03-15 11:56:58 - progress_bar.py[line:274] - INFO: epoch 005:   5709 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.273, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.7, ups=0.86, wpb=254.8, bsz=100, num_updates=30910, lr=1.09214e-06, gnorm=1.034, clip=70, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=59085
2023-03-15 11:57:05 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 11:57:10 - progress_bar.py[line:274] - INFO: epoch 005:   5720 / 6313 loss=0.475, loss_v1=0, loss_v2=0, nll_loss=0.252, ntokens=256.4, nsentences=100, sample_size=256.4, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=204.7, ups=0.8, wpb=256.4, bsz=100, num_updates=30920, lr=1.07547e-06, gnorm=1.009, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=59098
2023-03-15 11:57:22 - progress_bar.py[line:274] - INFO: epoch 005:   5730 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=254.4, nsentences=100, sample_size=254.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.3, ups=0.87, wpb=254.4, bsz=100, num_updates=30930, lr=1.05879e-06, gnorm=1.078, clip=70, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59110
2023-03-15 11:57:34 - progress_bar.py[line:274] - INFO: epoch 005:   5740 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.268, ntokens=253.2, nsentences=100, sample_size=253.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.7, ups=0.88, wpb=253.2, bsz=100, num_updates=30940, lr=1.04212e-06, gnorm=1.048, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=59121
2023-03-15 11:57:46 - progress_bar.py[line:274] - INFO: epoch 005:   5750 / 6313 loss=0.521, loss_v1=0, loss_v2=0, nll_loss=0.298, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.8, ups=0.86, wpb=254.2, bsz=100, num_updates=30950, lr=1.02544e-06, gnorm=1.079, clip=80, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=59133
2023-03-15 11:57:57 - progress_bar.py[line:274] - INFO: epoch 005:   5760 / 6313 loss=0.48, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=223.3, ups=0.88, wpb=254, bsz=100, num_updates=30960, lr=1.00877e-06, gnorm=1.022, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59145
2023-03-15 11:58:09 - progress_bar.py[line:274] - INFO: epoch 005:   5770 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.1, ups=0.86, wpb=255.8, bsz=100, num_updates=30970, lr=9.92097e-07, gnorm=1.122, clip=70, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=59157
2023-03-15 11:58:21 - progress_bar.py[line:274] - INFO: epoch 005:   5780 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=253.8, nsentences=100, sample_size=253.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=224.5, ups=0.88, wpb=253.8, bsz=100, num_updates=30980, lr=9.75423e-07, gnorm=1.024, clip=60, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59168
2023-03-15 11:58:32 - progress_bar.py[line:274] - INFO: epoch 005:   5790 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.264, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.3, ups=0.87, wpb=253.3, bsz=100, num_updates=30990, lr=9.58749e-07, gnorm=1.014, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59180
2023-03-15 11:58:44 - progress_bar.py[line:274] - INFO: epoch 005:   5800 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=254.5, nsentences=100, sample_size=254.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223, ups=0.88, wpb=254.5, bsz=100, num_updates=31000, lr=9.42075e-07, gnorm=1.076, clip=60, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=59192
2023-03-15 11:58:56 - progress_bar.py[line:274] - INFO: epoch 005:   5810 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.285, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.1, ups=0.87, wpb=252, bsz=100, num_updates=31010, lr=9.25401e-07, gnorm=1.022, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59203
2023-03-15 11:59:08 - progress_bar.py[line:274] - INFO: epoch 005:   5820 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.5, ups=0.86, wpb=253.7, bsz=100, num_updates=31020, lr=9.08727e-07, gnorm=1.091, clip=70, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=59215
2023-03-15 11:59:19 - progress_bar.py[line:274] - INFO: epoch 005:   5830 / 6313 loss=0.539, loss_v1=0, loss_v2=0, nll_loss=0.311, ntokens=249.7, nsentences=100, sample_size=249.7, sample_size_v1=0, sample_size_v2=0, ppl=1.24, wps=215.8, ups=0.86, wpb=249.7, bsz=100, num_updates=31030, lr=8.92053e-07, gnorm=1.183, clip=70, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=59227
2023-03-15 11:59:31 - progress_bar.py[line:274] - INFO: epoch 005:   5840 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.297, ntokens=252, nsentences=100, sample_size=252, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=217.1, ups=0.86, wpb=252, bsz=100, num_updates=31040, lr=8.75379e-07, gnorm=1.128, clip=80, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=59239
2023-03-15 11:59:43 - progress_bar.py[line:274] - INFO: epoch 005:   5850 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.5, ups=0.88, wpb=252.9, bsz=100, num_updates=31050, lr=8.58705e-07, gnorm=1.037, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59251
2023-03-15 11:59:55 - progress_bar.py[line:274] - INFO: epoch 005:   5860 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.284, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=221.3, ups=0.88, wpb=251.4, bsz=100, num_updates=31060, lr=8.42032e-07, gnorm=1.085, clip=80, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59262
2023-03-15 12:00:06 - progress_bar.py[line:274] - INFO: epoch 005:   5870 / 6313 loss=0.491, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=254.2, nsentences=100, sample_size=254.2, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.4, ups=0.88, wpb=254.2, bsz=100, num_updates=31070, lr=8.25358e-07, gnorm=0.996, clip=40, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59274
2023-03-15 12:00:18 - progress_bar.py[line:274] - INFO: epoch 005:   5880 / 6313 loss=0.503, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=250.7, nsentences=100, sample_size=250.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=223.1, ups=0.89, wpb=250.7, bsz=100, num_updates=31080, lr=8.08684e-07, gnorm=1.148, clip=80, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59285
2023-03-15 12:00:30 - progress_bar.py[line:274] - INFO: epoch 005:   5890 / 6313 loss=0.517, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=216.5, ups=0.86, wpb=252.8, bsz=100, num_updates=31090, lr=7.9201e-07, gnorm=1.071, clip=70, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=59297
2023-03-15 12:00:41 - progress_bar.py[line:274] - INFO: epoch 005:   5900 / 6313 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.253, ntokens=258.5, nsentences=100, sample_size=258.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=229.7, ups=0.89, wpb=258.5, bsz=100, num_updates=31100, lr=7.75336e-07, gnorm=1.05, clip=20, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59309
2023-03-15 12:00:53 - progress_bar.py[line:274] - INFO: epoch 005:   5910 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.265, ntokens=255.8, nsentences=100, sample_size=255.8, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=224.1, ups=0.88, wpb=255.8, bsz=100, num_updates=31110, lr=7.58662e-07, gnorm=1.006, clip=60, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59321
2023-03-15 12:01:04 - progress_bar.py[line:274] - INFO: epoch 005:   5920 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.288, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.8, ups=0.87, wpb=252.5, bsz=100, num_updates=31120, lr=7.41988e-07, gnorm=1.003, clip=50, loss_scale=512, train_wall=11, gb_free=10, ema_decay=0.9999, wall=59332
2023-03-15 12:01:16 - progress_bar.py[line:274] - INFO: epoch 005:   5930 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.277, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.9, ups=0.87, wpb=251.7, bsz=100, num_updates=31130, lr=7.25314e-07, gnorm=1.105, clip=50, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59344
2023-03-15 12:01:28 - progress_bar.py[line:274] - INFO: epoch 005:   5940 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.271, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=222.3, ups=0.87, wpb=254.3, bsz=100, num_updates=31140, lr=7.0864e-07, gnorm=1.068, clip=60, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59355
2023-03-15 12:01:40 - progress_bar.py[line:274] - INFO: epoch 005:   5950 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.5, ups=0.87, wpb=254, bsz=100, num_updates=31150, lr=6.91967e-07, gnorm=1.125, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59367
2023-03-15 12:01:51 - progress_bar.py[line:274] - INFO: epoch 005:   5960 / 6313 loss=0.5, loss_v1=0, loss_v2=0, nll_loss=0.272, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=219.8, ups=0.87, wpb=251.9, bsz=100, num_updates=31160, lr=6.75293e-07, gnorm=1.058, clip=80, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59379
2023-03-15 12:02:03 - progress_bar.py[line:274] - INFO: epoch 005:   5970 / 6313 loss=0.494, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.9, ups=0.87, wpb=252.9, bsz=100, num_updates=31170, lr=6.58619e-07, gnorm=1.186, clip=70, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59391
2023-03-15 12:02:15 - progress_bar.py[line:274] - INFO: epoch 005:   5980 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.28, ntokens=255.1, nsentences=100, sample_size=255.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220, ups=0.86, wpb=255.1, bsz=100, num_updates=31180, lr=6.41945e-07, gnorm=1.046, clip=60, loss_scale=512, train_wall=12, gb_free=10.3, ema_decay=0.9999, wall=59402
2023-03-15 12:02:27 - progress_bar.py[line:274] - INFO: epoch 005:   5990 / 6313 loss=0.487, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=254.3, nsentences=100, sample_size=254.3, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.1, ups=0.86, wpb=254.3, bsz=100, num_updates=31190, lr=6.25271e-07, gnorm=1.045, clip=60, loss_scale=512, train_wall=12, gb_free=10.1, ema_decay=0.9999, wall=59414
2023-03-15 12:02:39 - progress_bar.py[line:274] - INFO: epoch 005:   6000 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.6, ups=0.86, wpb=253.5, bsz=100, num_updates=31200, lr=6.08597e-07, gnorm=1.008, clip=50, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=59426
2023-03-15 12:02:51 - progress_bar.py[line:274] - INFO: epoch 005:   6010 / 6313 loss=0.515, loss_v1=0, loss_v2=0, nll_loss=0.29, ntokens=252.3, nsentences=100, sample_size=252.3, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.6, ups=0.87, wpb=252.3, bsz=100, num_updates=31210, lr=5.91923e-07, gnorm=1.133, clip=80, loss_scale=512, train_wall=11, gb_free=9.7, ema_decay=0.9999, wall=59438
2023-03-15 12:03:02 - progress_bar.py[line:274] - INFO: epoch 005:   6020 / 6313 loss=0.507, loss_v1=0, loss_v2=0, nll_loss=0.283, ntokens=251.9, nsentences=100, sample_size=251.9, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.9, ups=0.88, wpb=251.9, bsz=100, num_updates=31220, lr=5.75249e-07, gnorm=1.024, clip=50, loss_scale=512, train_wall=11, gb_free=10.2, ema_decay=0.9999, wall=59450
2023-03-15 12:03:14 - progress_bar.py[line:274] - INFO: epoch 005:   6030 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.276, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=220.7, ups=0.88, wpb=252.1, bsz=100, num_updates=31230, lr=5.58575e-07, gnorm=1.097, clip=70, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59461
2023-03-15 12:03:25 - progress_bar.py[line:274] - INFO: epoch 005:   6040 / 6313 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.251, ntokens=251.7, nsentences=100, sample_size=251.7, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=219.9, ups=0.87, wpb=251.7, bsz=100, num_updates=31240, lr=5.41901e-07, gnorm=0.994, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59473
2023-03-15 12:03:37 - progress_bar.py[line:274] - INFO: epoch 005:   6050 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.301, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=222.1, ups=0.88, wpb=253.5, bsz=100, num_updates=31250, lr=5.25228e-07, gnorm=1.038, clip=60, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59485
2023-03-15 12:03:49 - progress_bar.py[line:274] - INFO: epoch 005:   6060 / 6313 loss=0.493, loss_v1=0, loss_v2=0, nll_loss=0.266, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.1, ups=0.88, wpb=252.1, bsz=100, num_updates=31260, lr=5.08554e-07, gnorm=1.059, clip=40, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59496
2023-03-15 12:04:01 - progress_bar.py[line:274] - INFO: epoch 005:   6070 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=218.9, ups=0.87, wpb=252.8, bsz=100, num_updates=31270, lr=4.9188e-07, gnorm=1.073, clip=70, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=59508
2023-03-15 12:04:12 - progress_bar.py[line:274] - INFO: epoch 005:   6080 / 6313 loss=0.497, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=252.9, nsentences=100, sample_size=252.9, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.6, ups=0.88, wpb=252.9, bsz=100, num_updates=31280, lr=4.75206e-07, gnorm=1.018, clip=70, loss_scale=512, train_wall=11, gb_free=10.9, ema_decay=0.9999, wall=59520
2023-03-15 12:04:24 - progress_bar.py[line:274] - INFO: epoch 005:   6090 / 6313 loss=0.505, loss_v1=0, loss_v2=0, nll_loss=0.279, ntokens=252.5, nsentences=100, sample_size=252.5, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.7, ups=0.86, wpb=252.5, bsz=100, num_updates=31290, lr=4.58532e-07, gnorm=1.041, clip=60, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=59532
2023-03-15 12:04:36 - progress_bar.py[line:274] - INFO: epoch 005:   6100 / 6313 loss=0.484, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=255.6, nsentences=100, sample_size=255.6, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=223.7, ups=0.88, wpb=255.6, bsz=100, num_updates=31300, lr=4.41858e-07, gnorm=0.995, clip=50, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59543
2023-03-15 12:04:47 - progress_bar.py[line:274] - INFO: epoch 005:   6110 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=254, nsentences=100, sample_size=254, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=220.6, ups=0.87, wpb=254, bsz=100, num_updates=31310, lr=4.25184e-07, gnorm=1.078, clip=70, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59555
2023-03-15 12:04:59 - progress_bar.py[line:274] - INFO: epoch 005:   6120 / 6313 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=253.3, nsentences=100, sample_size=253.3, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=219, ups=0.86, wpb=253.3, bsz=100, num_updates=31320, lr=4.0851e-07, gnorm=1.052, clip=60, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=59567
2023-03-15 12:05:11 - progress_bar.py[line:274] - INFO: epoch 005:   6130 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.269, ntokens=253.4, nsentences=100, sample_size=253.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=221.5, ups=0.87, wpb=253.4, bsz=100, num_updates=31330, lr=3.91836e-07, gnorm=1.049, clip=50, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59579
2023-03-15 12:05:23 - progress_bar.py[line:274] - INFO: epoch 005:   6140 / 6313 loss=0.506, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222, ups=0.88, wpb=253.5, bsz=100, num_updates=31340, lr=3.75163e-07, gnorm=1.056, clip=50, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59590
2023-03-15 12:05:34 - progress_bar.py[line:274] - INFO: epoch 005:   6150 / 6313 loss=0.514, loss_v1=0, loss_v2=0, nll_loss=0.292, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.3, ups=0.87, wpb=253.5, bsz=100, num_updates=31350, lr=3.58489e-07, gnorm=1.101, clip=70, loss_scale=512, train_wall=12, gb_free=10.9, ema_decay=0.9999, wall=59602
2023-03-15 12:05:46 - progress_bar.py[line:274] - INFO: epoch 005:   6160 / 6313 loss=0.508, loss_v1=0, loss_v2=0, nll_loss=0.282, ntokens=252.6, nsentences=100, sample_size=252.6, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=223.4, ups=0.88, wpb=252.6, bsz=100, num_updates=31360, lr=3.41815e-07, gnorm=1.029, clip=60, loss_scale=512, train_wall=11, gb_free=10.6, ema_decay=0.9999, wall=59613
2023-03-15 12:05:58 - progress_bar.py[line:274] - INFO: epoch 005:   6170 / 6313 loss=0.486, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=255, nsentences=100, sample_size=255, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=220.7, ups=0.87, wpb=255, bsz=100, num_updates=31370, lr=3.25141e-07, gnorm=1.056, clip=50, loss_scale=512, train_wall=12, gb_free=10.5, ema_decay=0.9999, wall=59625
2023-03-15 12:06:10 - progress_bar.py[line:274] - INFO: epoch 005:   6180 / 6313 loss=0.49, loss_v1=0, loss_v2=0, nll_loss=0.26, ntokens=251.4, nsentences=100, sample_size=251.4, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=217.1, ups=0.86, wpb=251.4, bsz=100, num_updates=31380, lr=3.08467e-07, gnorm=1.1, clip=90, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=59637
2023-03-15 12:06:21 - progress_bar.py[line:274] - INFO: epoch 005:   6190 / 6313 loss=0.474, loss_v1=0, loss_v2=0, nll_loss=0.248, ntokens=254.6, nsentences=100, sample_size=254.6, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=221.1, ups=0.87, wpb=254.6, bsz=100, num_updates=31390, lr=2.91793e-07, gnorm=0.97, clip=30, loss_scale=512, train_wall=11, gb_free=10.5, ema_decay=0.9999, wall=59649
2023-03-15 12:06:33 - progress_bar.py[line:274] - INFO: epoch 005:   6200 / 6313 loss=0.509, loss_v1=0, loss_v2=0, nll_loss=0.286, ntokens=252.7, nsentences=100, sample_size=252.7, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=219.6, ups=0.87, wpb=252.7, bsz=100, num_updates=31400, lr=2.75119e-07, gnorm=1.136, clip=60, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=59661
2023-03-15 12:06:45 - progress_bar.py[line:274] - INFO: epoch 005:   6210 / 6313 loss=0.502, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=251.1, nsentences=100, sample_size=251.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=217.4, ups=0.87, wpb=251.1, bsz=100, num_updates=31410, lr=2.58445e-07, gnorm=1.094, clip=50, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=59672
2023-03-15 12:06:57 - progress_bar.py[line:274] - INFO: epoch 005:   6220 / 6313 loss=0.482, loss_v1=0, loss_v2=0, nll_loss=0.254, ntokens=253.5, nsentences=100, sample_size=253.5, sample_size_v1=0, sample_size_v2=0, ppl=1.19, wps=221.6, ups=0.87, wpb=253.5, bsz=100, num_updates=31420, lr=2.41771e-07, gnorm=1.022, clip=50, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=59684
2023-03-15 12:07:08 - progress_bar.py[line:274] - INFO: epoch 005:   6230 / 6313 loss=0.492, loss_v1=0, loss_v2=0, nll_loss=0.263, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=217.6, ups=0.86, wpb=252.1, bsz=100, num_updates=31430, lr=2.25098e-07, gnorm=1.025, clip=40, loss_scale=1024, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=59696
2023-03-15 12:07:17 - trainer.py[line:1007] - INFO: NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 512.0
2023-03-15 12:07:21 - progress_bar.py[line:274] - INFO: epoch 005:   6241 / 6313 loss=0.496, loss_v1=0, loss_v2=0, nll_loss=0.267, ntokens=252.1, nsentences=100, sample_size=252.1, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=202.3, ups=0.8, wpb=252.1, bsz=100, num_updates=31440, lr=2.08424e-07, gnorm=1.041, clip=40, loss_scale=512, train_wall=12, gb_free=10.4, ema_decay=0.9999, wall=59709
2023-03-15 12:07:33 - progress_bar.py[line:274] - INFO: epoch 005:   6251 / 6313 loss=0.513, loss_v1=0, loss_v2=0, nll_loss=0.291, ntokens=254.8, nsentences=100, sample_size=254.8, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=228.9, ups=0.9, wpb=254.8, bsz=100, num_updates=31450, lr=1.9175e-07, gnorm=1.178, clip=100, loss_scale=512, train_wall=11, gb_free=10.8, ema_decay=0.9999, wall=59720
2023-03-15 12:07:44 - progress_bar.py[line:274] - INFO: epoch 005:   6261 / 6313 loss=0.522, loss_v1=0, loss_v2=0, nll_loss=0.294, ntokens=250.9, nsentences=100, sample_size=250.9, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=220.6, ups=0.88, wpb=250.9, bsz=100, num_updates=31460, lr=1.75076e-07, gnorm=1.06, clip=50, loss_scale=512, train_wall=11, gb_free=10.7, ema_decay=0.9999, wall=59732
2023-03-15 12:07:56 - progress_bar.py[line:274] - INFO: epoch 005:   6271 / 6313 loss=0.489, loss_v1=0, loss_v2=0, nll_loss=0.261, ntokens=253.7, nsentences=100, sample_size=253.7, sample_size_v1=0, sample_size_v2=0, ppl=1.2, wps=218.9, ups=0.86, wpb=253.7, bsz=100, num_updates=31470, lr=1.58402e-07, gnorm=1.111, clip=80, loss_scale=512, train_wall=12, gb_free=10.2, ema_decay=0.9999, wall=59744
2023-03-15 12:08:08 - progress_bar.py[line:274] - INFO: epoch 005:   6281 / 6313 loss=0.523, loss_v1=0, loss_v2=0, nll_loss=0.293, ntokens=250, nsentences=100, sample_size=250, sample_size_v1=0, sample_size_v2=0, ppl=1.23, wps=218.4, ups=0.87, wpb=250, bsz=100, num_updates=31480, lr=1.41728e-07, gnorm=1.187, clip=100, loss_scale=512, train_wall=11, gb_free=10.4, ema_decay=0.9999, wall=59755
2023-03-15 12:08:19 - progress_bar.py[line:274] - INFO: epoch 005:   6291 / 6313 loss=0.504, loss_v1=0, loss_v2=0, nll_loss=0.278, ntokens=253.1, nsentences=100, sample_size=253.1, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=218.4, ups=0.86, wpb=253.1, bsz=100, num_updates=31490, lr=1.25054e-07, gnorm=1.084, clip=70, loss_scale=512, train_wall=12, gb_free=10.6, ema_decay=0.9999, wall=59767
2023-03-15 12:08:31 - progress_bar.py[line:274] - INFO: epoch 005:   6301 / 6313 loss=0.511, loss_v1=0, loss_v2=0, nll_loss=0.287, ntokens=254.1, nsentences=100, sample_size=254.1, sample_size_v1=0, sample_size_v2=0, ppl=1.22, wps=222.1, ups=0.87, wpb=254.1, bsz=100, num_updates=31500, lr=1.0838e-07, gnorm=1.02, clip=40, loss_scale=512, train_wall=11, gb_free=10.3, ema_decay=0.9999, wall=59779
2023-03-15 12:08:43 - progress_bar.py[line:274] - INFO: epoch 005:   6311 / 6313 loss=0.501, loss_v1=0, loss_v2=0, nll_loss=0.275, ntokens=252.8, nsentences=100, sample_size=252.8, sample_size_v1=0, sample_size_v2=0, ppl=1.21, wps=215.5, ups=0.85, wpb=252.8, bsz=100, num_updates=31510, lr=9.17064e-08, gnorm=1.108, clip=80, loss_scale=512, train_wall=12, gb_free=11.1, ema_decay=0.9999, wall=59791
2023-03-15 12:08:45 - train.py[line:506] - INFO: begin validation on "valid" subset
2023-03-15 12:08:47 - train.py[line:549] - INFO: 0 / 7052
2023-03-15 12:08:47 - train.py[line:551] - INFO: load:1.53 valid_run:0.00 task_valid:0.00 collect_output:0.00
2023-03-15 12:08:47 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 8.04 GiB (GPU 4; 39.59 GiB total capacity; 10.09 GiB already allocated; 6.03 GiB free; 30.79 GiB reserved in total by PyTorch)
2023-03-15 12:08:47 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 12:08:47 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 12:08:47 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 12:08:47 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 12:08:47 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   10329 MB |   11922 MB |    8097 TB |    8097 TB |
|       from large pool |   10184 MB |   11777 MB |    8090 TB |    8090 TB |
|       from small pool |     145 MB |     145 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| Active memory         |   10329 MB |   11922 MB |    8097 TB |    8097 TB |
|       from large pool |   10184 MB |   11777 MB |    8090 TB |    8090 TB |
|       from small pool |     145 MB |     145 MB |       6 TB |       6 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   31532 MB |   31534 MB |  331462 MB |  299930 MB |
|       from large pool |   31386 MB |   31386 MB |  330912 MB |  299526 MB |
|       from small pool |     146 MB |     148 MB |     550 MB |     404 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   21202 MB |   27015 MB |    8564 TB |    8564 TB |
|       from large pool |   21201 MB |   27013 MB |    8556 TB |    8556 TB |
|       from small pool |       0 MB |       2 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3670    |    3684    |  354047 K  |  354044 K  |
|       from large pool |     563    |     575    |  126556 K  |  126555 K  |
|       from small pool |    3107    |    3114    |  227491 K  |  227488 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3670    |    3684    |  354047 K  |  354044 K  |
|       from large pool |     563    |     575    |  126556 K  |  126555 K  |
|       from small pool |    3107    |    3114    |  227491 K  |  227488 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     121    |     767    |     647    |
|       from large pool |      47    |      47    |     492    |     445    |
|       from small pool |      73    |      74    |     275    |     202    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      92    |  236318 K  |  236318 K  |
|       from large pool |      54    |      57    |   60733 K  |   60733 K  |
|       from small pool |      30    |      38    |  175584 K  |  175584 K  |
|===========================================================================|

2023-03-15 12:08:47 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-03-15 12:08:56 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 7.86 GiB (GPU 3; 39.59 GiB total capacity; 9.98 GiB already allocated; 1.45 GiB free; 35.09 GiB reserved in total by PyTorch)
2023-03-15 12:08:56 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 12:08:56 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 12:08:56 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 12:08:56 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 11        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   10220 MB |   16960 MB |    8077 TB |    8077 TB |
|       from large pool |   10075 MB |   16814 MB |    8070 TB |    8070 TB |
|       from small pool |     145 MB |     155 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| Active memory         |   10220 MB |   16960 MB |    8077 TB |    8077 TB |
|       from large pool |   10075 MB |   16814 MB |    8070 TB |    8070 TB |
|       from small pool |     145 MB |     155 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   35932 MB |   37344 MB |  201256 MB |  165324 MB |
|       from large pool |   35786 MB |   37182 MB |  200916 MB |  165130 MB |
|       from small pool |     146 MB |     162 MB |     340 MB |     194 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   25711 MB |   25711 MB |    8169 TB |    8169 TB |
|       from large pool |   25710 MB |   25710 MB |    8161 TB |    8161 TB |
|       from small pool |       0 MB |       5 MB |       8 TB |       8 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3681    |    3695    |  354105 K  |  354101 K  |
|       from large pool |     563    |     575    |  126085 K  |  126084 K  |
|       from small pool |    3118    |    3128    |  228020 K  |  228016 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3681    |    3695    |  354105 K  |  354101 K  |
|       from large pool |     563    |     575    |  126085 K  |  126084 K  |
|       from small pool |    3118    |    3128    |  228020 K  |  228016 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     184    |     201    |     527    |     343    |
|       from large pool |     111    |     120    |     357    |     246    |
|       from small pool |      73    |      81    |     170    |      97    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     121    |     133    |  240398 K  |  240397 K  |
|       from large pool |      93    |      97    |   62549 K  |   62549 K  |
|       from small pool |      28    |      43    |  177848 K  |  177848 K  |
|===========================================================================|

2023-03-15 12:08:56 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 12:08:56 - trainer.py[line:1163] - WARNING: ran out of memory in validation step, retrying batch
2023-03-15 12:08:57 - trainer.py[line:1414] - WARNING: OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 7.86 GiB (GPU 3; 39.59 GiB total capacity; 15.21 GiB already allocated; 5.94 GiB free; 30.61 GiB reserved in total by PyTorch)
2023-03-15 12:08:57 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 12:08:57 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 12:08:57 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

2023-03-15 12:08:57 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   15579 MB |   17149 MB |    8077 TB |    8077 TB |
|       from large pool |   15462 MB |   17031 MB |    8070 TB |    8070 TB |
|       from small pool |     117 MB |     155 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| Active memory         |   15579 MB |   17149 MB |    8077 TB |    8077 TB |
|       from large pool |   15462 MB |   17031 MB |    8070 TB |    8070 TB |
|       from small pool |     117 MB |     155 MB |       7 TB |       7 TB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   31340 MB |   37344 MB |  201260 MB |  169920 MB |
|       from large pool |   31220 MB |   37182 MB |  200916 MB |  169696 MB |
|       from small pool |     120 MB |     162 MB |     344 MB |     224 MB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   15760 MB |   25718 MB |    8169 TB |    8169 TB |
|       from large pool |   15757 MB |   25714 MB |    8161 TB |    8161 TB |
|       from small pool |       2 MB |       5 MB |       8 TB |       8 TB |
|---------------------------------------------------------------------------|
| Allocations           |    3037    |    3695    |  354109 K  |  354106 K  |
|       from large pool |     447    |     575    |  126086 K  |  126086 K  |
|       from small pool |    2590    |    3128    |  228022 K  |  228019 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3037    |    3695    |  354109 K  |  354106 K  |
|       from large pool |     447    |     575    |  126086 K  |  126086 K  |
|       from small pool |    2590    |    3128    |  228022 K  |  228019 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     201    |     529    |     421    |
|       from large pool |      48    |     120    |     357    |     309    |
|       from small pool |      60    |      81    |     172    |     112    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     170    |  240401 K  |  240400 K  |
|       from large pool |      49    |     100    |   62550 K  |   62550 K  |
|       from small pool |      51    |      73    |  177850 K  |  177850 K  |
|===========================================================================|

2023-03-15 12:08:57 - trainer.py[line:1417] - WARNING: |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 4                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |
|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |
|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|===========================================================================|

Traceback (most recent call last):
  File "/data/private/yutianyu/OFA/trainer.py", line 1155, in valid_step
    sample, model, self.criterion, **extra_kwargs
  File "/data/private/yutianyu/OFA/tasks/mm_tasks/vqa_gen.py", line 294, in valid_step
    lprobs = eval_model.get_normalized_probs(decoder_out, log_probs=True)
  File "/data/private/yutianyu/OFA/models/ofa/unify_transformer.py", line 481, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_model.py", line 83, in get_normalized_probs_scriptable
    return self.decoder.get_normalized_probs(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_decoder.py", line 67, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_decoder.py", line 92, in get_normalized_probs_scriptable
    return utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/utils.py", line 521, in log_softmax
    return F.log_softmax(x, dim=dim, dtype=torch.float32)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/nn/functional.py", line 1674, in log_softmax
    ret = input.log_softmax(dim, dtype=dtype)
RuntimeError: CUDA out of memory. Tried to allocate 7.86 GiB (GPU 3; 39.59 GiB total capacity; 9.98 GiB already allocated; 1.45 GiB free; 35.09 GiB reserved in total by PyTorch)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "../../train.py", line 635, in <module>
    cli_main()
  File "../../train.py", line 628, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 374, in call_main
    distributed_main(cfg.distributed_training.device_id, main, cfg, kwargs)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/distributed/utils.py", line 348, in distributed_main
    main(cfg, **kwargs)
  File "../../train.py", line 206, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "../../train.py", line 332, in train
    cfg, trainer, task, epoch_itr, valid_subsets, end_of_epoch
  File "../../train.py", line 418, in validate_and_save
    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)
  File "../../train.py", line 556, in validate
    logging_output, logits, sample_ids, time_info = trainer.valid_step(sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/data/private/yutianyu/OFA/trainer.py", line 1170, in valid_step
    return self.valid_step(sample, raise_oom=True, do_distill=do_distill)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/contextlib.py", line 74, in inner
    return func(*args, **kwds)
  File "/data/private/yutianyu/OFA/trainer.py", line 1171, in valid_step
    raise e
  File "/data/private/yutianyu/OFA/trainer.py", line 1155, in valid_step
    sample, model, self.criterion, **extra_kwargs
  File "/data/private/yutianyu/OFA/tasks/mm_tasks/vqa_gen.py", line 294, in valid_step
    lprobs = eval_model.get_normalized_probs(decoder_out, log_probs=True)
  File "/data/private/yutianyu/OFA/models/ofa/unify_transformer.py", line 481, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_model.py", line 83, in get_normalized_probs_scriptable
    return self.decoder.get_normalized_probs(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_decoder.py", line 67, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/models/fairseq_decoder.py", line 92, in get_normalized_probs_scriptable
    return utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/fairseq/utils.py", line 521, in log_softmax
    return F.log_softmax(x, dim=dim, dtype=torch.float32)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/nn/functional.py", line 1674, in log_softmax
    ret = input.log_softmax(dim, dtype=dtype)
RuntimeError: CUDA out of memory. Tried to allocate 7.86 GiB (GPU 3; 39.59 GiB total capacity; 15.21 GiB already allocated; 5.94 GiB free; 30.61 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/home/yutianyu/miniconda3/envs/OFA/lib/python3.7/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/home/yutianyu/miniconda3/envs/OFA/bin/python3', '-u', '../../train.py', '--local_rank=4', '/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E0.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E1.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E2.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E3.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E4.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E5.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E6.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E7.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E8.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E9.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E10.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E11.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E12.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E13.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E14.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E15.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E16.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E17.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E18.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E19.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E20.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E21.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E22.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E23.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E24.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E25.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E26.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E27.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E28.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E29.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E30.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E31.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E32.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E33.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E34.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E35.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E36.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E37.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E38.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E39.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E40.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E41.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E42.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E43.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E44.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E45.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E46.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E47.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E48.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E49.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E50.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E51.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E52.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E53.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E54.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E55.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E56.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E57.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E58.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E59.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E60.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E61.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E62.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E63.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E64.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E65.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E66.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E67.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E68.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E69.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E70.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E71.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E72.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E73.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E74.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E75.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E76.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E77.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E78.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_train_NA1_E79.tsv,/data/private/yutianyu/datasets/OFA_data/sgg/50_way/mask_val_1500.tsv', '--selected-cols=0,5,2,3,4', '--data-buffer-size', '10', '--tensorboard-logdir=./vqa_tensorboard/50_way', '--bpe-dir=../../utils/BPE', '--user-dir=../../ofa_module', '--restore-file=/data/private/yutianyu/datasets/OFA_data/sgg/../checkpoints/ofa_base.pt', '--reset-optimizer', '--reset-dataloader', '--reset-meters', '--save-dir=./vqa_checkpoints/50_way/1_B20_A1_E5_0.05_5e-5_480', '--task=vqa_gen', '--arch=ofa_base', '--criterion=adjust_label_smoothed_cross_entropy', '--label-smoothing=0.1', '--label-proxy', 'answer', '--distill', 'default', '--distill-alpha=1.0', '--batch-size=20', '--batch-size-valid=8', '--update-freq=1', '--encoder-normalize-before', '--decoder-normalize-before', '--share-decoder-input-output-embed', '--share-all-embeddings', '--layernorm-embedding', '--patch-layernorm-embedding', '--code-layernorm-embedding', '--resnet-drop-path-rate=0.0', '--encoder-drop-path-rate=0.1', '--decoder-drop-path-rate=0.1', '--dropout=0.1', '--attention-dropout=0.0', '--weight-decay=0.01', '--optimizer=adam', '--adam-betas=(0.9,0.999)', '--adam-eps=1e-08', '--clip-norm=1.0', '--lr-scheduler=polynomial_decay', '--lr=5e-5', '--max-epoch=5', '--warmup-ratio=0.05', '--log-format=simple', '--log-interval=10', '--fixed-validation-seed=7', '--save-interval=1', '--validate-interval=1', '--save-interval-updates=50000', '--validate-interval-updates=50000', '--best-checkpoint-metric=R@100', '--maximize-best-checkpoint-metric', '--max-src-length=128', '--max-object-length=30', '--max-tgt-length=30', '--find-unused-parameters', '--freeze-encoder-embedding', '--freeze-decoder-embedding', '--ans2label-file=/data/private/yutianyu/datasets/OFA_data/sgg/50_way/50_way_ans2label.pkl', '--valid-batch-size=51', '--add-type-embedding', '--scale-attn', '--scale-fc', '--scale-heads', '--disable-entangle', '--num-bins=1000', '--patch-image-size=480', '--prompt-type=prev_output', '--fp16', '--fp16-scale-window=512', '--add-object', '--uses-ema', '--store-ema', '--ema-fp32', '--ema-decay=0.9999', '--ema-start-update=0', '--val-inference-type=allcand', '--num-workers=8']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 581708
Killing subprocess 581709
Killing subprocess 581710
Killing subprocess 581711
Killing subprocess 581712
